{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb1463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "処理(1): 各測定器のRAWデータを Time_sec + 値列 に正規化し、CSV/PNG を出力\n",
    "- ディレクトリ: BASE_DIR/{subject_id}/        ← 入力は ID のみ（人名フォルダなし）\n",
    "- 出力先: 上記フォルダ内の RAW/\n",
    "- サンプリング周波数は仕様で固定（ファイル内の時刻は使わない）\n",
    "    Pulse=1000Hz / Thermo=10Hz / Skinos=0.1Hz(=10s) / Face=15Hz\n",
    "- MAT: ch1=Pulse, ch2=Sweat（2ch想定）。1D連結(+datastart/dataend)にも対応し、別ファイルに保存\n",
    "- Skinos は {subject_id}_skinos.* を最優先で検索（拡張子・大小文字ゆれ対応／CSV優先）\n",
    "- Face は CSV/Excel（.xlsx/.xls）にも対応（ファイル名優先：A/B/C）\n",
    "- ログ出力はオフセット非表示の [OK]/[SKIP] 形式\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "# ===================== ユーザー設定 =====================\n",
    "BASE_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "# 入力は「IDのみ」\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10031\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\"\n",
    "]\n",
    "\n",
    "# サンプリング周波数（Hz）— 仕様固定\n",
    "FS_PULSE  = 1000.0  # Pulse および Sweat（2chとも 1000Hz で扱う）\n",
    "FS_THERMO = 10.0    # Thermo\n",
    "FS_SKINOS = 0.1     # Skinos (= 10s)\n",
    "FS_FACE   = 15.0    # Face\n",
    "\n",
    "# MATのチャンネル扱い（0始まりの番号）：ch1=Pulse, ch2=Sweat\n",
    "PULSE_CH_IDX = 0\n",
    "SWEAT_CH_IDX = 1\n",
    "# =======================================================\n",
    "\n",
    "\n",
    "# ===================== 共通ユーティリティ =====================\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def generate_time(n: int, fs_hz: float) -> np.ndarray:\n",
    "    \"\"\"サンプル数 n をサンプリング周波数 fs_hz に基づく等間隔時刻（秒）に変換（先頭t=0）\"\"\"\n",
    "    if fs_hz <= 0:\n",
    "        raise ValueError(f\"fs_hz must be positive, got {fs_hz}\")\n",
    "    return np.arange(n, dtype=float) / fs_hz\n",
    "\n",
    "def _fmt_mmss(x: float, _pos) -> str:\n",
    "    x = float(x); m = int(x // 60); s = int(x % 60)\n",
    "    return f\"{m:02d}:{s:02d}\"\n",
    "\n",
    "def plot_lines(df: pd.DataFrame, title_ascii: str, out_png: str) -> None:\n",
    "    \"\"\"Time_sec + 値列を折れ線で描画しPNG保存（x軸は mm:ss）\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = plt.gca()\n",
    "    for c in df.columns:\n",
    "        if c == \"Time_sec\":\n",
    "            continue\n",
    "        ax.plot(df[\"Time_sec\"], df[c], linewidth=1.5, label=c)\n",
    "    ax.set_title(title_ascii, fontsize=30)\n",
    "    ax.set_xlabel(\"Time (mm:ss)\", fontsize=24)\n",
    "    ax.set_ylabel(\"Value\", fontsize=24)\n",
    "    ax.tick_params(axis=\"both\", labelsize=20)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(_fmt_mmss))\n",
    "    if len(df.columns) > 2:\n",
    "        ax.legend(fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def finalize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Time_sec のNaN/全NaN行を整理し、昇順整列\"\"\"\n",
    "    df = df.dropna(subset=[\"Time_sec\"])\n",
    "    value_cols = [c for c in df.columns if c != \"Time_sec\"]\n",
    "    if value_cols:\n",
    "        df = df.dropna(how=\"all\", subset=value_cols)\n",
    "    return df.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "\n",
    "def first_existing(paths: List[str]) -> Optional[str]:\n",
    "    \"\"\"複数候補から最初に存在するパスを返す\"\"\"\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def find_skinos_file(subject_dir: str, subject_id: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Skinosは '{id}_skinos.*' を最優先に、広めに '*skinos*' も検索。\n",
    "    優先順: 更新日時が新しいもの／Excelは除外（CSV優先）\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        os.path.join(subject_dir, f\"{subject_id}_skinos.*\"),\n",
    "        os.path.join(subject_dir, f\"{subject_id}-skinos.*\"),\n",
    "        os.path.join(subject_dir, \"*skinos*.*\"),           # フォールバック\n",
    "    ]\n",
    "    files: List[str] = []\n",
    "    for pat in patterns:\n",
    "        files.extend(glob.glob(pat))\n",
    "        stem = pat.replace(\"skinos\", \"[sS]kinos\")  # 大小文字ゆれ\n",
    "        files.extend(glob.glob(stem))\n",
    "    files = [p for p in files if not p.lower().endswith((\".xlsx\", \".xls\"))]\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    return files[0]\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ===================== MAT: Pulse + Sweat（ch1/ch2=1000Hz） =====================\n",
    "def _to_1d(arr) -> np.ndarray:\n",
    "    a = np.asarray(arr)\n",
    "    if a.ndim == 2:\n",
    "        # サンプル次元が長い向きに揃える\n",
    "        if a.shape[0] < a.shape[1]:\n",
    "            a = a.T\n",
    "    return np.ravel(a).astype(float)\n",
    "\n",
    "def _split_by_datastart_dataend(data: np.ndarray, m) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    LabChart 由来の 1D 連結データを datastart/dataend で分割し、各chの配列リストを返す。\n",
    "    datastart/dataend は 1始まりのインデックスであることが多い点に注意。\n",
    "    \"\"\"\n",
    "    if (\"datastart\" not in m) or (\"dataend\" not in m):\n",
    "        return []\n",
    "    ds = np.asarray(m[\"datastart\"]).astype(int).ravel()\n",
    "    de = np.asarray(m[\"dataend\"]).astype(int).ravel()\n",
    "    if ds.ndim != 1 or de.ndim != 1 or len(ds) != len(de):\n",
    "        return []\n",
    "    segs = []\n",
    "    for i in range(len(ds)):\n",
    "        s = int(ds[i]) - 1  # 1-based → 0-based\n",
    "        e = int(de[i])      # Python のスライスは終端を含まない\n",
    "        s = max(s, 0)\n",
    "        e = min(e, data.shape[0])\n",
    "        if e > s:\n",
    "            segs.append(data[s:e])\n",
    "    return segs\n",
    "\n",
    "def process_pulse_sweat_mat(mat_path: str,\n",
    "                            out_pulse_csv: str, out_pulse_png: str,\n",
    "                            out_sweat_csv: str, out_sweat_png: str) -> Tuple[bool, bool]:\n",
    "    \"\"\"\n",
    "    MAT内の Pulse(ch1) / Sweat(ch2) を抽出して別ファイルに保存する（両chとも 1000Hz 固定）。\n",
    "    優先順位:\n",
    "      1) 'pulse' / 'sweat' キーがあればそれを使う\n",
    "      2) 'data' が 2D → 列インデックス（PULSE_CH_IDX/SWEAT_CH_IDX）\n",
    "      3) 'data' が 1D かつ 'datastart'/'dataend' あり → 連結を分割（ch1/ch2…）\n",
    "      4) 'data' が 1D 単独 → Pulseのみ\n",
    "    \"\"\"\n",
    "    if not os.path.exists(mat_path):\n",
    "        raise FileNotFoundError(mat_path)\n",
    "\n",
    "    m = loadmat(mat_path, squeeze_me=True, struct_as_record=False)\n",
    "    keys = {k.lower(): k for k in m.keys() if not k.startswith(\"__\")}\n",
    "\n",
    "    pulse_arr: Optional[np.ndarray] = None\n",
    "    sweat_arr: Optional[np.ndarray] = None\n",
    "\n",
    "    # 1) 個別キー\n",
    "    if \"pulse\" in keys:\n",
    "        pulse_arr = _to_1d(m[keys[\"pulse\"]])\n",
    "    elif \"Pulse\" in m:\n",
    "        pulse_arr = _to_1d(m[\"Pulse\"])\n",
    "\n",
    "    if \"sweat\" in keys:\n",
    "        sweat_arr = _to_1d(m[keys[\"sweat\"]])\n",
    "    elif \"Sweat\" in m:\n",
    "        sweat_arr = _to_1d(m[\"Sweat\"])\n",
    "\n",
    "    # 2/3/4) data から抽出\n",
    "    if (pulse_arr is None or sweat_arr is None) and (\"data\" in m):\n",
    "        data = np.asarray(m[\"data\"]).astype(float)\n",
    "        if data.ndim == 2:\n",
    "            if data.shape[0] < data.shape[1]:\n",
    "                data = data.T\n",
    "            n_ch = data.shape[1]\n",
    "            if pulse_arr is None and (0 <= PULSE_CH_IDX < n_ch):\n",
    "                pulse_arr = data[:, PULSE_CH_IDX]\n",
    "            if sweat_arr is None and (0 <= SWEAT_CH_IDX < n_ch):\n",
    "                sweat_arr = data[:, SWEAT_CH_IDX]\n",
    "        elif data.ndim == 1:\n",
    "            segs = _split_by_datastart_dataend(data, m)\n",
    "            if segs:\n",
    "                if pulse_arr is None and len(segs) > PULSE_CH_IDX:\n",
    "                    pulse_arr = segs[PULSE_CH_IDX]\n",
    "                if sweat_arr is None and len(segs) > SWEAT_CH_IDX:\n",
    "                    sweat_arr = segs[SWEAT_CH_IDX]\n",
    "            else:\n",
    "                if pulse_arr is None:\n",
    "                    pulse_arr = data\n",
    "\n",
    "    pulse_done = False\n",
    "    sweat_done = False\n",
    "\n",
    "    # 保存（Pulse, 1000Hz）\n",
    "    if pulse_arr is not None:\n",
    "        t = generate_time(len(pulse_arr), FS_PULSE)\n",
    "        df_pulse = pd.DataFrame({\"Time_sec\": t, \"Pulse\": np.ravel(pulse_arr).astype(float)})\n",
    "        df_pulse = finalize_df(df_pulse)\n",
    "        df_pulse.to_csv(out_pulse_csv, index=False)\n",
    "        plot_lines(df_pulse, \"Pulse\", out_pulse_png)\n",
    "        pulse_done = True\n",
    "\n",
    "    # 保存（Sweat, 1000Hz）\n",
    "    if sweat_arr is not None:\n",
    "        t = generate_time(len(sweat_arr), FS_PULSE)\n",
    "        df_sweat = pd.DataFrame({\"Time_sec\": t, \"Sweat\": np.ravel(sweat_arr).astype(float)})\n",
    "        df_sweat = finalize_df(df_sweat)\n",
    "        df_sweat.to_csv(out_sweat_csv, index=False)\n",
    "        plot_lines(df_sweat, \"Sweat\", out_sweat_png)\n",
    "        sweat_done = True\n",
    "\n",
    "    return pulse_done, sweat_done\n",
    "\n",
    "def process_pulse_mat(mat_path: str, out_csv: str, out_png: str) -> None:\n",
    "    \"\"\"\n",
    "    ラッパ：MATから Pulse(+Sweat) を出力（両ch=1000Hz）。\n",
    "    - 既存呼び出し互換を保ちつつ、Sweat も自動で出力（別ファイル）\n",
    "    \"\"\"\n",
    "    base_dir = os.path.dirname(out_csv)\n",
    "    subject_id = os.path.basename(out_csv).split(\"_\")[0]\n",
    "    out_sweat_csv = os.path.join(base_dir, f\"{subject_id}_Sweat.csv\")\n",
    "    out_sweat_png = os.path.join(base_dir, f\"{subject_id}_Sweat.png\")\n",
    "\n",
    "    pulse_ok, _sweat_ok = process_pulse_sweat_mat(\n",
    "        mat_path,\n",
    "        out_pulse_csv=out_csv, out_pulse_png=out_png,\n",
    "        out_sweat_csv=out_sweat_csv, out_sweat_png=out_sweat_png,\n",
    "    )\n",
    "    if not pulse_ok:\n",
    "        raise RuntimeError(\"Pulse channel not found in MAT.\")\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ===================== デバイス別処理（Thermo / Skinos / Face） =====================\n",
    "def process_thermo_csv(csv_path: str, out_csv: str, out_png: str) -> None:\n",
    "    \"\"\"\n",
    "    Thermo（CSV）\n",
    "    - 入力: {subject_id}.CSV（先頭に 'Time' を含むヘッダ行がある形式）\n",
    "    - 使用列: 'U1-1[C]' -> 'Thermo1'\n",
    "    - 出力: CSV(Time_sec, Thermo1) + PNG\n",
    "    - 時刻: FS_THERMO(=10Hz) で生成\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(csv_path)\n",
    "    header_row = None\n",
    "    with open(csv_path, \"r\", encoding=\"cp932\", errors=\"ignore\") as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            s = line.strip()\n",
    "            if s.startswith('\"Time\"') or s.startswith(\"Time\"):\n",
    "                header_row = idx\n",
    "                break\n",
    "    if header_row is None:\n",
    "        raise ValueError(\"Thermo: ヘッダ(Time, ...)が見つかりません。\")\n",
    "\n",
    "    df_raw = pd.read_csv(csv_path, header=header_row, encoding=\"cp932\")\n",
    "    col_name = \"U1-1[C]\"\n",
    "    if col_name not in df_raw.columns:\n",
    "        raise KeyError(f\"Thermo: 必要列 '{col_name}' なし。columns={list(df_raw.columns)}\")\n",
    "\n",
    "    y = pd.to_numeric(df_raw[col_name], errors=\"coerce\").to_numpy()\n",
    "    t = generate_time(len(y), FS_THERMO)\n",
    "    out = pd.DataFrame({\"Time_sec\": t, \"Thermo1\": y})\n",
    "    out = finalize_df(out)\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    plot_lines(out, \"Thermo\", out_png)\n",
    "\n",
    "def process_skinos_csv(csv_path: str, out_csv: str, out_png: str) -> None:\n",
    "    \"\"\"\n",
    "    Skinos（CSV: *_skinos.* を想定）\n",
    "    - 入力: header=1（2行目が列名）\n",
    "    - 使用列:\n",
    "        'Instance_Sweat(mg/cm^2/min)' -> 'Sweat_Rate'\n",
    "        'Heart_Rate(bpm)'            -> 'Heart_Rate'\n",
    "        'Skin_Temperature(degree C)' -> 'Skin_Temp'\n",
    "    - 出力: CSV(Time_sec, Sweat_Rate, Heart_Rate, Skin_Temp) + PNG\n",
    "    - 時刻: FS_SKINOS(=0.1Hz, 10s間隔) で生成\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(csv_path)\n",
    "    with open(csv_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        df_raw = pd.read_csv(f, header=1)\n",
    "\n",
    "    need = [\n",
    "        \"Instance_Sweat(mg/cm^2/min)\",\n",
    "        \"Heart_Rate(bpm)\",\n",
    "        \"Skin_Temperature(degree C)\",\n",
    "    ]\n",
    "    for c in need:\n",
    "        if c not in df_raw.columns:\n",
    "            raise KeyError(f\"Skinos: 必要列 '{c}' なし。columns={list(df_raw.columns)}\")\n",
    "\n",
    "    sweat = pd.to_numeric(df_raw[need[0]], errors=\"coerce\").to_numpy()\n",
    "    hr    = pd.to_numeric(df_raw[need[1]],  errors=\"coerce\").to_numpy()\n",
    "    temp  = pd.to_numeric(df_raw[need[2]],  errors=\"coerce\").to_numpy()\n",
    "\n",
    "    t = generate_time(len(df_raw), FS_SKINOS)\n",
    "    out = pd.DataFrame({\"Time_sec\": t, \"Sweat_Rate\": sweat, \"Heart_Rate\": hr, \"Skin_Temp\": temp})\n",
    "    out = finalize_df(out)\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    plot_lines(out, \"Skinos\", out_png)\n",
    "\n",
    "def process_face_table(csv_or_xlsx_path: str, expected_prefix: str, out_csv: str, out_png: str,\n",
    "                       encoding: str = \"cp932\") -> None:\n",
    "    \"\"\"\n",
    "    FaceA/FaceB/FaceC（表形式：CSV もしくは Excel）\n",
    "    - 入力: {subject_id}A/B/C.CSV または {subject_id}_A/B/C.xlsx など\n",
    "      先頭にメタ行、ヘッダに「番号/測定日付/測定時間」などがある形式を想定\n",
    "    - 抽出列（中身のA/B/Cは自動検出）:\n",
    "        'BOX <letter> MAX.' -> '{expected_prefix}_BoxMax'\n",
    "        'BOX <letter> AVE.' -> '{expected_prefix}_BoxAve'\n",
    "      ※ expected_prefix（'FaceA'|'FaceB'|'FaceC'）で出力名を固定（ファイル名優先）\n",
    "    - 出力: CSV(Time_sec, {prefix}_BoxMax, {prefix}_BoxAve) + PNG\n",
    "    - 時刻: FS_FACE(=15Hz) で生成\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_or_xlsx_path):\n",
    "        raise FileNotFoundError(csv_or_xlsx_path)\n",
    "    if expected_prefix not in (\"FaceA\", \"FaceB\", \"FaceC\"):\n",
    "        raise ValueError(\"expected_prefix must be 'FaceA', 'FaceB', or 'FaceC'.\")\n",
    "\n",
    "    ext = os.path.splitext(csv_or_xlsx_path)[1].lower()\n",
    "\n",
    "    if ext in (\".xlsx\", \".xls\"):\n",
    "        # Excel: まずヘッダ行を検出するため、header=None で読む\n",
    "        df0 = pd.read_excel(csv_or_xlsx_path, sheet_name=0, header=None, dtype=str)\n",
    "        header_row = None\n",
    "        for i in range(min(len(df0), 80)):  # 余裕をみて80行まで探索\n",
    "            row_vals = \"\".join([str(v) for v in df0.iloc[i].tolist() if pd.notna(v)])\n",
    "            if (\"番号\" in row_vals) and (\"測定日付\" in row_vals) and (\"測定時間\" in row_vals):\n",
    "                header_row = i\n",
    "                break\n",
    "        if header_row is None:\n",
    "            raise ValueError(f\"{csv_or_xlsx_path}: ヘッダ行が見つかりません。\")\n",
    "        # ヘッダ行を列名にして再構築\n",
    "        cols = df0.iloc[header_row].tolist()\n",
    "        df_raw = df0.iloc[header_row+1:].copy()\n",
    "        df_raw.columns = cols\n",
    "    else:\n",
    "        # CSV: ヘッダ行スキャン\n",
    "        header_row = None\n",
    "        with open(csv_or_xlsx_path, \"r\", encoding=encoding, errors=\"ignore\") as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if (\"番号\" in line) and (\"測定日付\" in line) and (\"測定時間\" in line):\n",
    "                    header_row = idx\n",
    "                    break\n",
    "        if header_row is None:\n",
    "            raise ValueError(f\"{csv_or_xlsx_path}: ヘッダ行が見つかりません。\")\n",
    "        df_raw = pd.read_csv(csv_or_xlsx_path, header=header_row, encoding=encoding,\n",
    "                             engine=\"python\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    # 中身の A/B/C 列を検出。まず expected を優先、無ければ他を順に採用\n",
    "    want_letter = expected_prefix[-1]  # 'A' or 'B' or 'C'\n",
    "    letters = [want_letter] + [L for L in (\"A\", \"B\", \"C\") if L != want_letter]\n",
    "\n",
    "    def has_cols(letter: str) -> bool:\n",
    "        return {f\"BOX {letter} MAX.\", f\"BOX {letter} AVE.\"}.issubset(df_raw.columns)\n",
    "\n",
    "    src_letter = None\n",
    "    for L in letters:\n",
    "        if has_cols(L):\n",
    "            src_letter = L\n",
    "            break\n",
    "    if src_letter is None:\n",
    "        raise KeyError(\n",
    "            f\"{csv_or_xlsx_path}: Faceの必要列（BOX A/B/C MAX./AVE.）が見つかりません。\"\n",
    "            f\"columns={list(df_raw.columns)}\"\n",
    "        )\n",
    "\n",
    "    max_vals = pd.to_numeric(df_raw[f\"BOX {src_letter} MAX.\"], errors=\"coerce\").to_numpy()\n",
    "    ave_vals = pd.to_numeric(df_raw[f\"BOX {src_letter} AVE.\"], errors=\"coerce\").to_numpy()\n",
    "\n",
    "    n = min(len(max_vals), len(ave_vals))\n",
    "    t = generate_time(n, FS_FACE)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"Time_sec\": t,\n",
    "        f\"{expected_prefix}_BoxMax\": max_vals[:n],\n",
    "        f\"{expected_prefix}_BoxAve\": ave_vals[:n],\n",
    "    })\n",
    "    out = finalize_df(out)\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    plot_lines(out, expected_prefix, out_png)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# =============== ログ出力（要求フォーマット） ===============\n",
    "def log_subject_header(subject_id: str) -> None:\n",
    "    print(f\"# Subject {subject_id}\")\n",
    "\n",
    "def log_ok(device: str, out_csv: str) -> None:\n",
    "    print(f\"[OK]  {device} -> {out_csv}\")\n",
    "\n",
    "def log_skip(device: str, reason: str) -> None:\n",
    "    print(f\"[SKIP] {device}: {reason}\")\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ===================== メイン処理 =====================\n",
    "def main_1() -> None:\n",
    "    for subject_id in SUBJECT_IDS:\n",
    "        subject_dir = os.path.join(BASE_DIR, subject_id)  # 人名なし（IDのみ）\n",
    "        raw_dir     = os.path.join(subject_dir, \"RAW\")\n",
    "        ensure_dir(raw_dir)\n",
    "\n",
    "        log_subject_header(subject_id)\n",
    "\n",
    "        # 入力ファイル\n",
    "        path_mat    = os.path.join(subject_dir, f\"{subject_id}.mat\")  # Pulse+Sweat\n",
    "        path_thermo = os.path.join(subject_dir, f\"{subject_id}.CSV\")\n",
    "\n",
    "        # Face（CSV or Excel の両対応）\n",
    "        faceA_candidates = [\n",
    "            os.path.join(subject_dir, f\"{subject_id}A.CSV\"),\n",
    "            os.path.join(subject_dir, f\"{subject_id}_A.csv\"),\n",
    "            os.path.join(subject_dir, f\"{subject_id}_A.xlsx\"),\n",
    "            os.path.join(subject_dir, f\"{subject_id}A.xlsx\"),\n",
    "        ]\n",
    "        faceB_candidates = [\n",
    "            os.path.join(subject_dir, f\"{subject_id}B.CSV\"),\n",
    "            os.path.join(subject_dir, f\"{subject_id}_B.csv\"),\n",
    "            os.path.join(subject_dir, f\"{subject_id}_B.xlsx\"),\n",
    "            os.path.join(subject_dir, f\"{subject_id}B.xlsx\"),\n",
    "        ]\n",
    "        faceC_candidates = [\n",
    "            os.path.join(subject_dir, f\"{subject_id}C.CSV\"),\n",
    "            os.path.join(subject_dir, f\"{subject_id}_C.csv\"),\n",
    "            os.path.join(subject_dir, f\"{subject_id}_C.xlsx\"),\n",
    "            os.path.join(subject_dir, f\"{subject_id}C.xlsx\"),\n",
    "        ]\n",
    "        path_faceA = first_existing(faceA_candidates)\n",
    "        path_faceB = first_existing(faceB_candidates)\n",
    "        path_faceC = first_existing(faceC_candidates)\n",
    "\n",
    "        # Skinos（{id}_skinos.* を最優先）\n",
    "        path_skinos = find_skinos_file(subject_dir, subject_id)\n",
    "\n",
    "        # 出力（RAW配下）\n",
    "        out_pulse_csv  = os.path.join(raw_dir, f\"{subject_id}_Pulse.csv\")\n",
    "        out_pulse_png  = os.path.join(raw_dir, f\"{subject_id}_Pulse.png\")\n",
    "        out_sweat_csv  = os.path.join(raw_dir, f\"{subject_id}_Sweat.csv\")\n",
    "        out_sweat_png  = os.path.join(raw_dir, f\"{subject_id}_Sweat.png\")\n",
    "        out_thermo_csv = os.path.join(raw_dir, f\"{subject_id}_Thermo.csv\")\n",
    "        out_thermo_png = os.path.join(raw_dir, f\"{subject_id}_Thermo.png\")\n",
    "        out_skinos_csv = os.path.join(raw_dir, f\"{subject_id}_Skinos.csv\")\n",
    "        out_skinos_png = os.path.join(raw_dir, f\"{subject_id}_Skinos.png\")\n",
    "        out_faceA_csv  = os.path.join(raw_dir, f\"{subject_id}_FaceA.csv\")\n",
    "        out_faceA_png  = os.path.join(raw_dir, f\"{subject_id}_FaceA.png\")\n",
    "        out_faceB_csv  = os.path.join(raw_dir, f\"{subject_id}_FaceB.csv\")\n",
    "        out_faceB_png  = os.path.join(raw_dir, f\"{subject_id}_FaceB.png\")\n",
    "        out_faceC_csv  = os.path.join(raw_dir, f\"{subject_id}_FaceC.csv\")\n",
    "        out_faceC_png  = os.path.join(raw_dir, f\"{subject_id}_FaceC.png\")\n",
    "\n",
    "        # Pulse (+Sweat) from MAT\n",
    "        if os.path.exists(path_mat):\n",
    "            try:\n",
    "                process_pulse_mat(path_mat, out_pulse_csv, out_pulse_png)\n",
    "                log_ok(\"Pulse\", out_pulse_csv)\n",
    "                if os.path.exists(out_sweat_csv):\n",
    "                    log_ok(\"Sweat\", out_sweat_csv)\n",
    "                else:\n",
    "                    log_skip(\"Sweat\", \"channel not found in MAT\")\n",
    "            except Exception as e:\n",
    "                log_skip(\"Pulse\", str(e))\n",
    "                log_skip(\"Sweat\", \"skipped due to Pulse error\")\n",
    "        else:\n",
    "            log_skip(\"Pulse\", f\"file not found: {path_mat}\")\n",
    "            log_skip(\"Sweat\", \"file not found (MAT required)\")\n",
    "\n",
    "        # Thermo\n",
    "        if os.path.exists(path_thermo):\n",
    "            try:\n",
    "                process_thermo_csv(path_thermo, out_thermo_csv, out_thermo_png)\n",
    "                log_ok(\"Thermo\", out_thermo_csv)\n",
    "            except Exception as e:\n",
    "                log_skip(\"Thermo\", str(e))\n",
    "        else:\n",
    "            log_skip(\"Thermo\", f\"file not found: {path_thermo}\")\n",
    "\n",
    "        # Skinos\n",
    "        if path_skinos and os.path.exists(path_skinos):\n",
    "            try:\n",
    "                process_skinos_csv(path_skinos, out_skinos_csv, out_skinos_png)\n",
    "                log_ok(\"Skinos\", out_skinos_csv)\n",
    "            except Exception as e:\n",
    "                log_skip(\"Skinos\", str(e))\n",
    "        else:\n",
    "            log_skip(\"Skinos\", \"file not found ({id}_skinos.* or *skinos*.*)\")\n",
    "\n",
    "        # FaceA\n",
    "        if path_faceA:\n",
    "            try:\n",
    "                process_face_table(path_faceA, \"FaceA\", out_faceA_csv, out_faceA_png)\n",
    "                log_ok(\"FaceA\", out_faceA_csv)\n",
    "            except Exception as e:\n",
    "                log_skip(\"FaceA\", str(e))\n",
    "        else:\n",
    "            log_skip(\"FaceA\", \"file not found (A: CSV/XLSX)\")\n",
    "\n",
    "        # FaceB\n",
    "        if path_faceB:\n",
    "            try:\n",
    "                process_face_table(path_faceB, \"FaceB\", out_faceB_csv, out_faceB_png)\n",
    "                log_ok(\"FaceB\", out_faceB_csv)\n",
    "            except Exception as e:\n",
    "                log_skip(\"FaceB\", str(e))\n",
    "        else:\n",
    "            log_skip(\"FaceB\", \"file not found (B: CSV/XLSX)\")\n",
    "\n",
    "        # FaceC\n",
    "        if path_faceC:\n",
    "            try:\n",
    "                process_face_table(path_faceC, \"FaceC\", out_faceC_csv, out_faceC_png)\n",
    "                log_ok(\"FaceC\", out_faceC_csv)\n",
    "            except Exception as e:\n",
    "                log_skip(\"FaceC\", str(e))\n",
    "        else:\n",
    "            log_skip(\"FaceC\", \"file not found (C: CSV/XLSX)\")\n",
    "\n",
    "    print(\"\\nAll done.\")\n",
    "\n",
    "\n",
    "# エントリポイント\n",
    "if __name__ == \"__main__\":\n",
    "    main_1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab35c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAW -> OFFSET\n",
    "- Pulse / Thermo / FaceA / FaceB / FaceC / Sweat: 被験者×生体信号のオフセットを Time_sec に加算\n",
    "    - FaceB/C は FaceA を継承（OFFSETSにFaceB/Cは不要）\n",
    "    - Sweat は Pulse を継承（OFFSETSにSweatは不要）\n",
    "- Skinos: 被験者ごとに「最初 x 秒」をカットし、t := t - x（x秒地点を t=0 とする）\n",
    "- PNG出力（mm:ss 軸 / linewidth=1.5 / ハイライト: HILIGHT_START〜HILIGHT_END）\n",
    "    - かつ ±PLOT_PAD_SEC の範囲のみ描画（= [HILIGHT_START-PLOT_PAD_SEC, HILIGHT_END+PLOT_PAD_SEC]）\n",
    "    - 赤ハッチ開始直前30秒を灰色の斜線帯で表示\n",
    "- scope画像: OFFSET/scope/ に、生体信号ごと1枚（横並び2窓）\n",
    "    - 左: 灰色帯開始（= HILIGHT_START-30）±15秒\n",
    "    - 右: 赤帯終了（= HILIGHT_END）±15秒\n",
    "    - 横軸は「秒」表記（mm:ssではない）\n",
    "- ログ:\n",
    "    # Subject {sid}\n",
    "    [OK]  {Device} -> {output_csv_path}\n",
    "    [SKIP] {Device}: {reason}\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.transforms import blended_transform_factory\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# ===================== ユーザー設定 =====================\n",
    "BASE_DIR: str = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "# 対象ID（氏名は使わない）\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10031\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "# ★被験者×生体信号のオフセット（秒）\n",
    "#   - FaceB/C は FaceA を継承、Sweat は Pulse を継承（OFFSETSに記述不要）\n",
    "OFFSETS: Dict[str, Dict[str, float]] = {\n",
    "    \"10061\": {\"Pulse\": 1770.0-5,  \"Thermo\": 1710.0+5,  \"FaceA\": 1770.0-10},\n",
    "    \"10063\": {\"Pulse\": 1770.0-5,  \"Thermo\": 1710.0,    \"FaceA\": 1740.0+5},\n",
    "    \"10064\": {\"Pulse\": 1770.0-1,  \"Thermo\": 1710.0,    \"FaceA\": 1740.0+5},\n",
    "\n",
    "    \"10071\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+1},\n",
    "    \"10072\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "    \"10073\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "    \"10074\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "\n",
    "    \"10081\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "    \"10082\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "    \"10083\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "    \"10084\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "\n",
    "    \"10091\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "    \"10092\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "    \"10093\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "    \"10094\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "\n",
    "    \"10101\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+10},\n",
    "    \"10102\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "    \"10103\": {\"Pulse\": 1590.0,    \"Thermo\": 1530.0,    \"FaceA\": 1560.0+5},\n",
    "}\n",
    "\n",
    "\n",
    "# ★Skinos の「最初 x 秒カット」— 被験者ごとに設定（未指定は 0.0）\n",
    "SKINOS_TRIM_SECS: Dict[str, float] = {\n",
    "    \"10031\": 0.0, \"10061\": 0.0, \"10062\": 600.0, \"10063\": 0.0, \"10064\": 0.0,\n",
    "    \"10071\": 0.0, \"10072\": 0.0, \"10073\": 0.0, \"10074\": 0.0,\n",
    "    \"10081\": 0.0, \"10082\": 0.0, \"10083\": 0.0, \"10084\": 0.0,\n",
    "    \"10091\": 30.0, \"10092\": 0.0, \"10093\": 0.0, \"10094\": 300.0,\n",
    "    \"10101\": 0.0, \"10102\": 0.0, \"10103\": 0.0,\n",
    "}\n",
    "# ※ カット後は常に t := t - x（x秒地点を t=0 に揃える）\n",
    "\n",
    "# 出力・描画オプション\n",
    "MAKE_PLOTS: bool = True\n",
    "CLAMP_MIN_ZERO: bool = False          # （オフセット後）最小時刻を0へ平行移動\n",
    "HILIGHT_START: float = 1800.0\n",
    "HILIGHT_END: float = 2400.0\n",
    "PLOT_PAD_SEC: float = 60.0            # 描画ウィンドウの前後余白（秒）\n",
    "TIME_COL: str = \"Time_sec\"\n",
    "\n",
    "# ===================== センサ設定（順序保持） =====================\n",
    "SENSORS: List[Dict[str, Any]] = [\n",
    "    {\"name\": \"Pulse\",  \"enabled\": True,  \"file\": \"{sid}_Pulse.csv\",\n",
    "     \"y_cols\": [\"Pulse\"], \"title\": \"Pulse\"},\n",
    "    {\"name\": \"Thermo\", \"enabled\": True,  \"file\": \"{sid}_Thermo.csv\",\n",
    "     \"y_cols\": [\"Thermo1\"], \"title\": \"Thermo\"},\n",
    "    {\"name\": \"Sweat\",  \"enabled\": True,  \"file\": \"{sid}_Sweat.csv\",\n",
    "     \"y_cols\": [\"Sweat\"], \"title\": \"Sweat\"},\n",
    "    {\"name\": \"Skinos\", \"enabled\": True,  \"file\": \"{sid}_Skinos.csv\",\n",
    "     \"y_cols\": [\"Sweat_Rate\", \"Heart_Rate\", \"Skin_Temp\"], \"title\": \"Skinos\"},\n",
    "    {\"name\": \"FaceA\",  \"enabled\": True,  \"file\": \"{sid}_FaceA.csv\",\n",
    "     \"y_cols\": [\"FaceA_BoxMax\", \"FaceA_BoxAve\"], \"title\": \"FaceA\"},\n",
    "    {\"name\": \"FaceB\",  \"enabled\": True,  \"file\": \"{sid}_FaceB.csv\",\n",
    "     \"y_cols\": [\"FaceB_BoxMax\", \"FaceB_BoxAve\"], \"title\": \"FaceB\"},\n",
    "    {\"name\": \"FaceC\",  \"enabled\": True,  \"file\": \"{sid}_FaceC.csv\",\n",
    "     \"y_cols\": [\"FaceC_BoxMax\", \"FaceC_BoxAve\"], \"title\": \"FaceC\"},\n",
    "]\n",
    "\n",
    "# ===================== ユーティリティ =====================\n",
    "def read_csv_robust(path: str) -> pd.DataFrame:\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"cp932\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.read_csv(path, encoding_errors=\"ignore\")\n",
    "\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def mmss_formatter(x: float, pos: int) -> str:\n",
    "    sign = \"-\" if x < 0 else \"\"\n",
    "    x = abs(x); m = int(x // 60); s = int(round(x - m*60))\n",
    "    if s == 60: m += 1; s = 0\n",
    "    return f\"{sign}{m:02d}:{s:02d}\"\n",
    "\n",
    "def apply_offset(df: pd.DataFrame, offset_sec: float, clamp_min_zero: bool) -> pd.DataFrame:\n",
    "    if TIME_COL not in df.columns:\n",
    "        raise ValueError(f\"{TIME_COL} not found: {list(df.columns)}\")\n",
    "    out = df.copy()\n",
    "    out[TIME_COL] = pd.to_numeric(out[TIME_COL], errors=\"coerce\") + float(offset_sec)\n",
    "    if clamp_min_zero:\n",
    "        min_t = out[TIME_COL].min()\n",
    "        if pd.notna(min_t) and min_t != 0:\n",
    "            out[TIME_COL] = out[TIME_COL] - min_t\n",
    "    return out\n",
    "\n",
    "def add_red_hatched_band(ax, x0: float, x1: float) -> None:\n",
    "    ax.axvspan(x0, x1, color=\"red\", alpha=0.08, zorder=0)\n",
    "    trans = blended_transform_factory(ax.transData, ax.transAxes)\n",
    "    rect = Rectangle((x0, 0), x1-x0, 1, transform=trans,\n",
    "                     fill=False, hatch=\"////\", edgecolor=\"red\",\n",
    "                     linewidth=0.0, zorder=1, alpha=0.5)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "def add_gray_hatched_before(ax, start: float, width: float = 30.0) -> None:\n",
    "    \"\"\"赤ハッチ開始直前の [start-width, start] を灰色の帯＋灰色ハッチで示す。\"\"\"\n",
    "    x0 = float(start) - float(width)\n",
    "    x1 = float(start)\n",
    "    ax.axvspan(x0, x1, color=\"gray\", alpha=0.10, zorder=0)\n",
    "    trans = blended_transform_factory(ax.transData, ax.transAxes)\n",
    "    rect = Rectangle((x0, 0), x1 - x0, 1,\n",
    "                     transform=trans, fill=False, hatch=\"////\",\n",
    "                     edgecolor=\"gray\", linewidth=0.0, zorder=1, alpha=0.5)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "def crop_for_window(df: pd.DataFrame,\n",
    "                    x_col: str,\n",
    "                    center_start: float,\n",
    "                    center_end: float,\n",
    "                    pad: float) -> pd.DataFrame:\n",
    "    \"\"\"[center_start - pad, center_end + pad] の範囲だけにデータを絞る（空なら元DFを返す）\"\"\"\n",
    "    x0 = center_start - pad\n",
    "    x1 = center_end + pad\n",
    "    xv = pd.to_numeric(df[x_col], errors=\"coerce\")\n",
    "    mask = (xv >= x0) & (xv <= x1)\n",
    "    dfw = df.loc[mask].copy()\n",
    "    return dfw if not dfw.empty else df\n",
    "\n",
    "def plot_timeseries(df: pd.DataFrame,\n",
    "                    y_cols: List[str],\n",
    "                    out_png: str,\n",
    "                    title: str,\n",
    "                    hilight_range: Tuple[float, float],\n",
    "                    pad_sec: float) -> None:\n",
    "    \"\"\"±pad_sec のウィンドウのみ描画し、その中に灰色(直前30秒)と赤ハッチ帯を重ねてPNG保存。\"\"\"\n",
    "    dfw = crop_for_window(df, TIME_COL, hilight_range[0], hilight_range[1], pad_sec)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    ax = plt.gca()\n",
    "    for c in y_cols:\n",
    "        if c in dfw.columns:\n",
    "            ax.plot(dfw[TIME_COL], pd.to_numeric(dfw[c], errors=\"coerce\"),\n",
    "                    label=c, linewidth=1.5)\n",
    "\n",
    "    # 直前30秒（灰）\n",
    "    add_gray_hatched_before(ax, start=hilight_range[0], width=30.0)\n",
    "    # 本番区間（赤）\n",
    "    add_red_hatched_band(ax, *hilight_range)\n",
    "\n",
    "    # 表示範囲固定\n",
    "    ax.set_xlim(hilight_range[0] - pad_sec, hilight_range[1] + pad_sec)\n",
    "\n",
    "    # 体裁（mm:ss）\n",
    "    ax.set_title(title, fontsize=30)\n",
    "    ax.set_xlabel(\"Time (mm:ss)\", fontsize=24)\n",
    "    ax.set_ylabel(\"Value\", fontsize=24)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(mmss_formatter))\n",
    "    ax.tick_params(axis=\"both\", labelsize=20)\n",
    "    ax.legend(fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# === 継承ロジック（Sweat→Pulse, FaceB/C→FaceA） ===\n",
    "def resolve_sensor_offset(per_sensor_offset: Dict[str, float], sensor_name: str) -> float:\n",
    "    \"\"\"継承ルール:\n",
    "       - FaceB/C → FaceA と同じ\n",
    "       - Sweat   → Pulse と同じ\n",
    "       - それ以外→ 自分のキー（無ければ 0.0）\n",
    "    \"\"\"\n",
    "    if sensor_name in (\"FaceB\", \"FaceC\"):\n",
    "        return float(per_sensor_offset.get(\"FaceA\", 0.0))\n",
    "    if sensor_name == \"Sweat\":\n",
    "        return float(per_sensor_offset.get(\"Pulse\", 0.0))\n",
    "    return float(per_sensor_offset.get(sensor_name, 0.0))\n",
    "\n",
    "# === scope 画像（灰色帯の開始前後 / 赤帯の終了前後）を横並びで保存 ===\n",
    "def plot_scope_two_windows(df: pd.DataFrame,\n",
    "                           y_cols: List[str],\n",
    "                           out_png: str,\n",
    "                           title: str,\n",
    "                           gray_start: float,\n",
    "                           red_end: float,\n",
    "                           half_window: float = 15.0) -> None:\n",
    "    \"\"\"\n",
    "    左: [gray_start-15, gray_start+15], 右: [red_end-15, red_end+15]\n",
    "    横軸は「秒」表記（mm:ss ではない）。\n",
    "    \"\"\"\n",
    "    import matplotlib.ticker as mticker\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "    windows = [(gray_start - half_window, gray_start + half_window),\n",
    "               (red_end   - half_window, red_end   + half_window)]\n",
    "    titles = [f\"{title}  (gray-start ±{int(half_window)}s)\",\n",
    "              f\"{title}  (red-end   ±{int(half_window)}s)\"]\n",
    "\n",
    "    for ax, (x0, x1), st in zip(axes, windows, titles):\n",
    "        # 窓データ\n",
    "        xv = pd.to_numeric(df[TIME_COL], errors=\"coerce\")\n",
    "        mask = (xv >= x0) & (xv <= x1)\n",
    "        sub = df.loc[mask].copy()\n",
    "\n",
    "        for c in y_cols:\n",
    "            if c in sub.columns:\n",
    "                ax.plot(sub[TIME_COL], pd.to_numeric(sub[c], errors=\"coerce\"),\n",
    "                        linewidth=1.5, label=c)\n",
    "\n",
    "        ax.set_xlim(x0, x1)\n",
    "        ax.set_title(st, fontsize=16)\n",
    "        ax.set_xlabel(\"Time (sec)\", fontsize=12)\n",
    "        ax.xaxis.set_major_locator(mticker.MaxNLocator(6))\n",
    "        ax.tick_params(axis=\"both\", labelsize=10)\n",
    "        # 中央基準線\n",
    "        ax.axvline((x0 + x1) / 2, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    axes[0].set_ylabel(\"Value\", fontsize=12)\n",
    "    # 凡例（右）\n",
    "    handles, labels = axes[-1].get_legend_handles_labels()\n",
    "    if handles:\n",
    "        axes[-1].legend(handles, labels, fontsize=10, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ===================== コア処理 =====================\n",
    "def process_sensor_offset(raw_dir: str,\n",
    "                          out_dir: str,\n",
    "                          sensor_cfg: Dict[str, Any],\n",
    "                          sid: str,\n",
    "                          per_sensor_offset: Dict[str, float]) -> str:\n",
    "    \"\"\"Pulse/Thermo/FaceA/FaceB/FaceC/Sweat: オフセット加算 + 保存 + PNG + scope画像\"\"\"\n",
    "    name = sensor_cfg[\"name\"]\n",
    "    in_csv = os.path.join(raw_dir, sensor_cfg[\"file\"].format(sid=sid))\n",
    "    if not os.path.exists(in_csv):\n",
    "        return f\"[SKIP] {name}: not found -> {in_csv}\"\n",
    "\n",
    "    try:\n",
    "        df = read_csv_robust(in_csv)\n",
    "        offset_val = resolve_sensor_offset(per_sensor_offset, name)\n",
    "        df2 = apply_offset(df, offset_val, CLAMP_MIN_ZERO)\n",
    "\n",
    "        ensure_dir(out_dir)\n",
    "        out_csv = os.path.join(out_dir, os.path.basename(in_csv))\n",
    "        df2.to_csv(out_csv, index=False)\n",
    "\n",
    "        if MAKE_PLOTS:\n",
    "            # 全体（mm:ss）PNG\n",
    "            out_png = os.path.splitext(out_csv)[0] + \".png\"\n",
    "            plot_timeseries(df=df2,\n",
    "                            y_cols=sensor_cfg[\"y_cols\"],\n",
    "                            out_png=out_png,\n",
    "                            title=sensor_cfg[\"title\"],\n",
    "                            hilight_range=(HILIGHT_START, HILIGHT_END),\n",
    "                            pad_sec=PLOT_PAD_SEC)\n",
    "\n",
    "            # scope 画像（秒表示・2窓）\n",
    "            scope_dir = os.path.join(out_dir, \"scope\")\n",
    "            ensure_dir(scope_dir)\n",
    "            gray_start = HILIGHT_START - 30.0\n",
    "            red_end = HILIGHT_END\n",
    "            out_scope = os.path.join(scope_dir, f\"{sid}_{name}_scope.png\")\n",
    "            plot_scope_two_windows(df=df2,\n",
    "                                   y_cols=sensor_cfg[\"y_cols\"],\n",
    "                                   out_png=out_scope,\n",
    "                                   title=sensor_cfg[\"title\"],\n",
    "                                   gray_start=gray_start,\n",
    "                                   red_end=red_end,\n",
    "                                   half_window=15.0)\n",
    "\n",
    "        return f\"[OK]  {name} -> {out_csv}\"\n",
    "    except Exception as e:\n",
    "        return f\"[SKIP] {name}: {e}\"\n",
    "\n",
    "def process_skinos_trim(raw_dir: str,\n",
    "                        out_dir: str,\n",
    "                        sensor_cfg: Dict[str, Any],\n",
    "                        sid: str) -> str:\n",
    "    \"\"\"\n",
    "    Skinos: 最初 x 秒をカット（Time_sec >= x の行のみ残す）し、常に t := t - x（x秒地点を t=0）。\n",
    "    オフセットは適用しない。scope画像も出力。\n",
    "    \"\"\"\n",
    "    name = sensor_cfg[\"name\"]\n",
    "    in_csv = os.path.join(raw_dir, sensor_cfg[\"file\"].format(sid=sid))\n",
    "    if not os.path.exists(in_csv):\n",
    "        return f\"[SKIP] {name}: not found -> {in_csv}\"\n",
    "\n",
    "    try:\n",
    "        df = read_csv_robust(in_csv)\n",
    "        if TIME_COL not in df.columns:\n",
    "            return f\"[SKIP] {name}: {TIME_COL} not found\"\n",
    "\n",
    "        trim_x = float(SKINOS_TRIM_SECS.get(sid, 0.0))\n",
    "        df[TIME_COL] = pd.to_numeric(df[TIME_COL], errors=\"coerce\")\n",
    "\n",
    "        df2 = df[df[TIME_COL] >= trim_x].copy()\n",
    "        if df2.empty:\n",
    "            return f\"[SKIP] {name}: empty after trim ({trim_x}s)\"\n",
    "\n",
    "        df2[TIME_COL] = df2[TIME_COL] - trim_x  # x秒地点を原点化\n",
    "\n",
    "        ensure_dir(out_dir)\n",
    "        out_csv = os.path.join(out_dir, os.path.basename(in_csv))\n",
    "        df2.to_csv(out_csv, index=False)\n",
    "\n",
    "        if MAKE_PLOTS:\n",
    "            # 全体（mm:ss）PNG\n",
    "            out_png = os.path.splitext(out_csv)[0] + \".png\"\n",
    "            plot_timeseries(df=df2,\n",
    "                            y_cols=sensor_cfg[\"y_cols\"],\n",
    "                            out_png=out_png,\n",
    "                            title=sensor_cfg[\"title\"],\n",
    "                            hilight_range=(HILIGHT_START, HILIGHT_END),\n",
    "                            pad_sec=PLOT_PAD_SEC)\n",
    "\n",
    "            # scope 画像（秒表示・2窓）\n",
    "            scope_dir = os.path.join(out_dir, \"scope\")\n",
    "            ensure_dir(scope_dir)\n",
    "            gray_start = HILIGHT_START - 30.0\n",
    "            red_end = HILIGHT_END\n",
    "            out_scope = os.path.join(scope_dir, f\"{sid}_{name}_scope.png\")\n",
    "            plot_scope_two_windows(df=df2,\n",
    "                                   y_cols=sensor_cfg[\"y_cols\"],\n",
    "                                   out_png=out_scope,\n",
    "                                   title=sensor_cfg[\"title\"],\n",
    "                                   gray_start=gray_start,\n",
    "                                   red_end=red_end,\n",
    "                                   half_window=15.0)\n",
    "\n",
    "        return f\"[OK]  {name} -> {out_csv}\"\n",
    "    except Exception as e:\n",
    "        return f\"[SKIP] {name}: {e}\"\n",
    "\n",
    "def process_subject(sid: str) -> str:\n",
    "    subj_dir = os.path.join(BASE_DIR, sid)\n",
    "    raw_dir = os.path.join(subj_dir, \"RAW\")\n",
    "    out_dir = os.path.join(subj_dir, \"OFFSET2\")\n",
    "\n",
    "    per_sensor_offset = OFFSETS.get(sid, {})  # 未登録は空→0.0扱い\n",
    "    logs = [f\"# Subject {sid}\"]\n",
    "\n",
    "    for sensor_cfg in SENSORS:\n",
    "        if not sensor_cfg.get(\"enabled\", True):\n",
    "            logs.append(f\"[SKIP] {sensor_cfg['name']}: skipped by config\")\n",
    "            continue\n",
    "\n",
    "        if sensor_cfg[\"name\"] == \"Skinos\":\n",
    "            logs.append(process_skinos_trim(raw_dir, out_dir, sensor_cfg, sid))\n",
    "        else:\n",
    "            logs.append(process_sensor_offset(raw_dir, out_dir, sensor_cfg, sid, per_sensor_offset))\n",
    "\n",
    "    return \"\\n\".join(logs)\n",
    "\n",
    "def main() -> None:\n",
    "    all_logs: List[str] = []\n",
    "    for sid in SUBJECT_IDS:\n",
    "        all_logs.append(process_subject(sid))\n",
    "    print(\"\\n\".join(all_logs))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2430a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===== ユーザー設定 =====\n",
    "BASE_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10061\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "\n",
    "# 対象デバイス（Skinosはトリム x を推定）\n",
    "DEVICES = [\n",
    "    (\"Pulse\",  \"{sid}_Pulse.csv\"),\n",
    "    (\"Thermo\", \"{sid}_Thermo.csv\"),\n",
    "    (\"Sweat\",  \"{sid}_Sweat.csv\"),\n",
    "    (\"FaceA\",  \"{sid}_FaceA.csv\"),\n",
    "    (\"FaceB\",  \"{sid}_FaceB.csv\"),\n",
    "    (\"FaceC\",  \"{sid}_FaceC.csv\"),\n",
    "    (\"Skinos\", \"{sid}_Skinos.csv\"),  # 特別扱い：トリム x 推定\n",
    "]\n",
    "TIME_COL = \"Time_sec\"\n",
    "\n",
    "# ===== ユーティリティ =====\n",
    "def read_csv_robust(path: str) -> pd.DataFrame:\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"cp932\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.read_csv(path, encoding_errors=\"ignore\")\n",
    "\n",
    "def _to_num(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "# ===== 推定ロジック =====\n",
    "def estimate_offset_general(df_raw: pd.DataFrame, df_off: pd.DataFrame) -> Tuple[float, bool, str]:\n",
    "    \"\"\"\n",
    "    一般デバイス: Δ ≈ median(Time_off - Time_raw)\n",
    "    - 行対応（長い側は切り詰め）\n",
    "    - CLAMP検知: min(Time_off)≈0 かつ min(Time_raw)>0 なら True\n",
    "    返り値: (offset_sec_est, clamp_detected, method_str)\n",
    "    \"\"\"\n",
    "    tr = _to_num(df_raw[TIME_COL])\n",
    "    to = _to_num(df_off[TIME_COL])\n",
    "\n",
    "    # 同じ行数に揃える（先頭揃え）\n",
    "    n = min(len(tr), len(to))\n",
    "    tr = tr.iloc[:n].reset_index(drop=True)\n",
    "    to = to.iloc[:n].reset_index(drop=True)\n",
    "\n",
    "    # 差の頑健推定（中央値）\n",
    "    diffs = (to - tr).dropna()\n",
    "    if diffs.empty:\n",
    "        return (float(\"nan\"), False, \"NA(empty diffs)\")\n",
    "\n",
    "    delta_med = float(np.median(diffs.values))\n",
    "\n",
    "    # CLAMP（最小0への再平行移動）が掛かっていると、真のΔは消える\n",
    "    clamp = (abs(float(to.min())) < 1e-6) and (float(tr.min()) > 0.0)\n",
    "    method = \"median(to - tr)\"\n",
    "    if clamp:\n",
    "        method += \" + CLAMP_DETECTED\"\n",
    "\n",
    "    return (delta_med, clamp, method)\n",
    "\n",
    "def estimate_trim_skinos(df_raw: pd.DataFrame, df_off: pd.DataFrame) -> Tuple[float, bool, str]:\n",
    "    \"\"\"\n",
    "    Skinos: RAWの先頭x秒をカットし、t := t - x で原点化（最小≈0）\n",
    "    推定: x ≈ RAWのうち保持分の先頭時刻\n",
    "      - 長さ差が head トリムに起因 → 近似的に raw[len_raw - len_off] を先頭とみなすとズレるので、\n",
    "        ここでは「offの最小は≈0」を利用し、rawの時刻列のうち、offと行数を合わせた先頭側の時刻中央値を採用。\n",
    "      - より単純に: x_est ≈ raw.time.min() if off.min≈0 and offが原点化直後のみ、だが\n",
    "        実運用では保持先頭の生データ時刻を使うのが安定。\n",
    "    返り値: (trim_x_sec_est, clamp_detected, method_str)\n",
    "    \"\"\"\n",
    "    tr = _to_num(df_raw[TIME_COL]).dropna().reset_index(drop=True)\n",
    "    to = _to_num(df_off[TIME_COL]).dropna().reset_index(drop=True)\n",
    "    if tr.empty or to.empty:\n",
    "        return (float(\"nan\"), False, \"NA(empty series)\")\n",
    "\n",
    "    clamp = (abs(float(to.min())) < 1e-6)  # 原点化されているはず\n",
    "    # 先頭合わせ（オフセットは0想定、長さは raw >= off のことが多い）\n",
    "    # off が原点から始まるので、off の先頭に対応する raw の時刻が trim_x の近似\n",
    "    # ここでは raw の最小値～先頭近傍の中央値を使って頑健化\n",
    "    # アプローチ: rawの先頭から len_off 件の時刻を取り、その中央値を trim_x とする\n",
    "    m = min(len(tr), len(to))\n",
    "    # 先頭 m 点の中央値（先頭のノイズに弱いなら、先頭 m の上位30%を除外する等の工夫も可）\n",
    "    trim_x = float(np.median(tr.iloc[:m].values))\n",
    "\n",
    "    return (trim_x, clamp, \"median(raw[:m]) with m=len_overlap & off.min≈0\")\n",
    "\n",
    "# ===== 本体 =====\n",
    "def main():\n",
    "    for sid in SUBJECT_IDS:\n",
    "        raw_dir = os.path.join(BASE_DIR, sid, \"RAW\")\n",
    "        off_dir = os.path.join(BASE_DIR, sid, \"OFFSET\")\n",
    "        print(f\"# Subject {sid}\")\n",
    "        for dev, fname_tmpl in DEVICES:\n",
    "            fn = fname_tmpl.format(sid=sid)\n",
    "            p_raw = os.path.join(raw_dir, fn)\n",
    "            p_off = os.path.join(off_dir, fn)\n",
    "\n",
    "            if not (os.path.exists(p_raw) and os.path.exists(p_off)):\n",
    "                print(f\"[SKIP] {dev}: missing file(s) -> RAW:{os.path.exists(p_raw)} OFF:{os.path.exists(p_off)}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                df_r = read_csv_robust(p_raw)\n",
    "                df_o = read_csv_robust(p_off)\n",
    "                if TIME_COL not in df_r.columns or TIME_COL not in df_o.columns:\n",
    "                    print(f\"[SKIP] {dev}: Time_sec not found\")\n",
    "                    continue\n",
    "\n",
    "                if dev == \"Skinos\":\n",
    "                    trim_x, clamp, method = estimate_trim_skinos(df_r, df_o)\n",
    "                    clamp_str = \" CLAMP\" if clamp else \"\"\n",
    "                    print(f\"[OK]  {dev}: trim_x ≈ {trim_x:.3f} s  ({method}{clamp_str})\")\n",
    "                else:\n",
    "                    delta, clamp, method = estimate_offset_general(df_r, df_o)\n",
    "                    clamp_str = \" CLAMP\" if clamp else \"\"\n",
    "                    print(f\"[OK]  {dev}: offset ≈ {delta:.3f} s  ({method}{clamp_str})\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP] {dev}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a26b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Skinos（OFFSET内CSV）に任意のオフセット（秒）を加算して **上書き保存** する専用セル。\n",
    "- 対象: BASE_DIR/{SID}/OFFSET/{SID}_Skinos.csv\n",
    "- 仕様: Time_sec := Time_sec + OFFSET_SEC[SID]\n",
    "- 入力: OFFSETS_SKINOS に {SID: offset_sec} を記述（秒, 負値可）\n",
    "- 出力: 同じCSVを上書き保存（必要に応じて .bak を作成）\n",
    "\n",
    "ログ形式:\n",
    "  # Subject {sid}\n",
    "  [OK]  Skinos -> overwrite {path} (Δ={offset:.3f}s, rows={n})\n",
    "  [SKIP] Skinos: {reason}\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# ===================== ユーザー設定 =====================\n",
    "BASE_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "# ここに「上書きしたいSIDとオフセット秒」を記入（例：{\"10061\": +2.5, \"10074\": -1.0}）\n",
    "OFFSETS_SKINOS: Dict[str, float] = {\n",
    "    \"10061\": -90.0, \"10063\": -90.0, \"10064\": -90.0,\n",
    "    \"10071\": -90.0, \"10072\": -90.0, \"10073\": -90.0, \"10074\": -90.0,\n",
    "    \"10081\": -90.0, \"10082\": -90.0, \"10083\": -90.0,\n",
    "    \"10091\": -90.0, \"10092\": -90.0, \"10093\": -90.0, \"10094\": -90.0,\n",
    "    \"10101\": -90.0, \"10102\": -90.0, \"10103\": -90.0,\n",
    "}\n",
    "\n",
    "\n",
    "# バックアップを残すなら True（{SID}_Skinos.csv.bak を作成）\n",
    "MAKE_BACKUP: bool = True\n",
    "\n",
    "TIME_COL = \"Time_sec\"\n",
    "\n",
    "# ===================== ユーティリティ =====================\n",
    "def read_csv_robust(path: str) -> pd.DataFrame:\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"cp932\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    # 最終フォールバック\n",
    "    return pd.read_csv(path, encoding_errors=\"ignore\")\n",
    "\n",
    "def process_one_sid(base_dir: str, sid: str, delta_sec: float) -> str:\n",
    "    subj_dir = os.path.join(base_dir, sid)\n",
    "    in_csv = os.path.join(subj_dir, \"OFFSET\", f\"{sid}_Skinos.csv\")\n",
    "    if not os.path.exists(in_csv):\n",
    "        return f\"[SKIP] Skinos: not found -> {in_csv}\"\n",
    "\n",
    "    try:\n",
    "        df = read_csv_robust(in_csv)\n",
    "        if TIME_COL not in df.columns:\n",
    "            return f\"[SKIP] Skinos: {TIME_COL} not found\"\n",
    "\n",
    "        # 数値化 + オフセット加算\n",
    "        df[TIME_COL] = pd.to_numeric(df[TIME_COL], errors=\"coerce\")\n",
    "        n_before = len(df)\n",
    "        df[TIME_COL] = df[TIME_COL] + float(delta_sec)\n",
    "\n",
    "        # バックアップ（任意）\n",
    "        if MAKE_BACKUP:\n",
    "            bak_path = in_csv + \".bak\"\n",
    "            try:\n",
    "                shutil.copy2(in_csv, bak_path)\n",
    "            except Exception:\n",
    "                # バックアップ失敗は処理続行（上書きが優先）\n",
    "                pass\n",
    "\n",
    "        # 上書き保存（エンコーディングは既定/安全側）\n",
    "        df.to_csv(in_csv, index=False, encoding=\"utf-8-sig\")\n",
    "        return f\"[OK]  Skinos -> overwrite {in_csv} (Δ={delta_sec:.3f}s, rows={n_before})\"\n",
    "    except Exception as e:\n",
    "        return f\"[SKIP] Skinos: {e}\"\n",
    "\n",
    "# ===================== 本体 =====================\n",
    "def main():\n",
    "    if not OFFSETS_SKINOS:\n",
    "        print(\"[SKIP] OFFSETS_SKINOS is empty (nothing to do)\")\n",
    "        return\n",
    "\n",
    "    for sid, delta in OFFSETS_SKINOS.items():\n",
    "        print(f\"# Subject {sid}\")\n",
    "        msg = process_one_sid(BASE_DIR, sid, delta)\n",
    "        print(msg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2df055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "scope 画像の集約コピー専用スクリプト（描画なし／元画像は変更しない）\n",
    "\n",
    "探索場所:\n",
    "  BASE_DIR/{sid}/OFFSET/scope/\n",
    "  BASE_DIR/{sid}/OFFSET2/scope/     # 任意\n",
    "\n",
    "対象ファイル:\n",
    "  - \"{sid}_*_scope.png\" をすべて対象\n",
    "  - 例: \"{sid}_{Sensor}_scope.png\", \"{sid}_各生体信号_scope.png\" など\n",
    "\n",
    "出力先:\n",
    "  BASE_DIR/SCOPE/ に複製保存（shutil.copy2 でタイムスタンプも維持）\n",
    "\n",
    "重名対策:\n",
    "  同名ファイルが既に存在する場合は、自動で \"_1\", \"_2\", … を末尾に付けて保存。\n",
    "\n",
    "ログ:\n",
    "  # Subject {sid}\n",
    "  [OK]   copy -> {src_rel}  =>  {dst_rel}\n",
    "  [SKIP] {reason}\n",
    "\n",
    "※ 元ファイルの上書き・変更は行わない。\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Tuple\n",
    "import os, glob, shutil\n",
    "\n",
    "# ===================== ユーザー設定 =====================\n",
    "BASE_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "# 対象SID（必要な分だけ）\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10031\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "# 画像を探すサブディレクトリ\n",
    "SOURCE_OFFSET_DIRNAMES: List[str] = [\"OFFSET\"]  # 片方だけでもOK\n",
    "SCOPE_SUBDIR = \"scope\"\n",
    "\n",
    "# 複製保存先\n",
    "CENTRAL_SCOPE_DIR = os.path.join(BASE_DIR, \"SCOPE\")\n",
    "\n",
    "# 既存ファイルがあっても強制上書きするなら True（既定: False = 自動リネーム）\n",
    "OVERWRITE = False\n",
    "\n",
    "# ===================== ユーティリティ =====================\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def _safe_dest_path(dst_dir: str, base_filename: str, overwrite: bool=False) -> str:\n",
    "    \"\"\"\n",
    "    保存先に同名がある場合、_1, _2, ... を付けて衝突回避。\n",
    "    overwrite=True のときはそのまま上書きパスを返す。\n",
    "    \"\"\"\n",
    "    dst = os.path.join(dst_dir, base_filename)\n",
    "    if overwrite or not os.path.exists(dst):\n",
    "        return dst\n",
    "    root, ext = os.path.splitext(base_filename)\n",
    "    k = 1\n",
    "    while True:\n",
    "        cand = os.path.join(dst_dir, f\"{root}_{k}{ext}\")\n",
    "        if not os.path.exists(cand):\n",
    "            return cand\n",
    "        k += 1\n",
    "\n",
    "def _rel(path: str, base: str) -> str:\n",
    "    try:\n",
    "        return os.path.relpath(path, base)\n",
    "    except Exception:\n",
    "        return path\n",
    "\n",
    "# ===================== 本体 =====================\n",
    "def main():\n",
    "    ensure_dir(CENTRAL_SCOPE_DIR)\n",
    "    total_found = 0\n",
    "    total_copied = 0\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        print(f\"# Subject {sid}\")\n",
    "        subj_root = os.path.join(BASE_DIR, sid)\n",
    "        if not os.path.isdir(subj_root):\n",
    "            print(f\"[SKIP] subject root not found -> {subj_root}\")\n",
    "            continue\n",
    "\n",
    "        found_for_sid = 0\n",
    "        copied_for_sid = 0\n",
    "\n",
    "        for offdir in SOURCE_OFFSET_DIRNAMES:\n",
    "            scope_dir = os.path.join(subj_root, offdir, SCOPE_SUBDIR)\n",
    "            if not os.path.isdir(scope_dir):\n",
    "                print(f\"[SKIP] scope dir not found -> { _rel(scope_dir, BASE_DIR) }\")\n",
    "                continue\n",
    "\n",
    "            # パターン: \"{sid}_*_scope.png\" を広めに拾う\n",
    "            pattern = os.path.join(scope_dir, f\"{sid}_*_scope.png\")\n",
    "            files = sorted(glob.glob(pattern))\n",
    "            if not files:\n",
    "                print(f\"[SKIP] no files -> { _rel(pattern, BASE_DIR) }\")\n",
    "                continue\n",
    "\n",
    "            for src in files:\n",
    "                found_for_sid += 1\n",
    "                base = os.path.basename(src)\n",
    "                dst = _safe_dest_path(CENTRAL_SCOPE_DIR, base, overwrite=OVERWRITE)\n",
    "                try:\n",
    "                    shutil.copy2(src, dst)  # タイムスタンプ等を維持\n",
    "                    copied_for_sid += 1\n",
    "                    print(f\"[OK]   copy -> { _rel(src, BASE_DIR) }  =>  { _rel(dst, BASE_DIR) }\")\n",
    "                except Exception as e:\n",
    "                    print(f\"[SKIP] copy failed -> { _rel(src, BASE_DIR) }: {e}\")\n",
    "\n",
    "        total_found += found_for_sid\n",
    "        total_copied += copied_for_sid\n",
    "        if found_for_sid == 0:\n",
    "            print(\"[SKIP] no scope images for this subject\")\n",
    "\n",
    "    print(\"\\n===== SUMMARY =====\")\n",
    "    print(f\"found : {total_found}\")\n",
    "    print(f\"copied: {total_copied}\")\n",
    "    print(f\"dest  : { _rel(CENTRAL_SCOPE_DIR, BASE_DIR) }\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f504a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "(3a) Feature Engineering - FaceTemp_raw（LP=1 Hz、サンプル時刻そのまま）\n",
    "----------------------------------------------------------------\n",
    "入力:\n",
    "  {BASE_DIR}\\{sid}\\OFFSET\\{sid}_FaceA.csv\n",
    "  {BASE_DIR}\\{sid}\\OFFSET\\{sid}_FaceB.csv\n",
    "  {BASE_DIR}\\{sid}\\OFFSET\\{sid}_FaceC.csv\n",
    "出力:\n",
    "  {BASE_DIR}\\{sid}\\FEATURE\\{sid}_FaceTemp_raw.csv (UTF-8-SIG)\n",
    "\n",
    "ログ:\n",
    "  # Subject {sid}\n",
    "  [OK]  FaceTemp_raw -> {output_csv_path}\n",
    "  または\n",
    "  [SKIP] FaceTemp_raw: {reason}\n",
    "\n",
    "処理概要:\n",
    "  1) FaceA/B/C の Max/Mean 列を自動検出（'box a/b/c' などを優先）\n",
    "  2) 1 Hz ローパス（Butterworth, order=2, filtfilt）。不均一サンプリングは等間隔へ一旦補間→filt→元時刻に逆補間\n",
    "  3) Time_sec 内積で inner join（重複時刻は平均化）\n",
    "  4) ご指定の出力9列を作成（ABはA/Bの平均、CはC単独、ABCは3点平均）\n",
    "     - AB_max      = (Amax + Bmax)/2\n",
    "     - C_max       = Cmax\n",
    "     - ABC_max     = (Amax + Bmax + Cmax)/3\n",
    "     - AB_mean     = (Amean + Bmean)/2\n",
    "     - C_mean      = Cmean\n",
    "     - ABC_mean    = (Amean + Bmean + Cmean)/3\n",
    "     - AB_maxdiff  = |Amax - Bmax|/2\n",
    "     - AB_meandiff = |Amean - Bmean|/2\n",
    "備考:\n",
    "  - ハイパスは既定オフ（HP_CUTOFF_HZ=None）\n",
    "  - 欠損はフィルタ前に線形補間（端は最近傍延長）、フィルタ後に欠損位置へNaNを戻す\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "# ===== ユーザー指定 =====\n",
    "BASE_DIR: str = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "# 対象ID（氏名なし）\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10041\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "# ===== フィルタ設定（温度は 1 Hz ローパス）=====\n",
    "LP_CUTOFF_HZ: float = 1.0                  # ローパス\n",
    "HP_CUTOFF_HZ: float | None = None          # 例: 0.01。既定は None（Face温度では通常HPしない）\n",
    "BUTTER_ORDER: int = 2\n",
    "UNIFORMITY_TOL: float = 0.02               # サンプリング間隔の変動係数が2%以内なら等間隔とみなす\n",
    "\n",
    "\n",
    "# ---------- ユーティリティ ----------\n",
    "def _normalize_colname(col: str) -> str:\n",
    "    s = col.lower()\n",
    "    s = re.sub(r\"[．。]\", \".\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def _select_face_columns(df: pd.DataFrame, box_hint: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Face系CSVから Max/Mean 列を自動検出。\n",
    "    優先: 'box a/b/c' を含み、かつ 'max' / 'ave|mean' を含む列。\n",
    "    \"\"\"\n",
    "    norm = {c: _normalize_colname(c) for c in df.columns}\n",
    "\n",
    "    def pick(words):\n",
    "        return [c for c in df.columns if all(w in norm[c] for w in words)]\n",
    "\n",
    "    # Max 列\n",
    "    max_c = pick([f\"box {box_hint}\", \"max\"]) or pick([\"max\"])\n",
    "    # Mean 列（'ave' 'mean' あたりを広めに拾う）\n",
    "    mean_c = (pick([f\"box {box_hint}\", \"ave\"]) or\n",
    "              pick([f\"box {box_hint}\", \"mean\"]) or\n",
    "              pick([\"ave\"]) or pick([\"mean\"]))\n",
    "    if not max_c or not mean_c:\n",
    "        raise ValueError(f\"required columns not found for box {box_hint}. columns={list(df.columns)}\")\n",
    "    return max_c[0], mean_c[0]\n",
    "\n",
    "def _estimate_fs(times: np.ndarray) -> tuple[float, bool]:\n",
    "    \"\"\"サンプリング周期の代表値から fs 推定と“ほぼ等間隔”かどうか。\"\"\"\n",
    "    dt = np.diff(times)\n",
    "    dt = dt[np.isfinite(dt)]\n",
    "    if dt.size == 0:\n",
    "        return 0.0, False\n",
    "    dt_med = float(np.median(dt))\n",
    "    if dt_med <= 0:\n",
    "        return 0.0, False\n",
    "    cv = float(np.std(dt) / dt_med)\n",
    "    fs = 1.0 / dt_med\n",
    "    return fs, (cv <= UNIFORMITY_TOL)\n",
    "\n",
    "def _make_filter(fs: float, lp: float | None, hp: float | None):\n",
    "    nyq = 0.5 * fs\n",
    "    if lp and hp:\n",
    "        wn = [max(1e-6, hp/nyq), min(0.999999, lp/nyq)]\n",
    "        b, a = butter(BUTTER_ORDER, wn, btype=\"bandpass\")\n",
    "    elif lp:\n",
    "        wn = min(0.999999, lp/nyq)\n",
    "        b, a = butter(BUTTER_ORDER, wn, btype=\"lowpass\")\n",
    "    elif hp:\n",
    "        wn = max(1e-6, hp/nyq)\n",
    "        b, a = butter(BUTTER_ORDER, wn, btype=\"highpass\")\n",
    "    else:\n",
    "        b, a = None, None\n",
    "    return b, a\n",
    "\n",
    "def _filter_series_with_resample(times: np.ndarray, values: np.ndarray,\n",
    "                                 lp: float | None, hp: float | None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    不均一サンプリングに頑健なフィルタ：\n",
    "      ・等間隔でなければ一旦等間隔に線形補間→filtfilt→元の時刻に逆補間\n",
    "      ・等間隔なら欠損を線形補間→filtfilt→欠損位置へNaNを復元\n",
    "    \"\"\"\n",
    "    fs, is_uniform = _estimate_fs(times)\n",
    "    vals = np.asarray(values, float)\n",
    "\n",
    "    if not np.isfinite(vals).any():\n",
    "        return vals\n",
    "\n",
    "    if not is_uniform:\n",
    "        t0, t1 = float(times[0]), float(times[-1])\n",
    "        dt = 1.0 / max(1.0, fs if fs > 0 else 10.0)  # fs不明時は 10 Hz 仮置き\n",
    "        tg = np.arange(t0, t1 + 1e-9, dt)\n",
    "        valid = np.isfinite(vals)\n",
    "        if valid.sum() < 2:\n",
    "            return vals\n",
    "        vg = np.interp(tg, times[valid], vals[valid])\n",
    "        b, a = _make_filter(1.0/dt, lp, hp)\n",
    "        vf = vg if b is None else filtfilt(b, a, vg, method=\"gust\")\n",
    "        return np.interp(times, tg, vf)\n",
    "    else:\n",
    "        b, a = _make_filter(fs, lp, hp)\n",
    "        if b is None:\n",
    "            return vals\n",
    "        valid = np.isfinite(vals)\n",
    "        if valid.sum() < 2:\n",
    "            return vals\n",
    "        vlin = np.interp(times, times[valid], vals[valid])\n",
    "        vf = filtfilt(b, a, vlin, method=\"gust\")\n",
    "        out = vf.copy()\n",
    "        out[~valid] = np.nan\n",
    "        return out\n",
    "\n",
    "def _prepare_face_df(df: pd.DataFrame, label: str, box_hint: str) -> pd.DataFrame:\n",
    "    \"\"\"FaceA/B/CのDFから Time_sec, Face{label}_Max/Mean を生成（重複時刻は平均）＋LP適用。\"\"\"\n",
    "    if \"Time_sec\" not in df.columns:\n",
    "        raise ValueError(\"Time_sec column missing\")\n",
    "\n",
    "    max_col, mean_col = _select_face_columns(df, box_hint=box_hint)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"Time_sec\"] = pd.to_numeric(df[\"Time_sec\"], errors=\"coerce\")\n",
    "    df[max_col]    = pd.to_numeric(df[max_col],    errors=\"coerce\")\n",
    "    df[mean_col]   = pd.to_numeric(df[mean_col],   errors=\"coerce\")\n",
    "\n",
    "    # 同一時刻は平均\n",
    "    tmp = df[[\"Time_sec\", max_col, mean_col]].groupby(\"Time_sec\", as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    # フィルタ（LP=1 Hz、HPは任意）\n",
    "    t  = tmp[\"Time_sec\"].to_numpy(float)\n",
    "    vx = tmp[max_col].to_numpy(float)\n",
    "    vm = tmp[mean_col].to_numpy(float)\n",
    "\n",
    "    vx_f = _filter_series_with_resample(t, vx, lp=LP_CUTOFF_HZ, hp=HP_CUTOFF_HZ)\n",
    "    vm_f = _filter_series_with_resample(t, vm, lp=LP_CUTOFF_HZ, hp=HP_CUTOFF_HZ)\n",
    "\n",
    "    tmp[max_col]  = vx_f\n",
    "    tmp[mean_col] = vm_f\n",
    "\n",
    "    # 列名整形\n",
    "    rename_map = {\n",
    "        max_col:  f\"Face{label}_Max\",\n",
    "        mean_col: f\"Face{label}_Mean\",\n",
    "    }\n",
    "    tmp = tmp.rename(columns=rename_map)\n",
    "    return tmp[[\"Time_sec\", f\"Face{label}_Max\", f\"Face{label}_Mean\"]]\n",
    "\n",
    "def merge_and_compute_face_features(dfA: pd.DataFrame, dfB: pd.DataFrame, dfC: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    FaceA/B/C を Time_sec で inner join し、指定9指標を算出。\n",
    "    出力列（この順）:\n",
    "      Time_sec,\n",
    "      AB_max, C_max, ABC_max,\n",
    "      AB_mean, C_mean, ABC_mean,\n",
    "      AB_maxdiff, AB_meandiff\n",
    "    \"\"\"\n",
    "    merged = pd.merge(dfA, dfB, on=\"Time_sec\", how=\"inner\", validate=\"one_to_one\")\n",
    "    merged = pd.merge(merged, dfC, on=\"Time_sec\", how=\"inner\", validate=\"one_to_one\")\n",
    "    if merged.empty:\n",
    "        raise ValueError(\"no overlapping Time_sec among FaceA/B/C\")\n",
    "\n",
    "    Amax = merged[\"FaceA_Max\"].to_numpy(float)\n",
    "    Bmax = merged[\"FaceB_Max\"].to_numpy(float)\n",
    "    Cmax = merged[\"FaceC_Max\"].to_numpy(float)\n",
    "\n",
    "    Amean = merged[\"FaceA_Mean\"].to_numpy(float)\n",
    "    Bmean = merged[\"FaceB_Mean\"].to_numpy(float)\n",
    "    Cmean = merged[\"FaceC_Mean\"].to_numpy(float)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"Time_sec\":   merged[\"Time_sec\"].to_numpy(float),\n",
    "\n",
    "        \"AB_max\":     (Amax + Bmax) / 2.0,\n",
    "        \"C_max\":      Cmax,\n",
    "        \"ABC_max\":    (Amax + Bmax + Cmax) / 3.0,\n",
    "\n",
    "        \"AB_mean\":    (Amean + Bmean) / 2.0,\n",
    "        \"C_mean\":     Cmean,\n",
    "        \"ABC_mean\":   (Amean + Bmean + Cmean) / 3.0,\n",
    "\n",
    "        \"AB_maxdiff\": 0.5 * np.abs(Amax - Bmax),\n",
    "        \"AB_meandiff\":0.5 * np.abs(Amean - Bmean),\n",
    "    })\n",
    "\n",
    "    # 列順固定\n",
    "    cols = [\n",
    "        \"Time_sec\",\n",
    "        \"AB_max\", \"C_max\", \"ABC_max\",\n",
    "        \"AB_mean\", \"C_mean\", \"ABC_mean\",\n",
    "        \"AB_maxdiff\", \"AB_meandiff\",\n",
    "    ]\n",
    "    return out[cols]\n",
    "\n",
    "def process_face_temp_raw_for_subject(base_root: str, sid: str) -> None:\n",
    "    \"\"\"1被験者分: FaceTemp_raw を作成（ご指定9指標）\"\"\"\n",
    "    subject_dir = os.path.join(base_root, f\"{sid}\")\n",
    "    offset_dir  = os.path.join(subject_dir, \"OFFSET\")\n",
    "    feature_dir = os.path.join(subject_dir, \"FEATURE\")\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    path_A = os.path.join(offset_dir, f\"{sid}_FaceA.csv\")\n",
    "    path_B = os.path.join(offset_dir, f\"{sid}_FaceB.csv\")\n",
    "    path_C = os.path.join(offset_dir, f\"{sid}_FaceC.csv\")\n",
    "    out_path = os.path.join(feature_dir, f\"{sid}_FaceTemp_raw.csv\")\n",
    "\n",
    "    print(f\"# Subject {sid}\")\n",
    "    for p in (path_A, path_B, path_C):\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[SKIP] FaceTemp_raw: missing -> {p}\")\n",
    "            return\n",
    "\n",
    "    try:\n",
    "        dfA = pd.read_csv(path_A, encoding=\"utf-8-sig\")\n",
    "        dfB = pd.read_csv(path_B, encoding=\"utf-8-sig\")\n",
    "        dfC = pd.read_csv(path_C, encoding=\"utf-8-sig\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] FaceTemp_raw: failed to read CSVs ({e})\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        dfA_prep = _prepare_face_df(dfA, label=\"A\", box_hint=\"a\")\n",
    "        dfB_prep = _prepare_face_df(dfB, label=\"B\", box_hint=\"b\")\n",
    "        dfC_prep = _prepare_face_df(dfC, label=\"C\", box_hint=\"c\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] FaceTemp_raw: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        out = merge_and_compute_face_features(dfA_prep, dfB_prep, dfC_prep)\n",
    "        out.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[OK]  FaceTemp_raw -> {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] FaceTemp_raw: {e}\")\n",
    "\n",
    "def main_3a_facetemp_raw():\n",
    "    \"\"\"処理(3a): 全被験者の FaceTemp_raw を作成（LP=1Hz, HP=任意）\"\"\"\n",
    "    for sid in SUBJECT_IDS:\n",
    "        process_face_temp_raw_for_subject(BASE_DIR, sid)\n",
    "\n",
    "# 実行\n",
    "if __name__ == \"__main__\":\n",
    "    main_3a_facetemp_raw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c558bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "(3a') FaceTemp (30s Epoch Aggregates) — 新FaceTemp_raw仕様に対応\n",
    "----------------------------------------------------------------\n",
    "入力 : FEATURE/{sid}_FaceTemp_raw.csv\n",
    "      （列: Time_sec, AB_max, C_max, ABC_max, AB_mean, C_mean, ABC_mean, AB_maxdiff, AB_meandiff）\n",
    "出力 : FEATURE/{sid}_FaceTemp.csv\n",
    "      （列: Epoch_start, Epoch_end, <各入力列>_mean/_std/_slope）\n",
    "仕様 :\n",
    "- 解析区間は 1800〜2400 秒\n",
    "- エポックは 30 秒幅、右端アンカーは 1800+30, 1800+60, …, 2400\n",
    "- 補間は行わない（該当エポックにサンプルが無ければ NaN）\n",
    "- slope はエポック内で y ≈ beta * t + alpha の beta（単位/秒）\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===== パスと対象ID =====\n",
    "BASE_DIR: str = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10041\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "# ===== エポック設定 =====\n",
    "EPOCH_WIN = 30\n",
    "START_SEC = 1800\n",
    "END_SEC   = 2400\n",
    "\n",
    "# ===== 入力で期待する FaceTemp_raw の列 =====\n",
    "EXPECTED_COLS = [\n",
    "    \"AB_max\", \"C_max\", \"ABC_max\",\n",
    "    \"AB_mean\", \"C_mean\", \"ABC_mean\",\n",
    "    \"AB_maxdiff\", \"AB_meandiff\",\n",
    "]\n",
    "\n",
    "def _epoch_stats_from_raw(df_raw: pd.DataFrame,\n",
    "                          start_sec: int,\n",
    "                          end_sec: int,\n",
    "                          win_sec: int) -> pd.DataFrame:\n",
    "    \"\"\"FaceTemp_raw（サンプル時刻粒度）から30秒エポックの mean/std/slope を算出。\"\"\"\n",
    "    if \"Time_sec\" not in df_raw.columns:\n",
    "        raise ValueError(\"required column missing: Time_sec\")\n",
    "\n",
    "    # 実在する列だけを対象にする（空ならエラー）\n",
    "    value_cols = [c for c in EXPECTED_COLS if c in df_raw.columns]\n",
    "    if not value_cols:\n",
    "        raise ValueError(f\"no expected FaceTemp columns found. need any of {EXPECTED_COLS}\")\n",
    "\n",
    "    df = df_raw.copy()\n",
    "    df[\"Time_sec\"] = pd.to_numeric(df[\"Time_sec\"], errors=\"coerce\")\n",
    "\n",
    "    for c in value_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # 区間抽出＆時刻昇順\n",
    "    df = df[(df[\"Time_sec\"] >= start_sec) & (df[\"Time_sec\"] <= end_sec)].sort_values(\"Time_sec\")\n",
    "    times = df[\"Time_sec\"].to_numpy(dtype=float)\n",
    "\n",
    "    anchors = np.arange(start_sec + win_sec, end_sec + 1, win_sec, dtype=float)\n",
    "    epoch_starts = anchors - win_sec\n",
    "    epoch_ends   = anchors.copy()\n",
    "\n",
    "    # 出力器\n",
    "    out = {\"Epoch_start\": epoch_starts.copy(), \"Epoch_end\": epoch_ends.copy()}\n",
    "    for col in value_cols:\n",
    "        out[f\"{col}_mean\"]  = np.full_like(anchors, np.nan, dtype=float)\n",
    "        out[f\"{col}_std\"]   = np.full_like(anchors, np.nan, dtype=float)\n",
    "        out[f\"{col}_slope\"] = np.full_like(anchors, np.nan, dtype=float)\n",
    "\n",
    "    # 走査（右端アンカーごとの trailing 30秒）\n",
    "    idx_left = 0\n",
    "    for i, t_end in enumerate(anchors):\n",
    "        t_start = t_end - win_sec\n",
    "        while idx_left < len(times) and times[idx_left] < t_start:\n",
    "            idx_left += 1\n",
    "        idx_right = idx_left\n",
    "        while idx_right < len(times) and times[idx_right] <= t_end:\n",
    "            idx_right += 1\n",
    "\n",
    "        if idx_right - idx_left <= 0:\n",
    "            continue\n",
    "\n",
    "        seg = df.iloc[idx_left:idx_right]\n",
    "        t = seg[\"Time_sec\"].to_numpy(dtype=float)\n",
    "\n",
    "        for col in value_cols:\n",
    "            y = seg[col].to_numpy(dtype=float)\n",
    "            valid = np.isfinite(y)\n",
    "            if not valid.any():\n",
    "                continue\n",
    "\n",
    "            v = y[valid]\n",
    "            out[f\"{col}_mean\"][i] = float(np.mean(v))\n",
    "            out[f\"{col}_std\"][i]  = float(np.std(v, ddof=1)) if v.size >= 2 else 0.0\n",
    "\n",
    "            # slope は有効点が2点以上のときのみ\n",
    "            msk = np.isfinite(t) & np.isfinite(y)\n",
    "            if msk.sum() >= 2:\n",
    "                beta, _ = np.polyfit(t[msk], y[msk], 1)  # y ≈ beta*t + alpha\n",
    "                out[f\"{col}_slope\"][i] = float(beta)\n",
    "\n",
    "    # 列順を固定\n",
    "    cols = [\"Epoch_start\", \"Epoch_end\"]\n",
    "    for col in value_cols:\n",
    "        cols += [f\"{col}_mean\", f\"{col}_std\", f\"{col}_slope\"]\n",
    "    return pd.DataFrame(out, columns=cols)\n",
    "\n",
    "def process_face_temp_epoch_for_subject(base_root: str, sid: str) -> None:\n",
    "    \"\"\"{sid}_FaceTemp_raw.csv → 30秒エポック集計 {sid}_FaceTemp.csv\"\"\"\n",
    "    subject_dir = os.path.join(base_root, f\"{sid}\")  # 氏名なしパス\n",
    "    feature_dir = os.path.join(subject_dir, \"FEATURE\")\n",
    "    in_path  = os.path.join(feature_dir, f\"{sid}_FaceTemp_raw.csv\")\n",
    "    out_path = os.path.join(feature_dir, f\"{sid}_FaceTemp.csv\")\n",
    "\n",
    "    print(f\"# Subject {sid} (FaceTemp epoch)\")\n",
    "    if not os.path.exists(in_path):\n",
    "        print(f\"[SKIP] FaceTemp: FaceTemp_raw not found -> {in_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_raw = pd.read_csv(in_path, encoding=\"utf-8-sig\")\n",
    "        out_df = _epoch_stats_from_raw(df_raw, START_SEC, END_SEC, EPOCH_WIN)\n",
    "        out_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[OK]  FaceTemp -> {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] FaceTemp: {e}\")\n",
    "\n",
    "def main_3a_facetemp_epoch(base_root: str, subject_ids: List[str]):\n",
    "    for sid in subject_ids:\n",
    "        process_face_temp_epoch_for_subject(base_root, sid)\n",
    "\n",
    "# 実行例:\n",
    "if __name__ == \"__main__\":\n",
    "    main_3a_facetemp_epoch(BASE_DIR, SUBJECT_IDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c5a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "(3b) RR extraction (RAW) & corrected (RAW2) & 30s-epoch RR export\n",
    "+ 30s Pulse window plots with beat markers (○=normal, ✕=anomaly)\n",
    "-----------------------------------------------------------------\n",
    "入力 : {BASE_DIR}\\{sid}\\OFFSET\\{sid}_Pulse.csv\n",
    "出力 : {BASE_DIR}\\{sid}\\FEATURE\\\n",
    "  - {sid}_RR_raw.csv     …… 検出そのまま（Time_sec, RR_interval_sec, HeartRate_BPM）\n",
    "  - {sid}_RR_raw.png     …… RR_raw の時系列（IQR±k逸脱は赤丸）\n",
    "  - {sid}_RR_raw2.csv    …… “中間ピーク削除”で修正後のビート系列（同3列）\n",
    "  - {sid}_RR.png         …… 修正後 RR の時系列プロット\n",
    "  - {sid}_RR.csv         …… 修正後（raw2）を用いた 30秒エポック平均RR\n",
    "                             （Epoch_start, Epoch_end, RR_interval）\n",
    "  - Pulse/{sid}_Pulse_{start}-{end}.png …… 30秒ごとの脈拍画像（filt後の波形に検出位置を重畳）\n",
    "       ・正常: 赤丸 ○ / 異常: 赤い ✕（IQR±k逸脱 or NaN RR）\n",
    "仕様:\n",
    "- 解析区間: 1800–2400秒（変更は START_TIME, END_TIME を編集）\n",
    "- 検出: bandpass(0.5–8Hz) → find_peaks(distance≥0.30s) → SPKI/NPKI適応閾値\n",
    "- RR妥当域: 0.15–2.0 s\n",
    "- 修正: 「短すぎるRRのみ」中間ピーク削除（長すぎるRRは図示のみで未補間）\n",
    "- 30秒エポックは右端アンカー（1800+30,…,2400）の trailing 30秒\n",
    "- 補間なし。窓内に有効RRが無ければ NaN\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "\n",
    "# ===== ユーザー環境 =====\n",
    "BASE_DIR: str = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10041\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "# ===== 時間・図の設定 =====\n",
    "START_TIME: int = 1800\n",
    "END_TIME:   int = 2400\n",
    "WIN_30:     int = 30\n",
    "\n",
    "DPI = 300\n",
    "FIGSIZE = (12, 4)\n",
    "LINEWIDTH = 1.5\n",
    "TITLE_FONTSIZE = 30\n",
    "LABEL_FONTSIZE = 24\n",
    "TICK_FONTSIZE  = 20\n",
    "\n",
    "# ===== フィルタ設定（PPG想定） =====\n",
    "LOWCUT_HZ, HIGHCUT_HZ = 0.5, 8.0\n",
    "FILTER_ORDER = 2\n",
    "\n",
    "# ===== 検出・RR判定 =====\n",
    "RR_MIN_SEC: float = 0.15\n",
    "RR_MAX_SEC: float = 2.0\n",
    "PEAK_MIN_DISTANCE_SEC: float = 0.30\n",
    "TH_K: float = 0.18       # 適応閾値係数（SPKI/NPKI から決定）\n",
    "\n",
    "# ===== IQR 閾値 =====\n",
    "IQR_K: float = 3.0\n",
    "\n",
    "\n",
    "# ---------- ヘルパ ----------\n",
    "def mmss_formatter(x: float, _pos=None) -> str:\n",
    "    m = int(x) // 60\n",
    "    s = int(x) % 60\n",
    "    return f\"{m}:{s:02d}\"\n",
    "\n",
    "def bandpass_filter(signal: np.ndarray, fs: int,\n",
    "                    lowcut: float = LOWCUT_HZ, highcut: float = HIGHCUT_HZ,\n",
    "                    order: int = FILTER_ORDER) -> np.ndarray:\n",
    "    nyq = 0.5 * fs\n",
    "    low = max(1e-6, lowcut / nyq)\n",
    "    high = min(0.999999, highcut / nyq)\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def detect_waveform_column(df: pd.DataFrame):\n",
    "    cols = [c for c in df.columns if c != \"Time_sec\"]\n",
    "    # よくある列名の自動検出\n",
    "    for key in [\"Pulse\", \"PPG\", \"pulse\", \"ppg\"]:\n",
    "        if key in cols:\n",
    "            return key\n",
    "    return cols[0] if cols else None\n",
    "\n",
    "def rolling_trailing_ranges(times: np.ndarray, anchors: np.ndarray, win_sec: int):\n",
    "    \"\"\"各アンカー t について [t-win_sec, t] に入る times のスライス (l, r) を返す（rは非包含）\"\"\"\n",
    "    n = len(times)\n",
    "    l = 0\n",
    "    for t in anchors:\n",
    "        start = t - win_sec\n",
    "        while l < n and times[l] < start:\n",
    "            l += 1\n",
    "        r = l\n",
    "        while r < n and times[r] <= t:\n",
    "            r += 1\n",
    "        yield l, r\n",
    "\n",
    "def iqr_bounds(x: pd.Series, k: float = IQR_K):\n",
    "    s = x.dropna()\n",
    "    if s.empty:\n",
    "        return -np.inf, np.inf\n",
    "    q1, q3 = s.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    return float(q1 - k*iqr), float(q3 + k*iqr)\n",
    "\n",
    "def plot_rr(time_sec: np.ndarray, rr_sec: np.ndarray, title: str, out_png: str,\n",
    "            highlight_mask: np.ndarray | None = None):\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.plot(time_sec, rr_sec, marker='o', linestyle='-', linewidth=LINEWIDTH, label='RR')\n",
    "    if highlight_mask is not None and np.any(highlight_mask):\n",
    "        ax.plot(time_sec[highlight_mask], rr_sec[highlight_mask], 'o',\n",
    "                color='red', markerfacecolor='none', markersize=8, markeredgewidth=2,\n",
    "                label='anomaly (IQR±k)')\n",
    "    ax.set_xlabel('Time (mm:ss)', fontsize=LABEL_FONTSIZE)\n",
    "    ax.set_ylabel('RR Interval (sec)', fontsize=LABEL_FONTSIZE)\n",
    "    ax.set_title(title, fontsize=TITLE_FONTSIZE)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(mmss_formatter))\n",
    "    ax.tick_params(labelsize=TICK_FONTSIZE)\n",
    "    ax.grid(True); ax.legend(fontsize=TICK_FONTSIZE)\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=DPI); plt.close(fig)\n",
    "\n",
    "def plot_pulse_window(times: np.ndarray, signal: np.ndarray,\n",
    "                      beat_times: np.ndarray,\n",
    "                      rr_raw: np.ndarray, outlier_mask: np.ndarray,\n",
    "                      t0: float, t1: float, out_png: str):\n",
    "    \"\"\"\n",
    "    30秒窓の脈波を描画。正常=赤丸○、異常=赤い✕。\n",
    "    異常条件: IQR±k逸脱 or 無効RR（NaN） ※先頭ビートのNaNは許容\n",
    "    \"\"\"\n",
    "    win_mask = (times >= t0) & (times <= t1)\n",
    "    if np.count_nonzero(win_mask) < 2:\n",
    "        return  # データ点が少なすぎる窓はスキップ\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE)\n",
    "    ax.plot(times[win_mask], signal[win_mask], linewidth=LINEWIDTH, label='Pulse (bandpass)')\n",
    "\n",
    "    # この窓に入る検出ビート\n",
    "    idx = np.where((beat_times >= t0) & (beat_times <= t1))[0]\n",
    "    for j in idx:\n",
    "        t = beat_times[j]\n",
    "        is_anom = bool(outlier_mask[j]) or (np.isnan(rr_raw[j]) and j != 0)\n",
    "        y = np.interp(t, times[win_mask], signal[win_mask])\n",
    "        if is_anom:\n",
    "            ax.plot([t], [y], marker='x', color='red', markersize=10, linestyle='None')\n",
    "        else:\n",
    "            ax.plot([t], [y], marker='o', markerfacecolor='none', markeredgecolor='red',\n",
    "                    markersize=8, linestyle='None')\n",
    "\n",
    "    ax.set_xlim(t0, t1)\n",
    "    ax.set_xlabel('Time (mm:ss)', fontsize=LABEL_FONTSIZE)\n",
    "    ax.set_ylabel('Pulse (a.u.)', fontsize=LABEL_FONTSIZE)\n",
    "    ax.set_title(f\"Pulse {int(t0)}–{int(t1)} sec\", fontsize=TITLE_FONTSIZE)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(mmss_formatter))\n",
    "    ax.tick_params(labelsize=TICK_FONTSIZE)\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout(); plt.savefig(out_png, dpi=DPI); plt.close(fig)\n",
    "\n",
    "\n",
    "# ---------- メイン処理 ----------\n",
    "def process_rr_extraction_for_subject(base_root: str, sid: str) -> None:\n",
    "    \"\"\"(3b) 1被験者分の RR 抽出（RAW/RAW2保存）＋30sエポックRR出力＋30s脈拍画像出力\"\"\"\n",
    "    subj_dir = os.path.join(base_root, f\"{sid}\")\n",
    "    in_csv  = os.path.join(subj_dir, 'OFFSET', f'{sid}_Pulse.csv')\n",
    "    out_dir = os.path.join(subj_dir, 'FEATURE')\n",
    "    pulse_plot_dir = os.path.join(out_dir, 'Pulse')  # 30秒ごとの脈拍画像保存先\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(pulse_plot_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"# Subject {sid}\")\n",
    "\n",
    "    if not os.path.exists(in_csv):\n",
    "        print(f\"[SKIP] {sid}: input not found -> {in_csv}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(in_csv)\n",
    "    if \"Time_sec\" not in df.columns:\n",
    "        print(f\"[SKIP] {sid}: Time_sec missing\")\n",
    "        return\n",
    "\n",
    "    # 対象範囲\n",
    "    df = df.sort_values('Time_sec').reset_index(drop=True)\n",
    "    df = df[(df['Time_sec'] >= START_TIME) & (df['Time_sec'] <= END_TIME)].reset_index(drop=True)\n",
    "    times = df['Time_sec'].to_numpy(float)\n",
    "\n",
    "    wave_col = detect_waveform_column(df)\n",
    "    if wave_col is None:\n",
    "        print(f\"[SKIP] {sid}: waveform column not found\")\n",
    "        return\n",
    "    sig = pd.to_numeric(df[wave_col], errors=\"coerce\").to_numpy(float)\n",
    "\n",
    "    if times.size < 2:\n",
    "        print(f\"[SKIP] {sid}: not enough samples\")\n",
    "        return\n",
    "\n",
    "    # fs推定\n",
    "    dt = float(np.mean(np.diff(times)))\n",
    "    if dt <= 0:\n",
    "        print(f\"[SKIP] {sid}: invalid Time_sec sequence\")\n",
    "        return\n",
    "    fs = int(round(1.0 / dt))\n",
    "\n",
    "    # バンドパス→ピーク検出\n",
    "    filt = bandpass_filter(sig, fs)\n",
    "    distance = max(1, int(PEAK_MIN_DISTANCE_SEC * fs))\n",
    "    peaks, _ = find_peaks(filt, distance=distance)\n",
    "    if peaks.size < 3:\n",
    "        print(f\"[SKIP] {sid}: insufficient peaks\")\n",
    "        return\n",
    "\n",
    "    # 適応しきい値（SPKI/NPKI 初期化）\n",
    "    SPKI = float(np.percentile(filt[peaks], 90))\n",
    "    NPKI = float(np.percentile(filt[peaks], 10))\n",
    "    thr = NPKI + TH_K * (SPKI - NPKI)\n",
    "\n",
    "    beat_t, beat_v = [], []\n",
    "    for idx in peaks:\n",
    "        val = float(filt[idx]); t = float(times[idx])\n",
    "        if val > thr:\n",
    "            if beat_t:\n",
    "                rr_tmp = t - beat_t[-1]\n",
    "                if not (RR_MIN_SEC <= rr_tmp <= RR_MAX_SEC):\n",
    "                    # 異常RRなら NPKI を更新し次へ\n",
    "                    NPKI = 0.125 * val + 0.875 * NPKI\n",
    "                    thr   = NPKI + TH_K * (SPKI - NPKI)\n",
    "                    continue\n",
    "            SPKI = 0.125 * val + 0.875 * SPKI\n",
    "            beat_t.append(t); beat_v.append(val)\n",
    "        else:\n",
    "            NPKI = 0.125 * val + 0.875 * NPKI\n",
    "        thr = NPKI + TH_K * (SPKI - NPKI)\n",
    "\n",
    "    if len(beat_t) < 3:\n",
    "        print(f\"[SKIP] {sid}: insufficient beats after thresholding\")\n",
    "        return\n",
    "\n",
    "    # ===== RR_raw.csv =====\n",
    "    rr = np.insert(np.diff(beat_t), 0, np.nan)\n",
    "    valid = np.isfinite(rr) & (rr >= RR_MIN_SEC) & (rr <= RR_MAX_SEC)\n",
    "    rr_raw = rr.copy()\n",
    "    rr_raw[~valid] = np.nan\n",
    "\n",
    "    df_rr_raw = pd.DataFrame({\n",
    "        \"Time_sec\": beat_t,\n",
    "        \"RR_interval_sec\": rr_raw,\n",
    "        \"HeartRate_BPM\": [np.nan if not np.isfinite(x) else 60.0/x for x in rr_raw],\n",
    "    })\n",
    "    raw_csv = os.path.join(out_dir, f\"{sid}_RR_raw.csv\")\n",
    "    df_rr_raw.to_csv(raw_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK]  RR_raw -> {raw_csv}\")\n",
    "\n",
    "    # 異常判定: IQR±k\n",
    "    low_iqr, up_iqr = iqr_bounds(df_rr_raw['RR_interval_sec'])\n",
    "    out_mask = df_rr_raw['RR_interval_sec'].lt(low_iqr) | df_rr_raw['RR_interval_sec'].gt(up_iqr)\n",
    "\n",
    "    # 図：RR_raw.png（IQR±k 逸脱を赤丸）\n",
    "    raw_png = os.path.join(out_dir, f\"{sid}_RR_raw.png\")\n",
    "    plot_rr(df_rr_raw[\"Time_sec\"].to_numpy(float),\n",
    "            df_rr_raw[\"RR_interval_sec\"].to_numpy(float),\n",
    "            title=f\"{sid} RR Interval (RAW)\",\n",
    "            out_png=raw_png,\n",
    "            highlight_mask=out_mask.to_numpy())\n",
    "    print(f\"[IQR] k={IQR_K}, lower={low_iqr:.3f}, upper={up_iqr:.3f}\")\n",
    "\n",
    "    # ===== RR_raw2.csv（修正: 中間ピーク削除：短すぎるRRのみ）=====\n",
    "    def correct_rr_by_middle_peak_deletion(qrs_times, lower, upper):\n",
    "        qrs = list(map(float, qrs_times))\n",
    "        i = 1\n",
    "        while i < len(qrs):\n",
    "            rr = qrs[i] - qrs[i-1]\n",
    "            if np.isfinite(rr) and rr < lower and i+1 < len(qrs):\n",
    "                t_prev, t_mid, t_next = qrs[i-1], qrs[i], qrs[i+1]\n",
    "                new_rr = t_next - t_prev\n",
    "                if lower <= new_rr <= upper:\n",
    "                    del qrs[i]\n",
    "                    continue  # 同インデックスで再評価\n",
    "            i += 1\n",
    "        return qrs\n",
    "\n",
    "    beat_t_corr = correct_rr_by_middle_peak_deletion(beat_t, low_iqr, up_iqr)\n",
    "    rr_corr = np.insert(np.diff(beat_t_corr), 0, np.nan)\n",
    "    df_rr_corr = pd.DataFrame({\n",
    "        \"Time_sec\": beat_t_corr,\n",
    "        \"RR_interval_sec\": rr_corr,\n",
    "        \"HeartRate_BPM\": [np.nan if not np.isfinite(x) else 60.0/x for x in rr_corr],\n",
    "    })\n",
    "    raw2_csv = os.path.join(out_dir, f\"{sid}_RR_raw2.csv\")\n",
    "    df_rr_corr.to_csv(raw2_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK]  RR_raw2 -> {raw2_csv}\")\n",
    "\n",
    "    # 図：修正後 RR.png\n",
    "    corr_png = os.path.join(out_dir, f\"{sid}_RR.png\")\n",
    "    plot_rr(np.asarray(beat_t_corr, float),\n",
    "            rr_corr,\n",
    "            title=f\"{sid} RR Interval (corrected by deletion)\",\n",
    "            out_png=corr_png)\n",
    "\n",
    "    # ===== 30秒ごとの脈拍画像（Pulse/{sid}_Pulse_{start}-{end}.png）=====\n",
    "    anchors_30 = np.arange(START_TIME + WIN_30, END_TIME + 1, WIN_30, dtype=float)\n",
    "    ep_st = anchors_30 - WIN_30\n",
    "    ep_ed = anchors_30.copy()\n",
    "\n",
    "    # 脈拍プロットは RAW の検出結果（beat_t, rr_raw, out_mask）で描画\n",
    "    beat_times_raw = np.asarray(beat_t, float)\n",
    "    rr_raw_arr     = df_rr_raw[\"RR_interval_sec\"].to_numpy(float)\n",
    "    outlier_arr    = out_mask.to_numpy()\n",
    "\n",
    "    for s, e in zip(ep_st, ep_ed):\n",
    "        out_png = os.path.join(pulse_plot_dir, f\"{sid}_Pulse_{int(s)}-{int(e)}.png\")\n",
    "        plot_pulse_window(times, filt, beat_times_raw, rr_raw_arr, outlier_arr, float(s), float(e), out_png)\n",
    "\n",
    "    # ===== RR.csv（30秒エポック平均RR：raw2ベース）=====\n",
    "    times_beats = df_rr_corr[\"Time_sec\"].to_numpy(float)\n",
    "    rr_beats    = df_rr_corr[\"RR_interval_sec\"].to_numpy(float)\n",
    "    rr_epoch = np.full_like(anchors_30, np.nan, dtype=float)\n",
    "    for i, (l, r) in enumerate(rolling_trailing_ranges(times_beats, anchors_30, WIN_30)):\n",
    "        if r - l <= 0:\n",
    "            continue\n",
    "        w = rr_beats[l:r]\n",
    "        m = np.isfinite(w) & (w >= RR_MIN_SEC) & (w <= RR_MAX_SEC)\n",
    "        if np.any(m):\n",
    "            rr_epoch[i] = float(np.mean(w[m]))\n",
    "\n",
    "    df_epoch = pd.DataFrame({\n",
    "        \"Epoch_start\": ep_st,\n",
    "        \"Epoch_end\":   ep_ed,\n",
    "        \"RR_interval\": rr_epoch,\n",
    "    })\n",
    "    rr_csv = os.path.join(out_dir, f\"{sid}_RR.csv\")\n",
    "    df_epoch.to_csv(rr_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK]  RR (30s epoch from RAW2) -> {rr_csv}\")\n",
    "\n",
    "\n",
    "def main_3b_rr_extraction(base_root: str, subject_ids: List[str]):\n",
    "    for sid in subject_ids:\n",
    "        process_rr_extraction_for_subject(base_root, sid)\n",
    "\n",
    "# 実行:\n",
    "if __name__ == \"__main__\":\n",
    "    main_3b_rr_extraction(BASE_DIR, SUBJECT_IDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17573f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "(3c) RR-derived features from RRtime（補間なし, 30s/120sエポック出力）\n",
    "--------------------------------------------------------------------\n",
    "入力 : FEATURE/{sid}_RRtime.csv   （列: Time_sec ← R波検出時刻 [sec]）\n",
    "出力 : FEATURE/{sid}_<FeatureName>.csv\n",
    "       （各CSVは 3列: Epoch_start, Epoch_end, <Feature>）\n",
    "\n",
    "要件:\n",
    "- RRは Time_sec の差分から算出（= 連続R間隔）\n",
    "- 有効RRは生理境界 0.25〜2.0 s のみ\n",
    "- 30秒窓（1770〜2400）: HeartRate, RMSSD, SDSD, SD1, SD2, CSI, CVI, pNN50, RR_interval\n",
    "- 120秒窓（1800〜2400）: LF_power, HF_power, LF_HF_ratio（Lomb-Scargleで不等間隔のまま解析）\n",
    "- アンカー:\n",
    "  * 30s:  START_30+30, …, 2400  （START_30=1770）\n",
    "  * 120s: START_120+120, …, 2400（START_120=1800）\n",
    "- 各CSVの列名は Epoch_start, Epoch_end, <Feature> とする\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import lombscargle\n",
    "\n",
    "# ===== ユーザー環境（別セルにある場合はそちらを使用） =====\n",
    "try:\n",
    "    BASE_DIR\n",
    "except NameError:\n",
    "    BASE_DIR: str = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "try:\n",
    "    SUBJECT_IDS\n",
    "except NameError:\n",
    "    SUBJECT_IDS: List[str] = [\n",
    "        \"10041\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "        \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "        \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "        \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "        \"10101\",\"10102\",\"10103\",\n",
    "    ]\n",
    "\n",
    "# ===== 時間設定 =====\n",
    "START_30: int  = 1770   # 30秒窓の開始（右端アンカーは 1800, 1830, …, 2400）\n",
    "START_120: int = 1800   # 120秒窓の開始（右端アンカーは 1920, 2040, …, 2400）\n",
    "END_TIME: int   = 2400\n",
    "\n",
    "WIN_30:  int = 30\n",
    "WIN_120: int = 120\n",
    "\n",
    "# ===== RR 生理境界 =====\n",
    "RR_MIN: float = 0.25\n",
    "RR_MAX: float = 2.0\n",
    "\n",
    "# ===== 周波数領域（Hz） =====\n",
    "LF_BAND = (0.04, 0.15)\n",
    "HF_BAND = (0.15, 0.40)\n",
    "F_MAX   = 0.5\n",
    "N_FREQ  = 512\n",
    "\n",
    "\n",
    "# ---------- ヘルパ ----------\n",
    "def rolling_trailing_ranges(times: np.ndarray, anchors: np.ndarray, win_sec: int):\n",
    "    \"\"\"各アンカー t に対し [t-win_sec, t] に含まれる times の slice (l, r)（rは非包含）を返す\"\"\"\n",
    "    n = len(times)\n",
    "    l = 0\n",
    "    for t in anchors:\n",
    "        start = t - win_sec\n",
    "        while l < n and times[l] < start:\n",
    "            l += 1\n",
    "        r = l\n",
    "        while r < n and times[r] <= t:\n",
    "            r += 1\n",
    "        yield l, r\n",
    "\n",
    "def valid_rr_mask(rr: np.ndarray) -> np.ndarray:\n",
    "    return np.isfinite(rr) & (rr >= RR_MIN) & (rr <= RR_MAX)\n",
    "\n",
    "def successive_diffs(rr: np.ndarray) -> np.ndarray:\n",
    "    if rr.size < 2:\n",
    "        return np.array([], dtype=float)\n",
    "    return np.diff(rr)\n",
    "\n",
    "def pnn50(drr: np.ndarray) -> float:\n",
    "    if drr.size == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs(drr) > 0.05))\n",
    "\n",
    "def sd1_sd2(rr: np.ndarray):\n",
    "    \"\"\"Poincaré解析に基づく SD1/SD2（標本分散, ddof=1）。不定形は NaN を返す。\"\"\"\n",
    "    if rr.size < 2:\n",
    "        return np.nan, np.nan\n",
    "    drr = successive_diffs(rr)\n",
    "    if drr.size < 2:\n",
    "        return np.nan, np.nan\n",
    "    sdrr   = np.std(rr, ddof=1)\n",
    "    sddiff = np.std(drr, ddof=1)\n",
    "    if not (np.isfinite(sdrr) and np.isfinite(sddiff)):\n",
    "        return np.nan, np.nan\n",
    "    sd1 = np.sqrt(0.5) * sddiff\n",
    "    val = 2.0 * (sdrr ** 2) - 0.5 * (sddiff ** 2)\n",
    "    sd2 = np.sqrt(val) if val > 0 else np.nan\n",
    "    return sd1, sd2\n",
    "\n",
    "def lomb_band_power(t_sec: np.ndarray, rr_sec: np.ndarray, band):\n",
    "    \"\"\"不等間隔 RR(t) を Lomb-Scargle で解析し、帯域パワーを積分（相対量）\"\"\"\n",
    "    if t_sec.size < 4 or rr_sec.size < 4:\n",
    "        return np.nan\n",
    "    tt = t_sec - t_sec[0]\n",
    "    x  = rr_sec - np.nanmean(rr_sec)\n",
    "    f = np.linspace(0.0001, F_MAX, N_FREQ)\n",
    "    w = 2.0 * np.pi * f\n",
    "    try:\n",
    "        p = lombscargle(tt, x, w, precenter=False, normalize=True)\n",
    "    except TypeError:\n",
    "        p = lombscargle(tt, x, w, precenter=False)\n",
    "        var = np.nanvar(x)\n",
    "        if var > 0:\n",
    "            p = p / var\n",
    "    m = (f >= band[0]) & (f <= band[1])\n",
    "    if not np.any(m):\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(np.trapezoid(p[m], f[m]))\n",
    "    except AttributeError:\n",
    "        return float(np.trapz(p[m], f[m]))\n",
    "\n",
    "def save_epoch_csv(feature_dir: str, sid: str, name: str,\n",
    "                   ep_start: np.ndarray, ep_end: np.ndarray, values: np.ndarray) -> str:\n",
    "    path = os.path.join(feature_dir, f\"{sid}_{name}.csv\")\n",
    "    pd.DataFrame({\"Epoch_start\": ep_start, \"Epoch_end\": ep_end, name: values}) \\\n",
    "      .to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    return path\n",
    "\n",
    "\n",
    "# ---------- メイン ----------\n",
    "def compute_rr_features_from_rrtime_for_subject(base_root: str, sid: str) -> None:\n",
    "    \"\"\"\n",
    "    入力: FEATURE/{sid}_RRtime.csv（列: Time_sec=R波時刻[sec]）\n",
    "    出力: 30s系8+1種, 120s系3種 を別CSVに保存\n",
    "    \"\"\"\n",
    "    subject_dir = os.path.join(base_root, f\"{sid}\")\n",
    "    feature_dir = os.path.join(subject_dir, \"FEATURE\")\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    rrtime_csv = os.path.join(feature_dir, f\"{sid}_RRtime.csv\")\n",
    "    print(f\"# Subject {sid} (RR-features from RRtime)\")\n",
    "\n",
    "    if not os.path.exists(rrtime_csv):\n",
    "        print(f\"[SKIP] RR-features: RRtime CSV not found -> {rrtime_csv}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(rrtime_csv, encoding=\"utf-8-sig\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] RR-features: failed to read RRtime ({e})\")\n",
    "        return\n",
    "\n",
    "    if \"Time_sec\" not in df.columns:\n",
    "        print(\"[SKIP] RR-features: Time_sec column missing\")\n",
    "        return\n",
    "\n",
    "    # R波時刻の整形\n",
    "    t_beats = pd.to_numeric(df[\"Time_sec\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "    t_beats = t_beats[np.isfinite(t_beats)]\n",
    "    if t_beats.size < 3:\n",
    "        print(\"[SKIP] RR-features: too few R-peaks\")\n",
    "        return\n",
    "    t_beats = np.unique(np.sort(t_beats))  # 単調化 & 重複除去\n",
    "\n",
    "    # RR列（先頭はNaN）\n",
    "    rr_all = np.insert(np.diff(t_beats), 0, np.nan)\n",
    "\n",
    "    # ------ アンカー定義 ------\n",
    "    anchors_30  = np.arange(START_30  + WIN_30,  END_TIME + 1, WIN_30,  dtype=float)\n",
    "    anchors_120 = np.arange(START_120 + WIN_120, END_TIME + 1, WIN_120, dtype=float)\n",
    "\n",
    "    ep30_st,  ep30_ed  = anchors_30 - WIN_30,  anchors_30.copy()\n",
    "    ep120_st, ep120_ed = anchors_120 - WIN_120, anchors_120.copy()\n",
    "\n",
    "    # ===== 30秒窓 =====\n",
    "    HR    = np.full_like(anchors_30, np.nan, dtype=float)\n",
    "    RMSSD = np.full_like(anchors_30, np.nan, dtype=float)\n",
    "    SDSD  = np.full_like(anchors_30, np.nan, dtype=float)\n",
    "    SD1   = np.full_like(anchors_30, np.nan, dtype=float)\n",
    "    SD2   = np.full_like(anchors_30, np.nan, dtype=float)\n",
    "    CSI   = np.full_like(anchors_30, np.nan, dtype=float)\n",
    "    CVI   = np.full_like(anchors_30, np.nan, dtype=float)\n",
    "    PNN50 = np.full_like(anchors_30, np.nan, dtype=float)\n",
    "    RRmn  = np.full_like(anchors_30, np.nan, dtype=float)  # 30sのRR平均\n",
    "\n",
    "    for i, (l, r) in enumerate(rolling_trailing_ranges(t_beats, anchors_30, WIN_30)):\n",
    "        if r - l <= 1:\n",
    "            continue\n",
    "        rr_w = rr_all[l:r]\n",
    "        rr_w = rr_w[valid_rr_mask(rr_w)]\n",
    "        if rr_w.size < 2:\n",
    "            continue\n",
    "\n",
    "        drr = successive_diffs(rr_w)\n",
    "        RRmn[i]  = float(np.mean(rr_w)) if rr_w.size > 0 else np.nan\n",
    "        HR[i]    = 60.0 / RRmn[i] if np.isfinite(RRmn[i]) and RRmn[i] > 0 else np.nan\n",
    "        RMSSD[i] = float(np.sqrt(np.mean(drr ** 2))) if drr.size > 0 else np.nan\n",
    "        SDSD[i]  = float(np.std(drr, ddof=1)) if drr.size >= 2 else np.nan\n",
    "        sd1, sd2 = sd1_sd2(rr_w)\n",
    "        SD1[i], SD2[i] = sd1, sd2\n",
    "        if np.isfinite(sd1) and sd1 > 0 and np.isfinite(sd2) and sd2 > 0:\n",
    "            CSI[i] = sd2 / sd1\n",
    "            CVI[i] = np.log10(sd1 * sd2)\n",
    "        PNN50[i] = pnn50(drr)\n",
    "\n",
    "    # 保存（30秒窓）\n",
    "    try:\n",
    "        for name, arr in [\n",
    "            (\"HeartRate\", HR),\n",
    "            (\"RMSSD\", RMSSD),\n",
    "            (\"SDSD\", SDSD),\n",
    "            (\"SD1\", SD1),\n",
    "            (\"SD2\", SD2),\n",
    "            (\"CSI\", CSI),\n",
    "            (\"CVI\", CVI),\n",
    "            (\"pNN50\", PNN50),\n",
    "            (\"RR_interval\", RRmn),  # ★ RRはmeanのみ\n",
    "        ]:\n",
    "            p = save_epoch_csv(feature_dir, sid, name, ep30_st, ep30_ed, arr)\n",
    "            print(f\"[OK]  30s -> {p}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] RR-features: save failed (30s) ({e})\")\n",
    "\n",
    "    # ===== 120秒窓（周波数領域） =====\n",
    "    LFp = np.full_like(anchors_120, np.nan, dtype=float)\n",
    "    HFp = np.full_like(anchors_120, np.nan, dtype=float)\n",
    "    LFr = np.full_like(anchors_120, np.nan, dtype=float)\n",
    "\n",
    "    for i, (l, r) in enumerate(rolling_trailing_ranges(t_beats, anchors_120, WIN_120)):\n",
    "        if r - l <= 3:\n",
    "            continue\n",
    "        # 120s窓に入るRRとその対応時刻（先頭NaNは除外）\n",
    "        rr_w = rr_all[l:r]\n",
    "        t_w  = t_beats[l:r]\n",
    "        # 先頭NaN除去のため、同じ範囲で再計算\n",
    "        rr_w = np.diff(t_w)\n",
    "        if rr_w.size < 4:\n",
    "            continue\n",
    "        m = valid_rr_mask(rr_w)\n",
    "        rr_w = rr_w[m]; t_w = t_w[1:][m]\n",
    "        if rr_w.size < 4:\n",
    "            continue\n",
    "\n",
    "        lf = lomb_band_power(t_w, rr_w, LF_BAND)\n",
    "        hf = lomb_band_power(t_w, rr_w, HF_BAND)\n",
    "        LFp[i] = lf\n",
    "        HFp[i] = hf\n",
    "        if np.isfinite(lf) and np.isfinite(hf) and hf > 0:\n",
    "            LFr[i] = lf / hf\n",
    "\n",
    "    try:\n",
    "        for name, arr in [\n",
    "            (\"LF_power\", LFp),\n",
    "            (\"HF_power\", HFp),\n",
    "            (\"LF_HF_ratio\", LFr),\n",
    "        ]:\n",
    "            p = save_epoch_csv(feature_dir, sid, name, ep120_st, ep120_ed, arr)\n",
    "            print(f\"[OK]  120s -> {p}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] RR-features: save failed (120s) ({e})\")\n",
    "\n",
    "\n",
    "def main_3c_rr_features_from_rrtime(base_root: str, subject_ids: List[str]):\n",
    "    for sid in subject_ids:\n",
    "        compute_rr_features_from_rrtime_for_subject(base_root, sid)\n",
    "\n",
    "\n",
    "# 実行例:\n",
    "if __name__ == \"__main__\":\n",
    "    main_3c_rr_features_from_rrtime(BASE_DIR, SUBJECT_IDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495b7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "(3d) Feature Engineering - Skinos（30s epoch, mean/std/slope with watch_ prefix）\n",
    "-------------------------------------------------------------------------------\n",
    "入力:\n",
    "  {BASE_DIR}\\{sid}\\OFFSET\\{sid}_Skinos.csv\n",
    "    期待列: Time_sec（= 時刻[sec]）, その他は数値列（例: Sweat_Rate, Heart_Rate, Skin_Temp）\n",
    "\n",
    "出力:\n",
    "  {BASE_DIR}\\{sid}\\FEATURE\\{sid}_Skinos.csv\n",
    "    列: Epoch_start, Epoch_end, watch_<metric>_mean, watch_<metric>_std, watch_<metric>_slope\n",
    "\n",
    "仕様:\n",
    "  - 30秒エポックは「後ろ詰め」の移動窓：[t-30, t]。アンカーは 1800, 1830, …, 2400\n",
    "  - 特徴量の計算区間は 1770〜2400（= 最初のエポック [1770,1800] を含めるため）\n",
    "  - 補間なし。エポックにサンプルが無い場合は mean/slope=NaN、stdは n>=2 のときのみ算出（未満は 0.0）\n",
    "  - OFFSET にファイルが無ければ SKIP\n",
    "  - Time_sec 列が無い場合は、列名に 'time' を含む列を自動検出して Time_sec として扱う\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===== ユーザー環境（別セルにある場合はそちらを使用） =====\n",
    "try:\n",
    "    BASE_DIR\n",
    "except NameError:\n",
    "    BASE_DIR: str = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "try:\n",
    "    SUBJECT_IDS\n",
    "except NameError:\n",
    "    SUBJECT_IDS: List[str] = [\n",
    "        \"10041\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "        \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "        \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "        \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "        \"10101\",\"10102\",\"10103\",\n",
    "    ]\n",
    "\n",
    "# ===== 時間設定 =====\n",
    "START_30: int = 1770   # 計算範囲の下限（最初のエポック [1770,1800] のため）\n",
    "GRID_START: int = 1800  # アンカー開始\n",
    "GRID_END:   int = 2400\n",
    "WIN_30:     int = 30\n",
    "\n",
    "# ---------- ヘルパ ----------\n",
    "def _rolling_trailing_ranges(times: np.ndarray, anchors: np.ndarray, win_sec: int):\n",
    "    \"\"\"各アンカー t に対し [t-win_sec, t] に含まれる times の slice (l, r)（rは非包含）を返す\"\"\"\n",
    "    n = len(times)\n",
    "    l = 0\n",
    "    for t in anchors:\n",
    "        start = t - win_sec\n",
    "        while l < n and times[l] < start:\n",
    "            l += 1\n",
    "        r = l\n",
    "        while r < n and times[r] <= t:\n",
    "            r += 1\n",
    "        yield l, r\n",
    "\n",
    "def _read_csv_any(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        # 日本語Windows想定のフォールバック\n",
    "        return pd.read_csv(path, encoding=\"cp932\")\n",
    "\n",
    "def _ensure_time_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Time_sec が無ければ 'time' を含む列を探して Time_sec にリネーム\"\"\"\n",
    "    if \"Time_sec\" in df.columns:\n",
    "        return df\n",
    "    cand = [c for c in df.columns if \"time\" in str(c).lower()]\n",
    "    if not cand:\n",
    "        raise ValueError(\"Time_sec column missing (no column contains 'time').\")\n",
    "    df = df.rename(columns={cand[0]: \"Time_sec\"}).copy()\n",
    "    return df\n",
    "\n",
    "def _pick_metric_columns(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Time_sec 以外の数値列を候補として採用（少なくとも1つ有効値がある列）\"\"\"\n",
    "    metrics = []\n",
    "    for c in df.columns:\n",
    "        if c == \"Time_sec\":\n",
    "            continue\n",
    "        v = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if np.isfinite(v).any():\n",
    "            metrics.append(c)\n",
    "    if not metrics:\n",
    "        raise ValueError(\"no numeric metric columns found.\")\n",
    "    return metrics\n",
    "\n",
    "def _epoch_aggregate_watch(df: pd.DataFrame,\n",
    "                           anchors: np.ndarray,\n",
    "                           win_sec: int,\n",
    "                           metric_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"30秒エポックで mean/std/slope を計算。列名は watch_<metric>_{mean|std|slope}。\"\"\"\n",
    "    epoch_starts = anchors - win_sec\n",
    "    out = {\n",
    "        \"Epoch_start\": epoch_starts.copy().astype(int),\n",
    "        \"Epoch_end\":   anchors.copy().astype(int),\n",
    "    }\n",
    "    # 先に全列を初期化\n",
    "    for col in metric_cols:\n",
    "        base = f\"watch_{col}\"\n",
    "        out[f\"{base}_mean\"]  = np.full_like(anchors, np.nan, dtype=float)\n",
    "        out[f\"{base}_std\"]   = np.full_like(anchors, np.nan, dtype=float)  # 標準偏差\n",
    "        out[f\"{base}_slope\"] = np.full_like(anchors, np.nan, dtype=float)  # 単位/秒\n",
    "\n",
    "    times = df[\"Time_sec\"].to_numpy(dtype=float)\n",
    "\n",
    "    for i, (l, r) in enumerate(_rolling_trailing_ranges(times, anchors, win_sec)):\n",
    "        if r - l <= 0:\n",
    "            continue\n",
    "        seg = df.iloc[l:r]\n",
    "        t = pd.to_numeric(seg[\"Time_sec\"], errors=\"coerce\").to_numpy(dtype=float)\n",
    "\n",
    "        for col in metric_cols:\n",
    "            y = pd.to_numeric(seg[col], errors=\"coerce\").to_numpy(dtype=float)\n",
    "            valid = np.isfinite(y)\n",
    "            if not valid.any():\n",
    "                continue\n",
    "\n",
    "            v = y[valid]\n",
    "            out[f\"watch_{col}_mean\"][i] = float(np.mean(v))\n",
    "            out[f\"watch_{col}_std\"][i]  = float(np.std(v, ddof=1)) if v.size >= 2 else 0.0\n",
    "\n",
    "            # slope は有効点が2点以上のときのみ\n",
    "            msk = np.isfinite(t) & np.isfinite(y)\n",
    "            if msk.sum() >= 2:\n",
    "                beta, _alpha = np.polyfit(t[msk], y[msk], 1)  # y ≈ beta*t + alpha\n",
    "                out[f\"watch_{col}_slope\"][i] = float(beta)\n",
    "\n",
    "    # 列順を固定\n",
    "    cols_order = [\"Epoch_start\", \"Epoch_end\"]\n",
    "    for col in metric_cols:\n",
    "        base = f\"watch_{col}\"\n",
    "        cols_order += [f\"{base}_mean\", f\"{base}_std\", f\"{base}_slope\"]\n",
    "\n",
    "    return pd.DataFrame(out, columns=cols_order)\n",
    "\n",
    "# ---------- メイン ----------\n",
    "def process_skinos_features_for_subject(base_root: str, sid: str) -> None:\n",
    "    \"\"\"OFFSET/{sid}_Skinos.csv → FEATURE/{sid}_Skinos.csv（30秒エポック mean/std/slope）\"\"\"\n",
    "    subject_dir = os.path.join(base_root, f\"{sid}\")\n",
    "    offset_path = os.path.join(subject_dir, \"OFFSET\", f\"{sid}_Skinos.csv\")\n",
    "    feature_dir = os.path.join(subject_dir, \"FEATURE\")\n",
    "    out_path = os.path.join(feature_dir, f\"{sid}_Skinos.csv\")\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"# Subject {sid}\")\n",
    "\n",
    "    if not os.path.exists(offset_path):\n",
    "        print(f\"[SKIP] Skinos: input not found -> {offset_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df0 = _read_csv_any(offset_path)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] Skinos: failed to read CSV ({e})\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df0 = _ensure_time_column(df0)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] Skinos: {e}\")\n",
    "        return\n",
    "\n",
    "    # 数値化 & 範囲フィルタ & 重複平均化\n",
    "    df0 = df0.copy()\n",
    "    df0[\"Time_sec\"] = pd.to_numeric(df0[\"Time_sec\"], errors=\"coerce\")\n",
    "    df0 = df0.loc[(df0[\"Time_sec\"] >= START_30) & (df0[\"Time_sec\"] <= GRID_END)]\n",
    "    if df0.empty:\n",
    "        print(\"[SKIP] Skinos: empty time window\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        metric_cols = _pick_metric_columns(df0)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] Skinos: {e}\")\n",
    "        return\n",
    "\n",
    "    # 同一 Time_sec の重複は平均化\n",
    "    df = df0[[\"Time_sec\"] + metric_cols].groupby(\"Time_sec\", as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    # アンカー（30s）: 1800, 1830, …, 2400\n",
    "    anchors_30 = np.arange(GRID_START, GRID_END + 1, WIN_30, dtype=float)\n",
    "\n",
    "    # 集計\n",
    "    try:\n",
    "        out_df = _epoch_aggregate_watch(df, anchors=anchors_30, win_sec=WIN_30, metric_cols=metric_cols)\n",
    "        out_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[OK]  Skinos -> {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] Skinos: failed to save ({e})\")\n",
    "\n",
    "def main_3d_skinos(base_root: str = BASE_DIR, subject_ids: List[str] = SUBJECT_IDS):\n",
    "    \"\"\"(3d) Skinos: 全被験者の 30秒エポック特徴を出力\"\"\"\n",
    "    for sid in subject_ids:\n",
    "        process_skinos_features_for_subject(base_root, sid)\n",
    "\n",
    "# 実行例:\n",
    "if __name__ == \"__main__\":\n",
    "    main_3d_skinos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac43fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "(3e) Sweat (30s Epoch Aggregates) — 平均/標準偏差/勾配\n",
    "----------------------------------------------------------------\n",
    "入力  : OFFSET/{sid}_Sweat.csv  （列: Time_sec, <sweat-like column>）\n",
    "出力  : FEATURE/{sid}_Sweat.csv （列: Epoch_start, Epoch_end,\n",
    "                                 Sweat_mean, Sweat_std, Sweat_slope）\n",
    "仕様  :\n",
    "- 解析区間は 1800〜2400 秒（右端アンカー: 1800+30, 1800+60, …, 2400）\n",
    "- 補間なし（エポック内にサンプルが無ければ NaN、std は n>=2 で算出、未満は 0.0）\n",
    "- 勾配 slope はエポック内で y ≈ beta * t + alpha の beta（単位/秒）\n",
    "- 入力の sweat 列は自動検出（優先: 'Sweat', 'GSR', 'EDA' を含む列名）\n",
    "- OFFSET/{sid}_Sweat.csv が無ければ例外（エラー）を投げる\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ルート\n",
    "BASE_DIR: str = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "# 対象ID（氏名なし）\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10041\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "# エポック設定\n",
    "EPOCH_WIN = 30\n",
    "START_SEC = 1800\n",
    "END_SEC   = 2400\n",
    "\n",
    "\n",
    "# ---- helpers ----\n",
    "def _pick_sweat_col(df: pd.DataFrame) -> Optional[str]:\n",
    "    \"\"\"Time_sec 以外で数値化できる列から汗量っぽい列を自動検出。\"\"\"\n",
    "    cand = [c for c in df.columns if c != \"Time_sec\"]\n",
    "    if not cand:\n",
    "        return None\n",
    "    # 数値だけに絞る\n",
    "    num_cand = []\n",
    "    for c in cand:\n",
    "        s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if s.notna().sum() > 0:\n",
    "            num_cand.append(c)\n",
    "    if not num_cand:\n",
    "        return None\n",
    "    # 優先キーワード\n",
    "    priority = [\"sweat\", \"gsr\", \"eda\"]\n",
    "    lower_map = {c: c.lower() for c in num_cand}\n",
    "    for key in priority:\n",
    "        hits = [c for c in num_cand if key in lower_map[c]]\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    # 見つからなければ最初の数値列\n",
    "    return num_cand[0]\n",
    "\n",
    "\n",
    "def _epoch_aggregate_sweat(df: pd.DataFrame,\n",
    "                           start_sec: int,\n",
    "                           end_sec: int,\n",
    "                           win_sec: int,\n",
    "                           col: str) -> pd.DataFrame:\n",
    "    \"\"\"Sweat 原系列で mean/std/slope を計算し DataFrame を返す。\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"Time_sec\"] = pd.to_numeric(df[\"Time_sec\"], errors=\"coerce\")\n",
    "    df[col]        = pd.to_numeric(df[col],        errors=\"coerce\")\n",
    "\n",
    "    # 指定区間・時刻順\n",
    "    df = df[(df[\"Time_sec\"] >= start_sec) & (df[\"Time_sec\"] <= end_sec)].sort_values(\"Time_sec\")\n",
    "    # 同一時刻の重複は平均\n",
    "    df = df.groupby(\"Time_sec\", as_index=False).mean(numeric_only=True)\n",
    "\n",
    "    times = df[\"Time_sec\"].to_numpy(float)\n",
    "    anchors = np.arange(start_sec + win_sec, end_sec + 1, win_sec, dtype=float)\n",
    "    ep_starts = anchors - win_sec\n",
    "    ep_ends   = anchors.copy()\n",
    "\n",
    "    # 出力バッファ\n",
    "    out = {\n",
    "        \"Epoch_start\": ep_starts.copy(),\n",
    "        \"Epoch_end\":   ep_ends.copy(),\n",
    "        \"Sweat_mean\":  np.full_like(anchors, np.nan, dtype=float),\n",
    "        \"Sweat_std\":   np.full_like(anchors, np.nan, dtype=float),\n",
    "        \"Sweat_slope\": np.full_like(anchors, np.nan, dtype=float),\n",
    "    }\n",
    "\n",
    "    idx_left = 0\n",
    "    vals = df[col].to_numpy(float)\n",
    "\n",
    "    for i, t_end in enumerate(anchors):\n",
    "        t_start = t_end - win_sec\n",
    "        # 左端を進める\n",
    "        while idx_left < len(times) and times[idx_left] < t_start:\n",
    "            idx_left += 1\n",
    "        # 右端（非包含）\n",
    "        idx_right = idx_left\n",
    "        while idx_right < len(times) and times[idx_right] <= t_end:\n",
    "            idx_right += 1\n",
    "\n",
    "        if idx_right - idx_left <= 0:\n",
    "            continue\n",
    "\n",
    "        seg_t = times[idx_left:idx_right]\n",
    "        seg_y = vals[idx_left:idx_right]\n",
    "        vmask = np.isfinite(seg_y)\n",
    "        if not vmask.any():\n",
    "            continue\n",
    "\n",
    "        v = seg_y[vmask]\n",
    "        out[\"Sweat_mean\"][i] = float(np.mean(v))\n",
    "        out[\"Sweat_std\"][i]  = float(np.std(v, ddof=1)) if v.size >= 2 else 0.0\n",
    "\n",
    "        # slope（有効点が2点以上）\n",
    "        msk = np.isfinite(seg_t) & np.isfinite(seg_y)\n",
    "        if msk.sum() >= 2:\n",
    "            beta, _alpha = np.polyfit(seg_t[msk], seg_y[msk], 1)\n",
    "            out[\"Sweat_slope\"][i] = float(beta)\n",
    "\n",
    "    return pd.DataFrame(out, columns=[\"Epoch_start\", \"Epoch_end\", \"Sweat_mean\", \"Sweat_std\", \"Sweat_slope\"])\n",
    "\n",
    "\n",
    "def process_sweat_epoch_for_subject(base_root: str, sid: str) -> None:\n",
    "    \"\"\"OFFSET/{sid}_Sweat.csv を読み、30秒エポック特徴 {sid}_Sweat.csv を出力。\"\"\"\n",
    "    subj_dir   = os.path.join(base_root, sid)\n",
    "    offset_csv = os.path.join(subj_dir, \"OFFSET\", f\"{sid}_Sweat.csv\")\n",
    "    out_csv    = os.path.join(subj_dir, \"FEATURE\", f\"{sid}_Sweat.csv\")\n",
    "    os.makedirs(os.path.join(subj_dir, \"FEATURE\"), exist_ok=True)\n",
    "\n",
    "    print(f\"# Subject {sid} (Sweat epoch)\")\n",
    "    if not os.path.exists(offset_csv):\n",
    "        raise FileNotFoundError(f\"OFFSET not found -> {offset_csv}\")\n",
    "\n",
    "    df0 = pd.read_csv(offset_csv, encoding=\"utf-8-sig\")\n",
    "    if \"Time_sec\" not in df0.columns:\n",
    "        raise ValueError(\"Time_sec column missing in Sweat CSV\")\n",
    "\n",
    "    col = _pick_sweat_col(df0)\n",
    "    if col is None:\n",
    "        raise ValueError(\"No numeric sweat-like column found in Sweat CSV\")\n",
    "\n",
    "    df = df0[[\"Time_sec\", col]].copy()\n",
    "    out_df = _epoch_aggregate_sweat(df, START_SEC, END_SEC, EPOCH_WIN, col)\n",
    "\n",
    "    out_df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK]  Sweat -> {out_csv}\")\n",
    "\n",
    "\n",
    "def main_3e_sweat_epoch(base_root: str, subject_ids: List[str]):\n",
    "    for sid in subject_ids:\n",
    "        process_sweat_epoch_for_subject(base_root, sid)\n",
    "\n",
    "\n",
    "# 実行例:\n",
    "main_3e_sweat_epoch(BASE_DIR, SUBJECT_IDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7438404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "心拍数ズレ検証（HR vs WatchHRMean）— ラグ最適化一括処理\n",
    "\n",
    "- 入力：\n",
    "  C:\\\\Users\\\\taiki\\\\OneDrive - Science Tokyo\\\\デスクトップ\\\\研究\\\\本実験結果\\\\<SID>\\\\FEATURE\\\\<SID>_HeartRate.csv\n",
    "  C:\\\\Users\\\\taiki\\\\OneDrive - Science Tokyo\\\\デスクトップ\\\\研究\\\\本実験結果\\\\<SID>\\\\FEATURE\\\\<SID>_Skinos.csv\n",
    "  ※ 1行目がラベル（ヘッダ），2行目から数値．\n",
    "  ※ 1列目=Epoch_start（秒），2列目=Epoch_end（秒），\n",
    "     HeartRate.csvの3列目=HeartRate（bpm），Skinos.csvの6列目=watch_Heart_Rate_mean（bpm）\n",
    "\n",
    "- 出力：C:\\\\Users\\\\taiki\\\\OneDrive - Science Tokyo\\\\デスクトップ\\\\研究\\\\本実験結果\\\\ズレ検証\\\n",
    "  詳細は関数内コメント参照\n",
    "\n",
    "- 仕様：\n",
    "  * Watch（Skinos）側をシフト（+で遅らせる，-で早める）して完全一致結合\n",
    "  * ラグ範囲：[-120, +120] 秒，刻み 5 秒（変更可）\n",
    "  * 指標：RMSE, AbsDiff_mean など\n",
    "  * 図の体裁：linewidth=1.5，フォント（Title=30, Label=24, Legend=20, Ticks=20），横軸 mm:ss\n",
    "\n",
    "実行方法：\n",
    "    python hr_watch_lag_analysis.py\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "# ======== ユーザ設定 ========\n",
    "BASE_DIR = r\"C:\\\\Users\\\\taiki\\\\OneDrive - Science Tokyo\\\\デスクトップ\\\\研究\\\\本実験結果\"\n",
    "OUT_ROOT = os.path.join(BASE_DIR, \"ズレ検証\")\n",
    "\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10061\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "# ラグ探索パラメータ\n",
    "LAG_MIN = -120  # 秒（watchを早める最大）\n",
    "LAG_MAX = 120   # 秒（watchを遅らせる最大）\n",
    "LAG_STEP = 5    # 秒\n",
    "MIN_OVERLAP = 3 # この未満は評価しない\n",
    "TARGET_METRIC = \"RMSE\"  # or \"AbsDiff_mean\"\n",
    "\n",
    "# 任意：手動ラグ（既知のオフセット）を適用したい場合に指定（秒）\n",
    "MANUAL_LAG_MAP: Dict[str, int] = {\n",
    "    # 例: \"10063\": 90,\n",
    "}\n",
    "\n",
    "# 図の体裁（ユーザ規約）\n",
    "LINEWIDTH = 1.5\n",
    "FS_TITLE = 30\n",
    "FS_LABEL = 24\n",
    "FS_LEGEND = 20\n",
    "FS_TICKS = 20\n",
    "\n",
    "# ======== ユーティリティ ========\n",
    "\n",
    "def mmss_formatter(x: float, pos: int) -> str:\n",
    "    try:\n",
    "        x = float(x)\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    m = int(x // 60)\n",
    "    s = int(x % 60)\n",
    "    return f\"{m:02d}:{s:02d}\"\n",
    "\n",
    "\n",
    "def ensure_dir(p: str) -> None:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "\n",
    "def epoch_midpoint(start: pd.Series, end: pd.Series) -> pd.Series:\n",
    "    return (pd.to_numeric(start, errors=\"coerce\") + pd.to_numeric(end, errors=\"coerce\")) / 2.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metrics:\n",
    "    SID: str\n",
    "    Lag_sec: int\n",
    "    N_overlap: int\n",
    "    Diff_mean: float\n",
    "    Diff_median: float\n",
    "    Diff_std: float\n",
    "    AbsDiff_mean: float\n",
    "    AbsDiff_median: float\n",
    "    AbsDiff_p95: float\n",
    "    RMSE: float\n",
    "\n",
    "    def to_row(self) -> Dict[str, object]:\n",
    "        return {\n",
    "            \"SID\": self.SID,\n",
    "            \"Lag_sec\": self.Lag_sec,\n",
    "            \"N_overlap\": self.N_overlap,\n",
    "            \"Diff_mean\": self.Diff_mean,\n",
    "            \"Diff_median\": self.Diff_median,\n",
    "            \"Diff_std\": self.Diff_std,\n",
    "            \"AbsDiff_mean\": self.AbsDiff_mean,\n",
    "            \"AbsDiff_median\": self.AbsDiff_median,\n",
    "            \"AbsDiff_p95\": self.AbsDiff_p95,\n",
    "            \"RMSE\": self.RMSE,\n",
    "        }\n",
    "\n",
    "\n",
    "# ======== IO ========\n",
    "\n",
    "def path_hr_csv(sid: str) -> str:\n",
    "    return os.path.join(BASE_DIR, sid, \"FEATURE\", f\"{sid}_HeartRate.csv\")\n",
    "\n",
    "\n",
    "def path_watch_csv(sid: str) -> str:\n",
    "    return os.path.join(BASE_DIR, sid, \"FEATURE\", f\"{sid}_Skinos.csv\")\n",
    "\n",
    "\n",
    "def load_hr_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"HeartRate.csv 読込（1行目がラベル，2行目から数値）\n",
    "    1列目: Epoch_start, 2列目: Epoch_end, 3列目: HeartRate\n",
    "    列名は存在しない可能性があるため，位置で取得する．\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"[ERROR] HR CSV not found: {path}\")\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(f\"[ERROR] HR CSV must have >=3 columns: {path}\")\n",
    "    out = pd.DataFrame({\n",
    "        \"Epoch_start\": pd.to_numeric(df.iloc[:, 0], errors=\"coerce\"),\n",
    "        \"Epoch_end\": pd.to_numeric(df.iloc[:, 1], errors=\"coerce\"),\n",
    "        \"HR\": pd.to_numeric(df.iloc[:, 2], errors=\"coerce\"),\n",
    "    })\n",
    "    # 欠損除外\n",
    "    out = out.dropna(subset=[\"Epoch_start\", \"Epoch_end\", \"HR\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_watch_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Skinos.csv 読込（1行目がラベル，2行目から数値）\n",
    "    1列目: Epoch_start, 2列目: Epoch_end, 6列目: watch_Heart_Rate_mean\n",
    "    列名は存在しない可能性があるため，位置で取得する．\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"[ERROR] Skinos CSV not found: {path}\")\n",
    "    df = pd.read_csv(path, header=0)\n",
    "    if df.shape[1] < 6:\n",
    "        raise ValueError(f\"[ERROR] Skinos CSV must have >=6 columns: {path}\")\n",
    "    out = pd.DataFrame({\n",
    "        \"Epoch_start\": pd.to_numeric(df.iloc[:, 0], errors=\"coerce\"),\n",
    "        \"Epoch_end\": pd.to_numeric(df.iloc[:, 1], errors=\"coerce\"),\n",
    "        \"WatchHRMean\": pd.to_numeric(df.iloc[:, 5], errors=\"coerce\"),\n",
    "    })\n",
    "    out = out.dropna(subset=[\"Epoch_start\", \"Epoch_end\", \"WatchHRMean\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======== コア処理 ========\n",
    "\n",
    "def shift_epochs(df: pd.DataFrame, seconds: int) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"Epoch_start\"] = pd.to_numeric(out[\"Epoch_start\"], errors=\"coerce\") + seconds\n",
    "    out[\"Epoch_end\"] = pd.to_numeric(out[\"Epoch_end\"], errors=\"coerce\") + seconds\n",
    "    return out\n",
    "\n",
    "\n",
    "def merge_on_epoch(hr_df: pd.DataFrame, watch_df: pd.DataFrame, watch_label: str) -> pd.DataFrame:\n",
    "    merged = pd.merge(\n",
    "        hr_df[[\"Epoch_start\", \"Epoch_end\", \"HR\"]],\n",
    "        watch_df[[\"Epoch_start\", \"Epoch_end\", \"WatchHRMean\"]],\n",
    "        on=[\"Epoch_start\", \"Epoch_end\"], how=\"inner\",\n",
    "    )\n",
    "    merged[\"Time_sec\"] = epoch_midpoint(merged[\"Epoch_start\"], merged[\"Epoch_end\"])\n",
    "    merged[\"Diff\"] = pd.to_numeric(merged[\"HR\"], errors=\"coerce\") - pd.to_numeric(merged[\"WatchHRMean\"], errors=\"coerce\")\n",
    "    merged[\"AbsDiff\"] = merged[\"Diff\"].abs()\n",
    "    cols = [\"Epoch_start\", \"Epoch_end\", \"Time_sec\", \"HR\", \"WatchHRMean\", \"Diff\", \"AbsDiff\"]\n",
    "    merged = merged[cols].sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "    merged = merged.rename(columns={\"WatchHRMean\": watch_label})\n",
    "    return merged\n",
    "\n",
    "\n",
    "def compute_metrics(df: pd.DataFrame, sid: str, lag_sec: int) -> Metrics:\n",
    "    if df.empty:\n",
    "        return Metrics(sid, lag_sec, 0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "    diff = pd.to_numeric(df[\"Diff\"], errors=\"coerce\").dropna()\n",
    "    if diff.empty:\n",
    "        return Metrics(sid, lag_sec, 0, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "    absdiff = diff.abs()\n",
    "    rmse = float(np.sqrt(np.mean(diff.values ** 2)))\n",
    "    p95 = float(np.quantile(absdiff, 0.95))\n",
    "    return Metrics(\n",
    "        SID=sid,\n",
    "        Lag_sec=lag_sec,\n",
    "        N_overlap=int(diff.shape[0]),\n",
    "        Diff_mean=float(diff.mean()),\n",
    "        Diff_median=float(diff.median()),\n",
    "        Diff_std=float(diff.std(ddof=1)) if diff.shape[0] > 1 else 0.0,\n",
    "        AbsDiff_mean=float(absdiff.mean()),\n",
    "        AbsDiff_median=float(absdiff.median()),\n",
    "        AbsDiff_p95=p95,\n",
    "        RMSE=rmse,\n",
    "    )\n",
    "\n",
    "\n",
    "def select_best(metrics_df: pd.DataFrame, target_metric: str = TARGET_METRIC) -> pd.Series:\n",
    "    if metrics_df.empty:\n",
    "        return pd.Series(dtype=object)\n",
    "    df = metrics_df.dropna(subset=[target_metric]).copy()\n",
    "    if df.empty:\n",
    "        return pd.Series(dtype=object)\n",
    "    # 1) target_metric 最小\n",
    "    df = df.sort_values([target_metric, \"AbsDiff_mean\", \"N_overlap\"], ascending=[True, True, False])\n",
    "    return df.iloc[0]\n",
    "\n",
    "\n",
    "# ======== 可視化 ========\n",
    "\n",
    "def _setup_time_axis(ax, title: str) -> None:\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(mmss_formatter))\n",
    "    ax.set_xlabel(\"Time (mm:ss)\", fontsize=FS_LABEL)\n",
    "    ax.set_ylabel(\"\", fontsize=FS_LABEL)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=FS_TICKS)\n",
    "    ax.set_title(title, fontsize=FS_TITLE)\n",
    "\n",
    "\n",
    "def plot_series(df: pd.DataFrame, title: str, outpng: str, hr_label: str = \"HeartRate\", watch_label: str = \"WatchHRMean\") -> None:\n",
    "    if df.empty:\n",
    "        return\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df[\"Time_sec\"], df[\"HR\"], linewidth=LINEWIDTH, label=hr_label)\n",
    "    plt.plot(df[\"Time_sec\"], df[watch_label], linewidth=LINEWIDTH, label=watch_label)\n",
    "    _setup_time_axis(plt.gca(), title)\n",
    "    plt.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpng, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_diff(df: pd.DataFrame, title: str, outpng: str) -> None:\n",
    "    if df.empty:\n",
    "        return\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(df[\"Time_sec\"], df[\"Diff\"], linewidth=LINEWIDTH, label=\"Diff = HR - Watch\")\n",
    "    _setup_time_axis(plt.gca(), title)\n",
    "    plt.axhline(0.0, linestyle=\"--\", linewidth=1)\n",
    "    plt.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpng, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_hist_absdiff(df: pd.DataFrame, title: str, outpng: str) -> None:\n",
    "    if df.empty:\n",
    "        return\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(pd.to_numeric(df[\"AbsDiff\"], errors=\"coerce\").dropna().values, bins=10)\n",
    "    plt.xlabel(\"Absolute Difference (bpm)\", fontsize=FS_LABEL)\n",
    "    plt.ylabel(\"Count\", fontsize=FS_LABEL)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=FS_TICKS)\n",
    "    plt.title(title, fontsize=FS_TITLE)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpng, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ======== メイン処理 ========\n",
    "\n",
    "def process_sid(sid: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series]:\n",
    "    sid_dir = os.path.join(OUT_ROOT, sid)\n",
    "    d_merged = os.path.join(sid_dir, \"MERGED\")\n",
    "    d_sweep = os.path.join(sid_dir, \"LAG_SWEEP\")\n",
    "    d_summary = os.path.join(sid_dir, \"SUMMARY\")\n",
    "    d_plots = os.path.join(sid_dir, \"PLOTS\")\n",
    "    for d in (sid_dir, d_merged, d_sweep, d_summary, d_plots):\n",
    "        ensure_dir(d)\n",
    "\n",
    "    # 読込\n",
    "    hr_path = path_hr_csv(sid)\n",
    "    watch_path = path_watch_csv(sid)\n",
    "    try:\n",
    "        hr_df = load_hr_csv(hr_path)\n",
    "        watch_df = load_watch_csv(watch_path)\n",
    "        print(f\"[OK] SID={sid} read HR: {len(hr_df)} rows, Watch: {len(watch_df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] SID={sid} read failed: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.Series(dtype=object)\n",
    "\n",
    "    # --- ノーシフト比較 ---\n",
    "    merged0 = merge_on_epoch(hr_df, watch_df, watch_label=\"WatchHRMean\")\n",
    "    merged0.to_csv(os.path.join(d_merged, \"HR_vs_Watch_noShift.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] SID={sid} no-shift overlap: {len(merged0)}\")\n",
    "\n",
    "    plot_series(merged0, \"HR vs WatchHRMean (no shift)\", os.path.join(d_plots, \"series_noShift.png\"), watch_label=\"WatchHRMean\")\n",
    "    plot_diff(merged0, \"Difference (no shift)\", os.path.join(d_plots, \"diff_noShift.png\"))\n",
    "\n",
    "    # --- ラグスイープ（watch側シフト） ---\n",
    "    rows: List[Dict[str, object]] = []\n",
    "    for lag in range(LAG_MIN, LAG_MAX + 1, LAG_STEP):\n",
    "        w_shift = shift_epochs(watch_df, lag)\n",
    "        merged = merge_on_epoch(hr_df, w_shift, watch_label=\"WatchHRMean_shift\")\n",
    "        if len(merged) < MIN_OVERLAP:\n",
    "            print(f\"[SKIP] SID={sid} lag={lag:+d}: overlap < {MIN_OVERLAP}\")\n",
    "            continue\n",
    "        # 保存（行が多いときは容量注意）\n",
    "        merged.to_csv(os.path.join(d_sweep, f\"LAG_{lag:+d}.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "        m = compute_metrics(merged, sid, lag)\n",
    "        rows.append(m.to_row())\n",
    "        print(f\"[OK] SID={sid} lag={lag:+4d} sec overlap={m.N_overlap} RMSE={m.RMSE:.3f} AbsMean={m.AbsDiff_mean:.3f}\")\n",
    "\n",
    "    metrics_df = pd.DataFrame(rows)\n",
    "    metrics_path = os.path.join(d_summary, \"lag_metrics.csv\")\n",
    "    metrics_df.to_csv(metrics_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    best = select_best(metrics_df, TARGET_METRIC)\n",
    "    if best.empty:\n",
    "        print(f\"[WARN] SID={sid} no best lag (no valid overlap)\")\n",
    "        return merged0, metrics_df, best\n",
    "\n",
    "    # --- ベストラグで可視化 ---\n",
    "    best_lag = int(best[\"Lag_sec\"])  # type: ignore\n",
    "    w_best = shift_epochs(watch_df, best_lag)\n",
    "    merged_best = merge_on_epoch(hr_df, w_best, watch_label=\"WatchHRMean_best\")\n",
    "\n",
    "    plot_series(merged_best, f\"HR vs WatchHRMean (best lag {best_lag:+d}s)\", os.path.join(d_plots, \"series_bestLag.png\"), watch_label=\"WatchHRMean_best\")\n",
    "    plot_diff(merged_best, f\"Difference (best lag {best_lag:+d}s)\", os.path.join(d_plots, \"diff_bestLag.png\"))\n",
    "    plot_hist_absdiff(merged_best, f\"|HR - Watch| (best lag {best_lag:+d}s)\", os.path.join(d_plots, \"hist_absdiff_bestLag.png\"))\n",
    "\n",
    "    # --- 手動ラグ（任意） ---\n",
    "    if sid in MANUAL_LAG_MAP:\n",
    "        man_lag = int(MANUAL_LAG_MAP[sid])\n",
    "        w_man = shift_epochs(watch_df, man_lag)\n",
    "        merged_man = merge_on_epoch(hr_df, w_man, watch_label=\"WatchHRMean_manual\")\n",
    "        merged_man.to_csv(os.path.join(d_sweep, f\"LAG_{man_lag:+d}_manual.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "        plot_series(merged_man, f\"HR vs WatchHRMean (manual lag {man_lag:+d}s)\", os.path.join(d_plots, \"series_manualLag.png\"), watch_label=\"WatchHRMean_manual\")\n",
    "        plot_diff(merged_man, f\"Difference (manual lag {man_lag:+d}s)\", os.path.join(d_plots, \"diff_manualLag.png\"))\n",
    "        plot_hist_absdiff(merged_man, f\"|HR - Watch| (manual lag {man_lag:+d}s)\", os.path.join(d_plots, \"hist_absdiff_manualLag.png\"))\n",
    "\n",
    "    return merged0, metrics_df, best\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    ensure_dir(OUT_ROOT)\n",
    "    best_rows: List[pd.Series] = []\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        merged0, metrics_df, best = process_sid(sid)\n",
    "        if best is not None and not isinstance(best, pd.Series):\n",
    "            # guard — but select_best returns Series\n",
    "            pass\n",
    "        if best is not None and not best.empty:\n",
    "            best_rows.append(best)\n",
    "\n",
    "    if best_rows:\n",
    "        all_best = pd.DataFrame(best_rows)\n",
    "        all_best = all_best[[\"SID\", \"Lag_sec\", \"N_overlap\", \"AbsDiff_mean\", \"RMSE\"]]\n",
    "        all_best = all_best.sort_values([\"SID\"]).reset_index(drop=True)\n",
    "        all_best.to_csv(os.path.join(OUT_ROOT, \"ALL_SID_best_lag.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[OK] saved -> {os.path.join(OUT_ROOT, 'ALL_SID_best_lag.csv')}\")\n",
    "    else:\n",
    "        print(\"[WARN] No best rows generated (no overlaps across SIDs?)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ceecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "(4) Epoch merge — FEATURE統合（Epoch_start/Epoch_endベース → 30秒グリッド）\n",
    "\n",
    "- FEATURE\\ の CSV を \"リストに列挙\" し、raw/raw2 付きは除外\n",
    "- 入力CSV: 1,2列目が Epoch_start/Epoch_end、3列目以降が特徴量\n",
    "- 出力CSV: 1,2列目が Epoch_start/Epoch_end、3列目 FMS、4列目以降が特徴量\n",
    "- HF_power / LF_power / LF_HF_ratio は 120秒系列（30秒化後、内部 NaN を bfill）\n",
    "- さらに 120秒系列については「最初の30秒(1770–1800)」も“直後(1800–1830)”の値で補完\n",
    "- FMS は被験者ごとのスコア列（30秒刻み）を挿入\n",
    "  * 本コードでは FMS_TEXT に“スペース区切り文字列”として与えた系列を使用\n",
    "  * 基準開始=1770s（=1800-30）、30s刻み、fms_text_shift_steps で時刻シフト可能（既定0）\n",
    "- 出力: {BASE_DIR}\\{sid}\\EPOCH\\{sid}_epoch.csv （上書き）\n",
    "依存: numpy, pandas\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===================== CONFIG =====================\n",
    "BASE_DIR: str = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "# --- 被験者IDのみ（氏名なし） ---\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10041\",\"10061\",\"10062\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\"10084\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "subjects: List[Tuple[str, str]] = [(sid, \"\") for sid in SUBJECT_IDS]\n",
    "\n",
    "# --- FMS（スペース区切りの 30秒刻み系列；開始=1770s） ---\n",
    "FMS_TEXT: Dict[str, str] = {\n",
    "    \"10041\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\",\n",
    "    \"10061\": \"0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 2 1\",\n",
    "    \"10062\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\",\n",
    "    \"10063\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10064\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\",\n",
    "    \"10071\": \"0 0 0 0 0 0 0 1 1 1 1 1 1 1 2 2 1 1 1 1 1\",\n",
    "    \"10072\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10073\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10074\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10081\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\",\n",
    "    \"10082\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\",\n",
    "    \"10083\": \"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\",\n",
    "    \"10084\": \"0 0 1 0 0 0 0 0 1 1 2 2 2 2 3 3 3 3 3 3 3\",\n",
    "    \"10091\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\",\n",
    "    \"10092\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10093\": \"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 2 2 3 3 4 4\",\n",
    "    \"10094\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 2 2\",\n",
    "    \"10101\": \"0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 3 3\",\n",
    "    \"10102\": \"0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 2 2 2 2 2\",\n",
    "    \"10103\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 2 3 3 4\",\n",
    "}\n",
    "fms_text_base_start: int   = 1770\n",
    "fms_text_step_sec: int     = 30\n",
    "fms_text_shift_steps: int  = 0   # 例: -1 で 1エポック前へ\n",
    "\n",
    "# FMS_MAP は今回は未使用（優先度: MAP > TEXT）。必要ならここに配列を入れる。\n",
    "FMS_MAP: Dict[str, List[float]] = {}\n",
    "\n",
    "# 出力グリッド（30s）。要求に合わせ 1770 開始（= 120秒系列の先頭30秒を埋めるため）\n",
    "grid_start_sec: int = 1770\n",
    "epoch_len_sec: int  = 30\n",
    "\n",
    "# 120秒エポック扱いのファイル名（末尾名）\n",
    "FEATURE_120S: set = {\"LF_power\", \"HF_power\", \"LF_HF_ratio\"}\n",
    "\n",
    "# 上書き保存\n",
    "overwrite: bool = True\n",
    "# ==================================================\n",
    "\n",
    "\n",
    "def list_candidate_files(feat_dir: Path) -> List[Path]:\n",
    "    \"\"\"FEATURE ディレクトリから統合候補CSVをリストで返す（*raw* を除外）。\"\"\"\n",
    "    files = sorted([p for p in feat_dir.glob(\"*.csv\")\n",
    "                    if (\"raw\" not in p.stem.lower() and \"raw2\" not in p.stem.lower())])\n",
    "    return files\n",
    "\n",
    "\n",
    "def ensure_epoch_columns(df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    先頭2列を Epoch_start / Epoch_end として解釈し、列名を揃える。\n",
    "    3列目以降を特徴量とみなす。列数<3なら None。\n",
    "    \"\"\"\n",
    "    if df is None or df.empty or df.shape[1] < 3:\n",
    "        return None\n",
    "    cols = list(df.columns)\n",
    "    df = df.rename(columns={cols[0]: \"Epoch_start\", cols[1]: \"Epoch_end\"}).copy()\n",
    "    # 整数秒に変換\n",
    "    try:\n",
    "        df[\"Epoch_start\"] = pd.to_numeric(df[\"Epoch_start\"], errors=\"coerce\").round().astype(int)\n",
    "        df[\"Epoch_end\"]   = pd.to_numeric(df[\"Epoch_end\"],   errors=\"coerce\").round().astype(int)\n",
    "    except Exception:\n",
    "        return None\n",
    "    # 単調性チェック（非減少）\n",
    "    s = df[\"Epoch_start\"].to_numpy()\n",
    "    if s.size >= 2 and not (s[1:] >= s[:-1]).all():\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_epoch_grid(start_sec: int, end_sec: int, step_sec: int) -> pd.DataFrame:\n",
    "    \"\"\"30秒グリッド（左閉右開）。\"\"\"\n",
    "    starts = np.arange(start_sec, end_sec, step_sec, dtype=int)\n",
    "    ends = starts + step_sec\n",
    "    return pd.DataFrame({\"Epoch_start\": starts, \"Epoch_end\": ends}, dtype=int)\n",
    "\n",
    "\n",
    "def resample_epoch_to_30s(df: pd.DataFrame, grid: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    任意長エポック（30/60/120sなど）→30秒グリッドへ展開。\n",
    "    ・B=[b,b+30) と E=[s,e) の重なり>0なら採用\n",
    "    ・複数重なりは各列の単純平均（全NaNはNaN）\n",
    "    \"\"\"\n",
    "    value_cols = list(df.columns)[2:]  # 3列目以降が特徴量\n",
    "    out = pd.DataFrame(index=grid.index, columns=value_cols, dtype=float)\n",
    "\n",
    "    s_arr = df[\"Epoch_start\"].to_numpy()\n",
    "    e_arr = df[\"Epoch_end\"].to_numpy()\n",
    "    vals = df[value_cols].to_numpy(dtype=float)\n",
    "\n",
    "    grid_se = grid[[\"Epoch_start\", \"Epoch_end\"]].to_numpy()\n",
    "    for i, (b_start, b_end) in enumerate(grid_se):\n",
    "        mask = ~((e_arr <= b_start) | (s_arr >= b_end))  # 重なり > 0\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        block = pd.DataFrame(vals[mask], columns=value_cols)\n",
    "        out.iloc[i, :] = block.mean(axis=0, skipna=True).to_numpy()\n",
    "    return out\n",
    "\n",
    "\n",
    "def merge_features_on_grid(grid: pd.DataFrame, frames: List[Tuple[str, pd.DataFrame]]) -> pd.DataFrame:\n",
    "    \"\"\"30秒グリッドに各FEATURE（30秒化済）を横結合。列衝突は 'col (feat)' に退避。\"\"\"\n",
    "    out = grid.copy()\n",
    "    for feat_name, df30 in frames:\n",
    "        for col in df30.columns:\n",
    "            new_col = col\n",
    "            if new_col in out.columns:\n",
    "                new_col = f\"{col} ({feat_name})\"\n",
    "            out[new_col] = df30[col].values\n",
    "    return out\n",
    "\n",
    "\n",
    "def bfill_internal_columns(merged: pd.DataFrame, cols: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    指定列について、最初の有効値 ~ 最後の有効値 の“内部 NaN”を bfill。\n",
    "    先頭より前/末尾より後は NaN のまま。\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        if col not in merged.columns:\n",
    "            continue\n",
    "        s = merged[col]\n",
    "        i0 = s.first_valid_index()\n",
    "        i1 = s.last_valid_index()\n",
    "        if i0 is None or i1 is None or i1 <= i0:\n",
    "            continue\n",
    "        merged.loc[i0:i1, col] = s.loc[i0:i1].bfill()\n",
    "\n",
    "\n",
    "def front_fill_first_bin_from_next(merged: pd.DataFrame, cols: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    120秒系列に限り、最初の30秒（Epoch_start==1770）の値を、\n",
    "    “直後(=最初の有効値)”で上書きする（1ビンのみ）。\n",
    "    \"\"\"\n",
    "    changed: List[str] = []\n",
    "    if merged.empty:\n",
    "        return changed\n",
    "    if int(merged.loc[0, \"Epoch_start\"]) != grid_start_sec:\n",
    "        return changed\n",
    "    for col in cols:\n",
    "        if col not in merged.columns:\n",
    "            continue\n",
    "        if pd.isna(merged.loc[0, col]):\n",
    "            idx = merged[col].first_valid_index()\n",
    "            if idx is not None:\n",
    "                merged.loc[0, col] = merged.loc[idx, col]\n",
    "                changed.append(col)\n",
    "    return changed\n",
    "\n",
    "\n",
    "# ===================== FMS helpers =====================\n",
    "def _fms_series_from_text_for_grid(sid: str, grid: pd.DataFrame) -> Optional[np.ndarray]:\n",
    "    \"\"\"FMS_TEXT[sid] を 1770s基準+シフトで時刻化し、30sグリッドに左結合して配列で返す。\"\"\"\n",
    "    txt = FMS_TEXT.get(sid)\n",
    "    if not txt:\n",
    "        return None\n",
    "    tokens = [t for t in str(txt).strip().split() if t != \"\"]\n",
    "    vals = np.array([float(t) for t in tokens], dtype=float)\n",
    "    n = vals.size\n",
    "    start0 = fms_text_base_start + fms_text_shift_steps * fms_text_step_sec\n",
    "    starts = np.arange(start0, start0 + n * fms_text_step_sec, fms_text_step_sec, dtype=int)\n",
    "    ends   = starts + fms_text_step_sec\n",
    "    fdf = pd.DataFrame({\"Epoch_start\": starts, \"Epoch_end\": ends, \"FMS\": vals})\n",
    "    merged = grid.merge(fdf, on=[\"Epoch_start\", \"Epoch_end\"], how=\"left\")\n",
    "    return merged[\"FMS\"].to_numpy()\n",
    "\n",
    "\n",
    "def estimate_fms_end_sec(sid: str, default_end: int) -> int:\n",
    "    \"\"\"\n",
    "    FMS_TEXT / FMS_MAP から、その被験者のFMSが占める最終 Epoch_end 秒を推定。\n",
    "    見つからない場合は default_end を返す。\n",
    "    \"\"\"\n",
    "    end_sec = default_end\n",
    "\n",
    "    if sid in FMS_MAP and FMS_MAP[sid]:\n",
    "        n = len(FMS_MAP[sid])\n",
    "        if n > 0:\n",
    "            end_sec = max(end_sec, grid_start_sec + n * epoch_len_sec)\n",
    "\n",
    "    if sid in FMS_TEXT and FMS_TEXT[sid].strip():\n",
    "        n = len([t for t in FMS_TEXT[sid].split() if t != \"\"])\n",
    "        if n > 0:\n",
    "            start0 = fms_text_base_start + fms_text_shift_steps * fms_text_step_sec\n",
    "            end_from_text = start0 + n * fms_text_step_sec\n",
    "            end_sec = max(end_sec, end_from_text)\n",
    "\n",
    "    return end_sec\n",
    "\n",
    "\n",
    "def make_fms_series_for_subject(sid: str, grid: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"優先順位: FMS_MAP → FMS_TEXT → NaN.\"\"\"\n",
    "    length = len(grid)\n",
    "    if sid in FMS_MAP and len(FMS_MAP[sid]) > 0:\n",
    "        arr = np.array(FMS_MAP[sid], dtype=float).flatten()\n",
    "        if arr.size < length:\n",
    "            arr = np.concatenate([arr, np.full(length - arr.size, np.nan)])\n",
    "        else:\n",
    "            arr = arr[:length]\n",
    "        print(f\"[OK]  FMS -> from FMS_MAP (len={length})\")\n",
    "        return arr\n",
    "    arr_txt = _fms_series_from_text_for_grid(sid, grid)\n",
    "    if arr_txt is not None:\n",
    "        print(f\"[OK]  FMS -> from FMS_TEXT (len={length})\")\n",
    "        return arr_txt\n",
    "    print(f\"[OK]  FMS -> NaN (len={length})\")\n",
    "    return np.full(length, np.nan, dtype=float)\n",
    "\n",
    "\n",
    "# ===================== MAIN =====================\n",
    "def main_4(subjects: List[Tuple[str, str]] = subjects, overwrite: bool = overwrite) -> None:\n",
    "    \"\"\"\n",
    "    工程(4) 実行：FEATURE → 30秒正規化 → 横結合 → FMS充填 → EPOCH保存。\n",
    "    ログ：\n",
    "      # Subject {sid}{name}\n",
    "      [OK]  FEATURE_DIR -> <path>\n",
    "      [OK]  LIST n files\n",
    "      [OK]  {FeatureName} -> <csv path>\n",
    "      [SKIP] {FeatureName}: columns mismatch or no features\n",
    "      [OK]  RESAMPLE {FeatureName} -> 30s\n",
    "      [OK]  BFILL(120s) -> <cols>\n",
    "      [OK]  FRONT-FILL(1770) -> <cols>   （該当があれば）\n",
    "      [OK]  EPOCH -> <out_csv>\n",
    "      [SKIP] EPOCH: exists\n",
    "    \"\"\"\n",
    "    base = Path(BASE_DIR)\n",
    "\n",
    "    for sid, name in subjects:\n",
    "        print(f\"# Subject {sid}{name}\")\n",
    "\n",
    "        # FEATURE を優先、無ければ FEATURES を見る\n",
    "        feat_dir = base / f\"{sid}{name}\" / \"FEATURE\"\n",
    "        if not feat_dir.exists():\n",
    "            alt = base / f\"{sid}{name}\" / \"FEATURES\"\n",
    "            feat_dir = alt if alt.exists() else None\n",
    "\n",
    "        out_dir = base / f\"{sid}{name}\" / \"EPOCH\"\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_csv = out_dir / f\"{sid}_epoch.csv\"\n",
    "\n",
    "        if out_csv.exists() and not overwrite:\n",
    "            print(\"[SKIP] EPOCH: exists\")\n",
    "            continue\n",
    "\n",
    "        if feat_dir is None:\n",
    "            print(\"[SKIP] FEATURES: not found\")\n",
    "            continue\n",
    "        print(f\"[OK]  FEATURE_DIR -> {str(feat_dir)}\")\n",
    "\n",
    "        # === 候補の列挙（リストに列挙） ===\n",
    "        candidates: List[Path] = list_candidate_files(feat_dir)\n",
    "        print(f\"[OK]  LIST {len(candidates)} files\")\n",
    "\n",
    "        # === 30秒グリッドの終端を見積もり ===\n",
    "        max_end = estimate_fms_end_sec(sid, default_end=grid_start_sec + epoch_len_sec)\n",
    "\n",
    "        frames_original: List[Tuple[str, pd.DataFrame]] = []\n",
    "        for p in candidates:\n",
    "            feat_name = p.stem  # 例: \"10041_LF_power\"\n",
    "            try:\n",
    "                raw = pd.read_csv(p)\n",
    "            except Exception:\n",
    "                print(f\"[SKIP] {feat_name}: read error\")\n",
    "                continue\n",
    "\n",
    "            df = ensure_epoch_columns(raw)\n",
    "            if df is None:\n",
    "                print(f\"[SKIP] {feat_name}: columns mismatch or no features\")\n",
    "                continue\n",
    "\n",
    "            value_cols = list(df.columns)[2:]\n",
    "            if not value_cols:\n",
    "                print(f\"[SKIP] {feat_name}: no feature columns (>=3rd) found\")\n",
    "                continue\n",
    "\n",
    "            print(f\"[OK]  {feat_name} -> {str(p)}\")\n",
    "\n",
    "            try:\n",
    "                max_end = max(max_end, int(df[\"Epoch_end\"].max()))\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            frames_original.append((feat_name, df))\n",
    "\n",
    "        # === 30秒グリッド作成（1770 開始） ===\n",
    "        grid = build_epoch_grid(grid_start_sec, max_end, epoch_len_sec)\n",
    "\n",
    "        # === 30秒へリサンプル & 連結 ===\n",
    "        resampled_frames: List[Tuple[str, pd.DataFrame]] = []\n",
    "        cols_120s_all: List[str] = []\n",
    "\n",
    "        for feat_name, df in frames_original:\n",
    "            df30 = resample_epoch_to_30s(df, grid)\n",
    "            resampled_frames.append((feat_name, df30))\n",
    "            print(f\"[OK]  RESAMPLE {feat_name} -> 30s\")\n",
    "\n",
    "            tail = feat_name.split(\"_\", 1)[-1]  # 例: \"LF_power\"\n",
    "            if tail in FEATURE_120S:\n",
    "                cols_120s_all.extend(list(df30.columns))\n",
    "\n",
    "        merged = merge_features_on_grid(grid, resampled_frames)\n",
    "\n",
    "        # === 120秒系列：内部のみ bfill → 先頭30秒フロントフィル ===\n",
    "        cols_120s_all = sorted(set([c for c in cols_120s_all if c in merged.columns]))\n",
    "        if cols_120s_all:\n",
    "            bfill_internal_columns(merged, cols_120s_all)\n",
    "            print(f\"[OK]  BFILL(120s) -> {', '.join(cols_120s_all)}\")\n",
    "            changed = front_fill_first_bin_from_next(merged, cols_120s_all)\n",
    "            if changed:\n",
    "                print(f\"[OK]  FRONT-FILL(1770) -> {', '.join(changed)}\")\n",
    "\n",
    "        # === FMS 列（3列目）を注入（既存があれば差し替え）===\n",
    "        if \"FMS\" in merged.columns:\n",
    "            merged.drop(columns=[\"FMS\"], inplace=True)\n",
    "        fms_series = make_fms_series_for_subject(sid, merged[[\"Epoch_start\", \"Epoch_end\"]])\n",
    "        merged.insert(2, \"FMS\", fms_series)\n",
    "\n",
    "        # === 保存 ===\n",
    "        try:\n",
    "            merged.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"[OK]  EPOCH -> {str(out_csv)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] EPOCH: save failed ({e})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_4()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
