{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9052a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell0: 特徴量設計\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 重要パラメータブロック\n",
    "# =========================\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\")\n",
    "\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\", \"10063\", \"10064\",\n",
    "    \"10071\", \"10072\", \"10073\", \"10074\",\n",
    "    \"10081\", \"10082\", \"10083\",\n",
    "    \"10091\", \"10092\", \"10093\", \"10094\",\n",
    "    \"10101\", \"10102\", \"10103\",\n",
    "]\n",
    "\n",
    "# 解析対象時間（秒）\n",
    "T_START = 1770.0\n",
    "T_END = 2400.0\n",
    "\n",
    "# スライディング窓の設定（本研究）\n",
    "WINDOW_SEC = 3           # 本研究: 特徴量窓幅 3秒\n",
    "SLIDE_STEP_SEC = 1       # 本研究: 窓終端 t を 1秒刻みで計算\n",
    "PC_LAG_SEC = 3           # 本研究: PCは「3秒前の窓」と比較（= 1つ前の3秒ブロック）\n",
    "\n",
    "# 参考: 先行研究③（VRジェットコースター LSTM）\n",
    "#   WINDOW_SEC   ≒ 3     # 同じく 3秒ローリング窓で rma/max/min/pc を計算\n",
    "#   SLIDE_STEP_SEC ≒ 0.5 # 0.5秒刻みで特徴列 f(t) を作成（2 Hz の時系列）\n",
    "#   シーケンス長  = 30   # 30ステップの入力系列 (= 過去 15秒分の f(t))\n",
    "#   ラベル        = 1つ  # 各 15秒ブロックの終端時刻に対応する CS(0/1) を1つ付与\n",
    "\n",
    "# pc の計算パラメータ\n",
    "PC_DEFAULT_VALUE = 0.0   # 前平均が無い/極小のときに入れる値\n",
    "PC_EPS = 1e-6            # 「ほぼゼロ判定」のしきい値\n",
    "\n",
    "# 窓終端の t の範囲（整数秒）\n",
    "T_MIN_OUT = int(T_START + WINDOW_SEC)   # 1770 + 3 = 1773\n",
    "T_MAX_OUT = int(T_END)                  # 2400\n",
    "\n",
    "# FMS_TEXT（与えられたものをそのまま使用）\n",
    "FMS_TEXT: Dict[str, str] = {\n",
    "    \"10061\": \"0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 2 1\",\n",
    "    \"10063\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10064\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\",\n",
    "    \"10071\": \"0 0 0 0 0 0 0 1 1 1 1 1 1 1 2 2 1 1 1 1 1\",\n",
    "    \"10072\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10073\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10074\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10081\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\",\n",
    "    \"10082\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\",\n",
    "    \"10083\": \"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\",\n",
    "    \"10091\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\",\n",
    "    \"10092\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10093\": \"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 2 2 3 3 4 4\",\n",
    "    \"10094\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 2 2\",\n",
    "    \"10101\": \"0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 3 3\",\n",
    "    \"10102\": \"0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 2 2 2 2 2\",\n",
    "    \"10103\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 2 2 3 3 4\",\n",
    "}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ユーティリティ関数\n",
    "# =========================\n",
    "\n",
    "def minmax_scale(values: np.ndarray, channel_name: str, sid: str) -> np.ndarray:\n",
    "    \"\"\"[1770,2400]内の値をmin-maxスケーリング。min==maxまたは全部NaNなら全0.\"\"\"\n",
    "    arr = np.asarray(values, float)\n",
    "    valid = np.isfinite(arr)\n",
    "    if not valid.any():\n",
    "        print(f\"[WARN] {sid} {channel_name}: all NaN -> set all zeros\")\n",
    "        return np.zeros_like(arr, float)\n",
    "    vmin = np.nanmin(arr[valid])\n",
    "    vmax = np.nanmax(arr[valid])\n",
    "    if vmax - vmin == 0:\n",
    "        print(f\"[WARN] {sid} {channel_name}: min==max ({vmin}) -> set all zeros\")\n",
    "        return np.zeros_like(arr, float)\n",
    "    return (arr - vmin) / (vmax - vmin)\n",
    "\n",
    "\n",
    "def compute_window_features_continuous(\n",
    "    times: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    t_grid: np.ndarray,\n",
    "    window_len: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    不均一サンプリング（1000Hz, 15Hzなど）について、\n",
    "    各 t (窓終端) に対する [t-window_len, t] の平均/最大/最小を二重ポインタで計算。\n",
    "    \"\"\"\n",
    "    times = np.asarray(times, float)\n",
    "    values = np.asarray(values, float)\n",
    "    n = len(times)\n",
    "    means = np.full(t_grid.shape, np.nan, dtype=float)\n",
    "    vmaxs = np.full(t_grid.shape, np.nan, dtype=float)\n",
    "    vmins = np.full(t_grid.shape, np.nan, dtype=float)\n",
    "    start = 0\n",
    "    end = -1\n",
    "    for i, t in enumerate(t_grid):\n",
    "        w_start = t - window_len\n",
    "        # 左端を前に進める\n",
    "        while start < n and times[start] < w_start:\n",
    "            start += 1\n",
    "        # 右端を前に進める\n",
    "        while end + 1 < n and times[end + 1] <= t:\n",
    "            end += 1\n",
    "        if end >= start and start < n:\n",
    "            seg = values[start:end + 1]\n",
    "            if seg.size > 0:\n",
    "                valid = np.isfinite(seg)\n",
    "                if valid.any():\n",
    "                    segv = seg[valid]\n",
    "                    means[i] = segv.mean()\n",
    "                    vmaxs[i] = segv.max()\n",
    "                    vmins[i] = segv.min()\n",
    "    return means, vmaxs, vmins\n",
    "\n",
    "\n",
    "def build_hr_1s_from_rr(\n",
    "    r_times: np.ndarray,\n",
    "    rr_intervals: np.ndarray,\n",
    "    t_start: int,\n",
    "    t_end: int,\n",
    "    sid: str,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    RR時刻列から、t_start〜t_end の1秒グリッドHR_1s(t)を作る。\n",
    "    ルール:\n",
    "      区間 [R_k, R_{k+1}) にいる t のHRは HR_k = 60/RR_k\n",
    "\n",
    "    ※ 有効RRがゼロ件なら即エラー（仕様4）\n",
    "    \"\"\"\n",
    "    r_times = np.asarray(r_times, float)\n",
    "    rr_intervals = np.asarray(rr_intervals, float)\n",
    "    # 有効なRRだけ\n",
    "    mask = np.isfinite(rr_intervals) & (rr_intervals > 0)\n",
    "    if not mask.any():\n",
    "        raise RuntimeError(f\"[ERROR] {sid} RRtime: no valid RR_interval_sec (>0)\")\n",
    "\n",
    "    r_times = r_times[mask]\n",
    "    rr_intervals = rr_intervals[mask]\n",
    "\n",
    "    t_grid = np.arange(int(t_start), int(t_end) + 1)\n",
    "\n",
    "    hr_k = 60.0 / rr_intervals\n",
    "    # 時刻順\n",
    "    order = np.argsort(r_times)\n",
    "    r_times = r_times[order]\n",
    "    hr_k = hr_k[order]\n",
    "\n",
    "    hr_1s = np.full_like(t_grid, np.nan, dtype=float)\n",
    "    k = 0\n",
    "    n = len(r_times)\n",
    "    for i, t in enumerate(t_grid):\n",
    "        # 次のR波が t 以下なら進める\n",
    "        while k + 1 < n and r_times[k + 1] <= t:\n",
    "            k += 1\n",
    "        if r_times[k] <= t:\n",
    "            hr_1s[i] = hr_k[k]\n",
    "\n",
    "    return t_grid, hr_1s\n",
    "\n",
    "\n",
    "def compute_hr_features(\n",
    "    hr_norm: np.ndarray,\n",
    "    t_grid: np.ndarray,\n",
    "    t_min: int,\n",
    "    t_max: int,\n",
    "    window_sec: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    HR_norm(1Hz) から、t=t_min..t_max について\n",
    "    ・window_sec 秒分の rma\n",
    "    ・window_sec 秒分の max/min\n",
    "    ・PC_LAG_SEC 秒前の窓との pc\n",
    "    を計算。\n",
    "    \"\"\"\n",
    "    T0 = int(t_grid[0])\n",
    "    mask = (t_grid >= t_min) & (t_grid <= t_max)\n",
    "    t_out = t_grid[mask]\n",
    "    n_out = len(t_out)\n",
    "\n",
    "    rma = np.full(n_out, np.nan, float)\n",
    "    vmax = np.full(n_out, np.nan, float)\n",
    "    vmin = np.full(n_out, np.nan, float)\n",
    "    pc = np.full(n_out, PC_DEFAULT_VALUE, float)\n",
    "\n",
    "    # 1Hzなので「window_sec サンプル」を窓にする\n",
    "    win_samples = int(window_sec)\n",
    "    lag_sec = int(PC_LAG_SEC)\n",
    "\n",
    "    # rma/max/min\n",
    "    for i, t in enumerate(t_out):\n",
    "        idx_end = int(t - T0)\n",
    "        idx_start = idx_end - win_samples + 1\n",
    "        if idx_start < 0:\n",
    "            continue\n",
    "        seg = hr_norm[idx_start: idx_end + 1]\n",
    "        valid = np.isfinite(seg)\n",
    "        if valid.any():\n",
    "            segv = seg[valid]\n",
    "            rma[i] = segv.mean()\n",
    "            vmax[i] = segv.max()\n",
    "            vmin[i] = segv.min()\n",
    "\n",
    "    # pc（lag_sec 秒前の窓との変化率）\n",
    "    for i, t in enumerate(t_out):\n",
    "        if t - lag_sec < t_min:\n",
    "            pc[i] = PC_DEFAULT_VALUE\n",
    "            continue\n",
    "        j = int((t - lag_sec) - t_min)  # t_out[j] = t - lag_sec\n",
    "        prev = rma[j]\n",
    "        cur = rma[i]\n",
    "        if np.isfinite(prev) and abs(prev) > PC_EPS and np.isfinite(cur):\n",
    "            pc[i] = (cur - prev) / prev\n",
    "        else:\n",
    "            pc[i] = PC_DEFAULT_VALUE\n",
    "\n",
    "    return t_out, rma, vmax, vmin, pc\n",
    "\n",
    "\n",
    "def forward_fill_to_1s(\n",
    "    times: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    t_start: int,\n",
    "    t_end: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    不規則（ここでは10秒刻み）な時系列を、t_start..t_end の1秒グリッドにforward fill。\n",
    "    \"\"\"\n",
    "    times = np.asarray(times, float)\n",
    "    values = np.asarray(values, float)\n",
    "    order = np.argsort(times)\n",
    "    times = times[order]\n",
    "    values = values[order]\n",
    "\n",
    "    t_grid = np.arange(int(t_start), int(t_end) + 1)\n",
    "    # 各 t について、times <= t の最後のインデックスを searchsorted で取得\n",
    "    idx = np.searchsorted(times, t_grid, side=\"right\") - 1\n",
    "    out = np.full_like(t_grid, np.nan, float)\n",
    "    valid = idx >= 0\n",
    "    out[valid] = values[idx[valid]]\n",
    "    return t_grid, out\n",
    "\n",
    "\n",
    "def compute_pc_from_mean(\n",
    "    mean_arr: np.ndarray,\n",
    "    t_grid: np.ndarray,\n",
    "    t_min: int,\n",
    "    t_max: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"3秒（PC_LAG_SEC秒）前の平均との変化率を、mean_arr(t)とt_gridから計算。\"\"\"\n",
    "    mask = (t_grid >= t_min) & (t_grid <= t_max)\n",
    "    t_out = t_grid[mask]\n",
    "    pc = np.full(t_out.shape, PC_DEFAULT_VALUE, float)\n",
    "    lag_sec = int(PC_LAG_SEC)\n",
    "\n",
    "    for i, t in enumerate(t_out):\n",
    "        if t - lag_sec < t_min:\n",
    "            pc[i] = PC_DEFAULT_VALUE\n",
    "            continue\n",
    "        j = int((t - lag_sec) - t_min)\n",
    "        prev = mean_arr[j]\n",
    "        cur = mean_arr[i]\n",
    "        if np.isfinite(prev) and abs(prev) > PC_EPS and np.isfinite(cur):\n",
    "            pc[i] = (cur - prev) / prev\n",
    "        else:\n",
    "            pc[i] = PC_DEFAULT_VALUE\n",
    "    return pc\n",
    "\n",
    "\n",
    "def build_fms_series_for_t_grid(sid: str, t_grid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"FMS_TEXT から t_grid(整数秒) 用の FMS(t) を作る。\"\"\"\n",
    "    text = FMS_TEXT[sid]\n",
    "    fms_list = [int(x) for x in text.split()]\n",
    "    if len(fms_list) != 21:\n",
    "        raise ValueError(f\"[FMS] {sid}: expected 21 values, got {len(fms_list)}\")\n",
    "    fms_arr = np.array(fms_list, int)\n",
    "    out = np.zeros_like(t_grid, int)\n",
    "    for i, t in enumerate(t_grid):\n",
    "        idx = int((t - T_START) // 30)\n",
    "        if idx < 0:\n",
    "            idx = 0\n",
    "        elif idx > 20:\n",
    "            idx = 20\n",
    "        out[i] = fms_arr[idx]\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# メイン処理\n",
    "# =========================\n",
    "\n",
    "def process_subject(sid: str) -> None:\n",
    "    print(f\"[INFO] Subject {sid} start\")\n",
    "\n",
    "    # ---- 入力パス ----\n",
    "    offset_dir = BASE_DIR / sid / \"OFFSET\"\n",
    "    feat_dir = BASE_DIR / sid / \"FEATURE\"\n",
    "    out_dir = BASE_DIR / sid / \"FEATURE2\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    path_pulse = offset_dir / f\"{sid}_Pulse.csv\"\n",
    "    path_sweat = offset_dir / f\"{sid}_Sweat.csv\"\n",
    "    path_faceA = offset_dir / f\"{sid}_FaceA.csv\"\n",
    "    path_faceB = offset_dir / f\"{sid}_FaceB.csv\"\n",
    "    path_skinos = offset_dir / f\"{sid}_Skinos.csv\"\n",
    "    path_rr = feat_dir / f\"{sid}_RRtime.csv\"\n",
    "\n",
    "    # ---- 出力時刻グリッド（窓終端） ----\n",
    "    t_out = np.arange(T_MIN_OUT, T_MAX_OUT + 1, int(SLIDE_STEP_SEC))  # 1773..2400\n",
    "\n",
    "    # ---- FMS 列 ----\n",
    "    fms_out = build_fms_series_for_t_grid(sid, t_out)\n",
    "\n",
    "    # =====================\n",
    "    # Pulse: 1000Hz  (仕様2: [1770,2400]にデータがなければ即エラー)\n",
    "    # =====================\n",
    "    df_pulse = pd.read_csv(path_pulse)\n",
    "    df_pulse = df_pulse[(df_pulse[\"Time_sec\"] >= T_START) & (df_pulse[\"Time_sec\"] <= T_END)].copy()\n",
    "    if df_pulse.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} Pulse: no data in [{T_START}, {T_END}]\")\n",
    "    df_pulse = df_pulse.sort_values(\"Time_sec\")\n",
    "    pulse_norm = minmax_scale(df_pulse[\"Pulse\"].to_numpy(), \"Pulse\", sid)\n",
    "    times_pulse = df_pulse[\"Time_sec\"].to_numpy()\n",
    "    pulse_mean3, pulse_max3, pulse_min3 = compute_window_features_continuous(\n",
    "        times_pulse, pulse_norm, t_out, window_len=WINDOW_SEC\n",
    "    )\n",
    "    pulse_pc3 = compute_pc_from_mean(pulse_mean3, t_out, T_MIN_OUT, T_MAX_OUT)\n",
    "\n",
    "    # =====================\n",
    "    # Sweat (GSR): 1000Hz  (仕様2)\n",
    "    # =====================\n",
    "    df_sweat = pd.read_csv(path_sweat)\n",
    "    df_sweat = df_sweat[(df_sweat[\"Time_sec\"] >= T_START) & (df_sweat[\"Time_sec\"] <= T_END)].copy()\n",
    "    if df_sweat.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} Sweat: no data in [{T_START}, {T_END}]\")\n",
    "    df_sweat = df_sweat.sort_values(\"Time_sec\")\n",
    "    gsr_norm = minmax_scale(df_sweat[\"Sweat\"].to_numpy(), \"Sweat\", sid)\n",
    "    times_gsr = df_sweat[\"Time_sec\"].to_numpy()\n",
    "    gsr_mean3, gsr_max3, gsr_min3 = compute_window_features_continuous(\n",
    "        times_gsr, gsr_norm, t_out, window_len=WINDOW_SEC\n",
    "    )\n",
    "    gsr_pc3 = compute_pc_from_mean(gsr_mean3, t_out, T_MIN_OUT, T_MAX_OUT)\n",
    "\n",
    "    # =====================\n",
    "    # FaceA/B: 15Hz  (仕様2 & 仕様3)\n",
    "    # =====================\n",
    "    df_faceA = pd.read_csv(path_faceA)\n",
    "    df_faceB = pd.read_csv(path_faceB)\n",
    "    df_faceA = df_faceA[(df_faceA[\"Time_sec\"] >= T_START) & (df_faceA[\"Time_sec\"] <= T_END)].copy()\n",
    "    df_faceB = df_faceB[(df_faceB[\"Time_sec\"] >= T_START) & (df_faceB[\"Time_sec\"] <= T_END)].copy()\n",
    "\n",
    "    if df_faceA.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} FaceA: no data in [{T_START}, {T_END}]\")\n",
    "    if df_faceB.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} FaceB: no data in [{T_START}, {T_END}]\")\n",
    "\n",
    "    df_faceA = df_faceA.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "    df_faceB = df_faceB.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "\n",
    "    # ---- 仕様3: FaceA/B の長さ＆Time_sec一致チェック ----\n",
    "    if len(df_faceA) != len(df_faceB):\n",
    "        raise RuntimeError(f\"[ERROR] {sid} FaceA/FaceB length mismatch: \"\n",
    "                           f\"{len(df_faceA)} vs {len(df_faceB)}\")\n",
    "    if not np.allclose(df_faceA[\"Time_sec\"].to_numpy(),\n",
    "                       df_faceB[\"Time_sec\"].to_numpy()):\n",
    "        raise RuntimeError(f\"[ERROR] {sid} FaceA/FaceB Time_sec mismatch\")\n",
    "\n",
    "    # 完全一致前提で結合\n",
    "    df_face = pd.DataFrame({\n",
    "        \"Time_sec\": df_faceA[\"Time_sec\"].to_numpy(),\n",
    "        \"FaceA_BoxAve\": df_faceA[\"FaceA_BoxAve\"].to_numpy(),\n",
    "        \"FaceB_BoxAve\": df_faceB[\"FaceB_BoxAve\"].to_numpy(),\n",
    "    })\n",
    "\n",
    "    # 簡易1秒ローパス（移動平均：15サンプル窓, center=True）\n",
    "    sA = df_face[\"FaceA_BoxAve\"].astype(float)\n",
    "    sB = df_face[\"FaceB_BoxAve\"].astype(float)\n",
    "    faceA_lp = sA.rolling(window=15, center=True, min_periods=1).mean().to_numpy()\n",
    "    faceB_lp = sB.rolling(window=15, center=True, min_periods=1).mean().to_numpy()\n",
    "\n",
    "    # IQRベース外れ値除去＋線形補間\n",
    "    def clean_face_channel(arr: np.ndarray) -> np.ndarray:\n",
    "        x = arr.astype(float).copy()\n",
    "        med = np.nanmedian(x)\n",
    "        q1 = np.nanpercentile(x, 25)\n",
    "        q3 = np.nanpercentile(x, 75)\n",
    "        iqr = q3 - q1\n",
    "        if iqr <= 0:\n",
    "            s = pd.Series(x)\n",
    "            return s.interpolate(method=\"linear\", limit_direction=\"both\").to_numpy()\n",
    "        lower = med - 3 * iqr\n",
    "        upper = med + 3 * iqr\n",
    "        mask_out = (x < lower) | (x > upper)\n",
    "        x[mask_out] = np.nan\n",
    "        s = pd.Series(x)\n",
    "        x_filled = s.interpolate(method=\"linear\", limit_direction=\"both\").to_numpy()\n",
    "        return x_filled\n",
    "\n",
    "    faceA_clean = clean_face_channel(faceA_lp)\n",
    "    faceB_clean = clean_face_channel(faceB_lp)\n",
    "\n",
    "    # min-max正規化\n",
    "    faceA_norm = minmax_scale(faceA_clean, \"FaceA_BoxAve\", sid)\n",
    "    faceB_norm = minmax_scale(faceB_clean, \"FaceB_BoxAve\", sid)\n",
    "\n",
    "    # 和＆差\n",
    "    face_sum = faceA_norm + faceB_norm\n",
    "    face_diff = faceB_norm - faceA_norm\n",
    "    times_face = df_face[\"Time_sec\"].to_numpy()\n",
    "\n",
    "    # 3秒窓特徴（平均のみ使用）\n",
    "    face_sum_mean3, _, _ = compute_window_features_continuous(\n",
    "        times_face, face_sum, t_out, window_len=WINDOW_SEC\n",
    "    )\n",
    "    face_diff_mean3, _, _ = compute_window_features_continuous(\n",
    "        times_face, face_diff, t_out, window_len=WINDOW_SEC\n",
    "    )\n",
    "    face_sum_pc3 = compute_pc_from_mean(face_sum_mean3, t_out, T_MIN_OUT, T_MAX_OUT)\n",
    "    face_diff_pc3 = compute_pc_from_mean(face_diff_mean3, t_out, T_MIN_OUT, T_MAX_OUT)\n",
    "\n",
    "    # =====================\n",
    "    # RRtime -> HR_1s -> HR 3秒窓特徴  (仕様2 & 仕様4)\n",
    "    # =====================\n",
    "    df_rr = pd.read_csv(path_rr)\n",
    "    if df_rr.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} RRtime: file is empty\")\n",
    "\n",
    "    # 仕様2: [1770,2400] に R波が1つもない場合はエラー\n",
    "    df_rr_win = df_rr[(df_rr[\"Time_sec\"] >= T_START) & (df_rr[\"Time_sec\"] <= T_END)]\n",
    "    if df_rr_win.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} RRtime: no R waves in [{T_START}, {T_END}]\")\n",
    "\n",
    "    r_times = df_rr[\"Time_sec\"].to_numpy()\n",
    "    rr_int = df_rr[\"RR_interval_sec\"].to_numpy()\n",
    "    t_grid_hr, hr_1s = build_hr_1s_from_rr(r_times, rr_int, int(T_START), int(T_END), sid)\n",
    "\n",
    "    # [1770,2400]だけ抜き出してmin-max\n",
    "    hr_norm = minmax_scale(hr_1s, \"HR_1s\", sid)\n",
    "    # HR 3秒窓特徴（1Hz）\n",
    "    t_hr_out, hr_rma3, hr_max3, hr_min3, hr_pc3 = compute_hr_features(\n",
    "        hr_norm, t_grid_hr, T_MIN_OUT, T_MAX_OUT, window_sec=WINDOW_SEC\n",
    "    )\n",
    "    # t_hr_out と t_out は同じはず\n",
    "    assert np.array_equal(t_hr_out, t_out), \"[BUG] HR t_grid mismatch\"\n",
    "\n",
    "    # =====================\n",
    "    # Skinos: 10秒刻み -> 1秒刻み  (仕様2)\n",
    "    # =====================\n",
    "    df_skin = pd.read_csv(path_skinos)\n",
    "    df_skin = df_skin[(df_skin[\"Time_sec\"] >= T_START) & (df_skin[\"Time_sec\"] <= T_END)].copy()\n",
    "    if df_skin.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} Skinos: no data in [{T_START}, {T_END}]\")\n",
    "    df_skin = df_skin.sort_values(\"Time_sec\")\n",
    "\n",
    "    skin_sweat_norm = minmax_scale(df_skin[\"Sweat_Rate\"].to_numpy(), \"Skinos_Sweat_Rate\", sid)\n",
    "    skin_hr_norm = minmax_scale(df_skin[\"Heart_Rate\"].to_numpy(), \"Skinos_Heart_Rate\", sid)\n",
    "    skin_temp_norm = minmax_scale(df_skin[\"Skin_Temp\"].to_numpy(), \"Skinos_Skin_Temp\", sid)\n",
    "\n",
    "    times_skin = df_skin[\"Time_sec\"].to_numpy()\n",
    "    t_grid_skin, skin_sweat_1s = forward_fill_to_1s(times_skin, skin_sweat_norm, int(T_START), int(T_END))\n",
    "    _, skin_hr_1s = forward_fill_to_1s(times_skin, skin_hr_norm, int(T_START), int(T_END))\n",
    "    _, skin_temp_1s = forward_fill_to_1s(times_skin, skin_temp_norm, int(T_START), int(T_END))\n",
    "\n",
    "    # t_grid_skin は1770..2400なので、t_out(1773..2400)に合わせて切り出し\n",
    "    mask_skin = (t_grid_skin >= T_MIN_OUT) & (t_grid_skin <= T_MAX_OUT)\n",
    "    assert np.array_equal(t_grid_skin[mask_skin], t_out), \"[BUG] Skinos t_grid mismatch\"\n",
    "    skin_sweat_out = skin_sweat_1s[mask_skin]\n",
    "    skin_hr_out = skin_hr_1s[mask_skin]\n",
    "    skin_temp_out = skin_temp_1s[mask_skin]\n",
    "\n",
    "    # =====================\n",
    "    # DataFrame にまとめて保存\n",
    "    # =====================\n",
    "    df_out = pd.DataFrame({\n",
    "        \"Time_sec\": t_out.astype(int),\n",
    "        \"FMS\": fms_out.astype(int),\n",
    "        \"Pulse_rma3\": pulse_mean3,\n",
    "        \"Pulse_max3\": pulse_max3,\n",
    "        \"Pulse_min3\": pulse_min3,\n",
    "        \"Pulse_pc3\": pulse_pc3,\n",
    "        \"HR_rma3\": hr_rma3,\n",
    "        \"HR_max3\": hr_max3,\n",
    "        \"HR_min3\": hr_min3,\n",
    "        \"HR_pc3\": hr_pc3,\n",
    "        \"GSR_rma3\": gsr_mean3,\n",
    "        \"GSR_max3\": gsr_max3,\n",
    "        \"GSR_min3\": gsr_min3,\n",
    "        \"GSR_pc3\": gsr_pc3,\n",
    "        \"FaceSum_mean3\": face_sum_mean3,\n",
    "        \"FaceDiff_mean3\": face_diff_mean3,\n",
    "        \"FaceSum_pc3\": face_sum_pc3,\n",
    "        \"FaceDiff_pc3\": face_diff_pc3,\n",
    "        \"Skinos_SweatRate\": skin_sweat_out,\n",
    "        \"Skinos_HeartRate\": skin_hr_out,\n",
    "        \"Skinos_SkinTemp\": skin_temp_out,\n",
    "    })\n",
    "\n",
    "    out_path = out_dir / f\"{sid}_3sFeat_1sSlide.csv\"\n",
    "    df_out.to_csv(out_path, index=False)\n",
    "    print(f\"[INFO] Subject {sid} done -> {out_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for sid in SUBJECT_IDS:\n",
    "        process_subject(sid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell1-LSTM: 3秒窓特徴量LSTM（LOSO＋ROC-AUC）\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# パス・基本設定\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\")\n",
    "\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\", \"10063\", \"10064\",\n",
    "    \"10071\", \"10072\", \"10073\", \"10074\",\n",
    "    \"10081\", \"10082\", \"10083\",\n",
    "    \"10091\", \"10092\", \"10093\", \"10094\",\n",
    "    \"10101\", \"10102\", \"10103\",\n",
    "]\n",
    "\n",
    "# このCell用の出力ディレクトリ\n",
    "CELL_NAME = \"Cell1-LSTM\"\n",
    "OUT_DIR = BASE_DIR / \"解析\" / \"Cell1\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Cell: {CELL_NAME}, OUT_DIR = {OUT_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 時間・シーケンス仕様\n",
    "# -----------------------------\n",
    "WINDOW_SEC = 3          # 3秒窓（既にFEATURE2で反映済み）\n",
    "SLIDE_STEP_SEC = 1      # 1秒刻み（既にFEATURE2で反映済み）\n",
    "\n",
    "# LSTM に入れる過去ステップ数（= 過去 SEQ_LEN 秒分）\n",
    "SEQ_LEN = 30\n",
    "\n",
    "# FEATURE2 での最初の出力時刻（T_START+WINDOW_SEC = 1770+3）\n",
    "BASE_T_MIN = 1773\n",
    "\n",
    "# ターゲットの最小時刻：最初の出力時刻＋(SEQ_LEN-1)\n",
    "# 例：BASE_T_MIN=1773, SEQ_LEN=30 → 1773+29 = 1802\n",
    "TARGET_T_MIN = BASE_T_MIN + (SEQ_LEN - 1)\n",
    "TARGET_T_MAX = 2400     # 上限はこれまで通り 2400 秒\n",
    "\n",
    "# ラベル閾値：FMS >= 1 を陽性とする\n",
    "FMS_POS_THRESHOLD = 1\n",
    "\n",
    "# -----------------------------\n",
    "# LSTMハイパラ（変更候補は CSV に出力）\n",
    "# -----------------------------\n",
    "HIDDEN_SIZE = 32\n",
    "FC_HIDDEN_SIZE = 8\n",
    "DROPOUT_LSTM = 0.0\n",
    "DROPOUT_FC = 0.5\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 10\n",
    "WEIGHT_DECAY = 1e-4  # L2正則化（Adam の weight_decay）\n",
    "\n",
    "# -----------------------------\n",
    "# 特徴量ON/OFF設定\n",
    "# -----------------------------\n",
    "FEATURE_SWITCHES: List[Tuple[str, bool]] = [\n",
    "    (\"Pulse_rma3\",       True),\n",
    "    (\"Pulse_max3\",       True),\n",
    "    (\"Pulse_min3\",       True),\n",
    "    (\"Pulse_pc3\",        True),\n",
    "    (\"HR_rma3\",          True),\n",
    "    (\"HR_max3\",          True),\n",
    "    (\"HR_min3\",          True),\n",
    "    (\"HR_pc3\",           True),\n",
    "    (\"GSR_rma3\",         True),\n",
    "    (\"GSR_max3\",         True),\n",
    "    (\"GSR_min3\",         True),\n",
    "    (\"GSR_pc3\",          True),\n",
    "    (\"FaceSum_mean3\",    True),\n",
    "    (\"FaceDiff_mean3\",   True),\n",
    "    (\"FaceSum_pc3\",      True),\n",
    "    (\"FaceDiff_pc3\",     True),\n",
    "    (\"Skinos_SweatRate\", True),\n",
    "    (\"Skinos_HeartRate\", True),\n",
    "    (\"Skinos_SkinTemp\",  True),\n",
    "]\n",
    "\n",
    "FEATURE_COLS: List[str] = [name for name, use in FEATURE_SWITCHES if use]\n",
    "if len(FEATURE_COLS) == 0:\n",
    "    raise RuntimeError(\"[ERROR] FEATURE_SWITCHES: 有効な特徴量が0個です（すべてFalse）。\")\n",
    "\n",
    "N_FEATURES = len(FEATURE_COLS)\n",
    "print(f\"[INFO] Using {N_FEATURES} features:\", \", \".join(FEATURE_COLS))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM モデル定義\n",
    "# -----------------------------\n",
    "class LSTMMotionSickness(nn.Module):\n",
    "    \"\"\"\n",
    "    単方向1層LSTM → Dropout → FC(HIDDEN_SIZE→FC_HIDDEN_SIZE) → ReLU → FC → ロジット\n",
    "    出力はロジット（Sigmoidはloss/評価側で適用）\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = HIDDEN_SIZE,\n",
    "        fc_hidden_size: int = FC_HIDDEN_SIZE,\n",
    "        dropout_lstm: float = DROPOUT_LSTM,\n",
    "        dropout_fc: float = DROPOUT_FC,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout_lstm,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_fc)\n",
    "        self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(fc_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size)\n",
    "        return: ロジット (batch,)\n",
    "        \"\"\"\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        h_last = hn[-1]              # (batch, hidden_size)\n",
    "        z = self.dropout(h_last)\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.fc_out(z)           # (batch, 1)\n",
    "        return z.squeeze(-1)         # (batch,)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# データ読み込み & シーケンス生成\n",
    "# -----------------------------\n",
    "def load_subject_df(sid: str) -> pd.DataFrame:\n",
    "    \"\"\"FEATURE2/{sid}_3sFeat_1sSlide.csv を読み込む.\"\"\"\n",
    "    path = BASE_DIR / sid / \"FEATURE2\" / f\"{sid}_3sFeat_1sSlide.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"[ERROR] Subject {sid}: file not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sequences_for_subject(\n",
    "    sid: str,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    1被験者について:\n",
    "      - FEATURE2 CSVを読み込み\n",
    "      - FMS>=1 を陽性にした y(t) を作成\n",
    "      - t=TARGET_T_MIN〜TARGET_T_MAX の各時刻 t に対し，\n",
    "          X_seq(t) = [t-SEQ_LEN+1 .. t] のシーケンスを生成\n",
    "      - その際，特徴量内にNaNがあれば即エラー\n",
    "    \"\"\"\n",
    "    df = load_subject_df(sid)\n",
    "\n",
    "    # 必要列が揃っているかチェック\n",
    "    required_cols = [\"Time_sec\", \"FMS\"] + FEATURE_COLS\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: missing columns in FEATURE2 csv: {missing}\")\n",
    "\n",
    "    # NaNチェック（仕様：NaNがあれば即エラー）\n",
    "    if df[FEATURE_COLS].isna().values.any():\n",
    "        nan_mask = df[FEATURE_COLS].isna()\n",
    "        bad_idx = np.where(nan_mask.values)[0][0]\n",
    "        bad_time = df.loc[bad_idx, \"Time_sec\"]\n",
    "        bad_cols = list(nan_mask.columns[nan_mask.iloc[bad_idx]])\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Subject {sid}: NaN detected at Time_sec={bad_time}, cols={bad_cols}\"\n",
    "        )\n",
    "\n",
    "    times = df[\"Time_sec\"].to_numpy().astype(int)\n",
    "    fms = df[\"FMS\"].to_numpy().astype(int)\n",
    "    features = df[FEATURE_COLS].to_numpy().astype(np.float32)\n",
    "\n",
    "    # TARGET_T_MIN〜TARGET_T_MAX の範囲があるか\n",
    "    target_mask = (times >= TARGET_T_MIN) & (times <= TARGET_T_MAX)\n",
    "    if not target_mask.any():\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: no Time_sec in [{TARGET_T_MIN}, {TARGET_T_MAX}]\")\n",
    "\n",
    "    X_list: List[np.ndarray] = []\n",
    "    y_list: List[int] = []\n",
    "    t_list: List[int] = []\n",
    "    fms_list: List[int] = []\n",
    "\n",
    "    for idx in range(len(times)):\n",
    "        t = times[idx]\n",
    "        if t < TARGET_T_MIN or t > TARGET_T_MAX:\n",
    "            continue\n",
    "\n",
    "        if idx < SEQ_LEN - 1:\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: idx={idx}, Time_sec={t} has no enough history (need {SEQ_LEN}).\"\n",
    "            )\n",
    "\n",
    "        window_feat = features[idx - SEQ_LEN + 1: idx + 1, :]  # (SEQ_LEN, N_FEATURES)\n",
    "        if not np.isfinite(window_feat).all():\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: non-finite value in sequence ending at Time_sec={t}\"\n",
    "            )\n",
    "\n",
    "        # ラベル：FMS>=1\n",
    "        y = 1 if fms[idx] >= FMS_POS_THRESHOLD else 0\n",
    "\n",
    "        X_list.append(window_feat)\n",
    "        y_list.append(y)\n",
    "        t_list.append(t)\n",
    "        fms_list.append(int(fms[idx]))\n",
    "\n",
    "    X_seq = np.stack(X_list).astype(np.float32)   # (N_seq, SEQ_LEN, N_FEATURES)\n",
    "    y_seq = np.array(y_list, dtype=np.int64)\n",
    "    t_seq = np.array(t_list, dtype=np.int64)\n",
    "    fms_seq = np.array(fms_list, dtype=np.int64)\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] Subject {sid}: target Time_sec range = {t_seq[0]}–{t_seq[-1]}, \"\n",
    "        f\"N_seq = {len(t_seq)}, N_pos = {y_seq.sum()}, N_neg = {len(y_seq) - y_seq.sum()}\"\n",
    "    )\n",
    "\n",
    "    return X_seq, y_seq, t_seq, fms_seq\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LOSO 学習・評価ループ\n",
    "# -----------------------------\n",
    "def train_one_fold(\n",
    "    train_X: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    device: torch.device,\n",
    ") -> Tuple[LSTMMotionSickness, List[float]]:\n",
    "    \"\"\"\n",
    "    1つのLOSO foldについて，訓練データのみを使ってLSTMを学習する。\n",
    "    戻り値: (学習済みモデル, 各epochの平均train lossリスト)\n",
    "    \"\"\"\n",
    "    model = LSTMMotionSickness(input_size=N_FEATURES).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # 陽性割合をプリント\n",
    "    n_train = len(train_y)\n",
    "    n_pos = int(train_y.sum())\n",
    "    n_neg = n_train - n_pos\n",
    "    pos_ratio = n_pos / n_train if n_train > 0 else 0.0\n",
    "    print(\n",
    "        f\"[INFO] Train stats: N={n_train}, N_pos={n_pos}, N_neg={n_neg}, \"\n",
    "        f\"pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    # DataLoader 構築\n",
    "    X_tensor = torch.from_numpy(train_X).float()\n",
    "    y_tensor = torch.from_numpy(train_y).float()\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    epoch_loss_list: List[float] = []\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)           # (batch,)\n",
    "            loss = criterion(logits, batch_y) # BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / max(n_batches, 1)\n",
    "        epoch_loss_list.append(avg_loss)\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == N_EPOCHS:\n",
    "            print(f\"[INFO] Epoch {epoch:02d}/{N_EPOCHS} - train_loss={avg_loss:.4f}\")\n",
    "\n",
    "    return model, epoch_loss_list\n",
    "\n",
    "\n",
    "def main():\n",
    "    # デバイス選択\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    # 再現性のためにseed固定\n",
    "    torch.manual_seed(20251206)\n",
    "    np.random.seed(20251206)\n",
    "\n",
    "    # ---- 全被験者のシーケンスを構築 ----\n",
    "    X_by_sid: Dict[str, np.ndarray] = {}\n",
    "    y_by_sid: Dict[str, np.ndarray] = {}\n",
    "    t_by_sid: Dict[str, np.ndarray] = {}\n",
    "    fms_by_sid: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        print(f\"[INFO] ==== Build sequences: Subject {sid} ====\")\n",
    "        X_seq, y_seq, t_seq, fms_seq = build_sequences_for_subject(sid)\n",
    "        X_by_sid[sid] = X_seq\n",
    "        y_by_sid[sid] = y_seq\n",
    "        t_by_sid[sid] = t_seq\n",
    "        fms_by_sid[sid] = fms_seq\n",
    "\n",
    "    all_y_tmp = np.concatenate([y_by_sid[sid] for sid in SUBJECT_IDS])\n",
    "    print(\n",
    "        f\"[INFO] Overall (all subjects) target stats: \"\n",
    "        f\"N={len(all_y_tmp)}, N_pos={all_y_tmp.sum()}, \"\n",
    "        f\"pos_ratio={all_y_tmp.mean():.3f}\"\n",
    "    )\n",
    "\n",
    "    # ---- LOSO 学習・評価 ----\n",
    "    all_probs: List[np.ndarray] = []\n",
    "    all_true: List[np.ndarray] = []\n",
    "    pred_rows: List[pd.DataFrame] = []\n",
    "    fold_summary_rows: List[Dict] = []\n",
    "    epoch_loss_records: List[Dict] = []\n",
    "\n",
    "    for test_sid in SUBJECT_IDS:\n",
    "        print(f\"\\n[INFO] ===== LOSO fold: Test Subject {test_sid} =====\")\n",
    "\n",
    "        # 学習・テスト分割\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "        for sid in SUBJECT_IDS:\n",
    "            if sid == test_sid:\n",
    "                continue\n",
    "            train_X_list.append(X_by_sid[sid])\n",
    "            train_y_list.append(y_by_sid[sid])\n",
    "\n",
    "        train_X = np.concatenate(train_X_list, axis=0)\n",
    "        train_y = np.concatenate(train_y_list, axis=0)\n",
    "        test_X = X_by_sid[test_sid]\n",
    "        test_y = y_by_sid[test_sid]\n",
    "        test_t = t_by_sid[test_sid]\n",
    "        test_fms = fms_by_sid[test_sid]\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Fold data sizes: \"\n",
    "            f\"Train N_seq={len(train_y)}, Test N_seq={len(test_y)}\"\n",
    "        )\n",
    "\n",
    "        # 1 fold 学習\n",
    "        model, epoch_loss_list = train_one_fold(train_X, train_y, device=device)\n",
    "\n",
    "        # epochごとの loss をログ用に保存\n",
    "        for ep_idx, loss_val in enumerate(epoch_loss_list, start=1):\n",
    "            epoch_loss_records.append(\n",
    "                {\n",
    "                    \"SubjectID\": test_sid,\n",
    "                    \"Epoch\": ep_idx,\n",
    "                    \"TrainLoss\": loss_val,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # テスト被験者の予測確率\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "            logits = model(X_test_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()  # (N_test,)\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_true.append(test_y.astype(int))\n",
    "\n",
    "        # foldごとのROC-AUC\n",
    "        n_pos_test = int(test_y.sum())\n",
    "        n_neg_test = int(len(test_y) - n_pos_test)\n",
    "        if n_pos_test == 0 or n_neg_test == 0:\n",
    "            rocauc_fold = float(\"nan\")\n",
    "            print(\n",
    "                f\"[INFO] Subject {test_sid}: ROC-AUC undefined (N_pos={n_pos_test}, N_neg={n_neg_test})\"\n",
    "            )\n",
    "        else:\n",
    "            rocauc_fold = roc_auc_score(test_y, probs)\n",
    "            print(\n",
    "                f\"[INFO] Subject {test_sid}: ROC-AUC(test fold) = {rocauc_fold:.4f} \"\n",
    "                f\"(N_test={len(test_y)}, N_pos={n_pos_test}, N_neg={n_neg_test})\"\n",
    "            )\n",
    "\n",
    "        fold_summary_rows.append(\n",
    "            {\n",
    "                \"SubjectID\": test_sid,\n",
    "                \"N_test\": int(len(test_y)),\n",
    "                \"N_pos_test\": n_pos_test,\n",
    "                \"N_neg_test\": n_neg_test,\n",
    "                \"pos_ratio_test\": float(test_y.mean()),\n",
    "                \"ROC_AUC_test\": rocauc_fold,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # このfoldの予測詳細\n",
    "        df_fold = pd.DataFrame(\n",
    "            {\n",
    "                \"SubjectID\": test_sid,\n",
    "                \"Time_sec\": test_t,\n",
    "                \"FMS\": test_fms,\n",
    "                \"Label_bin\": test_y.astype(int),\n",
    "                \"Prob_FMS_ge1\": probs,\n",
    "            }\n",
    "        )\n",
    "        pred_rows.append(df_fold)\n",
    "\n",
    "    # ---- 全foldをまとめた ROC-AUC ----\n",
    "    y_all = np.concatenate(all_true)\n",
    "    p_all = np.concatenate(all_probs)\n",
    "\n",
    "    n_total = len(y_all)\n",
    "    n_pos = int(y_all.sum())\n",
    "    n_neg = n_total - n_pos\n",
    "    pos_ratio = n_pos / n_total if n_total > 0 else 0.0\n",
    "\n",
    "    print(\"\\n[INFO] ===== Overall LOSO result =====\")\n",
    "    print(\n",
    "        f\"[INFO] All folds combined: N={n_total}, N_pos={n_pos}, \"\n",
    "        f\"N_neg={n_neg}, pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    if n_pos == 0 or n_pos == n_total:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] ROC-AUC undefined: labels are all the same \"\n",
    "            f\"(N={n_total}, N_pos={n_pos}).\"\n",
    "        )\n",
    "\n",
    "    rocauc = roc_auc_score(y_all, p_all)\n",
    "    print(f\"[RESULT] Global ROC-AUC (LOSO, LSTM, FMS>=1) = {rocauc:.4f}\")\n",
    "\n",
    "    # ---- 結果保存 ----\n",
    "    # 1) ROC-AUC のサマリ（ハイパラ込み）\n",
    "    result_path = OUT_DIR / \"Cell1_LSTM_LOSO_ROCAUC.csv\"\n",
    "    df_result = pd.DataFrame(\n",
    "        {\n",
    "            \"ROC_AUC_global\": [rocauc],\n",
    "            \"N_total\": [n_total],\n",
    "            \"N_pos\": [n_pos],\n",
    "            \"N_neg\": [n_neg],\n",
    "            \"pos_ratio\": [pos_ratio],\n",
    "            \"N_features\": [N_FEATURES],\n",
    "            \"feature_list\": [\",\".join(FEATURE_COLS)],\n",
    "            # 変更候補ハイパラを全部記録\n",
    "            \"WINDOW_SEC\": [WINDOW_SEC],\n",
    "            \"SLIDE_STEP_SEC\": [SLIDE_STEP_SEC],\n",
    "            \"SEQ_LEN\": [SEQ_LEN],\n",
    "            \"HIDDEN_SIZE\": [HIDDEN_SIZE],\n",
    "            \"FC_HIDDEN_SIZE\": [FC_HIDDEN_SIZE],\n",
    "            \"DROPOUT_LSTM\": [DROPOUT_LSTM],\n",
    "            \"DROPOUT_FC\": [DROPOUT_FC],\n",
    "            \"LEARNING_RATE\": [LEARNING_RATE],\n",
    "            \"BATCH_SIZE\": [BATCH_SIZE],\n",
    "            \"N_EPOCHS\": [N_EPOCHS],\n",
    "            \"WEIGHT_DECAY\": [WEIGHT_DECAY],\n",
    "        }\n",
    "    )\n",
    "    df_result.to_csv(result_path, index=False)\n",
    "    print(f\"[INFO] Saved ROC-AUC result to: {result_path}\")\n",
    "\n",
    "    # 2) シーケンスごとの詳細予測\n",
    "    df_pred = pd.concat(pred_rows, ignore_index=True)\n",
    "    pred_path = OUT_DIR / \"Cell1_LSTM_LOSO_pred_detail.csv\"\n",
    "    df_pred.to_csv(pred_path, index=False)\n",
    "    print(f\"[INFO] Saved per-sequence predictions to: {pred_path}\")\n",
    "\n",
    "    # 3) foldごとの summary（被験者別 ROC-AUC）\n",
    "    df_fold_summary = pd.DataFrame(fold_summary_rows)\n",
    "    fold_summary_path = OUT_DIR / \"Cell1_LSTM_LOSO_fold_summary.csv\"\n",
    "    df_fold_summary.to_csv(fold_summary_path, index=False)\n",
    "    print(f\"[INFO] Saved per-fold summary to: {fold_summary_path}\")\n",
    "\n",
    "    # 4) epochごとの train loss\n",
    "    df_loss = pd.DataFrame(epoch_loss_records)\n",
    "    loss_path = OUT_DIR / \"Cell1_LSTM_LOSO_train_loss_by_epoch.csv\"\n",
    "    df_loss.to_csv(loss_path, index=False)\n",
    "    print(f\"[INFO] Saved train loss by epoch to: {loss_path}\")\n",
    "\n",
    "    # ---- 最後に、被験者ごとのROC-AUCを()付きでプリント ----\n",
    "    print(\"\\n[SUMMARY] ===== Per-subject ROC-AUC (LOSO) =====\")\n",
    "    print(f\"[SUMMARY] Global ROC-AUC (all folds combined) = {rocauc:.4f}\")\n",
    "\n",
    "    good_mask = df_fold_summary[\"ROC_AUC_test\"].notna() & (df_fold_summary[\"ROC_AUC_test\"] > 0.5)\n",
    "    bad_mask = df_fold_summary[\"ROC_AUC_test\"].notna() & (df_fold_summary[\"ROC_AUC_test\"] <= 0.5)\n",
    "    nan_mask = df_fold_summary[\"ROC_AUC_test\"].isna()\n",
    "\n",
    "    def format_sid_list(mask) -> str:\n",
    "        rows = df_fold_summary.loc[mask, [\"SubjectID\", \"ROC_AUC_test\"]]\n",
    "        if rows.empty:\n",
    "            return \"なし\"\n",
    "        return \", \".join(f\"{row.SubjectID}({row.ROC_AUC_test:.3f})\" for _, row in rows.iterrows())\n",
    "\n",
    "    good_str = format_sid_list(good_mask)\n",
    "    bad_str = format_sid_list(bad_mask)\n",
    "    nan_sids = df_fold_summary.loc[nan_mask, \"SubjectID\"].tolist()\n",
    "    nan_str = \", \".join(nan_sids) if len(nan_sids) > 0 else \"なし\"\n",
    "\n",
    "    print(f\"[SUMMARY] よく当たっている被験者(>0.5): {good_str}\")\n",
    "    print(f\"[SUMMARY] あまり当たっていない被験者(<=0.5): {bad_str}\")\n",
    "    print(f\"[SUMMARY] 評価不能(ROC-AUC算出不可): {nan_str}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell2-LSTM-SHAP: LSTM（30秒履歴, 19特徴量）のSHAP重要度可視化#\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# 前提チェック（Cell1-LSTM 実行済み想定）\n",
    "# --------------------------------\n",
    "required_globals = [\n",
    "    \"BASE_DIR\",\n",
    "    \"SUBJECT_IDS\",\n",
    "    \"FEATURE_COLS\",\n",
    "    \"SEQ_LEN\",\n",
    "    \"TARGET_T_MIN\",\n",
    "    \"TARGET_T_MAX\",\n",
    "    \"FMS_POS_THRESHOLD\",\n",
    "    \"LSTMMotionSickness\",\n",
    "    \"build_sequences_for_subject\",\n",
    "    \"HIDDEN_SIZE\",\n",
    "    \"FC_HIDDEN_SIZE\",\n",
    "    \"DROPOUT_LSTM\",\n",
    "    \"DROPOUT_FC\",\n",
    "    \"LEARNING_RATE\",\n",
    "    \"BATCH_SIZE\",\n",
    "    \"N_EPOCHS\",\n",
    "]\n",
    "missing = [name for name in required_globals if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"[Cell2-LSTM-SHAP] 必要な定義が見つかりません。\"\n",
    "        \"先に Cell1-LSTM を同じノートブック上で実行してください。\\n\"\n",
    "        f\"不足: {missing}\"\n",
    "    )\n",
    "\n",
    "N_FEATURES = len(FEATURE_COLS)\n",
    "\n",
    "# このCell用の出力ディレクトリ\n",
    "CELL_NAME = \"Cell2-LSTM-SHAP\"\n",
    "OUT_DIR = BASE_DIR / \"解析\" / \"Cell2\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO][SHAP] Cell: {CELL_NAME}, OUT_DIR = {OUT_DIR}\")\n",
    "\n",
    "# SHAP用のサンプル数設定\n",
    "N_BACKGROUND_MAX = 200   # 各foldで DeepExplainer の背景に使う最大サンプル数\n",
    "N_SHAP_EVAL_MAX = 2000   # 各foldで SHAP を計算する最大サンプル数（訓練シーケンス）\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# SHAP用ラッパーモデル\n",
    "#   base_model: (batch, seq_len, feat) -> (batch,)\n",
    "#   → DeepExplainer 用に (batch, 1) に変形\n",
    "# --------------------------------\n",
    "class WrappedLSTMForSHAP(nn.Module):\n",
    "    \"\"\"\n",
    "    SHAP用ラッパー:\n",
    "    - base_model の出力 (batch,) を (batch, 1) に変形して返す\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.base_model(x)      # (batch,)\n",
    "        return logits.unsqueeze(1)       # (batch, 1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1 fold 学習（Cell1と同仕様）\n",
    "# -----------------------------\n",
    "def train_one_fold_for_shap(\n",
    "    train_X: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    device: torch.device,\n",
    ") -> \"LSTMMotionSickness\":\n",
    "    \"\"\"\n",
    "    SHAP用：Cell1-LSTM と同じ設定で1fold分のLSTMを学習する。\n",
    "    \"\"\"\n",
    "    model = LSTMMotionSickness(input_size=N_FEATURES).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    n_train = len(train_y)\n",
    "    n_pos = int(train_y.sum())\n",
    "    n_neg = n_train - n_pos\n",
    "    pos_ratio = n_pos / n_train if n_train > 0 else 0.0\n",
    "    print(\n",
    "        f\"[INFO][SHAP] Train stats: N={n_train}, N_pos={n_pos}, N_neg={n_neg}, \"\n",
    "        f\"pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    X_tensor = torch.from_numpy(train_X).float()\n",
    "    y_tensor = torch.from_numpy(train_y).float()\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / max(n_batches, 1)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == N_EPOCHS:\n",
    "            print(f\"[INFO][SHAP] Epoch {epoch:02d}/{N_EPOCHS} - train_loss={avg_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# メイン処理（LOSO＋SHAP）\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # デバイス選択\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO][SHAP] Using device: {device}\")\n",
    "\n",
    "    torch.manual_seed(20251206)\n",
    "    np.random.seed(20251206)\n",
    "\n",
    "    # --- 全被験者のシーケンス構築（Cell1と同じ関数を再利用） ---\n",
    "    X_by_sid: Dict[str, np.ndarray] = {}\n",
    "    y_by_sid: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        print(f\"[INFO][SHAP] ==== Build sequences: Subject {sid} ====\")\n",
    "        X_seq, y_seq, t_seq, fms_seq = build_sequences_for_subject(sid)\n",
    "        X_by_sid[sid] = X_seq\n",
    "        y_by_sid[sid] = y_seq\n",
    "\n",
    "    # 全体の陽性割合（参考）\n",
    "    all_y_tmp = np.concatenate([y_by_sid[sid] for sid in SUBJECT_IDS])\n",
    "    print(\n",
    "        f\"[INFO][SHAP] Overall (all subjects) target stats: \"\n",
    "        f\"N={len(all_y_tmp)}, N_pos={all_y_tmp.sum()}, \"\n",
    "        f\"pos_ratio={all_y_tmp.mean():.3f}\"\n",
    "    )\n",
    "\n",
    "    # --- LOSO 各foldで SHAP を計算 ---\n",
    "    fold_importances: List[np.ndarray] = []\n",
    "    fold_meta: List[Dict[str, str]] = []\n",
    "\n",
    "    # beeswarm 用：各foldの shap と特徴量値（時間平均）を保持\n",
    "    shap_samples_list: List[np.ndarray] = []\n",
    "    feature_values_list: List[np.ndarray] = []\n",
    "\n",
    "    for fold_idx, test_sid in enumerate(SUBJECT_IDS):\n",
    "        print(f\"\\n[INFO][SHAP] ===== LOSO fold {fold_idx+1}/{len(SUBJECT_IDS)}: Test {test_sid} =====\")\n",
    "\n",
    "        # 学習データ（LOSO）\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "        for sid in SUBJECT_IDS:\n",
    "            if sid == test_sid:\n",
    "                continue\n",
    "            train_X_list.append(X_by_sid[sid])\n",
    "            train_y_list.append(y_by_sid[sid])\n",
    "\n",
    "        train_X = np.concatenate(train_X_list, axis=0)  # (N_train, SEQ_LEN, N_FEATURES)\n",
    "        train_y = np.concatenate(train_y_list, axis=0)  # (N_train,)\n",
    "\n",
    "        print(\n",
    "            f\"[INFO][SHAP] Fold data sizes: \"\n",
    "            f\"Train N_seq={len(train_y)}, Test N_seq={len(y_by_sid[test_sid])}\"\n",
    "        )\n",
    "\n",
    "        # --- モデル学習 ---\n",
    "        model = train_one_fold_for_shap(train_X, train_y, device=device)\n",
    "        model.eval()\n",
    "\n",
    "        # --- SHAP用の背景データ（background）をサンプリング ---\n",
    "        n_train = train_X.shape[0]\n",
    "        if n_train > N_BACKGROUND_MAX:\n",
    "            idx_bg = np.random.choice(n_train, size=N_BACKGROUND_MAX, replace=False)\n",
    "            bg_X = train_X[idx_bg]\n",
    "        else:\n",
    "            bg_X = train_X\n",
    "\n",
    "        print(f\"[INFO][SHAP] Using {len(bg_X)} samples as background\")\n",
    "        background = torch.from_numpy(bg_X).float().to(device)\n",
    "\n",
    "        # --- SHAPを計算する対象サンプル（訓練シーケンスのサブセット） ---\n",
    "        if n_train > N_SHAP_EVAL_MAX:\n",
    "            idx_eval = np.random.choice(n_train, size=N_SHAP_EVAL_MAX, replace=False)\n",
    "            X_eval = train_X[idx_eval]\n",
    "        else:\n",
    "            X_eval = train_X\n",
    "\n",
    "        print(f\"[INFO][SHAP] Computing SHAP on {len(X_eval)} training sequences\")\n",
    "        X_eval_tensor = torch.from_numpy(X_eval).float().to(device)\n",
    "\n",
    "        # --- DeepExplainer を構築 ---\n",
    "        wrapped_model = WrappedLSTMForSHAP(model).to(device)\n",
    "        explainer = shap.DeepExplainer(wrapped_model, background)\n",
    "\n",
    "        # shap_values: (N_eval, SEQ_LEN, N_FEATURES, [output_dim]) か，\n",
    "        # それを要素に持つリスト\n",
    "        # ★ additivity チェックをオフ（RNN系でよく落ちるので）\n",
    "        shap_values = explainer.shap_values(X_eval_tensor, check_additivity=False)\n",
    "\n",
    "        # 戻り値の型に応じて整形\n",
    "        if isinstance(shap_values, list):\n",
    "            sv = shap_values[0]\n",
    "        else:\n",
    "            sv = shap_values\n",
    "\n",
    "        if isinstance(sv, torch.Tensor):\n",
    "            sv = sv.detach().cpu().numpy()\n",
    "        else:\n",
    "            sv = np.array(sv)\n",
    "\n",
    "        # 出力次元が最後に1つだけ付いている場合は squeeze\n",
    "        # 例: (N_eval, SEQ_LEN, N_FEATURES, 1) → (N_eval, SEQ_LEN, N_FEATURES)\n",
    "        if sv.ndim == 4 and sv.shape[-1] == 1:\n",
    "            sv = sv[..., 0]\n",
    "\n",
    "        # 形チェック\n",
    "        if sv.ndim != 3 or sv.shape[1] != SEQ_LEN or sv.shape[2] != N_FEATURES:\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR][SHAP] Unexpected SHAP shape: {sv.shape}, \"\n",
    "                f\"expected (N_eval, {SEQ_LEN}, {N_FEATURES})\"\n",
    "            )\n",
    "\n",
    "        # --- beeswarm 用：時間方向で平均して 2次元にする ---\n",
    "        # sv_mean_t: (N_eval, N_FEATURES) … 30秒履歴の平均寄与\n",
    "        sv_mean_t = sv.mean(axis=1)\n",
    "\n",
    "        # 特徴量値も時間平均をとる（色付け用）\n",
    "        X_eval_np = X_eval_tensor.detach().cpu().numpy()  # (N_eval, SEQ_LEN, N_FEATURES)\n",
    "        X_mean_t = X_eval_np.mean(axis=1)                 # (N_eval, N_FEATURES)\n",
    "\n",
    "        shap_samples_list.append(sv_mean_t)\n",
    "        feature_values_list.append(X_mean_t)\n",
    "\n",
    "        # --- fold内の特徴重要度：mean(|SHAP|) over (samples, time) ---\n",
    "        abs_sv = np.abs(sv)\n",
    "        imp_fold = abs_sv.mean(axis=(0, 1))  # (N_FEATURES,)\n",
    "        fold_importances.append(imp_fold)\n",
    "        fold_meta.append({\"fold_idx\": fold_idx, \"test_subject\": test_sid})\n",
    "\n",
    "        # foldごとのトップ特徴をざっくり表示\n",
    "        order = np.argsort(-imp_fold)  # 降順\n",
    "        top_k = min(5, N_FEATURES)\n",
    "        print(\"[INFO][SHAP] Top features in this fold:\")\n",
    "        for i in range(top_k):\n",
    "            j = order[i]\n",
    "            print(f\"  {i+1}. {FEATURE_COLS[j]} : mean|SHAP| = {imp_fold[j]:.4e}\")\n",
    "\n",
    "    # --- fold間で平均して最終重要度を算出 ---\n",
    "    imp_mat = np.stack(fold_importances, axis=0)  # (n_folds, N_FEATURES)\n",
    "    mean_imp = imp_mat.mean(axis=0)               # (N_FEATURES,)\n",
    "    std_imp = imp_mat.std(axis=0)                 # (N_FEATURES,)\n",
    "\n",
    "    order_global = np.argsort(-mean_imp)          # 降順\n",
    "\n",
    "    # ランキング表を作成\n",
    "    rows = []\n",
    "    for rank, idx in enumerate(order_global, start=1):\n",
    "        rows.append(\n",
    "            {\n",
    "                \"rank\": rank,\n",
    "                \"feature\": FEATURE_COLS[idx],\n",
    "                \"mean_abs_shap\": float(mean_imp[idx]),\n",
    "                \"std_abs_shap\": float(std_imp[idx]),\n",
    "            }\n",
    "        )\n",
    "    df_rank = pd.DataFrame(rows)\n",
    "\n",
    "    # --- CSV保存（全体ランキング） ---\n",
    "    rank_path = OUT_DIR / \"Cell2_LSTM_SHAP_feature_importance.csv\"\n",
    "    df_rank.to_csv(rank_path, index=False)\n",
    "    print(f\"[INFO][SHAP] Saved SHAP feature ranking to: {rank_path}\")\n",
    "\n",
    "    # fold別の重要度行列も保存\n",
    "    df_fold_imp = pd.DataFrame(\n",
    "        imp_mat,\n",
    "        columns=[f\"SHAP_{name}\" for name in FEATURE_COLS],\n",
    "    )\n",
    "    df_fold_imp.insert(0, \"test_subject\", [m[\"test_subject\"] for m in fold_meta])\n",
    "    df_fold_imp.insert(0, \"fold_idx\", [m[\"fold_idx\"] for m in fold_meta])\n",
    "    fold_imp_path = OUT_DIR / \"Cell2_LSTM_SHAP_feature_importance_by_fold.csv\"\n",
    "    df_fold_imp.to_csv(fold_imp_path, index=False)\n",
    "    print(f\"[INFO][SHAP] Saved fold-wise SHAP importances to: {fold_imp_path}\")\n",
    "\n",
    "    # --- バー図で可視化（特徴重要度, mean|SHAP|） ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    idxs = order_global  # 重要度降順\n",
    "    y_pos = np.arange(len(idxs))\n",
    "\n",
    "    plt.barh(y_pos, mean_imp[idxs])\n",
    "    plt.yticks(y_pos, [FEATURE_COLS[i] for i in idxs], fontsize=20)\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.xlabel(\"Mean |SHAP| (over samples & time)\", fontsize=24)\n",
    "    plt.title(\"LSTM (30s history) SHAP feature importance\", fontsize=30)\n",
    "    plt.xticks(fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = OUT_DIR / \"Cell2_LSTM_SHAP_feature_importance_bar.png\"\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[INFO][SHAP] Saved SHAP bar plot to: {fig_path}\")\n",
    "\n",
    "    # --- SHAP summary beeswarm 図（全fold統合） ---\n",
    "    try:\n",
    "        shap_all = np.concatenate(shap_samples_list, axis=0)   # (N_total, N_FEATURES)\n",
    "        X_all_plot = np.concatenate(feature_values_list, axis=0)\n",
    "        X_all_df = pd.DataFrame(X_all_plot, columns=FEATURE_COLS)\n",
    "\n",
    "        shap.summary_plot(\n",
    "            shap_all,\n",
    "            X_all_df,\n",
    "            feature_names=FEATURE_COLS,\n",
    "            show=False,\n",
    "            max_display=len(FEATURE_COLS),\n",
    "        )\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlabel(\"SHAP value (impact on model output)\", fontsize=24)\n",
    "        fig.suptitle(\"LSTM (30s history) SHAP summary (all folds)\", fontsize=30, y=1.02)\n",
    "\n",
    "        for lbl in ax.get_xticklabels():\n",
    "            lbl.set_fontsize(20)\n",
    "        for lbl in ax.get_yticklabels():\n",
    "            lbl.set_fontsize(20)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        beeswarm_path = OUT_DIR / \"Cell2_LSTM_SHAP_summary_beeswarm.png\"\n",
    "        fig.savefig(beeswarm_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"[INFO][SHAP] Saved SHAP summary beeswarm plot to: {beeswarm_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN][SHAP] Failed to create SHAP summary beeswarm plot: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
