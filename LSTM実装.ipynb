{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f069e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell0: 特徴量設計\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 重要パラメータブロック\n",
    "# =========================\n",
    "\n",
    "BASE_DIR = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\")\n",
    "\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\", \"10063\", \"10064\",\n",
    "    \"10071\", \"10072\", \"10073\", \"10074\",\n",
    "    \"10081\", \"10082\", \"10083\",\n",
    "    \"10091\", \"10092\", \"10093\", \"10094\",\n",
    "    \"10101\", \"10102\", \"10103\",\n",
    "]\n",
    "\n",
    "# 解析対象時間（秒）\n",
    "T_START = 1770.0\n",
    "T_END = 2400.0\n",
    "\n",
    "# スライディング窓の設定（本研究）\n",
    "WINDOW_SEC = 3            # 特徴量窓幅 3秒\n",
    "SLIDE_STEP_SEC = 3      # 窓終端 t を 0.5秒刻みで計算（←ここを 1.0 に変えれば1秒刻みでも動く）\n",
    "PC_LAG_SEC = 3            # PCは「3秒前の窓」と比較（= 1つ前の3秒ブロック）\n",
    "\n",
    "# ラベル（FMS）の時間シフト設定\n",
    "#   FMS_SHIFT_SEC = 0 : シフト無し（t のFMSは t が属する30秒ブロック）\n",
    "#   FMS_SHIFT_SEC > 0 : 「t の FMS は t+FMS_SHIFT_SEC のFMSブロック」に対応（将来予測用）\n",
    "FMS_SHIFT_SEC = 0\n",
    "\n",
    "# 参考: 先行研究③（VRジェットコースター LSTM）\n",
    "#   WINDOW_SEC      ≒ 3     # 同じく 3秒ローリング窓で rma/max/min/pc を計算\n",
    "#   SLIDE_STEP_SEC  ≒ 0.5   # 0.5秒刻みで特徴列 f(t) を作成（2 Hz の時系列）\n",
    "#   シーケンス長    = 30    # 30ステップの入力系列 (= 過去 15秒分の f(t))\n",
    "#   ラベル          = 1つ   # 各 15秒ブロック終端時刻に対応する CS(0/1) を1つ付与\n",
    "\n",
    "# pc の計算パラメータ\n",
    "PC_DEFAULT_VALUE = 0.0   # 前平均が無い/極小のときに入れる値\n",
    "PC_EPS = 1e-6            # 「ほぼゼロ判定」のしきい値\n",
    "\n",
    "# 窓終端の t の範囲\n",
    "T_MIN_OUT = T_START + WINDOW_SEC   # 1770 + 3 = 1773.0\n",
    "T_MAX_OUT = T_END                  # 2400.0\n",
    "\n",
    "# FMS_TEXT（与えられたものをそのまま使用）\n",
    "FMS_TEXT: Dict[str, str] = {\n",
    "    \"10061\": \"0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 2 1\",\n",
    "    \"10063\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10064\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\",\n",
    "    \"10071\": \"0 0 0 0 0 0 0 1 1 1 1 1 1 1 2 2 1 1 1 1 1\",\n",
    "    \"10072\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10073\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10074\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10081\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\",\n",
    "    \"10082\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\",\n",
    "    \"10083\": \"0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\",\n",
    "    \"10091\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1\",\n",
    "    \"10092\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\",\n",
    "    \"10093\": \"0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 2 3 3 4 4\",\n",
    "    \"10094\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 2 2\",\n",
    "    \"10101\": \"0 0 0 0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 3 3\",\n",
    "    \"10102\": \"0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 2 2 2 2 2\",\n",
    "    \"10103\": \"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4\",\n",
    "}\n",
    "\n",
    "\n",
    "# MSSQ_percentile 読み込み（ID→0〜1スケールの辞書に）\n",
    "SUMMARY_PATH = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\summary_scores.csv\"\n",
    "_df_summary = pd.read_csv(SUMMARY_PATH)\n",
    "MSSQ_PCT_BY_ID: Dict[str, float] = {}\n",
    "for _, row in _df_summary.iterrows():\n",
    "    raw_id = row[\"ID\"]\n",
    "    # NaNなどはスキップ（念のため）\n",
    "    if pd.isna(raw_id):\n",
    "        continue\n",
    "    # ★ intにしてから文字列化 → \"10061.0\" 問題を回避\n",
    "    sid_key = str(int(raw_id))\n",
    "    MSSQ_PCT_BY_ID[sid_key] = float(row[\"MSSQ_percentile\"]) / 100.0\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ユーティリティ関数\n",
    "# =========================\n",
    "\n",
    "def minmax_scale(values: np.ndarray, channel_name: str, sid: str) -> np.ndarray:\n",
    "    \"\"\"[1770,2400]内の値をmin-maxスケーリング。min==maxまたは全部NaNなら全0.\"\"\"\n",
    "    arr = np.asarray(values, float)\n",
    "    valid = np.isfinite(arr)\n",
    "    if not valid.any():\n",
    "        print(f\"[WARN] {sid} {channel_name}: all NaN -> set all zeros\")\n",
    "        return np.zeros_like(arr, float)\n",
    "    vmin = np.nanmin(arr[valid])\n",
    "    vmax = np.nanmax(arr[valid])\n",
    "    if vmax - vmin == 0:\n",
    "        print(f\"[WARN] {sid} {channel_name}: min==max ({vmin}) -> set all zeros\")\n",
    "        return np.zeros_like(arr, float)\n",
    "    return (arr - vmin) / (vmax - vmin)\n",
    "\n",
    "\n",
    "def compute_window_features_continuous(\n",
    "    times: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    t_grid: np.ndarray,\n",
    "    window_len: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    不均一サンプリング（1000Hz, 15Hz, 1Hzなど）について、\n",
    "    各 t (窓終端) に対する [t-window_len, t] の平均/最大/最小を二重ポインタで計算。\n",
    "    \"\"\"\n",
    "    times = np.asarray(times, float)\n",
    "    values = np.asarray(values, float)\n",
    "    n = len(times)\n",
    "    means = np.full(t_grid.shape, np.nan, dtype=float)\n",
    "    vmaxs = np.full(t_grid.shape, np.nan, dtype=float)\n",
    "    vmins = np.full(t_grid.shape, np.nan, dtype=float)\n",
    "    start = 0\n",
    "    end = -1\n",
    "    for i, t in enumerate(t_grid):\n",
    "        w_start = t - window_len\n",
    "        while start < n and times[start] < w_start:\n",
    "            start += 1\n",
    "        while end + 1 < n and times[end + 1] <= t:\n",
    "            end += 1\n",
    "        if end >= start and start < n:\n",
    "            seg = values[start:end + 1]\n",
    "            if seg.size > 0:\n",
    "                valid = np.isfinite(seg)\n",
    "                if valid.any():\n",
    "                    segv = seg[valid]\n",
    "                    means[i] = segv.mean()\n",
    "                    vmaxs[i] = segv.max()\n",
    "                    vmins[i] = segv.min()\n",
    "    return means, vmaxs, vmins\n",
    "\n",
    "\n",
    "def build_hr_1s_from_rr(\n",
    "    r_times: np.ndarray,\n",
    "    rr_intervals: np.ndarray,\n",
    "    t_start: int,\n",
    "    t_end: int,\n",
    "    sid: str,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    RR時刻列から、t_start〜t_end の1秒グリッドHR_1s(t)を作る。\n",
    "    区間 [R_k, R_{k+1}) にいる t のHRは HR_k = 60/RR_k\n",
    "    ※ 有効RRがゼロ件なら即エラー\n",
    "    \"\"\"\n",
    "    r_times = np.asarray(r_times, float)\n",
    "    rr_intervals = np.asarray(rr_intervals, float)\n",
    "    mask = np.isfinite(rr_intervals) & (rr_intervals > 0)\n",
    "    if not mask.any():\n",
    "        raise RuntimeError(f\"[ERROR] {sid} RRtime: no valid RR_interval_sec (>0)\")\n",
    "\n",
    "    r_times = r_times[mask]\n",
    "    rr_intervals = rr_intervals[mask]\n",
    "\n",
    "    t_grid = np.arange(int(t_start), int(t_end) + 1)\n",
    "    hr_k = 60.0 / rr_intervals\n",
    "\n",
    "    order = np.argsort(r_times)\n",
    "    r_times = r_times[order]\n",
    "    hr_k = hr_k[order]\n",
    "\n",
    "    hr_1s = np.full_like(t_grid, np.nan, dtype=float)\n",
    "    k = 0\n",
    "    n = len(r_times)\n",
    "    for i, t in enumerate(t_grid):\n",
    "        while k + 1 < n and r_times[k + 1] <= t:\n",
    "            k += 1\n",
    "        if r_times[k] <= t:\n",
    "            hr_1s[i] = hr_k[k]\n",
    "\n",
    "    return t_grid, hr_1s\n",
    "\n",
    "\n",
    "def forward_fill_to_grid(\n",
    "    times: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    t_grid: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    不規則サンプリングを、任意の t_grid に forward fill する。\n",
    "    10秒刻み → 0.5秒刻み などもOK。\n",
    "    \"\"\"\n",
    "    times = np.asarray(times, float)\n",
    "    values = np.asarray(values, float)\n",
    "    order = np.argsort(times)\n",
    "    times = times[order]\n",
    "    values = values[order]\n",
    "\n",
    "    idx = np.searchsorted(times, t_grid, side=\"right\") - 1\n",
    "    out = np.full_like(t_grid, np.nan, float)\n",
    "    valid = idx >= 0\n",
    "    out[valid] = values[idx[valid]]\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_pc_from_mean(\n",
    "    mean_arr: np.ndarray,\n",
    "    step_sec: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    平均系列 mean_arr（等間隔 step_sec）から、PC_LAG_SEC 秒前との変化率を計算。\n",
    "    lag_steps = round(PC_LAG_SEC / step_sec) ステップ戻った点を prev とする。\n",
    "\n",
    "    例:\n",
    "      step_sec = 1.0, PC_LAG_SEC = 3  → 3ステップ前\n",
    "      step_sec = 0.5, PC_LAG_SEC = 3  → 6ステップ前\n",
    "    \"\"\"\n",
    "    mean_arr = np.asarray(mean_arr, float)\n",
    "    n = len(mean_arr)\n",
    "    pc = np.full(n, PC_DEFAULT_VALUE, float)\n",
    "\n",
    "    if n == 0:\n",
    "        return pc\n",
    "\n",
    "    lag_steps_f = PC_LAG_SEC / step_sec\n",
    "    lag_steps = int(round(lag_steps_f))\n",
    "    if not np.isclose(lag_steps_f, lag_steps):\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] PC_LAG_SEC({PC_LAG_SEC}) is not a multiple of step_sec({step_sec})\"\n",
    "        )\n",
    "\n",
    "    for i in range(n):\n",
    "        j = i - lag_steps\n",
    "        if j < 0:\n",
    "            pc[i] = PC_DEFAULT_VALUE\n",
    "            continue\n",
    "        prev = mean_arr[j]\n",
    "        cur = mean_arr[i]\n",
    "        if np.isfinite(prev) and abs(prev) > PC_EPS and np.isfinite(cur):\n",
    "            pc[i] = (cur - prev) / prev\n",
    "        else:\n",
    "            pc[i] = PC_DEFAULT_VALUE\n",
    "    return pc\n",
    "\n",
    "\n",
    "def build_fms_series_for_t_grid(sid: str, t_grid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    FMS_TEXT から t_grid 用の FMS(t) を作る。\n",
    "\n",
    "    ラベル仕様:\n",
    "      FMS_SHIFT_SEC = 0 のとき、\n",
    "        時刻 t のラベルは「t が属する30秒ブロック」の FMS\n",
    "        idx = floor((t - T_START) / 30)\n",
    "      FMS_SHIFT_SEC > 0 のとき、\n",
    "        時刻 t のラベルは「t + FMS_SHIFT_SEC が属する30秒ブロック」の FMS\n",
    "    \"\"\"\n",
    "    text = FMS_TEXT[sid]\n",
    "    fms_list = [int(x) for x in text.split()]\n",
    "    if len(fms_list) != 21:\n",
    "        raise ValueError(f\"[FMS] {sid}: expected 21 values, got {len(fms_list)}\")\n",
    "    fms_arr = np.array(fms_list, int)\n",
    "    out = np.zeros_like(t_grid, int)\n",
    "    for i, t in enumerate(t_grid):\n",
    "        idx = int((t - T_START + FMS_SHIFT_SEC) // 30)\n",
    "        if idx < 0:\n",
    "            idx = 0\n",
    "        elif idx > 20:\n",
    "            idx = 20\n",
    "        out[i] = fms_arr[idx]\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# メイン処理\n",
    "# =========================\n",
    "\n",
    "def process_subject(sid: str) -> None:\n",
    "    print(f\"[INFO] Subject {sid} start\")\n",
    "\n",
    "    # ---- MSSQ_percentile（0〜1）取得 ----\n",
    "    if sid not in MSSQ_PCT_BY_ID:\n",
    "        raise RuntimeError(f\"[ERROR] {sid}: MSSQ_percentile not found in summary_scores.csv\")\n",
    "    mssq_pct01 = MSSQ_PCT_BY_ID[sid]\n",
    "\n",
    "    # ---- 入力パス ----\n",
    "    offset_dir = BASE_DIR / sid / \"OFFSET\"\n",
    "    feat_dir = BASE_DIR / sid / \"FEATURE\"\n",
    "    out_dir = BASE_DIR / sid / \"FEATURE2\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    path_pulse = offset_dir / f\"{sid}_Pulse.csv\"\n",
    "    path_sweat = offset_dir / f\"{sid}_Sweat.csv\"\n",
    "    path_faceA = offset_dir / f\"{sid}_FaceA.csv\"\n",
    "    path_faceB = offset_dir / f\"{sid}_FaceB.csv\"\n",
    "    path_skinos = offset_dir / f\"{sid}_Skinos.csv\"\n",
    "    path_rr = feat_dir / f\"{sid}_RRtime.csv\"\n",
    "\n",
    "    # ---- 出力時刻グリッド（窓終端）----\n",
    "    # 0.5秒刻みでも 1秒刻みでもOK （T_MIN_OUT〜T_MAX_OUT を含むように少しだけ +step/2）\n",
    "    t_out = np.arange(T_MIN_OUT, T_MAX_OUT + SLIDE_STEP_SEC / 2, SLIDE_STEP_SEC)\n",
    "\n",
    "    # ---- FMS 列 ----\n",
    "    fms_out = build_fms_series_for_t_grid(sid, t_out)\n",
    "\n",
    "    # =====================\n",
    "    # Pulse: 1000Hz\n",
    "    # =====================\n",
    "    df_pulse = pd.read_csv(path_pulse)\n",
    "    df_pulse = df_pulse[(df_pulse[\"Time_sec\"] >= T_START) & (df_pulse[\"Time_sec\"] <= T_END)].copy()\n",
    "    if df_pulse.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} Pulse: no data in [{T_START}, {T_END}]\")\n",
    "    df_pulse = df_pulse.sort_values(\"Time_sec\")\n",
    "    pulse_norm = minmax_scale(df_pulse[\"Pulse\"].to_numpy(), \"Pulse\", sid)\n",
    "    times_pulse = df_pulse[\"Time_sec\"].to_numpy()\n",
    "    pulse_mean3, pulse_max3, pulse_min3 = compute_window_features_continuous(\n",
    "        times_pulse, pulse_norm, t_out, window_len=WINDOW_SEC\n",
    "    )\n",
    "    pulse_pc3 = compute_pc_from_mean(pulse_mean3, step_sec=SLIDE_STEP_SEC)\n",
    "\n",
    "    # =====================\n",
    "    # Sweat (GSR): 1000Hz\n",
    "    # =====================\n",
    "    df_sweat = pd.read_csv(path_sweat)\n",
    "    df_sweat = df_sweat[(df_sweat[\"Time_sec\"] >= T_START) & (df_sweat[\"Time_sec\"] <= T_END)].copy()\n",
    "    if df_sweat.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} Sweat: no data in [{T_START}, {T_END}]\")\n",
    "    df_sweat = df_sweat.sort_values(\"Time_sec\")\n",
    "    gsr_norm = minmax_scale(df_sweat[\"Sweat\"].to_numpy(), \"Sweat\", sid)\n",
    "    times_gsr = df_sweat[\"Time_sec\"].to_numpy()\n",
    "    gsr_mean3, gsr_max3, gsr_min3 = compute_window_features_continuous(\n",
    "        times_gsr, gsr_norm, t_out, window_len=WINDOW_SEC\n",
    "    )\n",
    "    gsr_pc3 = compute_pc_from_mean(gsr_mean3, step_sec=SLIDE_STEP_SEC)\n",
    "\n",
    "    # =====================\n",
    "    # FaceA/B: 15Hz\n",
    "    # =====================\n",
    "    df_faceA = pd.read_csv(path_faceA)\n",
    "    df_faceB = pd.read_csv(path_faceB)\n",
    "    df_faceA = df_faceA[(df_faceA[\"Time_sec\"] >= T_START) & (df_faceA[\"Time_sec\"] <= T_END)].copy()\n",
    "    df_faceB = df_faceB[(df_faceB[\"Time_sec\"] >= T_START) & (df_faceB[\"Time_sec\"] <= T_END)].copy()\n",
    "\n",
    "    if df_faceA.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} FaceA: no data in [{T_START}, {T_END}]\")\n",
    "    if df_faceB.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} FaceB: no data in [{T_START}, {T_END}]\")\n",
    "\n",
    "    df_faceA = df_faceA.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "    df_faceB = df_faceB.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "\n",
    "    if len(df_faceA) != len(df_faceB):\n",
    "        raise RuntimeError(f\"[ERROR] {sid} FaceA/FaceB length mismatch: \"\n",
    "                           f\"{len(df_faceA)} vs {len(df_faceB)}\")\n",
    "    if not np.allclose(df_faceA[\"Time_sec\"].to_numpy(),\n",
    "                       df_faceB[\"Time_sec\"].to_numpy()):\n",
    "        raise RuntimeError(f\"[ERROR] {sid} FaceA/FaceB Time_sec mismatch\")\n",
    "\n",
    "    df_face = pd.DataFrame({\n",
    "        \"Time_sec\": df_faceA[\"Time_sec\"].to_numpy(),\n",
    "        \"FaceA_BoxAve\": df_faceA[\"FaceA_BoxAve\"].to_numpy(),\n",
    "        \"FaceB_BoxAve\": df_faceB[\"FaceB_BoxAve\"].to_numpy(),\n",
    "    })\n",
    "\n",
    "    sA = df_face[\"FaceA_BoxAve\"].astype(float)\n",
    "    sB = df_face[\"FaceB_BoxAve\"].astype(float)\n",
    "    faceA_lp = sA.rolling(window=15, center=True, min_periods=1).mean().to_numpy()\n",
    "    faceB_lp = sB.rolling(window=15, center=True, min_periods=1).mean().to_numpy()\n",
    "\n",
    "    def clean_face_channel(arr: np.ndarray) -> np.ndarray:\n",
    "        x = arr.astype(float).copy()\n",
    "        med = np.nanmedian(x)\n",
    "        q1 = np.nanpercentile(x, 25)\n",
    "        q3 = np.nanpercentile(x, 75)\n",
    "        iqr = q3 - q1\n",
    "        if iqr <= 0:\n",
    "            s = pd.Series(x)\n",
    "            return s.interpolate(method=\"linear\", limit_direction=\"both\").to_numpy()\n",
    "        lower = med - 3 * iqr\n",
    "        upper = med + 3 * iqr\n",
    "        mask_out = (x < lower) | (x > upper)\n",
    "        x[mask_out] = np.nan\n",
    "        s = pd.Series(x)\n",
    "        x_filled = s.interpolate(method=\"linear\", limit_direction=\"both\").to_numpy()\n",
    "        return x_filled\n",
    "\n",
    "    faceA_clean = clean_face_channel(faceA_lp)\n",
    "    faceB_clean = clean_face_channel(faceB_lp)\n",
    "\n",
    "    faceA_norm = minmax_scale(faceA_clean, \"FaceA_BoxAve\", sid)\n",
    "    faceB_norm = minmax_scale(faceB_clean, \"FaceB_BoxAve\", sid)\n",
    "\n",
    "    face_sum = faceA_norm + faceB_norm\n",
    "    face_diff = faceB_norm - faceA_norm\n",
    "    times_face = df_face[\"Time_sec\"].to_numpy()\n",
    "\n",
    "    face_sum_mean3, _, _ = compute_window_features_continuous(\n",
    "        times_face, face_sum, t_out, window_len=WINDOW_SEC\n",
    "    )\n",
    "    face_diff_mean3, _, _ = compute_window_features_continuous(\n",
    "        times_face, face_diff, t_out, window_len=WINDOW_SEC\n",
    "    )\n",
    "    face_sum_pc3 = compute_pc_from_mean(face_sum_mean3, step_sec=SLIDE_STEP_SEC)\n",
    "    face_diff_pc3 = compute_pc_from_mean(face_diff_mean3, step_sec=SLIDE_STEP_SEC)\n",
    "\n",
    "    # =====================\n",
    "    # RRtime -> HR_1s -> HR 3秒窓特徴\n",
    "    # =====================\n",
    "    df_rr = pd.read_csv(path_rr)\n",
    "    if df_rr.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} RRtime: file is empty\")\n",
    "\n",
    "    df_rr_win = df_rr[(df_rr[\"Time_sec\"] >= T_START) & (df_rr[\"Time_sec\"] <= T_END)]\n",
    "    if df_rr_win.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} RRtime: no R waves in [{T_START}, {T_END}]\")\n",
    "\n",
    "    r_times = df_rr[\"Time_sec\"].to_numpy()\n",
    "    rr_int = df_rr[\"RR_interval_sec\"].to_numpy()\n",
    "    t_grid_hr, hr_1s = build_hr_1s_from_rr(r_times, rr_int, int(T_START), int(T_END), sid)\n",
    "\n",
    "    hr_norm = minmax_scale(hr_1s, \"HR_1s\", sid)\n",
    "\n",
    "    # HR_1s (1Hz) を「不均一サンプリング」とみなして、t_outで3秒窓特徴を取る\n",
    "    hr_rma3, hr_max3, hr_min3 = compute_window_features_continuous(\n",
    "        t_grid_hr.astype(float), hr_norm, t_out, window_len=WINDOW_SEC\n",
    "    )\n",
    "    hr_pc3 = compute_pc_from_mean(hr_rma3, step_sec=SLIDE_STEP_SEC)\n",
    "\n",
    "    # =====================\n",
    "    # Skinos: 10秒刻み -> t_out へ forward fill\n",
    "    # =====================\n",
    "    df_skin = pd.read_csv(path_skinos)\n",
    "    df_skin = df_skin[(df_skin[\"Time_sec\"] >= T_START) & (df_skin[\"Time_sec\"] <= T_END)].copy()\n",
    "    if df_skin.empty:\n",
    "        raise RuntimeError(f\"[ERROR] {sid} Skinos: no data in [{T_START}, {T_END}]\")\n",
    "    df_skin = df_skin.sort_values(\"Time_sec\")\n",
    "\n",
    "    skin_sweat_norm = minmax_scale(df_skin[\"Sweat_Rate\"].to_numpy(), \"Skinos_Sweat_Rate\", sid)\n",
    "    skin_hr_norm = minmax_scale(df_skin[\"Heart_Rate\"].to_numpy(), \"Skinos_HeartRate\", sid)\n",
    "    skin_temp_norm = minmax_scale(df_skin[\"Skin_Temp\"].to_numpy(), \"Skinos_SkinTemp\", sid)\n",
    "\n",
    "    times_skin = df_skin[\"Time_sec\"].to_numpy()\n",
    "    skin_sweat_out = forward_fill_to_grid(times_skin, skin_sweat_norm, t_out)\n",
    "    skin_hr_out = forward_fill_to_grid(times_skin, skin_hr_norm, t_out)\n",
    "    skin_temp_out = forward_fill_to_grid(times_skin, skin_temp_norm, t_out)\n",
    "\n",
    "    # MSSQ_percentile（0〜1）を全行に付与\n",
    "    mssq_col = np.full_like(t_out, mssq_pct01, dtype=float)\n",
    "\n",
    "    # =====================\n",
    "    # DataFrame にまとめて保存\n",
    "    # =====================\n",
    "    df_out = pd.DataFrame({\n",
    "        \"Time_sec\": t_out,               # 0.5刻みの時刻（float）\n",
    "        \"FMS\": fms_out.astype(int),\n",
    "        \"Pulse_rma3\": pulse_mean3,\n",
    "        \"Pulse_max3\": pulse_max3,\n",
    "        \"Pulse_min3\": pulse_min3,\n",
    "        \"Pulse_pc3\": pulse_pc3,\n",
    "        \"HR_rma3\": hr_rma3,\n",
    "        \"HR_max3\": hr_max3,\n",
    "        \"HR_min3\": hr_min3,\n",
    "        \"HR_pc3\": hr_pc3,\n",
    "        \"GSR_rma3\": gsr_mean3,\n",
    "        \"GSR_max3\": gsr_max3,\n",
    "        \"GSR_min3\": gsr_min3,\n",
    "        \"GSR_pc3\": gsr_pc3,\n",
    "        \"FaceSum_mean3\": face_sum_mean3,\n",
    "        \"FaceDiff_mean3\": face_diff_mean3,\n",
    "        \"FaceSum_pc3\": face_sum_pc3,\n",
    "        \"FaceDiff_pc3\": face_diff_pc3,\n",
    "        \"Skinos_SweatRate\": skin_sweat_out,\n",
    "        \"Skinos_HeartRate\": skin_hr_out,\n",
    "        \"Skinos_SkinTemp\": skin_temp_out,\n",
    "        \"MSSQ_percentile01\": mssq_col,   # 0〜1スケールのMSSQ百分位\n",
    "    })\n",
    "\n",
    "    out_path = out_dir / f\"{sid}_3sFeat_{SLIDE_STEP_SEC}sSlide.csv\"\n",
    "    df_out.to_csv(out_path, index=False)\n",
    "    print(f\"[INFO] Subject {sid} done -> {out_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for sid in SUBJECT_IDS:\n",
    "        process_subject(sid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell1-LSTM: LOSO＋ROC-AUC 単体\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# パス・基本設定\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\")\n",
    "\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\", \"10063\", \"10064\",\n",
    "    \"10071\", \"10072\", \"10073\", \"10074\",\n",
    "    \"10081\", \"10082\", \"10083\",\n",
    "    \"10091\", \"10092\", \"10093\", \"10094\",\n",
    "    \"10101\", \"10102\", \"10103\",\n",
    "]\n",
    "\n",
    "# このCell用の出力ディレクトリ\n",
    "CELL_NAME = \"Cell1-LSTM\"\n",
    "OUT_DIR = BASE_DIR / f\"解析{SLIDE_STEP_SEC}\" / \"Cell1\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Cell: {CELL_NAME}, OUT_DIR = {OUT_DIR}\")\n",
    "\n",
    "# 確率分布プロット用ディレクトリ\n",
    "PROB_PLOT_DIR = OUT_DIR / \"prob_dist\"\n",
    "PROB_PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Prob. plot dir = {PROB_PLOT_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 時間・シーケンス仕様\n",
    "# -----------------------------\n",
    "\n",
    "# LSTM に入れる過去ステップ数（= 過去 SEQ_LEN 秒分）\n",
    "SLIDE_STEP_SEC = 1\n",
    "SEQ_LEN = 15\n",
    "Time_LEN=SLIDE_STEP_SEC*SEQ_LEN\n",
    "print(f\"過去{Time_LEN}sのデータ\")\n",
    "\n",
    "# FEATURE2 での最初の出力時刻（T_START+WINDOW_SEC = 1770+3）\n",
    "BASE_T_MIN = 1773\n",
    "\n",
    "# ターゲットの最小時刻：最初の出力時刻＋(SEQ_LEN-1)\n",
    "# 例：BASE_T_MIN=1773, SEQ_LEN=30 → 1773+29 = 1802\n",
    "TARGET_T_MIN = BASE_T_MIN + (SEQ_LEN - 1)\n",
    "TARGET_T_MAX = 2400     # 上限はこれまで通り 2400 秒\n",
    "\n",
    "# ラベル閾値：FMS >= 1 を陽性とする\n",
    "FMS_POS_THRESHOLD = 1\n",
    "\n",
    "# -----------------------------\n",
    "# LSTMハイパラ（変更候補は CSV に出力）\n",
    "# -----------------------------\n",
    "HIDDEN_SIZE = 32\n",
    "FC_HIDDEN_SIZE = 8\n",
    "DROPOUT_LSTM = 0.0\n",
    "DROPOUT_FC = 0.5\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 30\n",
    "WEIGHT_DECAY = 1e-4  # L2正則化（Adam の weight_decay）\n",
    "\n",
    "# -----------------------------\n",
    "# 特徴量ON/OFF設定\n",
    "# -----------------------------\n",
    "FEATURE_SWITCHES: List[Tuple[str, bool]] = [\n",
    "    (\"Pulse_rma3\",       True),\n",
    "    (\"Pulse_max3\",       True),\n",
    "    (\"Pulse_min3\",       True),\n",
    "    (\"Pulse_pc3\",        True),\n",
    "    (\"HR_rma3\",          True),\n",
    "    (\"HR_max3\",          True),\n",
    "    (\"HR_min3\",          True),\n",
    "    (\"HR_pc3\",           True),\n",
    "    (\"GSR_rma3\",         True),\n",
    "    (\"GSR_max3\",         True),\n",
    "    (\"GSR_min3\",         True),\n",
    "    (\"GSR_pc3\",          True),\n",
    "    (\"FaceSum_mean3\",    True),\n",
    "    (\"FaceDiff_mean3\",   True),\n",
    "    (\"FaceSum_pc3\",      True),\n",
    "    (\"FaceDiff_pc3\",     True),\n",
    "    (\"Skinos_SweatRate\", True),\n",
    "    (\"Skinos_HeartRate\", False),\n",
    "    (\"Skinos_SkinTemp\",  True),\n",
    "    (\"MSSQ_percentile01\",  True),\n",
    "]\n",
    "\n",
    "FEATURE_COLS: List[str] = [name for name, use in FEATURE_SWITCHES if use]\n",
    "if len(FEATURE_COLS) == 0:\n",
    "    raise RuntimeError(\"[ERROR] FEATURE_SWITCHES: 有効な特徴量が0個です（すべてFalse）。\")\n",
    "\n",
    "N_FEATURES = len(FEATURE_COLS)\n",
    "print(f\"[INFO] Using {N_FEATURES} features:\", \", \".join(FEATURE_COLS))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM モデル定義\n",
    "# -----------------------------\n",
    "class LSTMMotionSickness(nn.Module):\n",
    "    \"\"\"\n",
    "    単方向1層LSTM → Dropout → FC(HIDDEN_SIZE→FC_HIDDEN_SIZE) → ReLU → FC → ロジット\n",
    "    出力はロジット（Sigmoidはloss/評価側で適用）\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = HIDDEN_SIZE,\n",
    "        fc_hidden_size: int = FC_HIDDEN_SIZE,\n",
    "        dropout_lstm: float = DROPOUT_LSTM,\n",
    "        dropout_fc: float = DROPOUT_FC,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout_lstm,  # num_layers=1 では実質無視される\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_fc)\n",
    "        self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(fc_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size)\n",
    "        return: ロジット (batch,)\n",
    "        \"\"\"\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        h_last = hn[-1]              # (batch, hidden_size)\n",
    "        z = self.dropout(h_last)\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.fc_out(z)           # (batch, 1)\n",
    "        return z.squeeze(-1)         # (batch,)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# データ読み込み & シーケンス生成\n",
    "# -----------------------------\n",
    "def load_subject_df(sid: str) -> pd.DataFrame:\n",
    "    \"\"\"FEATURE2/{sid}_3sFeat_1sSlide.csv を読み込む.\"\"\"\n",
    "    path = BASE_DIR / sid / \"FEATURE2\" / f\"{sid}_3sFeat_1sSlide.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"[ERROR] Subject {sid}: file not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sequences_for_subject(\n",
    "    sid: str,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    1被験者について:\n",
    "      - FEATURE2 CSVを読み込み\n",
    "      - FMS>=1 を陽性にした y(t) を作成\n",
    "      - t=TARGET_T_MIN〜TARGET_T_MAX の各時刻 t に対し，\n",
    "          X_seq(t) = [t-SEQ_LEN+1 .. t] のシーケンスを生成\n",
    "      - その際，特徴量内にNaNがあれば即エラー\n",
    "    戻り値:\n",
    "      X_seq: (N_seq, SEQ_LEN, N_FEATURES)\n",
    "      y_seq: (N_seq,)\n",
    "      t_seq: (N_seq,)\n",
    "      fms_seq: (N_seq,)\n",
    "    \"\"\"\n",
    "    df = load_subject_df(sid)\n",
    "\n",
    "    # 必要列が揃っているかチェック\n",
    "    required_cols = [\"Time_sec\", \"FMS\"] + FEATURE_COLS\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: missing columns in FEATURE2 csv: {missing}\")\n",
    "\n",
    "    # NaNチェック（仕様：NaNがあれば即エラー）\n",
    "    if df[FEATURE_COLS].isna().values.any():\n",
    "        nan_mask = df[FEATURE_COLS].isna()\n",
    "        bad_idx = np.where(nan_mask.values)[0][0]\n",
    "        bad_time = df.loc[bad_idx, \"Time_sec\"]\n",
    "        bad_cols = list(nan_mask.columns[nan_mask.iloc[bad_idx]])\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Subject {sid}: NaN detected at Time_sec={bad_time}, cols={bad_cols}\"\n",
    "        )\n",
    "\n",
    "    times = df[\"Time_sec\"].to_numpy().astype(int)\n",
    "    fms = df[\"FMS\"].to_numpy().astype(int)\n",
    "    features = df[FEATURE_COLS].to_numpy().astype(np.float32)\n",
    "\n",
    "    # TARGET_T_MIN〜TARGET_T_MAX の範囲があるか\n",
    "    target_mask = (times >= TARGET_T_MIN) & (times <= TARGET_T_MAX)\n",
    "    if not target_mask.any():\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: no Time_sec in [{TARGET_T_MIN}, {TARGET_T_MAX}]\")\n",
    "\n",
    "    X_list: List[np.ndarray] = []\n",
    "    y_list: List[int] = []\n",
    "    t_list: List[int] = []\n",
    "    fms_list: List[int] = []\n",
    "\n",
    "    for idx in range(len(times)):\n",
    "        t = times[idx]\n",
    "        if t < TARGET_T_MIN or t > TARGET_T_MAX:\n",
    "            continue\n",
    "\n",
    "        if idx < SEQ_LEN - 1:\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: idx={idx}, Time_sec={t} has no enough history (need {SEQ_LEN}).\"\n",
    "            )\n",
    "\n",
    "        window_feat = features[idx - SEQ_LEN + 1: idx + 1, :]  # (SEQ_LEN, N_FEATURES)\n",
    "        if not np.isfinite(window_feat).all():\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: non-finite value in sequence ending at Time_sec={t}\"\n",
    "            )\n",
    "\n",
    "        # ラベル：FMS>=1\n",
    "        y = 1 if fms[idx] >= FMS_POS_THRESHOLD else 0\n",
    "\n",
    "        X_list.append(window_feat)\n",
    "        y_list.append(y)\n",
    "        t_list.append(t)\n",
    "        fms_list.append(int(fms[idx]))\n",
    "\n",
    "    X_seq = np.stack(X_list).astype(np.float32)   # (N_seq, SEQ_LEN, N_FEATURES)\n",
    "    y_seq = np.array(y_list, dtype=np.int64)\n",
    "    t_seq = np.array(t_list, dtype=np.int64)\n",
    "    fms_seq = np.array(fms_list, dtype=np.int64)\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] Subject {sid}: target Time_sec range = {t_seq[0]}–{t_seq[-1]}, \"\n",
    "        f\"N_seq = {len(t_seq)}, N_pos = {y_seq.sum()}, N_neg = {len(y_seq) - y_seq.sum()}\"\n",
    "    )\n",
    "\n",
    "    return X_seq, y_seq, t_seq, fms_seq\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LOSO 学習・評価ループ\n",
    "# -----------------------------\n",
    "def train_one_fold(\n",
    "    train_X: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    device: torch.device,\n",
    ") -> Tuple[LSTMMotionSickness, List[float]]:\n",
    "    \"\"\"\n",
    "    1つのLOSO foldについて，訓練データのみを使ってLSTMを学習する。\n",
    "    戻り値: (学習済みモデル, 各epochの平均train lossリスト)\n",
    "    \"\"\"\n",
    "    model = LSTMMotionSickness(input_size=N_FEATURES).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # 陽性割合をプリント\n",
    "    n_train = len(train_y)\n",
    "    n_pos = int(train_y.sum())\n",
    "    n_neg = n_train - n_pos\n",
    "    pos_ratio = n_pos / n_train if n_train > 0 else 0.0\n",
    "    print(\n",
    "        f\"[INFO] Train stats: N={n_train}, N_pos={n_pos}, N_neg={n_neg}, \"\n",
    "        f\"pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    # DataLoader 構築\n",
    "    X_tensor = torch.from_numpy(train_X).float()\n",
    "    y_tensor = torch.from_numpy(train_y).float()\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    epoch_loss_list: List[float] = []\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)           # (batch,)\n",
    "            loss = criterion(logits, batch_y) # BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / max(n_batches, 1)\n",
    "        epoch_loss_list.append(avg_loss)\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == N_EPOCHS:\n",
    "            print(f\"[INFO] Epoch {epoch:02d}/{N_EPOCHS} - train_loss={avg_loss:.4f}\")\n",
    "\n",
    "    return model, epoch_loss_list\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 確率分布プロット\n",
    "# -----------------------------\n",
    "def plot_probability_distributions(\n",
    "    df_pred: pd.DataFrame,\n",
    "    out_dir: Path,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Foldごと（被験者ごと）と全体の predicted probability 分布をヒストグラムで保存する。\n",
    "\n",
    "    df_pred:\n",
    "        列: ['SubjectID', 'Time_sec', 'FMS', 'Label_bin', 'Prob_FMS_ge1']\n",
    "    out_dir:\n",
    "        画像を保存するディレクトリ\n",
    "    \"\"\"\n",
    "    # グラフ体裁（ユーザ指定）\n",
    "    TITLE_FONTSIZE = 30\n",
    "    LABEL_FONTSIZE = 24\n",
    "    TICK_FONTSIZE = 20\n",
    "    LEGEND_FONTSIZE = 20\n",
    "    LINEWIDTH = 1.5\n",
    "\n",
    "    # ---- 全体の分布（全fold結合） ----\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    for label, alpha, lab_name in [\n",
    "        (0, 0.6, \"Label=0 (FMS<1)\"),\n",
    "        (1, 0.6, \"Label=1 (FMS>=1)\")\n",
    "    ]:\n",
    "        vals = df_pred.loc[df_pred[\"Label_bin\"] == label, \"Prob_FMS_ge1\"].values\n",
    "        if len(vals) == 0:\n",
    "            continue\n",
    "        ax.hist(\n",
    "            vals,\n",
    "            bins=20,\n",
    "            range=(0.0, 1.0),\n",
    "            density=True,\n",
    "            alpha=alpha,\n",
    "            label=lab_name,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=LINEWIDTH,\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_xlabel(\"Predicted probability (FMS ≥ 1)\", fontsize=LABEL_FONTSIZE)\n",
    "    ax.set_ylabel(\"Density\", fontsize=LABEL_FONTSIZE)\n",
    "    ax.set_title(\"All subjects – Probability distribution\", fontsize=TITLE_FONTSIZE)\n",
    "    ax.tick_params(axis=\"both\", labelsize=TICK_FONTSIZE)\n",
    "    ax.legend(fontsize=LEGEND_FONTSIZE)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path_all = out_dir / \"Cell1_LSTM_ProbDist_ALL.png\"\n",
    "    fig.savefig(out_path_all, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"[INFO] Saved global probability distribution plot to: {out_path_all}\")\n",
    "\n",
    "    # ---- 被験者ごとの分布 ----\n",
    "    for sid, df_sub in df_pred.groupby(\"SubjectID\"):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "        for label, alpha, lab_name in [\n",
    "            (0, 0.6, \"Label=0 (FMS<1)\"),\n",
    "            (1, 0.6, \"Label=1 (FMS>=1)\")\n",
    "        ]:\n",
    "            vals = df_sub.loc[df_sub[\"Label_bin\"] == label, \"Prob_FMS_ge1\"].values\n",
    "            if len(vals) == 0:\n",
    "                continue\n",
    "            ax.hist(\n",
    "                vals,\n",
    "                bins=20,\n",
    "                range=(0.0, 1.0),\n",
    "                density=True,\n",
    "                alpha=alpha,\n",
    "                label=lab_name,\n",
    "                edgecolor=\"black\",\n",
    "                linewidth=LINEWIDTH,\n",
    "            )\n",
    "\n",
    "        ax.set_xlim(0.0, 1.0)\n",
    "        ax.set_xlabel(\"Predicted probability (FMS ≥ 1)\", fontsize=LABEL_FONTSIZE)\n",
    "        ax.set_ylabel(\"Density\", fontsize=LABEL_FONTSIZE)\n",
    "        ax.set_title(f\"Subject {sid} – Probability distribution\", fontsize=TITLE_FONTSIZE)\n",
    "        ax.tick_params(axis=\"both\", labelsize=TICK_FONTSIZE)\n",
    "        ax.legend(fontsize=LEGEND_FONTSIZE)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_path_sid = out_dir / f\"Cell1_LSTM_ProbDist_{sid}.png\"\n",
    "        fig.savefig(out_path_sid, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(f\"[INFO] Saved probability distribution plot for {sid} to: {out_path_sid}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # デバイス選択\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    # 再現性のためにseed固定\n",
    "    torch.manual_seed(20251206)\n",
    "    np.random.seed(20251206)\n",
    "\n",
    "    # ---- 全被験者のシーケンスを構築 ----\n",
    "    X_by_sid: Dict[str, np.ndarray] = {}\n",
    "    y_by_sid: Dict[str, np.ndarray] = {}\n",
    "    t_by_sid: Dict[str, np.ndarray] = {}\n",
    "    fms_by_sid: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        print(f\"[INFO] ==== Build sequences: Subject {sid} ====\")\n",
    "        X_seq, y_seq, t_seq, fms_seq = build_sequences_for_subject(sid)\n",
    "        X_by_sid[sid] = X_seq\n",
    "        y_by_sid[sid] = y_seq\n",
    "        t_by_sid[sid] = t_seq\n",
    "        fms_by_sid[sid] = fms_seq\n",
    "\n",
    "    all_y_tmp = np.concatenate([y_by_sid[sid] for sid in SUBJECT_IDS])\n",
    "    print(\n",
    "        f\"[INFO] Overall (all subjects) target stats: \"\n",
    "        f\"N={len(all_y_tmp)}, N_pos={all_y_tmp.sum()}, \"\n",
    "        f\"pos_ratio={all_y_tmp.mean():.3f}\"\n",
    "    )\n",
    "\n",
    "    # ---- LOSO 学習・評価 ----\n",
    "    all_probs: List[np.ndarray] = []\n",
    "    all_true: List[np.ndarray] = []\n",
    "    pred_rows: List[pd.DataFrame] = []\n",
    "    fold_summary_rows: List[Dict] = []\n",
    "    epoch_loss_records: List[Dict] = []\n",
    "\n",
    "    for test_sid in SUBJECT_IDS:\n",
    "        print(f\"\\n[INFO] ===== LOSO fold: Test Subject {test_sid} =====\")\n",
    "\n",
    "        # 学習・テスト分割\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "        for sid in SUBJECT_IDS:\n",
    "            if sid == test_sid:\n",
    "                continue\n",
    "            train_X_list.append(X_by_sid[sid])\n",
    "            train_y_list.append(y_by_sid[sid])\n",
    "\n",
    "        train_X = np.concatenate(train_X_list, axis=0)\n",
    "        train_y = np.concatenate(train_y_list, axis=0)\n",
    "        test_X = X_by_sid[test_sid]\n",
    "        test_y = y_by_sid[test_sid]\n",
    "        test_t = t_by_sid[test_sid]\n",
    "        test_fms = fms_by_sid[test_sid]\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Fold data sizes: \"\n",
    "            f\"Train N_seq={len(train_y)}, Test N_seq={len(test_y)}\"\n",
    "        )\n",
    "\n",
    "        # 1 fold 学習\n",
    "        model, epoch_loss_list = train_one_fold(train_X, train_y, device=device)\n",
    "\n",
    "        # epochごとの loss をログ用に保存\n",
    "        for ep_idx, loss_val in enumerate(epoch_loss_list, start=1):\n",
    "            epoch_loss_records.append(\n",
    "                {\n",
    "                    \"SubjectID\": test_sid,\n",
    "                    \"Epoch\": ep_idx,\n",
    "                    \"TrainLoss\": loss_val,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # テスト被験者の予測確率\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "            logits = model(X_test_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()  # (N_test,)\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_true.append(test_y.astype(int))\n",
    "\n",
    "        # foldごとのROC-AUC\n",
    "        n_pos_test = int(test_y.sum())\n",
    "        n_neg_test = int(len(test_y) - n_pos_test)\n",
    "        if n_pos_test == 0 or n_neg_test == 0:\n",
    "            rocauc_fold = float(\"nan\")\n",
    "            print(\n",
    "                f\"[INFO] Subject {test_sid}: ROC-AUC undefined (N_pos={n_pos_test}, N_neg={n_neg_test})\"\n",
    "            )\n",
    "        else:\n",
    "            rocauc_fold = roc_auc_score(test_y, probs)\n",
    "            print(\n",
    "                f\"[INFO] Subject {test_sid}: ROC-AUC(test fold) = {rocauc_fold:.4f} \"\n",
    "                f\"(N_test={len(test_y)}, N_pos={n_pos_test}, N_neg={n_neg_test})\"\n",
    "            )\n",
    "\n",
    "        fold_summary_rows.append(\n",
    "            {\n",
    "                \"SubjectID\": test_sid,\n",
    "                \"N_test\": int(len(test_y)),\n",
    "                \"N_pos_test\": n_pos_test,\n",
    "                \"N_neg_test\": n_neg_test,\n",
    "                \"pos_ratio_test\": float(test_y.mean()),\n",
    "                \"ROC_AUC_test\": rocauc_fold,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # このfoldの予測詳細\n",
    "        df_fold = pd.DataFrame(\n",
    "            {\n",
    "                \"SubjectID\": test_sid,\n",
    "                \"Time_sec\": test_t,\n",
    "                \"FMS\": test_fms,\n",
    "                \"Label_bin\": test_y.astype(int),\n",
    "                \"Prob_FMS_ge1\": probs,\n",
    "            }\n",
    "        )\n",
    "        pred_rows.append(df_fold)\n",
    "\n",
    "    # ---- 全foldをまとめた ROC-AUC ----\n",
    "    y_all = np.concatenate(all_true)\n",
    "    p_all = np.concatenate(all_probs)\n",
    "\n",
    "    n_total = len(y_all)\n",
    "    n_pos = int(y_all.sum())\n",
    "    n_neg = n_total - n_pos\n",
    "    pos_ratio = n_pos / n_total if n_total > 0 else 0.0\n",
    "\n",
    "    print(\"\\n[INFO] ===== Overall LOSO result =====\")\n",
    "    print(\n",
    "        f\"[INFO] All folds combined: N={n_total}, N_pos={n_pos}, \"\n",
    "        f\"N_neg={n_neg}, pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    if n_pos == 0 or n_pos == n_total:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] ROC-AUC undefined: labels are all the same \"\n",
    "            f\"(N={n_total}, N_pos={n_pos}).\"\n",
    "        )\n",
    "\n",
    "    rocauc = roc_auc_score(y_all, p_all)\n",
    "    print(f\"[RESULT] Global ROC-AUC (LOSO, LSTM, FMS>=1) = {rocauc:.4f}\")\n",
    "\n",
    "    # ---- 結果保存 ----\n",
    "    # 1) ROC-AUC のサマリ（ハイパラ込み）\n",
    "    result_path = OUT_DIR / \"Cell1_LSTM_LOSO_ROCAUC.csv\"\n",
    "    df_result = pd.DataFrame(\n",
    "        {\n",
    "            \"ROC_AUC_global\": [rocauc],\n",
    "            \"N_total\": [n_total],\n",
    "            \"N_pos\": [n_pos],\n",
    "            \"N_neg\": [n_neg],\n",
    "            \"pos_ratio\": [pos_ratio],\n",
    "            \"N_features\": [N_FEATURES],\n",
    "            \"feature_list\": [\",\".join(FEATURE_COLS)],\n",
    "            # 変更候補ハイパラを全部記録\n",
    "            \"WINDOW_SEC\": [WINDOW_SEC],\n",
    "            \"SLIDE_STEP_SEC\": [SLIDE_STEP_SEC],\n",
    "            \"SEQ_LEN\": [SEQ_LEN],\n",
    "            \"HIDDEN_SIZE\": [HIDDEN_SIZE],\n",
    "            \"FC_HIDDEN_SIZE\": [FC_HIDDEN_SIZE],\n",
    "            \"DROPOUT_LSTM\": [DROPOUT_LSTM],\n",
    "            \"DROPOUT_FC\": [DROPOUT_FC],\n",
    "            \"LEARNING_RATE\": [LEARNING_RATE],\n",
    "            \"BATCH_SIZE\": [BATCH_SIZE],\n",
    "            \"N_EPOCHS\": [N_EPOCHS],\n",
    "            \"WEIGHT_DECAY\": [WEIGHT_DECAY],\n",
    "        }\n",
    "    )\n",
    "    df_result.to_csv(result_path, index=False)\n",
    "    print(f\"[INFO] Saved ROC-AUC result to: {result_path}\")\n",
    "\n",
    "    # 2) シーケンスごとの詳細予測\n",
    "    df_pred = pd.concat(pred_rows, ignore_index=True)\n",
    "    pred_path = OUT_DIR / \"Cell1_LSTM_LOSO_pred_detail.csv\"\n",
    "    df_pred.to_csv(pred_path, index=False)\n",
    "    print(f\"[INFO] Saved per-sequence predictions to: {pred_path}\")\n",
    "\n",
    "    # 2.5) 確率分布プロット（foldごと＋全体）\n",
    "    plot_probability_distributions(df_pred, PROB_PLOT_DIR)\n",
    "\n",
    "    # 3) foldごとの summary（被験者別 ROC-AUC）\n",
    "    df_fold_summary = pd.DataFrame(fold_summary_rows)\n",
    "    fold_summary_path = OUT_DIR / \"Cell1_LSTM_LOSO_fold_summary.csv\"\n",
    "    df_fold_summary.to_csv(fold_summary_path, index=False)\n",
    "    print(f\"[INFO] Saved per-fold summary to: {fold_summary_path}\")\n",
    "\n",
    "    # 4) epochごとの train loss\n",
    "    df_loss = pd.DataFrame(epoch_loss_records)\n",
    "    loss_path = OUT_DIR / \"Cell1_LSTM_LOSO_train_loss_by_epoch.csv\"\n",
    "    df_loss.to_csv(loss_path, index=False)\n",
    "    print(f\"[INFO] Saved train loss by epoch to: {loss_path}\")\n",
    "\n",
    "    # ---- 最後に、被験者ごとのROC-AUCを()付きでプリント ----\n",
    "    print(\"\\n[SUMMARY] ===== Per-subject ROC-AUC (LOSO) =====\")\n",
    "    print(f\"[SUMMARY] Global ROC-AUC (all folds combined) = {rocauc:.4f}\")\n",
    "\n",
    "    good_mask = df_fold_summary[\"ROC_AUC_test\"].notna() & (df_fold_summary[\"ROC_AUC_test\"] > 0.5)\n",
    "    bad_mask = df_fold_summary[\"ROC_AUC_test\"].notna() & (df_fold_summary[\"ROC_AUC_test\"] <= 0.5)\n",
    "    nan_mask = df_fold_summary[\"ROC_AUC_test\"].isna()\n",
    "\n",
    "    def format_sid_list(mask) -> str:\n",
    "        rows = df_fold_summary.loc[mask, [\"SubjectID\", \"ROC_AUC_test\"]]\n",
    "        if rows.empty:\n",
    "            return \"なし\"\n",
    "        return \", \".join(f\"{row.SubjectID}({row.ROC_AUC_test:.3f})\" for _, row in rows.iterrows())\n",
    "\n",
    "    good_str = format_sid_list(good_mask)\n",
    "    bad_str = format_sid_list(bad_mask)\n",
    "    nan_sids = df_fold_summary.loc[nan_mask, \"SubjectID\"].tolist()\n",
    "    nan_str = \", \".join(nan_sids) if len(nan_sids) > 0 else \"なし\"\n",
    "\n",
    "    print(f\"[SUMMARY] よく当たっている被験者(>0.5): {good_str}\")\n",
    "    print(f\"[SUMMARY] あまり当たっていない被験者(<=0.5): {bad_str}\")\n",
    "    print(f\"[SUMMARY] 評価不能(ROC-AUC算出不可): {nan_str}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell1-LSTM: LOSO＋ROC-AUC Inner GroupKFold\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# パス・基本設定\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\")\n",
    "\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\", \"10063\", \"10064\",\n",
    "    \"10071\", \"10072\", \"10073\", \"10074\",\n",
    "    \"10081\", \"10082\", \"10083\",\n",
    "    \"10091\", \"10092\", \"10093\", \"10094\",\n",
    "    \"10101\", \"10102\", \"10103\",\n",
    "]\n",
    "\n",
    "# このCell用の出力ディレクトリ\n",
    "CELL_NAME = \"Cell1-LSTM\"\n",
    "OUT_DIR = BASE_DIR / \"解析\" / \"Cell1\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Cell: {CELL_NAME}, OUT_DIR = {OUT_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 時間・シーケンス仕様\n",
    "# -----------------------------\n",
    "WINDOW_SEC = 3          # 3秒窓（既にFEATURE2で反映済み）\n",
    "SLIDE_STEP_SEC = 1      # 1秒刻み（既にFEATURE2で反映済み）\n",
    "\n",
    "SEQ_LEN = 30            # LSTMに入れる過去ステップ数（= 過去30秒分）\n",
    "\n",
    "# FEATURE2 での最初の出力時刻（T_START+WINDOW_SEC = 1770+3）\n",
    "BASE_T_MIN = 1773\n",
    "\n",
    "# ターゲットの最小時刻：最初の出力時刻＋(SEQ_LEN-1)\n",
    "# 例：BASE_T_MIN=1773, SEQ_LEN=30 → 1773+29 = 1802\n",
    "TARGET_T_MIN = BASE_T_MIN + (SEQ_LEN - 1)\n",
    "\n",
    "TARGET_T_MAX = 2400     # 上限はこれまで通り 2400 秒\n",
    "\n",
    "# ラベル閾値：FMS >= 1 を陽性とする\n",
    "FMS_POS_THRESHOLD = 1\n",
    "\n",
    "# -----------------------------\n",
    "# LSTMハイパラ（単方向1層）\n",
    "# -----------------------------\n",
    "HIDDEN_SIZE = 32\n",
    "FC_HIDDEN_SIZE = 8\n",
    "DROPOUT_LSTM = 0.0\n",
    "DROPOUT_FC = 0.5\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 256\n",
    "WEIGHT_DECAY = 1e-4          # L2正則化（Adam の weight_decay）\n",
    "\n",
    "INNER_MAX_EPOCHS = 30        # inner GroupKFold で探索する最大エポック\n",
    "INNER_N_SPLITS = 4           # inner GroupKFold の分割数\n",
    "\n",
    "# -----------------------------\n",
    "# 特徴量ON/OFF設定\n",
    "# -----------------------------\n",
    "FEATURE_SWITCHES: List[Tuple[str, bool]] = [\n",
    "    (\"Pulse_rma3\",       True),\n",
    "    (\"Pulse_max3\",       True),\n",
    "    (\"Pulse_min3\",       True),\n",
    "    (\"Pulse_pc3\",        True),\n",
    "    (\"HR_rma3\",          True),\n",
    "    (\"HR_max3\",          True),\n",
    "    (\"HR_min3\",          True),\n",
    "    (\"HR_pc3\",           True),\n",
    "    (\"GSR_rma3\",         True),\n",
    "    (\"GSR_max3\",         True),\n",
    "    (\"GSR_min3\",         True),\n",
    "    (\"GSR_pc3\",          True),\n",
    "    (\"FaceSum_mean3\",    True),\n",
    "    (\"FaceDiff_mean3\",   True),\n",
    "    (\"FaceSum_pc3\",      True),\n",
    "    (\"FaceDiff_pc3\",     True),\n",
    "    (\"Skinos_SweatRate\", True),\n",
    "    (\"Skinos_HeartRate\", True),\n",
    "    (\"Skinos_SkinTemp\",  True),\n",
    "]\n",
    "\n",
    "FEATURE_COLS: List[str] = [name for name, use in FEATURE_SWITCHES if use]\n",
    "if len(FEATURE_COLS) == 0:\n",
    "    raise RuntimeError(\"[ERROR] FEATURE_SWITCHES: 有効な特徴量が0個です（すべてFalse）。\")\n",
    "\n",
    "N_FEATURES = len(FEATURE_COLS)\n",
    "print(f\"[INFO] Using {N_FEATURES} features:\", \", \".join(FEATURE_COLS))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM モデル定義\n",
    "# -----------------------------\n",
    "class LSTMMotionSickness(nn.Module):\n",
    "    \"\"\"\n",
    "    単方向1層LSTM → Dropout → FC(32→8) → ReLU → FC(8→1)\n",
    "    出力はロジット（Sigmoidはloss/評価側で適用）\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = HIDDEN_SIZE,\n",
    "        fc_hidden_size: int = FC_HIDDEN_SIZE,\n",
    "        dropout_lstm: float = DROPOUT_LSTM,\n",
    "        dropout_fc: float = DROPOUT_FC,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout_lstm,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_fc)\n",
    "        self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(fc_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size)\n",
    "        return: ロジット (batch,)\n",
    "        \"\"\"\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        # hn: (num_layers, batch, hidden_size) -> (batch, hidden_size)\n",
    "        h_last = hn[-1]\n",
    "        z = self.dropout(h_last)\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.fc_out(z)         # (batch, 1)\n",
    "        return z.squeeze(-1)       # (batch,)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# データ読み込み & シーケンス生成\n",
    "# -----------------------------\n",
    "def load_subject_df(sid: str) -> pd.DataFrame:\n",
    "    \"\"\"FEATURE2/{sid}_3sFeat_1sSlide.csv を読み込む.\"\"\"\n",
    "    path = BASE_DIR / sid / \"FEATURE2\" / f\"{sid}_3sFeat_1sSlide.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"[ERROR] Subject {sid}: file not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    # 時刻順に並べておく\n",
    "    df = df.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sequences_for_subject(\n",
    "    sid: str,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    1被験者について:\n",
    "      - FEATURE2 CSVを読み込み\n",
    "      - FMS>=1 を陽性にした y(t) を作成\n",
    "      - t=TARGET_T_MIN〜TARGET_T_MAX の各時刻 t に対し，\n",
    "          X_seq(t) = [t-SEQ_LEN+1 .. t] のシーケンスを生成\n",
    "      - その際，特徴量内にNaNがあれば即エラー\n",
    "\n",
    "    戻り値:\n",
    "      X_seq:   (N_seq, SEQ_LEN, N_FEATURES)\n",
    "      y_seq:   (N_seq,)      0/1ラベル\n",
    "      t_seq:   (N_seq,)      対応する Time_sec\n",
    "      fms_seq: (N_seq,)      元のFMSスコア（0〜4）\n",
    "    \"\"\"\n",
    "    df = load_subject_df(sid)\n",
    "\n",
    "    # 必要列が揃っているかチェック\n",
    "    required_cols = [\"Time_sec\", \"FMS\"] + FEATURE_COLS\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: missing columns in FEATURE2 csv: {missing}\")\n",
    "\n",
    "    # NaNチェック（仕様：NaNがあれば即エラー）\n",
    "    if df[FEATURE_COLS].isna().values.any():\n",
    "        nan_mask = df[FEATURE_COLS].isna()\n",
    "        bad_idx = np.where(nan_mask.values)[0][0]\n",
    "        bad_time = df.loc[bad_idx, \"Time_sec\"]\n",
    "        bad_cols = list(nan_mask.columns[nan_mask.iloc[bad_idx]])\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Subject {sid}: NaN detected at Time_sec={bad_time}, cols={bad_cols}\"\n",
    "        )\n",
    "\n",
    "    times = df[\"Time_sec\"].to_numpy().astype(int)\n",
    "    fms = df[\"FMS\"].to_numpy().astype(int)\n",
    "    features = df[FEATURE_COLS].to_numpy().astype(np.float32)\n",
    "\n",
    "    # ターゲット時刻のマスク：TARGET_T_MIN〜TARGET_T_MAX\n",
    "    target_mask = (times >= TARGET_T_MIN) & (times <= TARGET_T_MAX)\n",
    "    if not target_mask.any():\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: no Time_sec in [{TARGET_T_MIN}, {TARGET_T_MAX}]\")\n",
    "\n",
    "    X_list: List[np.ndarray] = []\n",
    "    y_list: List[int] = []\n",
    "    t_list: List[int] = []\n",
    "    fms_list: List[int] = []\n",
    "\n",
    "    for idx in range(len(times)):\n",
    "        t = times[idx]\n",
    "        if t < TARGET_T_MIN or t > TARGET_T_MAX:\n",
    "            continue\n",
    "\n",
    "        if idx < SEQ_LEN - 1:\n",
    "            # 理論上ここには来ないはずだが，一応チェック\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: idx={idx}, Time_sec={t} has no enough history (need {SEQ_LEN}).\"\n",
    "            )\n",
    "\n",
    "        window_feat = features[idx - SEQ_LEN + 1 : idx + 1, :]  # (SEQ_LEN, N_FEATURES)\n",
    "        if not np.isfinite(window_feat).all():\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: non-finite value in sequence ending at Time_sec={t}\"\n",
    "            )\n",
    "\n",
    "        # ラベル：FMS>=1\n",
    "        y = 1 if fms[idx] >= FMS_POS_THRESHOLD else 0\n",
    "\n",
    "        X_list.append(window_feat)\n",
    "        y_list.append(y)\n",
    "        t_list.append(t)\n",
    "        fms_list.append(int(fms[idx]))\n",
    "\n",
    "    X_seq = np.stack(X_list).astype(np.float32)        # (N_seq, SEQ_LEN, N_FEATURES)\n",
    "    y_seq = np.array(y_list, dtype=np.int64)           # (N_seq,)\n",
    "    t_seq = np.array(t_list, dtype=np.int64)           # (N_seq,)\n",
    "    fms_seq = np.array(fms_list, dtype=np.int64)       # (N_seq,)\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] Subject {sid}: target Time_sec range = {t_seq[0]}–{t_seq[-1]}, \"\n",
    "        f\"N_seq = {len(t_seq)}, N_pos = {y_seq.sum()}, N_neg = {len(y_seq) - y_seq.sum()}\"\n",
    "    )\n",
    "\n",
    "    return X_seq, y_seq, t_seq, fms_seq\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Inner GroupKFold (4分割) による best epoch 探索\n",
    "# -----------------------------\n",
    "def run_inner_groupkfold_for_outer_fold(\n",
    "    outer_test_sid: str,\n",
    "    train_X: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    train_groups: np.ndarray,\n",
    "    device: torch.device,\n",
    "    max_epochs: int = INNER_MAX_EPOCHS,\n",
    ") -> Tuple[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    外側 test 被験者 outer_test_sid に対して，\n",
    "    学習側16名を GroupKFold(n_splits=INNER_N_SPLITS) で分割し，\n",
    "    各 epoch の mean validation ROC-AUC を計算して best_epoch を返す。\n",
    "    \"\"\"\n",
    "    gkf = GroupKFold(n_splits=INNER_N_SPLITS)\n",
    "\n",
    "    val_auc_sum = np.zeros(max_epochs, dtype=float)\n",
    "    val_auc_cnt = np.zeros(max_epochs, dtype=int)\n",
    "\n",
    "    print(\n",
    "        f\"[INFO][InnerCV] Start {INNER_N_SPLITS}-fold GroupKFold CV \"\n",
    "        f\"for outer test subject {outer_test_sid}\"\n",
    "    )\n",
    "\n",
    "    for inner_fold, (tr_idx, val_idx) in enumerate(\n",
    "        gkf.split(train_X, train_y, groups=train_groups),\n",
    "        start=1,\n",
    "    ):\n",
    "        X_tr = train_X[tr_idx]\n",
    "        y_tr = train_y[tr_idx]\n",
    "        X_val = train_X[val_idx]\n",
    "        y_val = train_y[val_idx]\n",
    "\n",
    "        # DataLoader 構築\n",
    "        train_ds = TensorDataset(\n",
    "            torch.from_numpy(X_tr).float(),\n",
    "            torch.from_numpy(y_tr).float(),\n",
    "        )\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        # この fold 用のモデルを新規に作成\n",
    "        model = LSTMMotionSickness(input_size=N_FEATURES).to(device)\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "        )\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        print(\n",
    "            f\"[INFO][InnerCV]  Fold {inner_fold}: \"\n",
    "            f\"N_train={len(y_tr)}, N_val={len(y_val)}\"\n",
    "        )\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            # ---- 1 epoch 学習 ----\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            n_batches = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(batch_X)\n",
    "                loss = criterion(logits, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                n_batches += 1\n",
    "\n",
    "            # ---- validation ROC-AUC ----\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                Xv = torch.from_numpy(X_val).float().to(device)\n",
    "                logits_val = model(Xv)\n",
    "                prob_val = torch.sigmoid(logits_val).cpu().numpy()\n",
    "\n",
    "            n_pos = int(y_val.sum())\n",
    "            n_val = len(y_val)\n",
    "            n_neg = n_val - n_pos\n",
    "\n",
    "            if n_pos == 0 or n_neg == 0:\n",
    "                # この fold / epoch は ROC-AUC 未定義 → 平均には入れない\n",
    "                continue\n",
    "\n",
    "            auc = roc_auc_score(y_val.astype(int), prob_val)\n",
    "            val_auc_sum[epoch - 1] += auc\n",
    "            val_auc_cnt[epoch - 1] += 1\n",
    "\n",
    "    # ---- epoch ごとの平均 ROC-AUC ----\n",
    "    mean_val_auc = np.full(max_epochs, np.nan, dtype=float)\n",
    "    mask = val_auc_cnt > 0\n",
    "    mean_val_auc[mask] = val_auc_sum[mask] / val_auc_cnt[mask]\n",
    "\n",
    "    print(\n",
    "        f\"[INFO][InnerCV] mean validation ROC-AUC by epoch \"\n",
    "        f\"for outer test subject {outer_test_sid}:\"\n",
    "    )\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        m = mean_val_auc[epoch - 1]\n",
    "        if np.isnan(m):\n",
    "            print(f\"[INFO][InnerCV]  epoch={epoch:02d}: mean_val_ROC-AUC = nan (no valid folds)\")\n",
    "        else:\n",
    "            print(f\"[INFO][InnerCV]  epoch={epoch:02d}: mean_val_ROC-AUC = {m:.4f}\")\n",
    "\n",
    "    # 有効な epoch の中で最大のものを選択\n",
    "    if np.all(np.isnan(mean_val_auc)):\n",
    "        best_epoch = 1\n",
    "        best_auc = float(\"nan\")\n",
    "    else:\n",
    "        best_idx = int(np.nanargmax(mean_val_auc))\n",
    "        best_epoch = best_idx + 1\n",
    "        best_auc = mean_val_auc[best_idx]\n",
    "\n",
    "    print(\n",
    "        f\"[INFO][InnerCV] Selected best_epoch = {best_epoch} \"\n",
    "        f\"(mean_val_ROC-AUC = {best_auc}) for outer test subject {outer_test_sid}\"\n",
    "    )\n",
    "\n",
    "    return best_epoch, mean_val_auc\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Outer training（best_epoch だけ学習）\n",
    "# -----------------------------\n",
    "def train_outer_model(\n",
    "    outer_test_sid: str,\n",
    "    train_X: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    device: torch.device,\n",
    "    n_epochs: int,\n",
    ") -> Tuple[LSTMMotionSickness, List[Dict]]:\n",
    "    \"\"\"\n",
    "    inner GroupKFold で決まった best_epoch (= n_epochs) だけ\n",
    "    学習データ全体で学習する。\n",
    "    戻り値:\n",
    "      model: 学習済みモデル\n",
    "      loss_log: [{SubjectID, epoch, train_loss}, ...]\n",
    "    \"\"\"\n",
    "    model = LSTMMotionSickness(input_size=N_FEATURES).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    n_train = len(train_y)\n",
    "    n_pos = int(train_y.sum())\n",
    "    n_neg = n_train - n_pos\n",
    "    pos_ratio = n_pos / n_train if n_train > 0 else 0.0\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] Train stats (outer test {outer_test_sid}): \"\n",
    "        f\"N={n_train}, N_pos={n_pos}, N_neg={n_neg}, pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "    print(f\"[INFO] Train final model for outer test subject {outer_test_sid} with best_epoch={n_epochs}\")\n",
    "\n",
    "    X_tensor = torch.from_numpy(train_X).float()\n",
    "    y_tensor = torch.from_numpy(train_y).float()\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    loss_log: List[Dict] = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / max(n_batches, 1)\n",
    "        print(\n",
    "            f\"[INFO] Epoch {epoch:02d}/{n_epochs:02d} \"\n",
    "            f\"(outer test {outer_test_sid}) - train_loss={avg_loss:.4f}\"\n",
    "        )\n",
    "        loss_log.append(\n",
    "            {\n",
    "                \"SubjectID\": outer_test_sid,\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": avg_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return model, loss_log\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # デバイス選択\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    # 再現性のためにseed固定\n",
    "    torch.manual_seed(20251206)\n",
    "    np.random.seed(20251206)\n",
    "\n",
    "    # ---- 全被験者のシーケンスを構築 ----\n",
    "    X_by_sid: Dict[str, np.ndarray] = {}\n",
    "    y_by_sid: Dict[str, np.ndarray] = {}\n",
    "    t_by_sid: Dict[str, np.ndarray] = {}\n",
    "    fms_by_sid: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        print(f\"[INFO] ==== Build sequences: Subject {sid} ====\")\n",
    "        X_seq, y_seq, t_seq, fms_seq = build_sequences_for_subject(sid)\n",
    "        X_by_sid[sid] = X_seq\n",
    "        y_by_sid[sid] = y_seq\n",
    "        t_by_sid[sid] = t_seq\n",
    "        fms_by_sid[sid] = fms_seq\n",
    "\n",
    "    # 全体陽性割合（参考）\n",
    "    all_y_tmp = np.concatenate([y_by_sid[sid] for sid in SUBJECT_IDS])\n",
    "    print(\n",
    "        f\"[INFO] Overall (all subjects) target stats: \"\n",
    "        f\"N={len(all_y_tmp)}, N_pos={all_y_tmp.sum()}, \"\n",
    "        f\"pos_ratio={all_y_tmp.mean():.3f}\"\n",
    "    )\n",
    "\n",
    "    # ---- 外側 LOSO 学習・評価 ----\n",
    "    all_probs: List[np.ndarray] = []\n",
    "    all_true: List[np.ndarray] = []\n",
    "    pred_rows: List[pd.DataFrame] = []        # シーケンスごとの詳細保存用\n",
    "    fold_summary_rows: List[Dict] = []        # 被験者ごとのサマリ\n",
    "    train_loss_rows: List[Dict] = []          # outer training の loss ログ\n",
    "\n",
    "    for test_sid in SUBJECT_IDS:\n",
    "        print(f\"\\n[INFO] ===== LOSO fold: Test Subject {test_sid} =====\")\n",
    "\n",
    "        # 学習・テストデータをLOSOで分割\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "        train_groups_list = []\n",
    "\n",
    "        for sid in SUBJECT_IDS:\n",
    "            if sid == test_sid:\n",
    "                continue\n",
    "            X_sid = X_by_sid[sid]\n",
    "            y_sid = y_by_sid[sid]\n",
    "\n",
    "            train_X_list.append(X_sid)\n",
    "            train_y_list.append(y_sid)\n",
    "\n",
    "            n_seq_sid = len(y_sid)\n",
    "            train_groups_list.append(np.full(n_seq_sid, sid))\n",
    "\n",
    "        train_X = np.concatenate(train_X_list, axis=0)\n",
    "        train_y = np.concatenate(train_y_list, axis=0)\n",
    "        train_groups = np.concatenate(train_groups_list, axis=0)\n",
    "\n",
    "        test_X = X_by_sid[test_sid]\n",
    "        test_y = y_by_sid[test_sid]\n",
    "        test_t = t_by_sid[test_sid]\n",
    "        test_fms = fms_by_sid[test_sid]\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Fold data sizes: \"\n",
    "            f\"Train N_seq={len(train_y)}, Test N_seq={len(test_y)}\"\n",
    "        )\n",
    "\n",
    "        # ---- Inner GroupKFold で best_epoch 探索 ----\n",
    "        best_epoch, mean_val_auc = run_inner_groupkfold_for_outer_fold(\n",
    "            outer_test_sid=test_sid,\n",
    "            train_X=train_X,\n",
    "            train_y=train_y,\n",
    "            train_groups=train_groups,\n",
    "            device=device,\n",
    "            max_epochs=INNER_MAX_EPOCHS,\n",
    "        )\n",
    "\n",
    "        best_inner_auc = (\n",
    "            float(mean_val_auc[best_epoch - 1])\n",
    "            if not np.isnan(mean_val_auc[best_epoch - 1])\n",
    "            else float(\"nan\")\n",
    "        )\n",
    "\n",
    "        # ---- best_epoch だけ outer train で学習 ----\n",
    "        model, loss_log = train_outer_model(\n",
    "            outer_test_sid=test_sid,\n",
    "            train_X=train_X,\n",
    "            train_y=train_y,\n",
    "            device=device,\n",
    "            n_epochs=best_epoch,\n",
    "        )\n",
    "        train_loss_rows.extend(loss_log)\n",
    "\n",
    "        # ---- outer test 被験者の予測 ----\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "            logits = model(X_test_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_true.append(test_y.astype(int))\n",
    "\n",
    "        # per-fold ROC-AUC\n",
    "        n_test = len(test_y)\n",
    "        n_test_pos = int(test_y.sum())\n",
    "        n_test_neg = n_test - n_test_pos\n",
    "\n",
    "        if n_test_pos == 0 or n_test_neg == 0:\n",
    "            roc_auc_test = float(\"nan\")\n",
    "            print(\n",
    "                f\"[INFO] Subject {test_sid}: ROC-AUC undefined \"\n",
    "                f\"(N_pos={n_test_pos}, N_neg={n_test_neg})\"\n",
    "            )\n",
    "        else:\n",
    "            roc_auc_test = roc_auc_score(test_y.astype(int), probs)\n",
    "            print(\n",
    "                f\"[INFO] Subject {test_sid}: ROC-AUC(test fold) = {roc_auc_test:.4f} \"\n",
    "                f\"(N_test={n_test}, N_pos={n_test_pos}, N_neg={n_test_neg})\"\n",
    "            )\n",
    "\n",
    "        # fold summary 行を追加\n",
    "        fold_summary_rows.append(\n",
    "            dict(\n",
    "                SubjectID=test_sid,\n",
    "                N_train=len(train_y),\n",
    "                N_train_pos=int(train_y.sum()),\n",
    "                N_train_neg=int(len(train_y) - train_y.sum()),\n",
    "                N_test=n_test,\n",
    "                N_test_pos=n_test_pos,\n",
    "                N_test_neg=n_test_neg,\n",
    "                best_epoch=best_epoch,\n",
    "                best_inner_mean_val_roc_auc=best_inner_auc,\n",
    "                test_roc_auc=roc_auc_test,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # このfoldの予測詳細を保存用に集約\n",
    "        df_fold = pd.DataFrame(\n",
    "            {\n",
    "                \"SubjectID\": test_sid,\n",
    "                \"Time_sec\": test_t,\n",
    "                \"FMS\": test_fms,\n",
    "                \"Label_bin\": test_y.astype(int),\n",
    "                \"Prob_FMS_ge1\": probs,\n",
    "            }\n",
    "        )\n",
    "        pred_rows.append(df_fold)\n",
    "\n",
    "    # ---- 全foldをまとめた ROC-AUC ----\n",
    "    y_all = np.concatenate(all_true)\n",
    "    p_all = np.concatenate(all_probs)\n",
    "\n",
    "    n_total = len(y_all)\n",
    "    n_pos = int(y_all.sum())\n",
    "    n_neg = n_total - n_pos\n",
    "    pos_ratio = n_pos / n_total if n_total > 0 else 0.0\n",
    "\n",
    "    print(\"\\n[INFO] ===== Overall LOSO result =====\")\n",
    "    print(\n",
    "        f\"[INFO] All folds combined: N={n_total}, N_pos={n_pos}, \"\n",
    "        f\"N_neg={n_neg}, pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    if n_pos == 0 or n_pos == n_total:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] ROC-AUC undefined: labels are all the same \"\n",
    "            f\"(N={n_total}, N_pos={n_pos}).\"\n",
    "        )\n",
    "\n",
    "    rocauc_global = roc_auc_score(y_all, p_all)\n",
    "    print(f\"[RESULT] Global ROC-AUC (LOSO, LSTM, FMS>=1) = {rocauc_global:.4f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 結果保存\n",
    "    # -----------------------------\n",
    "    # 1) ROC-AUC のサマリ（ハイパラ込み）\n",
    "    result_path = OUT_DIR / \"Cell1_LSTM_LOSO_ROCAUC.csv\"\n",
    "    df_result = pd.DataFrame(\n",
    "        {\n",
    "            \"ROC_AUC_global\": [rocauc_global],\n",
    "            \"N_total\": [n_total],\n",
    "            \"N_pos\": [n_pos],\n",
    "            \"N_neg\": [n_neg],\n",
    "            \"pos_ratio\": [pos_ratio],\n",
    "            \"N_features\": [N_FEATURES],\n",
    "            \"feature_list\": [\",\".join(FEATURE_COLS)],\n",
    "            \"SEQ_LEN\": [SEQ_LEN],\n",
    "            \"HIDDEN_SIZE\": [HIDDEN_SIZE],\n",
    "            \"FC_HIDDEN_SIZE\": [FC_HIDDEN_SIZE],\n",
    "            \"DROPOUT_LSTM\": [DROPOUT_LSTM],\n",
    "            \"DROPOUT_FC\": [DROPOUT_FC],\n",
    "            \"LEARNING_RATE\": [LEARNING_RATE],\n",
    "            \"BATCH_SIZE\": [BATCH_SIZE],\n",
    "            \"WEIGHT_DECAY\": [WEIGHT_DECAY],\n",
    "            \"INNER_MAX_EPOCHS\": [INNER_MAX_EPOCHS],\n",
    "            \"INNER_N_SPLITS\": [INNER_N_SPLITS],\n",
    "        }\n",
    "    )\n",
    "    df_result.to_csv(result_path, index=False)\n",
    "    print(f\"[INFO] Saved ROC-AUC result to: {result_path}\")\n",
    "\n",
    "    # 2) シーケンスごとの詳細予測\n",
    "    df_pred = pd.concat(pred_rows, ignore_index=True)\n",
    "    pred_path = OUT_DIR / \"Cell1_LSTM_LOSO_pred_detail.csv\"\n",
    "    df_pred.to_csv(pred_path, index=False)\n",
    "    print(f\"[INFO] Saved per-sequence predictions to: {pred_path}\")\n",
    "\n",
    "    # 3) 被験者ごとのfoldサマリ\n",
    "    df_fold_summary = pd.DataFrame(fold_summary_rows)\n",
    "    fold_summary_path = OUT_DIR / \"Cell1_LSTM_LOSO_fold_summary.csv\"\n",
    "    df_fold_summary.to_csv(fold_summary_path, index=False)\n",
    "    print(f\"[INFO] Saved per-fold summary to: {fold_summary_path}\")\n",
    "\n",
    "    # 4) outer training の train loss ログ\n",
    "    df_train_loss = pd.DataFrame(train_loss_rows)\n",
    "    loss_path = OUT_DIR / \"Cell1_LSTM_LOSO_train_loss_by_epoch.csv\"\n",
    "    df_train_loss.to_csv(loss_path, index=False)\n",
    "    print(f\"[INFO] Saved train loss by epoch to: {loss_path}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # コンソール用の簡易サマリ\n",
    "    # -----------------------------\n",
    "    print(\"\\n[SUMMARY] ===== Per-subject ROC-AUC (LOSO) =====\")\n",
    "    print(f\"[SUMMARY] Global ROC-AUC (all folds combined) = {rocauc_global:.4f}\")\n",
    "\n",
    "    # ROC-AUC > 0.5 / <= 0.5 / NaN で分類\n",
    "    good_list = []\n",
    "    bad_list = []\n",
    "    nan_list = []\n",
    "\n",
    "    for row in fold_summary_rows:\n",
    "        sid = row[\"SubjectID\"]\n",
    "        auc = row[\"test_roc_auc\"]\n",
    "        if np.isnan(auc):\n",
    "            nan_list.append(sid)\n",
    "        elif auc > 0.5:\n",
    "            good_list.append(f\"{sid}({auc:.3f})\")\n",
    "        else:\n",
    "            bad_list.append(f\"{sid}({auc:.3f})\")\n",
    "\n",
    "    print(\"[SUMMARY] よく当たっている被験者(>0.5): \" + (\", \".join(good_list) if good_list else \"なし\"))\n",
    "    print(\"[SUMMARY] あまり当たっていない被験者(<=0.5): \" + (\", \".join(bad_list) if bad_list else \"なし\"))\n",
    "    print(\"[SUMMARY] 評価不能(ROC-AUC算出不可): \" + (\", \".join(nan_list) if nan_list else \"なし\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2254350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell2-LSTM-SHAP: SHAP重要度可視化\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# 前提チェック（Cell1-LSTM 実行済み想定）\n",
    "# --------------------------------\n",
    "required_globals = [\n",
    "    \"BASE_DIR\",\n",
    "    \"SUBJECT_IDS\",\n",
    "    \"FEATURE_COLS\",\n",
    "    \"SEQ_LEN\",\n",
    "    \"TARGET_T_MIN\",\n",
    "    \"TARGET_T_MAX\",\n",
    "    \"FMS_POS_THRESHOLD\",\n",
    "    \"LSTMMotionSickness\",\n",
    "    \"build_sequences_for_subject\",\n",
    "    \"HIDDEN_SIZE\",\n",
    "    \"FC_HIDDEN_SIZE\",\n",
    "    \"DROPOUT_LSTM\",\n",
    "    \"DROPOUT_FC\",\n",
    "    \"LEARNING_RATE\",\n",
    "    \"BATCH_SIZE\",\n",
    "    \"N_EPOCHS\",\n",
    "]\n",
    "missing = [name for name in required_globals if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"[Cell2-LSTM-SHAP] 必要な定義が見つかりません。\"\n",
    "        \"先に Cell1-LSTM を同じノートブック上で実行してください。\\n\"\n",
    "        f\"不足: {missing}\"\n",
    "    )\n",
    "\n",
    "N_FEATURES = len(FEATURE_COLS)\n",
    "\n",
    "# このCell用の出力ディレクトリ\n",
    "CELL_NAME = \"Cell2-LSTM-SHAP\"\n",
    "OUT_DIR = BASE_DIR / \"解析\" / \"Cell2\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO][SHAP] Cell: {CELL_NAME}, OUT_DIR = {OUT_DIR}\")\n",
    "\n",
    "# SHAP用のサンプル数設定\n",
    "N_BACKGROUND_MAX = 200   # 各foldで DeepExplainer の背景に使う最大サンプル数\n",
    "N_SHAP_EVAL_MAX = 2000   # 各foldで SHAP を計算する最大サンプル数（訓練シーケンス）\n",
    "\n",
    "\n",
    "# --------------------------------\n",
    "# SHAP用ラッパーモデル\n",
    "#   base_model: (batch, seq_len, feat) -> (batch,)\n",
    "#   → DeepExplainer 用に (batch, 1) に変形\n",
    "# --------------------------------\n",
    "class WrappedLSTMForSHAP(nn.Module):\n",
    "    \"\"\"\n",
    "    SHAP用ラッパー:\n",
    "    - base_model の出力 (batch,) を (batch, 1) に変形して返す\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.base_model(x)      # (batch,)\n",
    "        return logits.unsqueeze(1)       # (batch, 1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1 fold 学習（Cell1と同仕様）\n",
    "# -----------------------------\n",
    "def train_one_fold_for_shap(\n",
    "    train_X: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    device: torch.device,\n",
    ") -> \"LSTMMotionSickness\":\n",
    "    \"\"\"\n",
    "    SHAP用：Cell1-LSTM と同じ設定で1fold分のLSTMを学習する。\n",
    "    \"\"\"\n",
    "    model = LSTMMotionSickness(input_size=N_FEATURES).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    n_train = len(train_y)\n",
    "    n_pos = int(train_y.sum())\n",
    "    n_neg = n_train - n_pos\n",
    "    pos_ratio = n_pos / n_train if n_train > 0 else 0.0\n",
    "    print(\n",
    "        f\"[INFO][SHAP] Train stats: N={n_train}, N_pos={n_pos}, N_neg={n_neg}, \"\n",
    "        f\"pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    X_tensor = torch.from_numpy(train_X).float()\n",
    "    y_tensor = torch.from_numpy(train_y).float()\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / max(n_batches, 1)\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == N_EPOCHS:\n",
    "            print(f\"[INFO][SHAP] Epoch {epoch:02d}/{N_EPOCHS} - train_loss={avg_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# メイン処理（LOSO＋SHAP）\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # デバイス選択\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO][SHAP] Using device: {device}\")\n",
    "\n",
    "    torch.manual_seed(20251206)\n",
    "    np.random.seed(20251206)\n",
    "\n",
    "    # --- 全被験者のシーケンス構築（Cell1と同じ関数を再利用） ---\n",
    "    X_by_sid: Dict[str, np.ndarray] = {}\n",
    "    y_by_sid: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        print(f\"[INFO][SHAP] ==== Build sequences: Subject {sid} ====\")\n",
    "        X_seq, y_seq, t_seq, fms_seq = build_sequences_for_subject(sid)\n",
    "        X_by_sid[sid] = X_seq\n",
    "        y_by_sid[sid] = y_seq\n",
    "\n",
    "    # 全体の陽性割合（参考）\n",
    "    all_y_tmp = np.concatenate([y_by_sid[sid] for sid in SUBJECT_IDS])\n",
    "    print(\n",
    "        f\"[INFO][SHAP] Overall (all subjects) target stats: \"\n",
    "        f\"N={len(all_y_tmp)}, N_pos={all_y_tmp.sum()}, \"\n",
    "        f\"pos_ratio={all_y_tmp.mean():.3f}\"\n",
    "    )\n",
    "\n",
    "    # --- LOSO 各foldで SHAP を計算 ---\n",
    "    fold_importances: List[np.ndarray] = []\n",
    "    fold_meta: List[Dict[str, str]] = []\n",
    "\n",
    "    # beeswarm 用：各foldの shap と特徴量値（時間平均）を保持\n",
    "    shap_samples_list: List[np.ndarray] = []\n",
    "    feature_values_list: List[np.ndarray] = []\n",
    "\n",
    "    for fold_idx, test_sid in enumerate(SUBJECT_IDS):\n",
    "        print(f\"\\n[INFO][SHAP] ===== LOSO fold {fold_idx+1}/{len(SUBJECT_IDS)}: Test {test_sid} =====\")\n",
    "\n",
    "        # 学習データ（LOSO）\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "        for sid in SUBJECT_IDS:\n",
    "            if sid == test_sid:\n",
    "                continue\n",
    "            train_X_list.append(X_by_sid[sid])\n",
    "            train_y_list.append(y_by_sid[sid])\n",
    "\n",
    "        train_X = np.concatenate(train_X_list, axis=0)  # (N_train, SEQ_LEN, N_FEATURES)\n",
    "        train_y = np.concatenate(train_y_list, axis=0)  # (N_train,)\n",
    "\n",
    "        print(\n",
    "            f\"[INFO][SHAP] Fold data sizes: \"\n",
    "            f\"Train N_seq={len(train_y)}, Test N_seq={len(y_by_sid[test_sid])}\"\n",
    "        )\n",
    "\n",
    "        # --- モデル学習 ---\n",
    "        model = train_one_fold_for_shap(train_X, train_y, device=device)\n",
    "        model.eval()\n",
    "\n",
    "        # --- SHAP用の背景データ（background）をサンプリング ---\n",
    "        n_train = train_X.shape[0]\n",
    "        if n_train > N_BACKGROUND_MAX:\n",
    "            idx_bg = np.random.choice(n_train, size=N_BACKGROUND_MAX, replace=False)\n",
    "            bg_X = train_X[idx_bg]\n",
    "        else:\n",
    "            bg_X = train_X\n",
    "\n",
    "        print(f\"[INFO][SHAP] Using {len(bg_X)} samples as background\")\n",
    "        background = torch.from_numpy(bg_X).float().to(device)\n",
    "\n",
    "        # --- SHAPを計算する対象サンプル（訓練シーケンスのサブセット） ---\n",
    "        if n_train > N_SHAP_EVAL_MAX:\n",
    "            idx_eval = np.random.choice(n_train, size=N_SHAP_EVAL_MAX, replace=False)\n",
    "            X_eval = train_X[idx_eval]\n",
    "        else:\n",
    "            X_eval = train_X\n",
    "\n",
    "        print(f\"[INFO][SHAP] Computing SHAP on {len(X_eval)} training sequences\")\n",
    "        X_eval_tensor = torch.from_numpy(X_eval).float().to(device)\n",
    "\n",
    "        # --- DeepExplainer を構築 ---\n",
    "        wrapped_model = WrappedLSTMForSHAP(model).to(device)\n",
    "        explainer = shap.DeepExplainer(wrapped_model, background)\n",
    "\n",
    "        # shap_values: (N_eval, SEQ_LEN, N_FEATURES, [output_dim]) か，\n",
    "        # それを要素に持つリスト\n",
    "        # ★ additivity チェックをオフ（RNN系でよく落ちるので）\n",
    "        shap_values = explainer.shap_values(X_eval_tensor, check_additivity=False)\n",
    "\n",
    "        # 戻り値の型に応じて整形\n",
    "        if isinstance(shap_values, list):\n",
    "            sv = shap_values[0]\n",
    "        else:\n",
    "            sv = shap_values\n",
    "\n",
    "        if isinstance(sv, torch.Tensor):\n",
    "            sv = sv.detach().cpu().numpy()\n",
    "        else:\n",
    "            sv = np.array(sv)\n",
    "\n",
    "        # 出力次元が最後に1つだけ付いている場合は squeeze\n",
    "        # 例: (N_eval, SEQ_LEN, N_FEATURES, 1) → (N_eval, SEQ_LEN, N_FEATURES)\n",
    "        if sv.ndim == 4 and sv.shape[-1] == 1:\n",
    "            sv = sv[..., 0]\n",
    "\n",
    "        # 形チェック\n",
    "        if sv.ndim != 3 or sv.shape[1] != SEQ_LEN or sv.shape[2] != N_FEATURES:\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR][SHAP] Unexpected SHAP shape: {sv.shape}, \"\n",
    "                f\"expected (N_eval, {SEQ_LEN}, {N_FEATURES})\"\n",
    "            )\n",
    "\n",
    "        # --- beeswarm 用：時間方向で平均して 2次元にする ---\n",
    "        # sv_mean_t: (N_eval, N_FEATURES) … 30秒履歴の平均寄与\n",
    "        sv_mean_t = sv.mean(axis=1)\n",
    "\n",
    "        # 特徴量値も時間平均をとる（色付け用）\n",
    "        X_eval_np = X_eval_tensor.detach().cpu().numpy()  # (N_eval, SEQ_LEN, N_FEATURES)\n",
    "        X_mean_t = X_eval_np.mean(axis=1)                 # (N_eval, N_FEATURES)\n",
    "\n",
    "        shap_samples_list.append(sv_mean_t)\n",
    "        feature_values_list.append(X_mean_t)\n",
    "\n",
    "        # --- fold内の特徴重要度：mean(|SHAP|) over (samples, time) ---\n",
    "        abs_sv = np.abs(sv)\n",
    "        imp_fold = abs_sv.mean(axis=(0, 1))  # (N_FEATURES,)\n",
    "        fold_importances.append(imp_fold)\n",
    "        fold_meta.append({\"fold_idx\": fold_idx, \"test_subject\": test_sid})\n",
    "\n",
    "        # foldごとのトップ特徴をざっくり表示\n",
    "        order = np.argsort(-imp_fold)  # 降順\n",
    "        top_k = min(5, N_FEATURES)\n",
    "        print(\"[INFO][SHAP] Top features in this fold:\")\n",
    "        for i in range(top_k):\n",
    "            j = order[i]\n",
    "            print(f\"  {i+1}. {FEATURE_COLS[j]} : mean|SHAP| = {imp_fold[j]:.4e}\")\n",
    "\n",
    "    # --- fold間で平均して最終重要度を算出 ---\n",
    "    imp_mat = np.stack(fold_importances, axis=0)  # (n_folds, N_FEATURES)\n",
    "    mean_imp = imp_mat.mean(axis=0)               # (N_FEATURES,)\n",
    "    std_imp = imp_mat.std(axis=0)                 # (N_FEATURES,)\n",
    "\n",
    "    order_global = np.argsort(-mean_imp)          # 降順\n",
    "\n",
    "    # ランキング表を作成\n",
    "    rows = []\n",
    "    for rank, idx in enumerate(order_global, start=1):\n",
    "        rows.append(\n",
    "            {\n",
    "                \"rank\": rank,\n",
    "                \"feature\": FEATURE_COLS[idx],\n",
    "                \"mean_abs_shap\": float(mean_imp[idx]),\n",
    "                \"std_abs_shap\": float(std_imp[idx]),\n",
    "            }\n",
    "        )\n",
    "    df_rank = pd.DataFrame(rows)\n",
    "\n",
    "    # --- CSV保存（全体ランキング） ---\n",
    "    rank_path = OUT_DIR / \"Cell2_LSTM_SHAP_feature_importance.csv\"\n",
    "    df_rank.to_csv(rank_path, index=False)\n",
    "    print(f\"[INFO][SHAP] Saved SHAP feature ranking to: {rank_path}\")\n",
    "\n",
    "    # fold別の重要度行列も保存\n",
    "    df_fold_imp = pd.DataFrame(\n",
    "        imp_mat,\n",
    "        columns=[f\"SHAP_{name}\" for name in FEATURE_COLS],\n",
    "    )\n",
    "    df_fold_imp.insert(0, \"test_subject\", [m[\"test_subject\"] for m in fold_meta])\n",
    "    df_fold_imp.insert(0, \"fold_idx\", [m[\"fold_idx\"] for m in fold_meta])\n",
    "    fold_imp_path = OUT_DIR / \"Cell2_LSTM_SHAP_feature_importance_by_fold.csv\"\n",
    "    df_fold_imp.to_csv(fold_imp_path, index=False)\n",
    "    print(f\"[INFO][SHAP] Saved fold-wise SHAP importances to: {fold_imp_path}\")\n",
    "\n",
    "    # --- バー図で可視化（特徴重要度, mean|SHAP|） ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    idxs = order_global  # 重要度降順\n",
    "    y_pos = np.arange(len(idxs))\n",
    "\n",
    "    plt.barh(y_pos, mean_imp[idxs])\n",
    "    plt.yticks(y_pos, [FEATURE_COLS[i] for i in idxs], fontsize=20)\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    plt.xlabel(\"Mean |SHAP| (over samples & time)\", fontsize=24)\n",
    "    plt.title(\"LSTM (30s history) SHAP feature importance\", fontsize=30)\n",
    "    plt.xticks(fontsize=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig_path = OUT_DIR / \"Cell2_LSTM_SHAP_feature_importance_bar.png\"\n",
    "    plt.savefig(fig_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[INFO][SHAP] Saved SHAP bar plot to: {fig_path}\")\n",
    "\n",
    "    # --- SHAP summary beeswarm 図（全fold統合） ---\n",
    "    try:\n",
    "        shap_all = np.concatenate(shap_samples_list, axis=0)   # (N_total, N_FEATURES)\n",
    "        X_all_plot = np.concatenate(feature_values_list, axis=0)\n",
    "        X_all_df = pd.DataFrame(X_all_plot, columns=FEATURE_COLS)\n",
    "\n",
    "        shap.summary_plot(\n",
    "            shap_all,\n",
    "            X_all_df,\n",
    "            feature_names=FEATURE_COLS,\n",
    "            show=False,\n",
    "            max_display=len(FEATURE_COLS),\n",
    "        )\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        ax = plt.gca()\n",
    "        ax.set_xlabel(\"SHAP value (impact on model output)\", fontsize=24)\n",
    "        fig.suptitle(\"LSTM (30s history) SHAP summary (all folds)\", fontsize=30, y=1.02)\n",
    "\n",
    "        for lbl in ax.get_xticklabels():\n",
    "            lbl.set_fontsize(20)\n",
    "        for lbl in ax.get_yticklabels():\n",
    "            lbl.set_fontsize(20)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        beeswarm_path = OUT_DIR / \"Cell2_LSTM_SHAP_summary_beeswarm.png\"\n",
    "        fig.savefig(beeswarm_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"[INFO][SHAP] Saved SHAP summary beeswarm plot to: {beeswarm_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN][SHAP] Failed to create SHAP summary beeswarm plot: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d71dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell1-LSTM: LSTMパラメータ探索\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# パス・基本設定\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\")\n",
    "\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\", \"10063\", \"10064\",\n",
    "    \"10071\", \"10072\", \"10073\", \"10074\",\n",
    "    \"10081\", \"10082\", \"10083\",\n",
    "    \"10091\", \"10092\", \"10093\", \"10094\",\n",
    "    \"10101\", \"10102\", \"10103\",\n",
    "]\n",
    "\n",
    "# このCell用の出力ディレクトリ\n",
    "CELL_NAME = \"Cell1-LSTM\"\n",
    "OUT_DIR = BASE_DIR / \"解析\" / \"Cell1_12071\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Cell: {CELL_NAME}, OUT_DIR = {OUT_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 時間・シーケンス仕様（SEQ_LENは固定）\n",
    "# -----------------------------\n",
    "WINDOW_SEC = 3          # 3秒窓（FEATURE2で反映済み）\n",
    "SLIDE_STEP_SEC = 1      # 1秒刻み（FEATURE2で反映済み）\n",
    "\n",
    "# LSTM に入れる過去ステップ数（= 過去 SEQ_LEN 秒分） ← 固定\n",
    "SEQ_LEN = 10\n",
    "\n",
    "# FEATURE2 での最初の出力時刻（T_START+WINDOW_SEC = 1770+3）\n",
    "BASE_T_MIN = 1773\n",
    "\n",
    "# ターゲットの最小時刻：最初の出力時刻＋(SEQ_LEN-1)\n",
    "TARGET_T_MIN = BASE_T_MIN + (SEQ_LEN - 1)\n",
    "TARGET_T_MAX = 2400     # 上限はこれまで通り 2400 秒\n",
    "\n",
    "# ラベル閾値：FMS >= 1 を陽性とする\n",
    "FMS_POS_THRESHOLD = 1\n",
    "\n",
    "# -----------------------------\n",
    "# LSTMハイパラ（デフォルト値）\n",
    "#   → 実際には CONFIG_LIST 側で上書き\n",
    "# -----------------------------\n",
    "DEFAULT_FC_HIDDEN_SIZE = 8\n",
    "DEFAULT_DROPOUT_LSTM = 0.0\n",
    "DEFAULT_DROPOUT_FC = 0.5\n",
    "DEFAULT_LEARNING_RATE = 0.005\n",
    "DEFAULT_BATCH_SIZE = 256\n",
    "DEFAULT_N_EPOCHS = 10   # 基準としての値（実際の学習回数は config[\"N_EPOCHS\"]）\n",
    "\n",
    "# -----------------------------\n",
    "# 特徴量ON/OFF設定\n",
    "# -----------------------------\n",
    "FEATURE_SWITCHES: List[Tuple[str, bool]] = [\n",
    "    (\"Pulse_rma3\",       True),\n",
    "    (\"Pulse_max3\",       True),\n",
    "    (\"Pulse_min3\",       True),\n",
    "    (\"Pulse_pc3\",        True),\n",
    "    (\"HR_rma3\",          True),\n",
    "    (\"HR_max3\",          True),\n",
    "    (\"HR_min3\",          True),\n",
    "    (\"HR_pc3\",           True),\n",
    "    (\"GSR_rma3\",         True),\n",
    "    (\"GSR_max3\",         True),\n",
    "    (\"GSR_min3\",         True),\n",
    "    (\"GSR_pc3\",          True),\n",
    "    (\"FaceSum_mean3\",    True),\n",
    "    (\"FaceDiff_mean3\",   True),\n",
    "    (\"FaceSum_pc3\",      True),\n",
    "    (\"FaceDiff_pc3\",     True),\n",
    "    (\"Skinos_SweatRate\", True),\n",
    "    (\"Skinos_HeartRate\", False),\n",
    "    (\"Skinos_SkinTemp\",  True),\n",
    "    (\"MSSQ_percentile01\",  True),\n",
    "]\n",
    "\n",
    "FEATURE_COLS: List[str] = [name for name, use in FEATURE_SWITCHES if use]\n",
    "if len(FEATURE_COLS) == 0:\n",
    "    raise RuntimeError(\"[ERROR] FEATURE_SWITCHES: 有効な特徴量が0個です（すべてFalse）。\")\n",
    "\n",
    "N_FEATURES = len(FEATURE_COLS)\n",
    "print(f\"[INFO] Using {N_FEATURES} features:\", \", \".join(FEATURE_COLS))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM モデル定義\n",
    "# -----------------------------\n",
    "class LSTMMotionSickness(nn.Module):\n",
    "    \"\"\"\n",
    "    単方向1層LSTM → Dropout → FC(HIDDEN_SIZE→FC_HIDDEN_SIZE) → ReLU → FC → ロジット\n",
    "    出力はロジット（Sigmoidはloss/評価側で適用）\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        fc_hidden_size: int,\n",
    "        dropout_lstm: float,\n",
    "        dropout_fc: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout_lstm,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_fc)\n",
    "        self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(fc_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size)\n",
    "        return: ロジット (batch,)\n",
    "        \"\"\"\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        h_last = hn[-1]              # (batch, hidden_size)\n",
    "        z = self.dropout(h_last)\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.fc_out(z)           # (batch, 1)\n",
    "        return z.squeeze(-1)         # (batch,)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# データ読み込み & シーケンス生成\n",
    "# -----------------------------\n",
    "def load_subject_df(sid: str) -> pd.DataFrame:\n",
    "    \"\"\"FEATURE2/{sid}_3sFeat_1sSlide.csv を読み込む.\"\"\"\n",
    "    path = BASE_DIR / sid / \"FEATURE2\" / f\"{sid}_3sFeat_1sSlide.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"[ERROR] Subject {sid}: file not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sequences_for_subject(\n",
    "    sid: str,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    1被験者について:\n",
    "      - FEATURE2 CSVを読み込み\n",
    "      - FMS>=1 を陽性にした y(t) を作成\n",
    "      - t=TARGET_T_MIN〜TARGET_T_MAX の各時刻 t に対し，\n",
    "          X_seq(t) = [t-SEQ_LEN+1 .. t] のシーケンスを生成\n",
    "      - その際，特徴量内にNaNがあれば即エラー\n",
    "    \"\"\"\n",
    "    df = load_subject_df(sid)\n",
    "\n",
    "    # 必要列が揃っているかチェック\n",
    "    required_cols = [\"Time_sec\", \"FMS\"] + FEATURE_COLS\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: missing columns in FEATURE2 csv: {missing}\")\n",
    "\n",
    "    # NaNチェック（仕様：NaNがあれば即エラー）\n",
    "    if df[FEATURE_COLS].isna().values.any():\n",
    "        nan_mask = df[FEATURE_COLS].isna()\n",
    "        bad_idx = np.where(nan_mask.values)[0][0]\n",
    "        bad_time = df.loc[bad_idx, \"Time_sec\"]\n",
    "        bad_cols = list(nan_mask.columns[nan_mask.iloc[bad_idx]])\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Subject {sid}: NaN detected at Time_sec={bad_time}, cols={bad_cols}\"\n",
    "        )\n",
    "\n",
    "    times = df[\"Time_sec\"].to_numpy().astype(int)\n",
    "    fms = df[\"FMS\"].to_numpy().astype(int)\n",
    "    features = df[FEATURE_COLS].to_numpy().astype(np.float32)\n",
    "\n",
    "    # TARGET_T_MIN〜TARGET_T_MAX の範囲があるか\n",
    "    target_mask = (times >= TARGET_T_MIN) & (times <= TARGET_T_MAX)\n",
    "    if not target_mask.any():\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: no Time_sec in [{TARGET_T_MIN}, {TARGET_T_MAX}]\")\n",
    "\n",
    "    X_list: List[np.ndarray] = []\n",
    "    y_list: List[int] = []\n",
    "    t_list: List[int] = []\n",
    "    fms_list: List[int] = []\n",
    "\n",
    "    for idx in range(len(times)):\n",
    "        t = times[idx]\n",
    "        if t < TARGET_T_MIN or t > TARGET_T_MAX:\n",
    "            continue\n",
    "\n",
    "        if idx < SEQ_LEN - 1:\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: idx={idx}, Time_sec={t} has no enough history (need {SEQ_LEN}).\"\n",
    "            )\n",
    "\n",
    "        window_feat = features[idx - SEQ_LEN + 1: idx + 1, :]  # (SEQ_LEN, N_FEATURES)\n",
    "        if not np.isfinite(window_feat).all():\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: non-finite value in sequence ending at Time_sec={t}\"\n",
    "            )\n",
    "\n",
    "        # ラベル：FMS>=1\n",
    "        y = 1 if fms[idx] >= FMS_POS_THRESHOLD else 0\n",
    "\n",
    "        X_list.append(window_feat)\n",
    "        y_list.append(y)\n",
    "        t_list.append(t)\n",
    "        fms_list.append(int(fms[idx]))\n",
    "\n",
    "    X_seq = np.stack(X_list).astype(np.float32)   # (N_seq, SEQ_LEN, N_FEATURES)\n",
    "    y_seq = np.array(y_list, dtype=np.int64)\n",
    "    t_seq = np.array(t_list, dtype=np.int64)\n",
    "    fms_seq = np.array(fms_list, dtype=np.int64)\n",
    "\n",
    "    # ここではログを出さない（詳細はCSVで確認）\n",
    "\n",
    "    return X_seq, y_seq, t_seq, fms_seq\n",
    "\n",
    "\n",
    "def build_sequences_all_subjects() -> Tuple[\n",
    "    Dict[str, np.ndarray],\n",
    "    Dict[str, np.ndarray],\n",
    "    Dict[str, np.ndarray],\n",
    "    Dict[str, np.ndarray],\n",
    "]:\n",
    "    \"\"\"\n",
    "    固定SEQ_LENで全被験者のシーケンスを構築\n",
    "    \"\"\"\n",
    "    X_by_sid: Dict[str, np.ndarray] = {}\n",
    "    y_by_sid: Dict[str, np.ndarray] = {}\n",
    "    t_by_sid: Dict[str, np.ndarray] = {}\n",
    "    fms_by_sid: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        X_seq, y_seq, t_seq, fms_seq = build_sequences_for_subject(sid)\n",
    "        X_by_sid[sid] = X_seq\n",
    "        y_by_sid[sid] = y_seq\n",
    "        t_by_sid[sid] = t_seq\n",
    "        fms_by_sid[sid] = fms_seq\n",
    "\n",
    "    all_y_tmp = np.concatenate([y_by_sid[sid] for sid in SUBJECT_IDS])\n",
    "    print(\n",
    "        f\"[INFO] Overall (all subjects) target stats (SEQ_LEN={SEQ_LEN}): \"\n",
    "        f\"N={len(all_y_tmp)}, N_pos={all_y_tmp.sum()}, \"\n",
    "        f\"pos_ratio={all_y_tmp.mean():.3f}\"\n",
    "    )\n",
    "\n",
    "    return X_by_sid, y_by_sid, t_by_sid, fms_by_sid\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1 fold 学習\n",
    "# -----------------------------\n",
    "def train_one_fold(\n",
    "    train_X: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    device: torch.device,\n",
    "    config: Dict[str, Any],\n",
    ") -> Tuple[LSTMMotionSickness, List[float]]:\n",
    "    \"\"\"\n",
    "    1つのLOSO foldについて，訓練データのみを使ってLSTMを学習する。\n",
    "    戻り値: (学習済みモデル, 各epochの平均train lossリスト)\n",
    "    \"\"\"\n",
    "    model = LSTMMotionSickness(\n",
    "        input_size=N_FEATURES,\n",
    "        hidden_size=config[\"HIDDEN_SIZE\"],\n",
    "        fc_hidden_size=config[\"FC_HIDDEN_SIZE\"],\n",
    "        dropout_lstm=config[\"DROPOUT_LSTM\"],\n",
    "        dropout_fc=config[\"DROPOUT_FC\"],\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config[\"LEARNING_RATE\"],\n",
    "        weight_decay=config[\"WEIGHT_DECAY\"],\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    batch_size = config[\"BATCH_SIZE\"]\n",
    "    n_epochs = config[\"N_EPOCHS\"]\n",
    "\n",
    "    # DataLoader 構築\n",
    "    X_tensor = torch.from_numpy(train_X).float()\n",
    "    y_tensor = torch.from_numpy(train_y).float()\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    epoch_loss_list: List[float] = []\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)           # (batch,)\n",
    "            loss = criterion(logits, batch_y) # BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / max(n_batches, 1)\n",
    "        epoch_loss_list.append(avg_loss)\n",
    "\n",
    "        # 学習途中のログ出力はしない\n",
    "\n",
    "    return model, epoch_loss_list\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1 config について LOSO を回す\n",
    "# -----------------------------\n",
    "def run_loso_for_config(\n",
    "    config: Dict[str, Any],\n",
    "    device: torch.device,\n",
    "    X_by_sid: Dict[str, np.ndarray],\n",
    "    y_by_sid: Dict[str, np.ndarray],\n",
    "    t_by_sid: Dict[str, np.ndarray],\n",
    "    fms_by_sid: Dict[str, np.ndarray],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    与えられた設定(config)で LOSO を1周して評価＆CSV保存\n",
    "    \"\"\"\n",
    "    config_name = config[\"NAME\"]\n",
    "\n",
    "    all_probs: List[np.ndarray] = []\n",
    "    all_true: List[np.ndarray] = []\n",
    "    pred_rows: List[pd.DataFrame] = []\n",
    "    fold_summary_rows: List[Dict[str, Any]] = []\n",
    "    epoch_loss_records: List[Dict[str, Any]] = []\n",
    "\n",
    "    for test_sid in SUBJECT_IDS:\n",
    "        # 学習・テスト分割\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "        for sid in SUBJECT_IDS:\n",
    "            if sid == test_sid:\n",
    "                continue\n",
    "            train_X_list.append(X_by_sid[sid])\n",
    "            train_y_list.append(y_by_sid[sid])\n",
    "\n",
    "        train_X = np.concatenate(train_X_list, axis=0)\n",
    "        train_y = np.concatenate(train_y_list, axis=0)\n",
    "        test_X = X_by_sid[test_sid]\n",
    "        test_y = y_by_sid[test_sid]\n",
    "        test_t = t_by_sid[test_sid]\n",
    "        test_fms = fms_by_sid[test_sid]\n",
    "\n",
    "        # 1 fold 学習\n",
    "        model, epoch_loss_list = train_one_fold(train_X, train_y, device=device, config=config)\n",
    "\n",
    "        # epochごとの loss をログ用に保存（プリントはしない）\n",
    "        for ep_idx, loss_val in enumerate(epoch_loss_list, start=1):\n",
    "            epoch_loss_records.append(\n",
    "                {\n",
    "                    \"ConfigName\": config_name,\n",
    "                    \"SEQ_LEN\": SEQ_LEN,\n",
    "                    \"SubjectID\": test_sid,\n",
    "                    \"Epoch\": ep_idx,\n",
    "                    \"TrainLoss\": loss_val,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # テスト被験者の予測確率\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "            logits = model(X_test_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()  # (N_test,)\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_true.append(test_y.astype(int))\n",
    "\n",
    "        # foldごとのROC-AUC\n",
    "        n_pos_test = int(test_y.sum())\n",
    "        n_neg_test = int(len(test_y) - n_pos_test)\n",
    "        if n_pos_test == 0 or n_neg_test == 0:\n",
    "            rocauc_fold = float(\"nan\")\n",
    "        else:\n",
    "            rocauc_fold = roc_auc_score(test_y, probs)\n",
    "\n",
    "        fold_summary_rows.append(\n",
    "            {\n",
    "                \"ConfigName\": config_name,\n",
    "                \"SEQ_LEN\": SEQ_LEN,\n",
    "                \"SubjectID\": test_sid,\n",
    "                \"N_test\": int(len(test_y)),\n",
    "                \"N_pos_test\": n_pos_test,\n",
    "                \"N_neg_test\": n_neg_test,\n",
    "                \"pos_ratio_test\": float(test_y.mean()),\n",
    "                \"ROC_AUC_test\": rocauc_fold,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # このfoldの予測詳細\n",
    "        df_fold = pd.DataFrame(\n",
    "            {\n",
    "                \"ConfigName\": config_name,\n",
    "                \"SEQ_LEN\": SEQ_LEN,\n",
    "                \"SubjectID\": test_sid,\n",
    "                \"Time_sec\": test_t,\n",
    "                \"FMS\": test_fms,\n",
    "                \"Label_bin\": test_y.astype(int),\n",
    "                \"Prob_FMS_ge1\": probs,\n",
    "            }\n",
    "        )\n",
    "        pred_rows.append(df_fold)\n",
    "\n",
    "    # ---- 全foldをまとめた ROC-AUC ----\n",
    "    y_all = np.concatenate(all_true)\n",
    "    p_all = np.concatenate(all_probs)\n",
    "\n",
    "    n_total = len(y_all)\n",
    "    n_pos = int(y_all.sum())\n",
    "    n_neg = n_total - n_pos\n",
    "    pos_ratio = n_pos / n_total if n_total > 0 else 0.0\n",
    "\n",
    "    if n_pos == 0 or n_pos == n_total:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] ROC-AUC undefined: labels are all the same \"\n",
    "            f\"(N={n_total}, N_pos={n_pos}).\"\n",
    "        )\n",
    "\n",
    "    rocauc = roc_auc_score(y_all, p_all)\n",
    "\n",
    "    # ---- 結果保存 ----\n",
    "    # 1) ROC-AUC のサマリ（ハイパラ込み）\n",
    "    result_path = OUT_DIR / f\"Cell1-LSTM_{config_name}_LOSO_ROCAUC.csv\"\n",
    "    df_result = pd.DataFrame(\n",
    "        {\n",
    "            \"ConfigName\": [config_name],\n",
    "            \"ROC_AUC_global\": [rocauc],\n",
    "            \"N_total\": [n_total],\n",
    "            \"N_pos\": [n_pos],\n",
    "            \"N_neg\": [n_neg],\n",
    "            \"pos_ratio\": [pos_ratio],\n",
    "            \"N_features\": [N_FEATURES],\n",
    "            \"feature_list\": [\",\".join(FEATURE_COLS)],\n",
    "            \"SEQ_LEN\": [SEQ_LEN],\n",
    "            \"HIDDEN_SIZE\": [config[\"HIDDEN_SIZE\"]],\n",
    "            \"FC_HIDDEN_SIZE\": [config[\"FC_HIDDEN_SIZE\"]],\n",
    "            \"DROPOUT_LSTM\": [config[\"DROPOUT_LSTM\"]],\n",
    "            \"DROPOUT_FC\": [config[\"DROPOUT_FC\"]],\n",
    "            \"LEARNING_RATE\": [config[\"LEARNING_RATE\"]],\n",
    "            \"BATCH_SIZE\": [config[\"BATCH_SIZE\"]],\n",
    "            \"N_EPOCHS\": [config[\"N_EPOCHS\"]],\n",
    "            \"WEIGHT_DECAY\": [config[\"WEIGHT_DECAY\"]],\n",
    "        }\n",
    "    )\n",
    "    df_result.to_csv(result_path, index=False)\n",
    "\n",
    "    # 2) シーケンスごとの詳細予測\n",
    "    df_pred = pd.concat(pred_rows, ignore_index=True)\n",
    "    pred_path = OUT_DIR / f\"Cell1-LSTM_{config_name}_LOSO_pred_detail.csv\"\n",
    "    df_pred.to_csv(pred_path, index=False)\n",
    "\n",
    "    # 3) foldごとの summary（被験者別 ROC-AUC）\n",
    "    df_fold_summary = pd.DataFrame(fold_summary_rows)\n",
    "    fold_summary_path = OUT_DIR / f\"Cell1-LSTM_{config_name}_LOSO_fold_summary.csv\"\n",
    "    df_fold_summary.to_csv(fold_summary_path, index=False)\n",
    "\n",
    "    # 4) epochごとの train loss\n",
    "    df_loss = pd.DataFrame(epoch_loss_records)\n",
    "    loss_path = OUT_DIR / f\"Cell1-LSTM_{config_name}_LOSO_train_loss_by_epoch.csv\"\n",
    "    df_loss.to_csv(loss_path, index=False)\n",
    "\n",
    "    # config summary 用に返す情報（ログは main 側でまとめて出す）\n",
    "    return {\n",
    "        \"ConfigName\": config_name,\n",
    "        \"ROC_AUC_global\": rocauc,\n",
    "        \"N_total\": n_total,\n",
    "        \"N_pos\": n_pos,\n",
    "        \"N_neg\": n_neg,\n",
    "        \"pos_ratio\": pos_ratio,\n",
    "        \"N_features\": N_FEATURES,\n",
    "        \"feature_list\": \",\".join(FEATURE_COLS),\n",
    "        \"SEQ_LEN\": SEQ_LEN,\n",
    "        \"HIDDEN_SIZE\": config[\"HIDDEN_SIZE\"],\n",
    "        \"FC_HIDDEN_SIZE\": config[\"FC_HIDDEN_SIZE\"],\n",
    "        \"DROPOUT_LSTM\": config[\"DROPOUT_LSTM\"],\n",
    "        \"DROPOUT_FC\": config[\"DROPOUT_FC\"],\n",
    "        \"LEARNING_RATE\": config[\"LEARNING_RATE\"],\n",
    "        \"BATCH_SIZE\": config[\"BATCH_SIZE\"],\n",
    "        \"N_EPOCHS\": config[\"N_EPOCHS\"],\n",
    "        \"WEIGHT_DECAY\": config[\"WEIGHT_DECAY\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# メイン\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # デバイス選択\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    # 再現性のためにseed固定\n",
    "    torch.manual_seed(20251206)\n",
    "    np.random.seed(20251206)\n",
    "\n",
    "    # -------------------------\n",
    "    # 固定SEQ_LENで全被験者のシーケンス構築（1回だけ）\n",
    "    # -------------------------\n",
    "    X_by_sid, y_by_sid, t_by_sid, fms_by_sid = build_sequences_all_subjects()\n",
    "\n",
    "    # -------------------------\n",
    "    # ハイパラパターン一覧を定義\n",
    "    # -------------------------\n",
    "    HIDDEN_SIZE_LIST = [32, 64]\n",
    "    WEIGHT_DECAY_LIST = [0.0, 1e-6, 1e-5, 1e-4]\n",
    "    N_EPOCHS_LIST = [10, 30, 45, 60]\n",
    "\n",
    "    CONFIG_LIST: List[Dict[str, Any]] = []\n",
    "    for h in HIDDEN_SIZE_LIST:\n",
    "        for wd in WEIGHT_DECAY_LIST:\n",
    "            if wd == 0.0:\n",
    "                wd_tag = \"0\"\n",
    "            elif wd == 1e-4:\n",
    "                wd_tag = \"1e-4\"\n",
    "            elif wd == 1e-3:\n",
    "                wd_tag = \"1e-3\"\n",
    "            else:\n",
    "                wd_tag = f\"{wd}\"\n",
    "            for n_ep in N_EPOCHS_LIST:\n",
    "                name = f\"H{h}_WD{wd_tag}_EP{n_ep}\"\n",
    "                CONFIG_LIST.append(\n",
    "                    dict(\n",
    "                        NAME=name,\n",
    "                        HIDDEN_SIZE=h,\n",
    "                        FC_HIDDEN_SIZE=DEFAULT_FC_HIDDEN_SIZE,\n",
    "                        DROPOUT_LSTM=DEFAULT_DROPOUT_LSTM,\n",
    "                        DROPOUT_FC=DEFAULT_DROPOUT_FC,\n",
    "                        LEARNING_RATE=DEFAULT_LEARNING_RATE,\n",
    "                        BATCH_SIZE=DEFAULT_BATCH_SIZE,\n",
    "                        N_EPOCHS=n_ep,\n",
    "                        WEIGHT_DECAY=wd,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    print(f\"[INFO] Total number of configs = {len(CONFIG_LIST)}\")\n",
    "\n",
    "    config_summary_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    # -------------------------\n",
    "    # 各 config について LOSO 実験\n",
    "    # -------------------------\n",
    "    for i, config in enumerate(CONFIG_LIST, start=1):\n",
    "        summary = run_loso_for_config(\n",
    "            config=config,\n",
    "            device=device,\n",
    "            X_by_sid=X_by_sid,\n",
    "            y_by_sid=y_by_sid,\n",
    "            t_by_sid=t_by_sid,\n",
    "            fms_by_sid=fms_by_sid,\n",
    "        )\n",
    "        config_summary_rows.append(summary)\n",
    "\n",
    "        # ★ここで「パラメータとROC-AUC」だけをプリント\n",
    "        print(\n",
    "            f\"[RESULT] ({i}/{len(CONFIG_LIST)}) \"\n",
    "            f\"Config={summary['ConfigName']}, \"\n",
    "            f\"H={summary['HIDDEN_SIZE']}, \"\n",
    "            f\"WD={summary['WEIGHT_DECAY']}, \"\n",
    "            f\"N_EPOCHS={summary['N_EPOCHS']}, \"\n",
    "            f\"ROC_AUC_global={summary['ROC_AUC_global']:.4f}\"\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # 全configの summary を保存\n",
    "    # -------------------------\n",
    "    df_config_summary = pd.DataFrame(config_summary_rows)\n",
    "    summary_path = OUT_DIR / \"Cell1-LSTM_LOSO_config_summary.csv\"\n",
    "    df_config_summary.to_csv(summary_path, index=False)\n",
    "    print(f\"[INFO] Saved config summary to: {summary_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e119405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell1-LSTM: LSTMパラメータ探索（SLIDE_STEP_SEC × SEQ_LEN）\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# パス・基本設定\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\")\n",
    "\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\", \"10063\", \"10064\",\n",
    "    \"10071\", \"10072\", \"10073\", \"10074\",\n",
    "    \"10081\", \"10082\", \"10083\",\n",
    "    \"10091\", \"10092\", \"10093\", \"10094\",\n",
    "    \"10101\", \"10102\", \"10103\",\n",
    "]\n",
    "\n",
    "# 3秒窓特徴量（FEATURE2）のベース設定\n",
    "T_START = 1770           # 特徴量計算の開始時刻（秒）\n",
    "WINDOW_SEC = 3           # FEATURE2 で使用した窓幅（秒）\n",
    "BASE_T_MIN = float(T_START + WINDOW_SEC)  # 1773.0\n",
    "TARGET_T_MAX = 2400.0    # ターゲットの上限時刻（秒）\n",
    "\n",
    "# ラベル閾値：FMS >= 1 を陽性とする\n",
    "FMS_POS_THRESHOLD = 1\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LSTMハイパラ\n",
    "# -----------------------------\n",
    "HIDDEN_SIZE = 32\n",
    "FC_HIDDEN_SIZE = 8\n",
    "DROPOUT_LSTM = 0.0\n",
    "DROPOUT_FC = 0.5\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 30\n",
    "WEIGHT_DECAY = 1e-4  # L2正則化（Adam の weight_decay）\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 特徴量ON/OFF設定\n",
    "# -----------------------------\n",
    "FEATURE_SWITCHES: List[Tuple[str, bool]] = [\n",
    "    (\"Pulse_rma3\",       True),\n",
    "    (\"Pulse_max3\",       True),\n",
    "    (\"Pulse_min3\",       True),\n",
    "    (\"Pulse_pc3\",        True),\n",
    "    (\"HR_rma3\",          True),\n",
    "    (\"HR_max3\",          True),\n",
    "    (\"HR_min3\",          True),\n",
    "    (\"HR_pc3\",           True),\n",
    "    (\"GSR_rma3\",         True),\n",
    "    (\"GSR_max3\",         True),\n",
    "    (\"GSR_min3\",         True),\n",
    "    (\"GSR_pc3\",          True),\n",
    "    (\"FaceSum_mean3\",    True),\n",
    "    (\"FaceDiff_mean3\",   True),\n",
    "    (\"FaceSum_pc3\",      True),\n",
    "    (\"FaceDiff_pc3\",     True),\n",
    "    (\"Skinos_SweatRate\", True),\n",
    "    (\"Skinos_HeartRate\", False),\n",
    "    (\"Skinos_SkinTemp\",  True),\n",
    "    (\"MSSQ_percentile01\",  True),\n",
    "]\n",
    "\n",
    "FEATURE_COLS: List[str] = [name for name, use in FEATURE_SWITCHES if use]\n",
    "if len(FEATURE_COLS) == 0:\n",
    "    raise RuntimeError(\"[ERROR] FEATURE_SWITCHES: 有効な特徴量が0個です（すべてFalse）。\")\n",
    "\n",
    "N_FEATURES = len(FEATURE_COLS)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM モデル定義\n",
    "# -----------------------------\n",
    "class LSTMMotionSickness(nn.Module):\n",
    "    \"\"\"\n",
    "    単方向1層LSTM → Dropout → FC(HIDDEN_SIZE→FC_HIDDEN_SIZE) → ReLU → FC → ロジット\n",
    "    出力はロジット（Sigmoidはloss/評価側で適用）\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = HIDDEN_SIZE,\n",
    "        fc_hidden_size: int = FC_HIDDEN_SIZE,\n",
    "        dropout_lstm: float = DROPOUT_LSTM,\n",
    "        dropout_fc: float = DROPOUT_FC,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout_lstm,  # num_layers=1 では実質無視される\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_fc)\n",
    "        self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(fc_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size)\n",
    "        return: ロジット (batch,)\n",
    "        \"\"\"\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        h_last = hn[-1]              # (batch, hidden_size)\n",
    "        z = self.dropout(h_last)\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.fc_out(z)           # (batch, 1)\n",
    "        return z.squeeze(-1)         # (batch,)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# データ読み込み & シーケンス生成\n",
    "# -----------------------------\n",
    "def get_feature2_path(sid: str, slide_step_sec: float) -> Path:\n",
    "    \"\"\"\n",
    "    FEATURE2 CSV のファイルパスを返すヘルパー.\n",
    "\n",
    "    例:\n",
    "      slide_step_sec=0.5 -> {sid}_3sFeat_0.5sSlide.csv\n",
    "      slide_step_sec=1.0 -> {sid}_3sFeat_1sSlide.csv\n",
    "      slide_step_sec=1.5 -> {sid}_3sFeat_1.5sSlide.csv\n",
    "      slide_step_sec=3.0 -> {sid}_3sFeat_3sSlide.csv\n",
    "    \"\"\"\n",
    "    step_str = f\"{slide_step_sec:g}\"  # 0.5 -> '0.5', 1.0 -> '1', 3.0 -> '3'\n",
    "    fname = f\"{sid}_3sFeat_{step_str}sSlide.csv\"\n",
    "    return BASE_DIR / sid / \"FEATURE2\" / fname\n",
    "\n",
    "\n",
    "def load_subject_df(sid: str, slide_step_sec: float) -> pd.DataFrame:\n",
    "    \"\"\"FEATURE2/{sid}_3sFeat_{step}sSlide.csv を読み込む.\"\"\"\n",
    "    path = get_feature2_path(sid, slide_step_sec)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"[ERROR] Subject {sid}: file not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sequences_for_subject(\n",
    "    sid: str,\n",
    "    slide_step_sec: float,\n",
    "    seq_len: int,\n",
    "    target_t_min: float,\n",
    "    target_t_max: float,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    1被験者について:\n",
    "      - FEATURE2 CSVを読み込み\n",
    "      - FMS>=1 を陽性にした y(t) を作成\n",
    "      - t=target_t_min〜target_t_max の各時刻 t に対し，\n",
    "          X_seq(t) = [t-seq_len+1 .. t] のシーケンスを生成\n",
    "      - その際，特徴量内にNaNがあれば即エラー\n",
    "    戻り値:\n",
    "      X_seq: (N_seq, seq_len, N_FEATURES)\n",
    "      y_seq: (N_seq,)\n",
    "      t_seq: (N_seq,)\n",
    "      fms_seq: (N_seq,)\n",
    "    \"\"\"\n",
    "    df = load_subject_df(sid, slide_step_sec=slide_step_sec)\n",
    "\n",
    "    # 必要列が揃っているかチェック\n",
    "    required_cols = [\"Time_sec\", \"FMS\"] + FEATURE_COLS\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: missing columns in FEATURE2 csv: {missing}\")\n",
    "\n",
    "    # NaNチェック（仕様：NaNがあれば即エラー）\n",
    "    if df[FEATURE_COLS].isna().values.any():\n",
    "        nan_mask = df[FEATURE_COLS].isna()\n",
    "        bad_idx = np.where(nan_mask.values)[0][0]\n",
    "        bad_time = df.loc[bad_idx, \"Time_sec\"]\n",
    "        bad_cols = list(nan_mask.columns[nan_mask.iloc[bad_idx]])\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Subject {sid}: NaN detected at Time_sec={bad_time}, cols={bad_cols}\"\n",
    "        )\n",
    "\n",
    "    times = df[\"Time_sec\"].to_numpy().astype(float)\n",
    "    fms = df[\"FMS\"].to_numpy().astype(int)\n",
    "    features = df[FEATURE_COLS].to_numpy().astype(np.float32)\n",
    "\n",
    "    # target_t_min〜target_t_max の範囲があるか\n",
    "    target_mask = (times >= target_t_min) & (times <= target_t_max)\n",
    "    if not target_mask.any():\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Subject {sid}: no Time_sec in [{target_t_min}, {target_t_max}]\"\n",
    "        )\n",
    "\n",
    "    X_list: List[np.ndarray] = []\n",
    "    y_list: List[int] = []\n",
    "    t_list: List[float] = []\n",
    "    fms_list: List[int] = []\n",
    "\n",
    "    for idx, t in enumerate(times):\n",
    "        if t < target_t_min or t > target_t_max:\n",
    "            continue\n",
    "\n",
    "        # 万が一 target_t_min の設定のズレで履歴不足が出てもスキップする\n",
    "        if idx < seq_len - 1:\n",
    "            continue\n",
    "\n",
    "        window_feat = features[idx - seq_len + 1: idx + 1, :]  # (seq_len, N_FEATURES)\n",
    "        if not np.isfinite(window_feat).all():\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: non-finite value in sequence ending at Time_sec={t}\"\n",
    "            )\n",
    "\n",
    "        # ラベル：FMS>=1\n",
    "        y = 1 if fms[idx] >= FMS_POS_THRESHOLD else 0\n",
    "\n",
    "        X_list.append(window_feat)\n",
    "        y_list.append(y)\n",
    "        t_list.append(float(t))\n",
    "        fms_list.append(int(fms[idx]))\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Subject {sid}: no valid sequence constructed \"\n",
    "            f\"(check target_t_min={target_t_min}, seq_len={seq_len}, slide_step={slide_step_sec})\"\n",
    "        )\n",
    "\n",
    "    X_seq = np.stack(X_list).astype(np.float32)   # (N_seq, seq_len, N_FEATURES)\n",
    "    y_seq = np.array(y_list, dtype=np.int64)\n",
    "    t_seq = np.array(t_list, dtype=np.float64)\n",
    "    fms_seq = np.array(fms_list, dtype=np.int64)\n",
    "\n",
    "    return X_seq, y_seq, t_seq, fms_seq\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LOSO 学習・評価（1設定分）\n",
    "# -----------------------------\n",
    "def train_one_fold(\n",
    "    train_X: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    device: torch.device,\n",
    ") -> LSTMMotionSickness:\n",
    "    \"\"\"\n",
    "    1つのLOSO foldについて，訓練データのみを使ってLSTMを学習する。\n",
    "    戻り値: 学習済みモデル\n",
    "    \"\"\"\n",
    "    model = LSTMMotionSickness(input_size=N_FEATURES).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # DataLoader 構築\n",
    "    X_tensor = torch.from_numpy(train_X).float()\n",
    "    y_tensor = torch.from_numpy(train_y).float()\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)           # (batch,)\n",
    "            loss = criterion(logits, batch_y) # BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def run_experiment_for_config(\n",
    "    slide_step_sec: float,\n",
    "    seq_len: int,\n",
    "    device: torch.device,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    1 組の (SLIDE_STEP_SEC, SEQ_LEN) について LOSO 評価を行い，\n",
    "    Global ROC-AUC を返す。\n",
    "    \"\"\"\n",
    "    # 再現性のためにseed固定\n",
    "    torch.manual_seed(20251206)\n",
    "    np.random.seed(20251206)\n",
    "\n",
    "    # ---- 全被験者のシーケンスを構築 ----\n",
    "    X_by_sid: Dict[str, np.ndarray] = {}\n",
    "    y_by_sid: Dict[str, np.ndarray] = {}\n",
    "    t_by_sid: Dict[str, np.ndarray] = {}\n",
    "    fms_by_sid: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # slide_step_sec を考慮した最初のターゲット時刻\n",
    "    target_t_min = BASE_T_MIN + (seq_len - 1) * float(slide_step_sec)\n",
    "    target_t_max = TARGET_T_MAX\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        X_seq, y_seq, t_seq, fms_seq = build_sequences_for_subject(\n",
    "            sid,\n",
    "            slide_step_sec=slide_step_sec,\n",
    "            seq_len=seq_len,\n",
    "            target_t_min=target_t_min,\n",
    "            target_t_max=target_t_max,\n",
    "        )\n",
    "        X_by_sid[sid] = X_seq\n",
    "        y_by_sid[sid] = y_seq\n",
    "        t_by_sid[sid] = t_seq\n",
    "        fms_by_sid[sid] = fms_seq\n",
    "\n",
    "    # ---- LOSO 学習・評価 ----\n",
    "    all_probs: List[np.ndarray] = []\n",
    "    all_true: List[np.ndarray] = []\n",
    "\n",
    "    for test_sid in SUBJECT_IDS:\n",
    "        # 学習・テスト分割\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "        for sid in SUBJECT_IDS:\n",
    "            if sid == test_sid:\n",
    "                continue\n",
    "            train_X_list.append(X_by_sid[sid])\n",
    "            train_y_list.append(y_by_sid[sid])\n",
    "\n",
    "        train_X = np.concatenate(train_X_list, axis=0)\n",
    "        train_y = np.concatenate(train_y_list, axis=0)\n",
    "        test_X = X_by_sid[test_sid]\n",
    "        test_y = y_by_sid[test_sid]\n",
    "\n",
    "        # 1 fold 学習\n",
    "        model = train_one_fold(train_X, train_y, device=device)\n",
    "\n",
    "        # テスト被験者の予測確率\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "            logits = model(X_test_tensor)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()  # (N_test,)\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_true.append(test_y.astype(int))\n",
    "\n",
    "    # ---- 全foldをまとめた ROC-AUC ----\n",
    "    y_all = np.concatenate(all_true)\n",
    "    p_all = np.concatenate(all_probs)\n",
    "\n",
    "    n_total = len(y_all)\n",
    "    n_pos = int(y_all.sum())\n",
    "\n",
    "    if n_pos == 0 or n_pos == n_total:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] ROC-AUC undefined: labels are all the same \"\n",
    "            f\"(N={n_total}, N_pos={n_pos}).\"\n",
    "        )\n",
    "\n",
    "    rocauc = roc_auc_score(y_all, p_all)\n",
    "    return rocauc\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# main: (SLIDE_STEP_SEC, SEQ_LEN) グリッド探索\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # デバイス選択\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 探索したいパラメータ候補\n",
    "    slide_step_list = [0.5,1]\n",
    "    seq_len_list = [20 ,60]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # ヘッダ行を先に出しておく\n",
    "    print(\"SLIDE_STEP_SEC,SEQ_LEN,ROC_AUC_global\")\n",
    "\n",
    "    for slide_step_sec in slide_step_list:\n",
    "        for seq_len in seq_len_list:\n",
    "            rocauc = run_experiment_for_config(\n",
    "                slide_step_sec=slide_step_sec,\n",
    "                seq_len=seq_len,\n",
    "                device=device,\n",
    "            )\n",
    "            results.append({\n",
    "                \"SLIDE_STEP_SEC\": slide_step_sec,\n",
    "                \"SEQ_LEN\": seq_len,\n",
    "                \"Time_LEN_sec\": slide_step_sec * seq_len,\n",
    "                \"ROC_AUC_global\": rocauc,\n",
    "            })\n",
    "            # ここだけ出力される\n",
    "            print(f\"{slide_step_sec},{seq_len},{rocauc:.4f}\")\n",
    "\n",
    "    # 結果を DataFrame にして CSV 保存（画面には出さない）\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results = df_results.sort_values([\"SLIDE_STEP_SEC\", \"SEQ_LEN\"]).reset_index(drop=True)\n",
    "\n",
    "    summary_dir = BASE_DIR / \"ANALYSIS\" / \"機械学習\" / \"Cell1-LSTM_param_sweep\"\n",
    "    summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_path = summary_dir / \"Cell1_LSTM_slide_seq_ROCAUC.csv\"\n",
    "    df_results.to_csv(summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell1-LSTM: LOSO＋ROC Temperature Scaling(inner-LOSO)＋Label Smoothing\n",
    "#1207 ROCAUC改善 3時間\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# パス・基本設定\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\")\n",
    "\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\", \"10063\", \"10064\",\n",
    "    \"10071\", \"10072\", \"10073\", \"10074\",\n",
    "    \"10081\", \"10082\", \"10083\",\n",
    "    \"10091\", \"10092\", \"10093\", \"10094\",\n",
    "    \"10101\", \"10102\", \"10103\",\n",
    "]\n",
    "\n",
    "# このCell用の出力ディレクトリ\n",
    "CELL_NAME = \"Cell1-LSTM\"\n",
    "OUT_DIR = BASE_DIR / \"解析\" / \"Cell1\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Cell: {CELL_NAME}, OUT_DIR = {OUT_DIR}\")\n",
    "\n",
    "# 確率分布プロット用ディレクトリ\n",
    "PROB_PLOT_DIR = OUT_DIR / \"prob_dist\"\n",
    "PROB_PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO] Prob. plot dir = {PROB_PLOT_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 時間・シーケンス仕様\n",
    "# -----------------------------\n",
    "WINDOW_SEC = 3          # 3秒窓（既にFEATURE2で反映済み）\n",
    "SLIDE_STEP_SEC = 1      # 1秒刻み（既にFEATURE2で反映済み）\n",
    "\n",
    "# LSTM に入れる過去ステップ数（= 過去 SEQ_LEN 秒分）\n",
    "SEQ_LEN = 10\n",
    "\n",
    "# FEATURE2 での最初の出力時刻（T_START+WINDOW_SEC = 1770+3）\n",
    "BASE_T_MIN = 1773\n",
    "\n",
    "# ターゲットの最小時刻：最初の出力時刻＋(SEQ_LEN-1)\n",
    "# 例：BASE_T_MIN=1773, SEQ_LEN=30 → 1773+29 = 1802\n",
    "TARGET_T_MIN = BASE_T_MIN + (SEQ_LEN - 1)\n",
    "TARGET_T_MAX = 2400     # 上限はこれまで通り 2400 秒\n",
    "\n",
    "# ラベル閾値：FMS >= 1 を陽性とする\n",
    "FMS_POS_THRESHOLD = 1\n",
    "\n",
    "# -----------------------------\n",
    "# LSTMハイパラ（変更候補は CSV に出力）\n",
    "# -----------------------------\n",
    "HIDDEN_SIZE = 32\n",
    "FC_HIDDEN_SIZE = 8\n",
    "DROPOUT_LSTM = 0.0\n",
    "DROPOUT_FC = 0.5\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 30\n",
    "WEIGHT_DECAY = 1e-4  # L2正則化（Adam の weight_decay）\n",
    "\n",
    "# -----------------------------\n",
    "# Temperature Scaling 設定\n",
    "# -----------------------------\n",
    "TEMP_MAX_ITER = 200\n",
    "TEMP_LR = 0.01\n",
    "\n",
    "# -----------------------------\n",
    "# Label Smoothing 設定\n",
    "# -----------------------------\n",
    "LABEL_SMOOTHING = 0.0  # 0.1 ならOFF, 例: 0.05 で 0→0.05, 1→0.95\n",
    "\n",
    "# -----------------------------\n",
    "# 特徴量ON/OFF設定\n",
    "# -----------------------------\n",
    "FEATURE_SWITCHES: List[Tuple[str, bool]] = [\n",
    "    (\"Pulse_rma3\",       True),\n",
    "    (\"Pulse_max3\",       True),\n",
    "    (\"Pulse_min3\",       True),\n",
    "    (\"Pulse_pc3\",        True),\n",
    "    (\"HR_rma3\",          True),\n",
    "    (\"HR_max3\",          True),\n",
    "    (\"HR_min3\",          True),\n",
    "    (\"HR_pc3\",           True),\n",
    "    (\"GSR_rma3\",         True),\n",
    "    (\"GSR_max3\",         True),\n",
    "    (\"GSR_min3\",         True),\n",
    "    (\"GSR_pc3\",          True),\n",
    "    (\"FaceSum_mean3\",    True),\n",
    "    (\"FaceDiff_mean3\",   True),\n",
    "    (\"FaceSum_pc3\",      True),\n",
    "    (\"FaceDiff_pc3\",     True),\n",
    "    (\"Skinos_SweatRate\", True),\n",
    "    (\"Skinos_HeartRate\", False),\n",
    "    (\"Skinos_SkinTemp\",  True),\n",
    "    (\"MSSQ_percentile01\",  True),\n",
    "]\n",
    "\n",
    "FEATURE_COLS: List[str] = [name for name, use in FEATURE_SWITCHES if use]\n",
    "if len(FEATURE_COLS) == 0:\n",
    "    raise RuntimeError(\"[ERROR] FEATURE_SWITCHES: 有効な特徴量が0個です（すべてFalse）。\")\n",
    "\n",
    "N_FEATURES = len(FEATURE_COLS)\n",
    "print(f\"[INFO] Using {N_FEATURES} features:\", \", \".join(FEATURE_COLS))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LSTM モデル定義\n",
    "# -----------------------------\n",
    "class LSTMMotionSickness(nn.Module):\n",
    "    \"\"\"\n",
    "    単方向1層LSTM → Dropout → FC(HIDDEN_SIZE→FC_HIDDEN_SIZE) → ReLU → FC → ロジット\n",
    "    出力はロジット（Sigmoidはloss/評価側で適用）\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int = HIDDEN_SIZE,\n",
    "        fc_hidden_size: int = FC_HIDDEN_SIZE,\n",
    "        dropout_lstm: float = DROPOUT_LSTM,\n",
    "        dropout_fc: float = DROPOUT_FC,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=dropout_lstm,  # num_layers=1 では実質無視される\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_fc)\n",
    "        self.fc1 = nn.Linear(hidden_size, fc_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(fc_hidden_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, input_size)\n",
    "        return: ロジット (batch,)\n",
    "        \"\"\"\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        h_last = hn[-1]              # (batch, hidden_size)\n",
    "        z = self.dropout(h_last)\n",
    "        z = self.relu(self.fc1(z))\n",
    "        z = self.fc_out(z)           # (batch, 1)\n",
    "        return z.squeeze(-1)         # (batch,)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# データ読み込み & シーケンス生成\n",
    "# -----------------------------\n",
    "def load_subject_df(sid: str) -> pd.DataFrame:\n",
    "    \"\"\"FEATURE2/{sid}_3sFeat_1sSlide.csv を読み込む.\"\"\"\n",
    "    path = BASE_DIR / sid / \"FEATURE2\" / f\"{sid}_3sFeat_1sSlide.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"[ERROR] Subject {sid}: file not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.sort_values(\"Time_sec\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_sequences_for_subject(\n",
    "    sid: str,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    1被験者について:\n",
    "      - FEATURE2 CSVを読み込み\n",
    "      - FMS>=1 を陽性にした y(t) を作成\n",
    "      - t=TARGET_T_MIN〜TARGET_T_MAX の各時刻 t に対し，\n",
    "          X_seq(t) = [t-SEQ_LEN+1 .. t] のシーケンスを生成\n",
    "      - その際，特徴量内にNaNがあれば即エラー\n",
    "    戻り値:\n",
    "      X_seq: (N_seq, SEQ_LEN, N_FEATURES)\n",
    "      y_seq: (N_seq,)\n",
    "      t_seq: (N_seq,)\n",
    "      fms_seq: (N_seq,)\n",
    "    \"\"\"\n",
    "    df = load_subject_df(sid)\n",
    "\n",
    "    # 必要列が揃っているかチェック\n",
    "    required_cols = [\"Time_sec\", \"FMS\"] + FEATURE_COLS\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: missing columns in FEATURE2 csv: {missing}\")\n",
    "\n",
    "    # NaNチェック（仕様：NaNがあれば即エラー）\n",
    "    if df[FEATURE_COLS].isna().values.any():\n",
    "        nan_mask = df[FEATURE_COLS].isna()\n",
    "        bad_idx = np.where(nan_mask.values)[0][0]\n",
    "        bad_time = df.loc[bad_idx, \"Time_sec\"]\n",
    "        bad_cols = list(nan_mask.columns[nan_mask.iloc[bad_idx]])\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] Subject {sid}: NaN detected at Time_sec={bad_time}, cols={bad_cols}\"\n",
    "        )\n",
    "\n",
    "    times = df[\"Time_sec\"].to_numpy().astype(int)\n",
    "    fms = df[\"FMS\"].to_numpy().astype(int)\n",
    "    features = df[FEATURE_COLS].to_numpy().astype(np.float32)\n",
    "\n",
    "    # TARGET_T_MIN〜TARGET_T_MAX の範囲があるか\n",
    "    target_mask = (times >= TARGET_T_MIN) & (times <= TARGET_T_MAX)\n",
    "    if not target_mask.any():\n",
    "        raise RuntimeError(f\"[ERROR] Subject {sid}: no Time_sec in [{TARGET_T_MIN}, {TARGET_T_MAX}]\")\n",
    "\n",
    "    X_list: List[np.ndarray] = []\n",
    "    y_list: List[int] = []\n",
    "    t_list: List[int] = []\n",
    "    fms_list: List[int] = []\n",
    "\n",
    "    for idx in range(len(times)):\n",
    "        t = times[idx]\n",
    "        if t < TARGET_T_MIN or t > TARGET_T_MAX:\n",
    "            continue\n",
    "\n",
    "        if idx < SEQ_LEN - 1:\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: idx={idx}, Time_sec={t} has no enough history (need {SEQ_LEN}).\"\n",
    "            )\n",
    "\n",
    "        window_feat = features[idx - SEQ_LEN + 1: idx + 1, :]  # (SEQ_LEN, N_FEATURES)\n",
    "        if not np.isfinite(window_feat).all():\n",
    "            raise RuntimeError(\n",
    "                f\"[ERROR] Subject {sid}: non-finite value in sequence ending at Time_sec={t}\"\n",
    "            )\n",
    "\n",
    "        # ラベル：FMS>=1\n",
    "        y = 1 if fms[idx] >= FMS_POS_THRESHOLD else 0\n",
    "\n",
    "        X_list.append(window_feat)\n",
    "        y_list.append(y)\n",
    "        t_list.append(t)\n",
    "        fms_list.append(int(fms[idx]))\n",
    "\n",
    "    X_seq = np.stack(X_list).astype(np.float32)   # (N_seq, SEQ_LEN, N_FEATURES)\n",
    "    y_seq = np.array(y_list, dtype=np.int64)\n",
    "    t_seq = np.array(t_list, dtype=np.int64)\n",
    "    fms_seq = np.array(fms_list, dtype=np.int64)\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] Subject {sid}: target Time_sec range = {t_seq[0]}–{t_seq[-1]}, \"\n",
    "        f\"N_seq = {len(t_seq)}, N_pos = {y_seq.sum()}, N_neg = {len(y_seq) - y_seq.sum()}\"\n",
    "    )\n",
    "\n",
    "    return X_seq, y_seq, t_seq, fms_seq\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LOSO 学習・評価ループ（1モデル学習）\n",
    "# -----------------------------\n",
    "def train_one_fold(\n",
    "    train_X: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    device: torch.device,\n",
    ") -> Tuple[LSTMMotionSickness, List[float]]:\n",
    "    \"\"\"\n",
    "    1つのデータセット（train_X, train_y）についてLSTMを学習する。\n",
    "    戻り値: (学習済みモデル, 各epochの平均train lossリスト)\n",
    "    \"\"\"\n",
    "    model = LSTMMotionSickness(input_size=N_FEATURES).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # 陽性割合をプリント（元ラベルで）\n",
    "    n_train = len(train_y)\n",
    "    n_pos = int(train_y.sum())\n",
    "    n_neg = n_train - n_pos\n",
    "    pos_ratio = n_pos / n_train if n_train > 0 else 0.0\n",
    "    print(\n",
    "        f\"[INFO] Train stats: N={n_train}, N_pos={n_pos}, N_neg={n_neg}, \"\n",
    "        f\"pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    # DataLoader 構築\n",
    "    X_tensor = torch.from_numpy(train_X).float()\n",
    "    y_tensor = torch.from_numpy(train_y).float()  # 0/1\n",
    "\n",
    "    if LABEL_SMOOTHING > 0.0:\n",
    "        eps = float(LABEL_SMOOTHING)\n",
    "        # 0 -> eps, 1 -> 1-eps\n",
    "        y_smooth = y_tensor * (1.0 - eps) + (1.0 - y_tensor) * eps\n",
    "        print(f\"[INFO] Label smoothing enabled: eps={eps:.3f}\")\n",
    "    else:\n",
    "        y_smooth = y_tensor\n",
    "\n",
    "    dataset = TensorDataset(X_tensor, y_smooth)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    epoch_loss_list: List[float] = []\n",
    "\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch_X, batch_y_smooth in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y_smooth = batch_y_smooth.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_X)                 # (batch,)\n",
    "            loss = criterion(logits, batch_y_smooth)  # BCEWithLogitsLoss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        avg_loss = running_loss / max(n_batches, 1)\n",
    "        epoch_loss_list.append(avg_loss)\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1 or epoch == N_EPOCHS:\n",
    "            print(f\"[INFO] Epoch {epoch:02d}/{N_EPOCHS} - train_loss={avg_loss:.4f}\")\n",
    "\n",
    "    return model, epoch_loss_list\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Temperature Scaling 用クラス・関数\n",
    "# -----------------------------\n",
    "class TemperatureScaler(nn.Module):\n",
    "    \"\"\"\n",
    "    logits を 1/T でスケールするモジュール（T > 0）。\n",
    "    forward: logits -> logits / T\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # log_T をパラメータとして持ち，初期値 T=1\n",
    "        self.log_T = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        T = torch.exp(self.log_T)\n",
    "        return logits / T\n",
    "\n",
    "    def temperature(self) -> torch.Tensor:\n",
    "        return torch.exp(self.log_T)\n",
    "\n",
    "\n",
    "def fit_temperature_scaling(\n",
    "    calib_logits_np: np.ndarray,\n",
    "    calib_labels_np: np.ndarray,\n",
    "    device: torch.device,\n",
    "    max_iter: int = TEMP_MAX_ITER,\n",
    "    lr: float = TEMP_LR,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    calibration用データ (logits, labels) から Temperature T を学習する。\n",
    "    T は BCEWithLogitsLoss( logits/T, y ) を最小化するよう最適化。\n",
    "    返り値: T (float)\n",
    "    \"\"\"\n",
    "    calib_logits = torch.from_numpy(calib_logits_np.astype(np.float32)).to(device)\n",
    "    calib_labels = torch.from_numpy(calib_labels_np.astype(np.float32)).to(device)\n",
    "\n",
    "    # ラベルが全0または全1なら校正不能→T=1.0\n",
    "    n_pos = int(calib_labels.sum().item())\n",
    "    n_total = calib_labels.numel()\n",
    "    if n_pos == 0 or n_pos == n_total:\n",
    "        print(\n",
    "            f\"[WARN] [TempScaling] calibration labels are all same \"\n",
    "            f\"(N={n_total}, N_pos={n_pos}) -> skip (T=1.0)\"\n",
    "        )\n",
    "        return 1.0\n",
    "\n",
    "    scaler = TemperatureScaler().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(scaler.parameters(), lr=lr)\n",
    "\n",
    "    scaler.train()\n",
    "    for step in range(1, max_iter + 1):\n",
    "        optimizer.zero_grad()\n",
    "        scaled_logits = scaler(calib_logits)\n",
    "        loss = criterion(scaled_logits, calib_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50 == 0 or step == 1 or step == max_iter:\n",
    "            T_val = scaler.temperature().item()\n",
    "            print(\n",
    "                f\"[INFO] [TempScaling] step={step:03d}, \"\n",
    "                f\"loss={loss.item():.6f}, T={T_val:.4f}\"\n",
    "            )\n",
    "\n",
    "    T_final = scaler.temperature().item()\n",
    "    print(f\"[INFO] [TempScaling] Finished: T={T_final:.4f}\")\n",
    "    return T_final\n",
    "\n",
    "\n",
    "def build_calibration_data_inner_loso(\n",
    "    train_subject_ids: List[str],\n",
    "    X_by_sid: Dict[str, np.ndarray],\n",
    "    y_by_sid: Dict[str, np.ndarray],\n",
    "    device: torch.device,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    パターンB: Train側被験者のみで inner-LOSO を回し，\n",
    "    各 inner fold の val被験者に対する logits（out-of-fold予測）とラベルを集める。\n",
    "\n",
    "    返り値:\n",
    "      calib_logits: shape (N_train_all,)\n",
    "      calib_labels: shape (N_train_all,)\n",
    "    \"\"\"\n",
    "    all_logits: List[np.ndarray] = []\n",
    "    all_labels: List[np.ndarray] = []\n",
    "\n",
    "    for val_sid in train_subject_ids:\n",
    "        print(f\"[INFO] [Calib-InnerLOSO] val subject = {val_sid}\")\n",
    "        inner_train_sids = [s for s in train_subject_ids if s != val_sid]\n",
    "\n",
    "        inner_train_X = np.concatenate([X_by_sid[s] for s in inner_train_sids], axis=0)\n",
    "        inner_train_y = np.concatenate([y_by_sid[s] for s in inner_train_sids], axis=0)\n",
    "        val_X = X_by_sid[val_sid]\n",
    "        val_y = y_by_sid[val_sid]\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] [Calib-InnerLOSO] inner-train N={len(inner_train_y)}, \"\n",
    "            f\"val N={len(val_y)}\"\n",
    "        )\n",
    "\n",
    "        # inner fold モデル学習（ここでもラベルスムージング設定が効く）\n",
    "        inner_model, _ = train_one_fold(inner_train_X, inner_train_y, device=device)\n",
    "\n",
    "        # val被験者への logits を out-of-fold 予測として取得\n",
    "        inner_model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_val_tensor = torch.from_numpy(val_X).float().to(device)\n",
    "            logits_val = inner_model(X_val_tensor)  # (N_val,)\n",
    "            all_logits.append(logits_val.cpu().numpy())\n",
    "            all_labels.append(val_y.astype(int))\n",
    "\n",
    "    calib_logits = np.concatenate(all_logits, axis=0)\n",
    "    calib_labels = np.concatenate(all_labels, axis=0).astype(np.int64)\n",
    "\n",
    "    n_total = len(calib_labels)\n",
    "    n_pos = int(calib_labels.sum())\n",
    "    n_neg = n_total - n_pos\n",
    "    pos_ratio = n_pos / n_total if n_total > 0 else 0.0\n",
    "\n",
    "    print(\n",
    "        f\"[INFO] [Calib-InnerLOSO] Collected calibration data: \"\n",
    "        f\"N={n_total}, N_pos={n_pos}, N_neg={n_neg}, pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    return calib_logits, calib_labels\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 確率分布プロット\n",
    "# -----------------------------\n",
    "def plot_probability_distributions(\n",
    "    df_pred: pd.DataFrame,\n",
    "    out_dir: Path,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Foldごと（被験者ごと）と全体の predicted probability 分布をヒストグラムで保存する。\n",
    "\n",
    "    df_pred:\n",
    "        列: ['SubjectID', 'Time_sec', 'FMS', 'Label_bin',\n",
    "             'Prob_FMS_ge1_raw', 'Prob_FMS_ge1(=calibrated)']\n",
    "        ※ この関数では 'Prob_FMS_ge1'（= Temperature scaling 後）を使用。\n",
    "    out_dir:\n",
    "        画像を保存するディレクトリ\n",
    "    \"\"\"\n",
    "    # グラフ体裁（ユーザ指定）\n",
    "    TITLE_FONTSIZE = 30\n",
    "    LABEL_FONTSIZE = 24\n",
    "    TICK_FONTSIZE = 20\n",
    "    LEGEND_FONTSIZE = 20\n",
    "    LINEWIDTH = 1.5\n",
    "\n",
    "    prob_col = \"Prob_FMS_ge1\"  # calibrated\n",
    "\n",
    "    # ---- 全体の分布（全fold結合） ----\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    for label, alpha, lab_name in [\n",
    "        (0, 0.6, \"Label=0 (FMS<1)\"),\n",
    "        (1, 0.6, \"Label=1 (FMS>=1)\")\n",
    "    ]:\n",
    "        vals = df_pred.loc[df_pred[\"Label_bin\"] == label, prob_col].values\n",
    "        if len(vals) == 0:\n",
    "            continue\n",
    "        ax.hist(\n",
    "            vals,\n",
    "            bins=20,\n",
    "            range=(0.0, 1.0),\n",
    "            density=True,\n",
    "            alpha=alpha,\n",
    "            label=lab_name,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=LINEWIDTH,\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_xlabel(\"Calibrated probability (FMS ≥ 1)\", fontsize=LABEL_FONTSIZE)\n",
    "    ax.set_ylabel(\"Density\", fontsize=LABEL_FONTSIZE)\n",
    "    ax.set_title(\"All subjects – Calibrated probability distribution\", fontsize=TITLE_FONTSIZE)\n",
    "    ax.tick_params(axis=\"both\", labelsize=TICK_FONTSIZE)\n",
    "    ax.legend(fontsize=LEGEND_FONTSIZE)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out_path_all = out_dir / \"Cell1_LSTM_ProbDist_ALL.png\"\n",
    "    fig.savefig(out_path_all, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"[INFO] Saved global probability distribution plot to: {out_path_all}\")\n",
    "\n",
    "    # ---- 被験者ごとの分布 ----\n",
    "    for sid, df_sub in df_pred.groupby(\"SubjectID\"):\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "        for label, alpha, lab_name in [\n",
    "            (0, 0.6, \"Label=0 (FMS<1)\"),\n",
    "            (1, 0.6, \"Label=1 (FMS>=1)\")\n",
    "        ]:\n",
    "            vals = df_sub.loc[df_sub[\"Label_bin\"] == label, prob_col].values\n",
    "            if len(vals) == 0:\n",
    "                continue\n",
    "            ax.hist(\n",
    "                vals,\n",
    "                bins=20,\n",
    "                range=(0.0, 1.0),\n",
    "                density=True,\n",
    "                alpha=alpha,\n",
    "                label=lab_name,\n",
    "                edgecolor=\"black\",\n",
    "                linewidth=LINEWIDTH,\n",
    "            )\n",
    "\n",
    "        ax.set_xlim(0.0, 1.0)\n",
    "        ax.set_xlabel(\"Calibrated probability (FMS ≥ 1)\", fontsize=LABEL_FONTSIZE)\n",
    "        ax.set_ylabel(\"Density\", fontsize=LABEL_FONTSIZE)\n",
    "        ax.set_title(f\"Subject {sid} – Calibrated probability distribution\", fontsize=TITLE_FONTSIZE)\n",
    "        ax.tick_params(axis=\"both\", labelsize=TICK_FONTSIZE)\n",
    "        ax.legend(fontsize=LEGEND_FONTSIZE)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out_path_sid = out_dir / f\"Cell1_LSTM_ProbDist_{sid}.png\"\n",
    "        fig.savefig(out_path_sid, dpi=300)\n",
    "        plt.close(fig)\n",
    "        print(f\"[INFO] Saved probability distribution plot for {sid} to: {out_path_sid}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# main\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # デバイス選択\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "\n",
    "    # 再現性のためにseed固定\n",
    "    torch.manual_seed(20251206)\n",
    "    np.random.seed(20251206)\n",
    "\n",
    "    # ---- 全被験者のシーケンスを構築 ----\n",
    "    X_by_sid: Dict[str, np.ndarray] = {}\n",
    "    y_by_sid: Dict[str, np.ndarray] = {}\n",
    "    t_by_sid: Dict[str, np.ndarray] = {}\n",
    "    fms_by_sid: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    for sid in SUBJECT_IDS:\n",
    "        print(f\"[INFO] ==== Build sequences: Subject {sid} ====\")\n",
    "        X_seq, y_seq, t_seq, fms_seq = build_sequences_for_subject(sid)\n",
    "        X_by_sid[sid] = X_seq\n",
    "        y_by_sid[sid] = y_seq\n",
    "        t_by_sid[sid] = t_seq\n",
    "        fms_by_sid[sid] = fms_seq\n",
    "\n",
    "    all_y_tmp = np.concatenate([y_by_sid[sid] for sid in SUBJECT_IDS])\n",
    "    print(\n",
    "        f\"[INFO] Overall (all subjects) target stats: \"\n",
    "        f\"N={len(all_y_tmp)}, N_pos={all_y_tmp.sum()}, \"\n",
    "        f\"pos_ratio={all_y_tmp.mean():.3f}\"\n",
    "    )\n",
    "\n",
    "    # ---- LOSO 学習・評価 ----\n",
    "    all_probs_raw: List[np.ndarray] = []\n",
    "    all_probs_cal: List[np.ndarray] = []\n",
    "    all_true: List[np.ndarray] = []\n",
    "    pred_rows: List[pd.DataFrame] = []\n",
    "    fold_summary_rows: List[Dict] = []\n",
    "    epoch_loss_records: List[Dict] = []\n",
    "\n",
    "    for test_sid in SUBJECT_IDS:\n",
    "        print(f\"\\n[INFO] ===== LOSO fold: Test Subject {test_sid} =====\")\n",
    "\n",
    "        # 学習・テスト被験者\n",
    "        train_sids = [sid for sid in SUBJECT_IDS if sid != test_sid]\n",
    "\n",
    "        # ---------- inner-LOSO で Temperature Scaling 用データ構築 ----------\n",
    "        print(\"[INFO] ----- Build calibration data via inner-LOSO on train subjects -----\")\n",
    "        calib_logits, calib_labels = build_calibration_data_inner_loso(\n",
    "            train_subject_ids=train_sids,\n",
    "            X_by_sid=X_by_sid,\n",
    "            y_by_sid=y_by_sid,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Temperature T を学習\n",
    "        T_fold = fit_temperature_scaling(\n",
    "            calib_logits_np=calib_logits,\n",
    "            calib_labels_np=calib_labels,\n",
    "            device=device,\n",
    "        )\n",
    "        print(f\"[INFO] Fold (test={test_sid}) learned temperature T = {T_fold:.4f}\")\n",
    "\n",
    "        # ---------- 外側foldの学習データ（train_sids全部）で最終モデル学習 ----------\n",
    "        train_X_list = []\n",
    "        train_y_list = []\n",
    "        for sid in train_sids:\n",
    "            train_X_list.append(X_by_sid[sid])\n",
    "            train_y_list.append(y_by_sid[sid])\n",
    "\n",
    "        train_X = np.concatenate(train_X_list, axis=0)\n",
    "        train_y = np.concatenate(train_y_list, axis=0)\n",
    "        test_X = X_by_sid[test_sid]\n",
    "        test_y = y_by_sid[test_sid]\n",
    "        test_t = t_by_sid[test_sid]\n",
    "        test_fms = fms_by_sid[test_sid]\n",
    "\n",
    "        print(\n",
    "            f\"[INFO] Fold data sizes: \"\n",
    "            f\"Train N_seq={len(train_y)}, Test N_seq={len(test_y)}\"\n",
    "        )\n",
    "\n",
    "        # 1 fold 学習（外側fold用モデル）\n",
    "        model, epoch_loss_list = train_one_fold(train_X, train_y, device=device)\n",
    "\n",
    "        # epochごとの loss をログ用に保存\n",
    "        for ep_idx, loss_val in enumerate(epoch_loss_list, start=1):\n",
    "            epoch_loss_records.append(\n",
    "                {\n",
    "                    \"SubjectID\": test_sid,\n",
    "                    \"Epoch\": ep_idx,\n",
    "                    \"TrainLoss\": loss_val,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # テスト被験者の予測（logits → raw prob → calibrated prob）\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_test_tensor = torch.from_numpy(test_X).float().to(device)\n",
    "            logits_test = model(X_test_tensor)  # (N_test,)\n",
    "            probs_raw = torch.sigmoid(logits_test).cpu().numpy()\n",
    "            logits_scaled = logits_test / T_fold\n",
    "            probs_cal = torch.sigmoid(logits_scaled).cpu().numpy()\n",
    "\n",
    "        all_probs_raw.append(probs_raw)\n",
    "        all_probs_cal.append(probs_cal)\n",
    "        all_true.append(test_y.astype(int))\n",
    "\n",
    "        # foldごとのROC-AUC（raw & calibrated）\n",
    "        n_pos_test = int(test_y.sum())\n",
    "        n_neg_test = int(len(test_y) - n_pos_test)\n",
    "        if n_pos_test == 0 or n_neg_test == 0:\n",
    "            rocauc_fold_raw = float(\"nan\")\n",
    "            rocauc_fold_cal = float(\"nan\")\n",
    "            print(\n",
    "                f\"[INFO] Subject {test_sid}: ROC-AUC undefined (N_pos={n_pos_test}, N_neg={n_neg_test})\"\n",
    "            )\n",
    "        else:\n",
    "            rocauc_fold_raw = roc_auc_score(test_y, probs_raw)\n",
    "            rocauc_fold_cal = roc_auc_score(test_y, probs_cal)\n",
    "            print(\n",
    "                f\"[INFO] Subject {test_sid}: \"\n",
    "                f\"ROC-AUC raw = {rocauc_fold_raw:.4f}, \"\n",
    "                f\"calib = {rocauc_fold_cal:.4f} \"\n",
    "                f\"(N_test={len(test_y)}, N_pos={n_pos_test}, N_neg={n_neg_test})\"\n",
    "            )\n",
    "\n",
    "        fold_summary_rows.append(\n",
    "            {\n",
    "                \"SubjectID\": test_sid,\n",
    "                \"N_test\": int(len(test_y)),\n",
    "                \"N_pos_test\": n_pos_test,\n",
    "                \"N_neg_test\": n_neg_test,\n",
    "                \"pos_ratio_test\": float(test_y.mean()),\n",
    "                \"Temp_T\": float(T_fold),\n",
    "                \"ROC_AUC_test_raw\": rocauc_fold_raw,\n",
    "                \"ROC_AUC_test_calib\": rocauc_fold_cal,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # このfoldの予測詳細\n",
    "        df_fold = pd.DataFrame(\n",
    "            {\n",
    "                \"SubjectID\": test_sid,\n",
    "                \"Time_sec\": test_t,\n",
    "                \"FMS\": test_fms,\n",
    "                \"Label_bin\": test_y.astype(int),\n",
    "                \"Prob_FMS_ge1_raw\": probs_raw,\n",
    "                \"Prob_FMS_ge1\": probs_cal,  # Temperature scaling 後\n",
    "            }\n",
    "        )\n",
    "        pred_rows.append(df_fold)\n",
    "\n",
    "    # ---- 全foldをまとめた ROC-AUC ----\n",
    "    y_all = np.concatenate(all_true)\n",
    "    p_all_raw = np.concatenate(all_probs_raw)\n",
    "    p_all_cal = np.concatenate(all_probs_cal)\n",
    "\n",
    "    n_total = len(y_all)\n",
    "    n_pos = int(y_all.sum())\n",
    "    n_neg = n_total - n_pos\n",
    "    pos_ratio = n_pos / n_total if n_total > 0 else 0.0\n",
    "\n",
    "    print(\"\\n[INFO] ===== Overall LOSO result =====\")\n",
    "    print(\n",
    "        f\"[INFO] All folds combined: N={n_total}, N_pos={n_pos}, \"\n",
    "        f\"N_neg={n_neg}, pos_ratio={pos_ratio:.3f}\"\n",
    "    )\n",
    "\n",
    "    if n_pos == 0 or n_pos == n_total:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] ROC-AUC undefined: labels are all the same \"\n",
    "            f\"(N={n_total}, N_pos={n_pos}).\"\n",
    "        )\n",
    "\n",
    "    rocauc_raw = roc_auc_score(y_all, p_all_raw)\n",
    "    rocauc_cal = roc_auc_score(y_all, p_all_cal)\n",
    "    # Temperature scaling は単調変換なので raw と基本同じはず\n",
    "    print(f\"[RESULT] Global ROC-AUC raw   (LOSO, LSTM, FMS>=1) = {rocauc_raw:.4f}\")\n",
    "    print(f\"[RESULT] Global ROC-AUC calib (LOSO, LSTM, FMS>=1) = {rocauc_cal:.4f}\")\n",
    "\n",
    "    rocauc = rocauc_cal  # 基本的な代表値として calib を採用\n",
    "\n",
    "    # ---- 結果保存 ----\n",
    "    # 1) ROC-AUC のサマリ（ハイパラ込み）\n",
    "    result_path = OUT_DIR / \"Cell1_LSTM_LOSO_ROCAUC.csv\"\n",
    "    df_result = pd.DataFrame(\n",
    "        {\n",
    "            \"ROC_AUC_global\": [rocauc],\n",
    "            \"ROC_AUC_global_raw\": [rocauc_raw],\n",
    "            \"ROC_AUC_global_calib\": [rocauc_cal],\n",
    "            \"N_total\": [n_total],\n",
    "            \"N_pos\": [n_pos],\n",
    "            \"N_neg\": [n_neg],\n",
    "            \"pos_ratio\": [pos_ratio],\n",
    "            \"N_features\": [N_FEATURES],\n",
    "            \"feature_list\": [\",\".join(FEATURE_COLS)],\n",
    "            # 変更候補ハイパラを全部記録\n",
    "            \"WINDOW_SEC\": [WINDOW_SEC],\n",
    "            \"SLIDE_STEP_SEC\": [SLIDE_STEP_SEC],\n",
    "            \"SEQ_LEN\": [SEQ_LEN],\n",
    "            \"HIDDEN_SIZE\": [HIDDEN_SIZE],\n",
    "            \"FC_HIDDEN_SIZE\": [FC_HIDDEN_SIZE],\n",
    "            \"DROPOUT_LSTM\": [DROPOUT_LSTM],\n",
    "            \"DROPOUT_FC\": [DROPOUT_FC],\n",
    "            \"LEARNING_RATE\": [LEARNING_RATE],\n",
    "            \"BATCH_SIZE\": [BATCH_SIZE],\n",
    "            \"N_EPOCHS\": [N_EPOCHS],\n",
    "            \"WEIGHT_DECAY\": [WEIGHT_DECAY],\n",
    "            \"TEMP_MAX_ITER\": [TEMP_MAX_ITER],\n",
    "            \"TEMP_LR\": [TEMP_LR],\n",
    "            \"LABEL_SMOOTHING\": [LABEL_SMOOTHING],\n",
    "        }\n",
    "    )\n",
    "    df_result.to_csv(result_path, index=False)\n",
    "    print(f\"[INFO] Saved ROC-AUC result to: {result_path}\")\n",
    "\n",
    "    # 2) シーケンスごとの詳細予測\n",
    "    df_pred = pd.concat(pred_rows, ignore_index=True)\n",
    "    pred_path = OUT_DIR / \"Cell1_LSTM_LOSO_pred_detail.csv\"\n",
    "    df_pred.to_csv(pred_path, index=False)\n",
    "    print(f\"[INFO] Saved per-sequence predictions to: {pred_path}\")\n",
    "\n",
    "    # 2.5) 確率分布プロット（foldごと＋全体, calibrated prob）\n",
    "    plot_probability_distributions(df_pred, PROB_PLOT_DIR)\n",
    "\n",
    "    # 3) foldごとの summary（被験者別 ROC-AUC）\n",
    "    df_fold_summary = pd.DataFrame(fold_summary_rows)\n",
    "    # 後方互換用に calib を ROC_AUC_test としても持たせておく\n",
    "    df_fold_summary[\"ROC_AUC_test\"] = df_fold_summary[\"ROC_AUC_test_calib\"]\n",
    "\n",
    "    fold_summary_path = OUT_DIR / \"Cell1_LSTM_LOSO_fold_summary.csv\"\n",
    "    df_fold_summary.to_csv(fold_summary_path, index=False)\n",
    "    print(f\"[INFO] Saved per-fold summary to: {fold_summary_path}\")\n",
    "\n",
    "    # 4) epochごとの train loss\n",
    "    df_loss = pd.DataFrame(epoch_loss_records)\n",
    "    loss_path = OUT_DIR / \"Cell1_LSTM_LOSO_train_loss_by_epoch.csv\"\n",
    "    df_loss.to_csv(loss_path, index=False)\n",
    "    print(f\"[INFO] Saved train loss by epoch to: {loss_path}\")\n",
    "\n",
    "    # ---- 最後に、被験者ごとのROC-AUCを()付きでプリント（calibベース） ----\n",
    "    print(\"\\n[SUMMARY] ===== Per-subject ROC-AUC (LOSO, calibrated) =====\")\n",
    "    print(f\"[SUMMARY] Global ROC-AUC (all folds combined, calibrated) = {rocauc:.4f}\")\n",
    "\n",
    "    good_mask = df_fold_summary[\"ROC_AUC_test\"].notna() & (df_fold_summary[\"ROC_AUC_test\"] > 0.5)\n",
    "    bad_mask = df_fold_summary[\"ROC_AUC_test\"].notna() & (df_fold_summary[\"ROC_AUC_test\"] <= 0.5)\n",
    "    nan_mask = df_fold_summary[\"ROC_AUC_test\"].isna()\n",
    "\n",
    "    def format_sid_list(mask) -> str:\n",
    "        rows = df_fold_summary.loc[mask, [\"SubjectID\", \"ROC_AUC_test\"]]\n",
    "        if rows.empty:\n",
    "            return \"なし\"\n",
    "        return \", \".join(f\"{row.SubjectID}({row.ROC_AUC_test:.3f})\" for _, row in rows.iterrows())\n",
    "\n",
    "    good_str = format_sid_list(good_mask)\n",
    "    bad_str = format_sid_list(bad_mask)\n",
    "    nan_sids = df_fold_summary.loc[nan_mask, \"SubjectID\"].tolist()\n",
    "    nan_str = \", \".join(nan_sids) if len(nan_sids) > 0 else \"なし\"\n",
    "\n",
    "    print(f\"[SUMMARY] よく当たっている被験者(>0.5): {good_str}\")\n",
    "    print(f\"[SUMMARY] あまり当たっていない被験者(<=0.5): {bad_str}\")\n",
    "    print(f\"[SUMMARY] 評価不能(ROC-AUC算出不可): {nan_str}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
