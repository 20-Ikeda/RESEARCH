{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01b0b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Subject 0521因幡先生\n",
      "[OK]  CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\実験結果\\0521因幡先生\\EPOCH\\0521_epoch30_merged.csv\n",
      "# Subject 06021今村さん\n",
      "[OK]  CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\実験結果\\06021今村さん\\EPOCH\\06021_epoch30_merged.csv\n",
      "# Subject 06022梅野さん\n",
      "[OK]  CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\実験結果\\06022梅野さん\\EPOCH\\06022_epoch30_merged.csv\n",
      "# Subject 06271\n",
      "[OK]  CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\実験結果\\06271\\EPOCH\\06271_epoch30_merged.csv\n",
      "# Subject 06272\n",
      "[OK]  CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\実験結果\\06272\\EPOCH\\06272_epoch30_merged.csv\n",
      "# Subject 06273\n",
      "[OK]  CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\実験結果\\06273\\EPOCH\\06273_epoch30_merged.csv\n",
      "# Subject 06274\n",
      "[OK]  CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\実験結果\\06274\\EPOCH\\06274_epoch30_merged.csv\n",
      "# Subject 06275\n",
      "[OK]  CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\実験結果\\06275\\EPOCH\\06275_epoch30_merged.csv\n",
      "\n",
      "[INFO] df shape: (192, 30)\n",
      "[INFO] label_col: FMS\n",
      "[INFO] group_col: subject_id\n",
      "[INFO] #features: 25\n",
      "[INFO] head():\n",
      "   epoch_start  epoch_end  FMS       CSI       CVI  FaceTemp_Max  \\\n",
      "0         1380       1410  NaN  3.095303 -2.892448     35.705843   \n",
      "1         1410       1440  NaN  1.468646 -1.914324     35.575255   \n",
      "2         1440       1470  NaN  2.241789 -2.908496     35.679645   \n",
      "3         1470       1500  0.0  3.088283 -2.797115     35.724701   \n",
      "4         1500       1530  0.0  2.429628 -3.278578     35.705333   \n",
      "\n",
      "   FaceTemp_Mean  FaceTemp_Max_Diff  \n",
      "0      34.719335           0.299047  \n",
      "1      34.676973           0.389490  \n",
      "2      34.700333           0.322572  \n",
      "3      34.750499           0.283016  \n",
      "4      34.813969           0.368093  \n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: Load & assemble dataset =====\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 前提（ユーザー定義） ---\n",
    "subjects = [\n",
    "    (\"0521\", \"因幡先生\"),\n",
    "    (\"06021\", \"今村さん\"),\n",
    "    (\"06022\", \"梅野さん\"),\n",
    "    (\"06271\", \"\"),\n",
    "    (\"06272\", \"\"),\n",
    "    (\"06273\", \"\"),\n",
    "    (\"06274\", \"\"),\n",
    "    (\"06275\", \"\")\n",
    "]\n",
    "\n",
    "title_map = { 'Face_Temp_Max_mean': 'Face_Temp Max', 'Face_Temp_Mean_mean': 'Face_Temp Mean', 'Face_Temp_Max_Diff_mean': 'Face_Temp Max Diff', 'Face_Temp_Mean_Diff_mean': 'Face_Temp Mean Diff', 'HeartRate_BPM_mean': 'Pulse HR', 'RR_interval_sec_mean': 'RR Interval', 'RMSSD': 'RMSSD', 'watch_Heart_Rate(bpm)_mean': 'Watch HR', 'watch_Sweat_Rate(mg/cm^2/min)_mean': 'Watch ACC', 'watch_Skin_Temperature(C)_mean': 'Watch Temp', 'SDSD': 'SDSD', 'pNN50': 'pNN50', 'HF_power': 'HF Power', 'LF_power': 'LF Power', 'LF_HF_ratio': 'HF/LF', 'CSI': 'CSI', 'CVI': 'CVI', 'SD1': 'SD1', 'SD2': 'SD2' }\n",
    "interval_map = { 'Face_Temp_Max_mean': 30, 'Face_Temp_Mean_mean': 30, 'Face_Temp_Max_Diff_mean': 30, 'Face_Temp_Mean_Diff_mean': 30, 'HeartRate_BPM_mean': 30, 'RR_interval_sec_mean': 30, 'RMSSD': 30, 'watch_Heart_Rate(bpm)_mean': 30, 'watch_Sweat_Rate(mg/cm^2/min)_mean': 30, 'watch_Skin_Temperature(C)_mean': 30, 'SDSD': 30, 'pNN50': 60, 'HF_power': 120, 'LF_power': 120, 'LF_HF_ratio': 120, 'CSI': 30, 'CVI': 30, 'SD1': 30, 'SD2': 30 }\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\実験結果\"\n",
    "\n",
    "# --- 探索する候補パスのテンプレ ---\n",
    "def candidate_paths(base_dir, sid, name):\n",
    "    fname = f\"{sid}_epoch30_merged.csv\"\n",
    "    cand = [\n",
    "        os.path.join(base_dir, f\"{sid}{name}\", \"EPOCH\", fname),\n",
    "        os.path.join(base_dir, f\"{sid}{name}\", fname),\n",
    "        os.path.join(base_dir, fname),\n",
    "    ]\n",
    "    # nameが空文字のときの余分なスラッシュなども吸収\n",
    "    return list(dict.fromkeys([p.replace(\"//\", \"/\").replace(\"\\\\\\\\\", \"\\\\\") for p in cand]))\n",
    "\n",
    "# --- 列名を仕様どおりに正規化（位置で強制） ---\n",
    "def normalize_columns(df):\n",
    "    cols = list(df.columns)\n",
    "    if len(cols) < 4:\n",
    "        raise ValueError(f\"列数が不足しています（{len(cols)}列）。最低4列必要です。\")\n",
    "    # 位置ベースで先頭3列名を固定、それ以降は既存名を尊重（重複回避）\n",
    "    new_cols = [\"epoch_start\", \"epoch_end\", \"FMS\"] + cols[3:]\n",
    "    df = df.copy()\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "# --- 型の統一：4列目以降は数値に、FMSは数値化 ---\n",
    "def enforce_types(df):\n",
    "    df = df.copy()\n",
    "    # FMS\n",
    "    df[\"FMS\"] = pd.to_numeric(df[\"FMS\"], errors=\"coerce\")\n",
    "    # 4列目以降（特徴量）をfloat化\n",
    "    feature_cols = df.columns[3:]\n",
    "    for c in feature_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# --- 読み込み本体 ---\n",
    "all_frames = []\n",
    "for sid, name in subjects:\n",
    "    # ログの見出し\n",
    "    print(f\"# Subject {sid}{name}\")\n",
    "    found = None\n",
    "    for p in candidate_paths(BASE_DIR, sid, name):\n",
    "        if os.path.exists(p):\n",
    "            found = p\n",
    "            break\n",
    "\n",
    "    if found is None:\n",
    "        print(f\"[SKIP] CSV: {sid}_epoch30_merged.csv が見つかりません\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_i = pd.read_csv(found, encoding=\"utf-8-sig\")\n",
    "        df_i = normalize_columns(df_i)\n",
    "        df_i = enforce_types(df_i)\n",
    "        # 被験者情報の付与\n",
    "        df_i[\"subject_id\"] = sid\n",
    "        df_i[\"person_name\"] = name\n",
    "        all_frames.append(df_i)\n",
    "        print(f\"[OK]  CSV -> {found}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] CSV: 読み込み/正規化に失敗: {e}\")\n",
    "\n",
    "# --- 縦結合と基本チェック ---\n",
    "if len(all_frames) == 0:\n",
    "    raise RuntimeError(\"有効なCSVが1件も読み込めませんでした。パス設定を確認してください。\")\n",
    "\n",
    "df = pd.concat(all_frames, axis=0, ignore_index=True)\n",
    "\n",
    "# --- 学習用の列指定を確定 ---\n",
    "label_col = \"FMS\"\n",
    "group_col = \"subject_id\"\n",
    "# 4列目以降が特徴量、ただし末尾2列（subject_id, person_name）は除外\n",
    "feature_cols = list(df.columns[3:-2])\n",
    "\n",
    "# --- 確認出力（先頭行と基本統計） ---\n",
    "print(\"\\n[INFO] df shape:\", df.shape)\n",
    "print(\"[INFO] label_col:\", label_col)\n",
    "print(\"[INFO] group_col:\", group_col)\n",
    "print(\"[INFO] #features:\", len(feature_cols))\n",
    "print(\"[INFO] head():\")\n",
    "print(df.iloc[:5, :8])  # 先頭8列だけ表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72f1b45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]  Features dtype check -> all numeric (25 cols)\n",
      "[SKIP] 欠損あり列: 12 / 25 列 -> 前工程の補完を確認してください\n",
      "\n",
      "[INFO] Class balance per subject (y=1 counts / total):\n",
      "# Subject 0521\n",
      "[OK]  Class -> pos=2, neg=22, total=24\n",
      "# Subject 06021\n",
      "[OK]  Class -> pos=8, neg=16, total=24\n",
      "# Subject 06022\n",
      "[OK]  Class -> pos=1, neg=23, total=24\n",
      "# Subject 06271\n",
      "[OK]  Class -> pos=2, neg=22, total=24\n",
      "# Subject 06272\n",
      "[OK]  Class -> pos=12, neg=12, total=24\n",
      "# Subject 06273\n",
      "[OK]  Class -> pos=5, neg=19, total=24\n",
      "# Subject 06274\n",
      "[OK]  Class -> pos=0, neg=24, total=24\n",
      "# Subject 06275\n",
      "[OK]  Class -> pos=0, neg=24, total=24\n",
      "\n",
      "[SKIP] 単一クラスfold検出: 06274, 06275 -> 当該foldのROC AUCはNaNになります（仕様どおり継続）\n",
      "\n",
      "[INFO] Shapes:\n",
      "X_all: (192, 25) / y_all: (192,) / groups: (192,)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2: Label binarization & sanity checks =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 二値化（仕様：FMS ≥ 2 → 1）\n",
    "y_all = (df[label_col] >= 2).astype(int)\n",
    "\n",
    "# 特徴量行列とグループ列\n",
    "X_all = df[feature_cols].copy()\n",
    "groups = df[group_col].copy()\n",
    "\n",
    "# 型・欠損の最終チェック（必要最低限）\n",
    "# 特徴量はfloat想定：object等が混ざっていないか確認\n",
    "non_numeric = [c for c in X_all.columns if not np.issubdtype(X_all[c].dtype, np.number)]\n",
    "if non_numeric:\n",
    "    print(f\"[SKIP] 非数値列を検出: {non_numeric} -> 前処理でfloat化が必要です\")\n",
    "else:\n",
    "    print(f\"[OK]  Features dtype check -> all numeric ({len(feature_cols)} cols)\")\n",
    "\n",
    "# 欠損の有無（ここでは警告のみ。補完は前工程の責務）\n",
    "na_counts = X_all.isna().sum()\n",
    "n_cols_with_na = int((na_counts > 0).sum())\n",
    "if n_cols_with_na > 0:\n",
    "    print(f\"[SKIP] 欠損あり列: {n_cols_with_na} / {len(feature_cols)} 列 -> 前工程の補完を確認してください\")\n",
    "else:\n",
    "    print(\"[OK]  Missing check -> no NaNs in features\")\n",
    "\n",
    "# 被験者ごとのクラス分布を表示（LOSOの健全性確認）\n",
    "print(\"\\n[INFO] Class balance per subject (y=1 counts / total):\")\n",
    "warn_single_class = []\n",
    "for sid, grp in df.groupby(group_col):\n",
    "    y_sub = (grp[label_col] >= 2).astype(int)\n",
    "    pos = int(y_sub.sum())\n",
    "    tot = int(len(y_sub))\n",
    "    neg = tot - pos\n",
    "    print(f\"# Subject {sid}\")\n",
    "    print(f\"[OK]  Class -> pos={pos}, neg={neg}, total={tot}\")\n",
    "    if pos == 0 or neg == 0:\n",
    "        warn_single_class.append(sid)\n",
    "\n",
    "# 単一クラスfoldの警告（AUCはNaNになるため）\n",
    "if warn_single_class:\n",
    "    ids = \", \".join(map(str, warn_single_class))\n",
    "    print(f\"\\n[SKIP] 単一クラスfold検出: {ids} -> 当該foldのROC AUCはNaNになります（仕様どおり継続）\")\n",
    "else:\n",
    "    print(\"\\n[OK]  全foldで2クラス確認 -> ROC AUC計算は可能\")\n",
    "\n",
    "# 形状の要約\n",
    "print(\"\\n[INFO] Shapes:\")\n",
    "print(\"X_all:\", X_all.shape, \"/ y_all:\", y_all.shape, \"/ groups:\", groups.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dd5cd1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[ERROR] ベースラインにNaNがあります。詳細: {'06272': ['CSI', 'CVI', 'HeartRate', 'pNN50', 'RMSSD', 'RR_interval', 'SD1', 'SD2', 'SDSD']}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_mask\u001b[38;5;241m.\u001b[39many()\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     67\u001b[0m     bad \u001b[38;5;241m=\u001b[39m {sid: baseline_mat\u001b[38;5;241m.\u001b[39mcolumns[cols]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     68\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m sid, cols \u001b[38;5;129;01min\u001b[39;00m na_mask\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mto_series()\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m s: np\u001b[38;5;241m.\u001b[39mwhere(na_mask\u001b[38;5;241m.\u001b[39mloc[s])[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     69\u001b[0m            \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] ベースラインにNaNがあります。詳細: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# ---- スケーリング関数（フォールバック禁止版）----\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_delta\u001b[39m(X_sub: pd\u001b[38;5;241m.\u001b[39mDataFrame, b_vec: pd\u001b[38;5;241m.\u001b[39mSeries) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n",
      "\u001b[1;31mValueError\u001b[0m: [ERROR] ベースラインにNaNがあります。詳細: {'06272': ['CSI', 'CVI', 'HeartRate', 'pNN50', 'RMSSD', 'RR_interval', 'SD1', 'SD2', 'SDSD']}"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2.5 : Baseline(単一行) & Scaling (delta/relative/robust) =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- 必須カラム確認 ----\n",
    "for col in [\"epoch_start\", \"epoch_end\", \"FMS\"]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"[ERROR] 必須カラムが見つかりません: {col}\")\n",
    "\n",
    "# subject_id が無いCSVでも動くように（ただし単一被験者に限定）\n",
    "if \"subject_id\" not in df.columns:\n",
    "    df[\"subject_id\"] = \"SINGLE\"\n",
    "\n",
    "# ---- パラメータ ----\n",
    "BASELINE_START = 1470   # ← 仕様どおり\n",
    "BASELINE_END   = 1500   # ← 仕様どおり（半開区間: [1500,1530)）\n",
    "ML_START       = 1500\n",
    "ML_END         = 2100\n",
    "\n",
    "# スケーリング選択: 'delta' | 'relative' | 'robust'\n",
    "SCALING_MODE     = \"delta\"\n",
    "RELATIVE_PERCENT = False  # Trueで%（×100）\n",
    "\n",
    "# ---- 学習対象行（ML窓） ----\n",
    "df_ml = df[(df[\"epoch_start\"] >= ML_START) & (df[\"epoch_start\"] < ML_END)].copy()\n",
    "if df_ml.empty:\n",
    "    raise ValueError(f\"[ERROR] ML窓({ML_START} <= epoch_start < {ML_END})にデータがありません。\")\n",
    "\n",
    "# ---- 特徴量確認 ----\n",
    "if \"feature_cols\" not in globals():\n",
    "    # 4列目以降〜末尾2列手前（subject_id, person_nameなど）を想定\n",
    "    feature_cols = df.columns[3:]\n",
    "    # subject_id/名前系が紛れていたら除外（既に変数がある場合はそのまま使用）\n",
    "    feature_cols = [c for c in feature_cols if c not in {\"subject_id\", \"person_name\"}]\n",
    "\n",
    "if not feature_cols:\n",
    "    raise ValueError(\"[ERROR] feature_cols が空です。\")\n",
    "\n",
    "# すべて数値化（エラーはここでSTOP）\n",
    "try:\n",
    "    df_ml[feature_cols] = df_ml[feature_cols].astype(float)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"[ERROR] 特徴量に数値化できない列があります: {e}\")\n",
    "\n",
    "# ---- ベースライン行（各被験者ちょうど1行）抽出（フォールバック禁止）----\n",
    "bl_rows = []\n",
    "for sid, g in df.groupby(\"subject_id\"):\n",
    "    bl = g[(g[\"epoch_start\"] >= BASELINE_START) & (g[\"epoch_start\"] < BASELINE_END)]\n",
    "    if len(bl) != 1:\n",
    "        raise ValueError(\n",
    "            f\"[ERROR] ベースライン行がちょうど1行ではありません: subject={sid}, 該当行数={len(bl)} \"\n",
    "            f\"(必要条件: {BASELINE_START} <= epoch_start < {BASELINE_END})\"\n",
    "        )\n",
    "    bl_rows.append(bl.iloc[0].copy())\n",
    "\n",
    "baseline_df = pd.DataFrame(bl_rows).set_index(\"subject_id\")\n",
    "\n",
    "# ベースライン値（数値化 & 欠損チェック：1つでもNaNがあればSTOP）\n",
    "baseline_mat = baseline_df[feature_cols].copy()\n",
    "try:\n",
    "    baseline_mat = baseline_mat.astype(float)\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"[ERROR] ベースライン行に数値化できない値が含まれます: {e}\")\n",
    "\n",
    "na_mask = baseline_mat.isna()\n",
    "if na_mask.any().any():\n",
    "    bad = {sid: baseline_mat.columns[cols].tolist()\n",
    "           for sid, cols in na_mask.index.to_series().map(lambda s: np.where(na_mask.loc[s])[0]).items()\n",
    "           if len(cols) > 0}\n",
    "    raise ValueError(f\"[ERROR] ベースラインにNaNがあります。詳細: {bad}\")\n",
    "\n",
    "# ---- スケーリング関数（フォールバック禁止版）----\n",
    "def transform_delta(X_sub: pd.DataFrame, b_vec: pd.Series) -> pd.DataFrame:\n",
    "    return X_sub - b_vec\n",
    "\n",
    "def transform_relative(X_sub: pd.DataFrame, b_vec: pd.Series, percent: bool=False) -> pd.DataFrame:\n",
    "    denom = b_vec.abs()\n",
    "    zero_feats = denom.index[denom == 0.0].tolist()\n",
    "    if zero_feats:\n",
    "        raise ValueError(f\"[ERROR] relative変換の分母が0です（ベースライン=0）: features={zero_feats}\")\n",
    "    Z = (X_sub - b_vec) / denom\n",
    "    return Z * 100.0 if percent else Z\n",
    "\n",
    "def transform_robust(X_sub: pd.DataFrame, b_vec: pd.Series) -> pd.DataFrame:\n",
    "    # IQRは同被験者のML窓から算出（フォールバック無し）\n",
    "    q1 = X_sub.quantile(0.25, axis=0)\n",
    "    q3 = X_sub.quantile(0.75, axis=0)\n",
    "    iqr = q3 - q1\n",
    "    if iqr.isna().any():\n",
    "        raise ValueError(f\"[ERROR] robust変換でIQRがNaNの特徴があります: {iqr[iqr.isna()].index.tolist()}\")\n",
    "    nonpos = iqr.index[iqr <= 0.0].tolist()\n",
    "    if nonpos:\n",
    "        raise ValueError(f\"[ERROR] robust変換でIQR<=0の特徴があります（定数/ほぼ定数）: {nonpos}\")\n",
    "    return (X_sub - b_vec) / iqr\n",
    "\n",
    "# ---- 変換の実行（被験者単位）----\n",
    "scaled_blocks = []\n",
    "for sid, g in df_ml.groupby(\"subject_id\"):\n",
    "    X_sub = g[feature_cols].copy()\n",
    "\n",
    "    # ML窓側の欠損チェック：1つでもNaNがあれば即STOP\n",
    "    na_locs = np.where(X_sub.isna())\n",
    "    if len(na_locs[0]) > 0:\n",
    "        # どの行・列にNaNがあるか簡潔に表示\n",
    "        bad_rows = X_sub.index[na_locs[0]].unique().tolist()\n",
    "        bad_cols = list(set([feature_cols[j] for j in na_locs[1]]))\n",
    "        raise ValueError(f\"[ERROR] ML窓にNaNがあります: subject={sid}, rows={bad_rows[:5]}..., cols={bad_cols}\")\n",
    "\n",
    "    b_vec = baseline_mat.loc[sid]  # 1行のベクトル（NaNなしは前段で保証済み）\n",
    "\n",
    "    if SCALING_MODE == \"delta\":\n",
    "        X_trf = transform_delta(X_sub, b_vec)\n",
    "        mode_str = \"delta\"\n",
    "    elif SCALING_MODE == \"relative\":\n",
    "        X_trf = transform_relative(X_sub, b_vec, percent=RELATIVE_PERCENT)\n",
    "        mode_str = \"relative_pct\" if RELATIVE_PERCENT else \"relative_ratio\"\n",
    "    elif SCALING_MODE == \"robust\":\n",
    "        X_trf = transform_robust(X_sub, b_vec)\n",
    "        mode_str = \"robust_iqr\"\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR] 未知のSCALING_MODE: {SCALING_MODE}\")\n",
    "\n",
    "    X_trf.index = g.index\n",
    "    scaled_blocks.append(X_trf)\n",
    "\n",
    "# ---- 全被験者を縦結合 → X_scaled_all / y_all / groups ----\n",
    "X_scaled_all = pd.concat(scaled_blocks, axis=0).sort_index().astype(np.float32)\n",
    "y_all = (df_ml[\"FMS\"] >= 2).astype(int)\n",
    "groups = df_ml[\"subject_id\"]\n",
    "\n",
    "# ---- 保存（大文字ファイル名）----\n",
    "baseline_out = baseline_mat.copy()\n",
    "baseline_out.index.name = \"subject_id\"\n",
    "baseline_out.to_csv(\"BASELINE_VALUES.CSV\", encoding=\"utf-8-sig\")\n",
    "\n",
    "pd.DataFrame([{\n",
    "    \"mode\": mode_str,\n",
    "    \"baseline_window\": f\"[{BASELINE_START},{BASELINE_END})\",\n",
    "    \"ml_window\": f\"[{ML_START},{ML_END})\",\n",
    "    \"n_subjects\": df_ml[\"subject_id\"].nunique(),\n",
    "    \"n_rows_ml\": len(df_ml)\n",
    "}]).to_csv(\"SCALING_META.CSV\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[OK] Scaling completed (strict). mode={mode_str}  subjects={df_ml['subject_id'].nunique()}  rows={len(df_ml)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1148b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3: Modeling & SHAP helper functions (with deterministic settings) =====\n",
    "from typing import Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb  # for pred_contribs SHAP\n",
    "\n",
    "def fit_xgb_classifier(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    random_state: Optional[int] = None,  # 互換のため受け取るが未使用\n",
    ") -> XGBClassifier:\n",
    "    # 決定論性のため型を固定\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.int32)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        eval_metric=\"logloss\",\n",
    "        subsample=1.0,          # 明示\n",
    "        colsample_bytree=1.0,   # 明示\n",
    "        n_jobs=1,               # 単一スレッドで決定論的に\n",
    "        tree_method=\"hist\",\n",
    "        # xgboost<2.0 の場合は device 引数が無いので下行はコメントアウトしてください\n",
    "        device=\"cpu\",\n",
    "        seed=0, random_state=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def compute_train_shap_abs_mean(model: XGBClassifier, X_ref: pd.DataFrame) -> pd.Series:\n",
    "    # XGBoost組み込みSHAP（pred_contribs）で決定論的に\n",
    "    X_ref = X_ref.astype(np.float32)\n",
    "    dm = xgb.DMatrix(X_ref, feature_names=list(X_ref.columns))\n",
    "    contribs = model.get_booster().predict(dm, pred_contribs=True)  # (n_samples, n_features+1)\n",
    "    shap_vals = contribs[:, :-1]  # 最後の列はバイアス項\n",
    "    abs_mean = np.abs(shap_vals).mean(axis=0)\n",
    "    return pd.Series(abs_mean, index=X_ref.columns, name=\"abs_mean\")\n",
    "\n",
    "def evaluate_fold(\n",
    "    model: XGBClassifier,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    ") -> Dict[str, float]:\n",
    "    # 型を揃えて微小差を抑制\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, proba)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    return {\"roc_auc\": roc_auc, \"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Subject 0521因幡先生\n",
      "[OK]  Eval -> ROC AUC = 0.806, ACC = 0.850\n",
      "# Subject 06021因幡先生\n",
      "[OK]  Eval -> ROC AUC = 0.927, ACC = 0.650\n",
      "# Subject 06022今村さん\n",
      "[OK]  Eval -> ROC AUC = 0.947, ACC = 0.950\n",
      "# Subject 06271梅野さん\n",
      "[OK]  Eval -> ROC AUC = 0.972, ACC = 0.950\n",
      "# Subject 06272\n",
      "[OK]  Eval -> ROC AUC = 0.573, ACC = 0.400\n",
      "# Subject 06273\n",
      "[OK]  Eval -> ROC AUC = 0.333, ACC = 0.750\n",
      "# Subject 06274\n",
      "[SKIP] Eval: ROC AUC = NaN（評価不可）, ACC = 1.000\n",
      "# Subject 06275\n",
      "[SKIP] Eval: ROC AUC = NaN（評価不可）, ACC = 0.550\n",
      "[OK]  SHAP Ranking -> ./SHAP_FEATURE_RANKING.CSV\n",
      "[OK]  SHAP TopK -> ./SHAP_TOP10.CSV\n",
      "[OK]  Metrics -> ./LOSO_METRICS.CSV\n",
      "\n",
      "[INFO] shap_rank head():\n",
      "                          fold1     fold2     fold3     fold4     fold5  \\\n",
      "HF_power               2.250399  0.824969  1.744066  1.726002  1.807171   \n",
      "watch_Sweat_Rate_mean  0.969304  1.221709  1.010570  0.608620  1.153376   \n",
      "watch_Heart_Rate_mean  0.694299  0.679267  0.723402  0.957407  1.037829   \n",
      "FaceTemp_Max_Diff      0.592341  0.607686  0.525797  0.656857  1.194625   \n",
      "LF_HF_ratio            0.449161  0.166598  0.347581  0.448433  0.635763   \n",
      "FaceTemp_Mean_Diff     0.713726  0.775527  0.479957  0.307386  0.220311   \n",
      "FaceTemp_Mean          0.142268  0.430916  0.330061  0.416937  0.079258   \n",
      "watch_Heart_Rate_std   0.216076  0.086036  0.224922  0.240580  0.314187   \n",
      "pNN50                  0.073849  0.050742  0.331188  0.217748  0.062420   \n",
      "FaceTemp_Max           0.239173  0.130247  0.341557  0.207005  0.097170   \n",
      "\n",
      "                          fold6     fold7     fold8  mean_abs  \n",
      "HF_power               1.816076  2.191697  0.854549  1.651866  \n",
      "watch_Sweat_Rate_mean  0.775916  0.895790  1.032216  0.958438  \n",
      "watch_Heart_Rate_mean  1.134398  0.867341  1.093031  0.898372  \n",
      "FaceTemp_Max_Diff      0.792651  0.519652  0.776660  0.708284  \n",
      "LF_HF_ratio            0.112883  0.693237  0.518415  0.421509  \n",
      "FaceTemp_Mean_Diff     0.120180  0.255842  0.402906  0.409479  \n",
      "FaceTemp_Mean          0.298722  0.289307  0.428434  0.301988  \n",
      "watch_Heart_Rate_std   0.378469  0.126967  0.260681  0.230990  \n",
      "pNN50                  0.478648  0.154549  0.120347  0.186186  \n",
      "FaceTemp_Max           0.236927  0.125165  0.083440  0.182586  \n",
      "\n",
      "[INFO] metrics summary:\n",
      "         roc_auc  accuracy test_subject\n",
      "count   6.000000  8.000000            8\n",
      "unique       NaN       NaN            8\n",
      "top          NaN       NaN         0521\n",
      "freq         NaN       NaN            1\n",
      "mean    0.759747  0.762500          NaN\n",
      "std     0.255805  0.215058          NaN\n",
      "min     0.333333  0.400000          NaN\n",
      "25%     0.631076  0.625000          NaN\n",
      "50%     0.866319  0.800000          NaN\n",
      "75%     0.942297  0.950000          NaN\n",
      "max     0.972222  1.000000          NaN\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4 : LOSO → SHAP集計 & 保存（StandardScalerなし） =====\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# 前セルで X_scaled_all, y_all, groups, feature_cols が定義済み前提\n",
    "X_all = X_scaled_all.copy()   # 追加の標準化はしない\n",
    "y_all = y_all.copy()\n",
    "groups = groups.copy()\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# foldごとの |SHAP|平均を記録\n",
    "shap_abs_means = []\n",
    "metrics_rows = []\n",
    "\n",
    "fold_id = 0\n",
    "for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "    fold_id += 1\n",
    "    test_sid = pd.Series(groups).iloc[te_idx].unique()[0]\n",
    "\n",
    "    X_tr, X_te = X_all.iloc[tr_idx], X_all.iloc[te_idx]\n",
    "    y_tr, y_te = y_all.iloc[tr_idx], y_all.iloc[te_idx]\n",
    "\n",
    "    # 学習（Cell 3の決定論版）\n",
    "    model = fit_xgb_classifier(X_tr, y_tr)\n",
    "\n",
    "    # 学習データ上の SHAP（|値|平均）\n",
    "    abs_mean = compute_train_shap_abs_mean(model, X_tr).rename(f\"fold{fold_id}\")\n",
    "    shap_abs_means.append(abs_mean)\n",
    "\n",
    "    # 評価\n",
    "    proba = model.predict_proba(X_te.astype(np.float32))[:, 1]\n",
    "    if len(np.unique(y_te)) == 2:\n",
    "        roc_auc = roc_auc_score(y_te, proba)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "    acc = accuracy_score(y_te, (proba >= 0.5).astype(int))\n",
    "\n",
    "    metrics_rows.append({\"test_subject\": str(test_sid), \"roc_auc\": roc_auc, \"accuracy\": acc})\n",
    "\n",
    "# SHAPランキング（fold列＋mean_abs）\n",
    "shap_rank = pd.concat(shap_abs_means, axis=1)\n",
    "shap_rank[\"mean_abs\"] = shap_rank.mean(axis=1)\n",
    "shap_rank = shap_rank.sort_values(\"mean_abs\", ascending=False)\n",
    "\n",
    "# 保存（大文字ファイル名）\n",
    "shap_rank.to_csv(\"SHAP_FEATURE_RANKING.CSV\", encoding=\"utf-8-sig\")\n",
    "shap_rank.head(10).to_csv(\"SHAP_TOP10.CSV\", encoding=\"utf-8-sig\")\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "metrics_df.to_csv(\"LOSO_METRICS.CSV\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[OK]  SHAP Ranking -> ./SHAP_FEATURE_RANKING.CSV\")\n",
    "print(\"[OK]  SHAP TopK -> ./SHAP_TOP10.CSV\")\n",
    "print(\"[OK]  Metrics -> ./LOSO_METRICS.CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65b7c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]  SHAP Ranking (labeled) -> ./SHAP_FEATURE_RANKING_LABELED.CSV\n",
      "[OK]  SHAP Top10 (labeled)   -> ./SHAP_TOP10_LABELED.CSV\n",
      "\n",
      "[INFO] head(10):\n",
      "                                display_name  interval_sec  mean_abs\n",
      "HF_power                            HF Power         120.0  1.651866\n",
      "watch_Sweat_Rate_mean  watch_Sweat_Rate_mean           NaN  0.958438\n",
      "watch_Heart_Rate_mean  watch_Heart_Rate_mean           NaN  0.898372\n",
      "FaceTemp_Max_Diff          FaceTemp_Max_Diff           NaN  0.708284\n",
      "LF_HF_ratio                            HF/LF         120.0  0.421509\n",
      "FaceTemp_Mean_Diff        FaceTemp_Mean_Diff           NaN  0.409479\n",
      "FaceTemp_Mean                  FaceTemp_Mean           NaN  0.301988\n",
      "watch_Heart_Rate_std    watch_Heart_Rate_std           NaN  0.230990\n",
      "pNN50                                  pNN50          60.0  0.186186\n",
      "FaceTemp_Max                    FaceTemp_Max           NaN  0.182586\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 5: Label SHAP table with display names & intervals =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 既存のマップ（ノート上で定義済みのものを使う）\n",
    "# title_map, interval_map が既にある前提\n",
    "\n",
    "IN_PATH  = \"SHAP_FEATURE_RANKING.CSV\"\n",
    "OUT_PATH = \"SHAP_FEATURE_RANKING_LABELED.CSV\"\n",
    "\n",
    "shap_df = pd.read_csv(IN_PATH, encoding=\"utf-8-sig\", index_col=0)\n",
    "\n",
    "# 表示名と間隔を列として追加\n",
    "def _disp_name(feat: str) -> str:\n",
    "    return title_map.get(feat, feat)\n",
    "\n",
    "def _interval_sec(feat: str):\n",
    "    return interval_map.get(feat, np.nan)\n",
    "\n",
    "labeled = shap_df.copy()\n",
    "labeled.insert(0, \"display_name\", [ _disp_name(f) for f in labeled.index ])\n",
    "labeled.insert(1, \"interval_sec\", [ _interval_sec(f) for f in labeled.index ])\n",
    "\n",
    "# 上位10も別名で保存（任意）\n",
    "TOPK_LABELED = \"SHAP_TOP10_LABELED.CSV\"\n",
    "labeled.to_csv(OUT_PATH, encoding=\"utf-8-sig\")\n",
    "labeled.head(10).to_csv(TOPK_LABELED, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[OK]  SHAP Ranking (labeled) -> ./{OUT_PATH}\")\n",
    "print(f\"[OK]  SHAP Top10 (labeled)   -> ./{TOPK_LABELED}\")\n",
    "print(\"\\n[INFO] head(10):\")\n",
    "print(labeled.head(10)[[\"display_name\", \"interval_sec\", \"mean_abs\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81043540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]  Plot -> ./SHAP_RANKING_ALL.PNG\n",
      "[OK]  Plot -> ./SHAP_RANKING_TOP10.PNG\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6: Plot SHAP ranking (all features & Top10) =====\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 入力は前セルで作った labeled DataFrame を再利用\n",
    "# まだ残ってなければ CSV を再読込\n",
    "if \"labeled\" not in locals():\n",
    "    labeled = pd.read_csv(\"SHAP_FEATURE_RANKING_LABELED.CSV\", encoding=\"utf-8-sig\", index_col=0)\n",
    "\n",
    "# ソート済みのランキングを使用\n",
    "ranking_all = labeled.sort_values(\"mean_abs\", ascending=True)  # 棒を下から上に\n",
    "ranking_top10 = ranking_all.tail(10)\n",
    "\n",
    "# --- 全特徴量ランキング ---\n",
    "plt.figure(figsize=(8, max(6, len(ranking_all) * 0.35)))\n",
    "plt.barh(ranking_all[\"display_name\"], ranking_all[\"mean_abs\"])\n",
    "plt.xlabel(\"Mean |SHAP|\", fontsize=24)\n",
    "plt.ylabel(\"Features\", fontsize=24)\n",
    "plt.title(\"SHAP Feature Importance (All)\", fontsize=30)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SHAP_RANKING_ALL.PNG\", dpi=300)\n",
    "plt.close()\n",
    "print(\"[OK]  Plot -> ./SHAP_RANKING_ALL.PNG\")\n",
    "\n",
    "# --- Top10ランキング ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(ranking_top10[\"display_name\"], ranking_top10[\"mean_abs\"])\n",
    "plt.xlabel(\"Mean |SHAP|\", fontsize=24)\n",
    "plt.ylabel(\"Top 10 Features\", fontsize=24)\n",
    "plt.title(\"SHAP Feature Importance (Top10)\", fontsize=30)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SHAP_RANKING_TOP10.PNG\", dpi=300)\n",
    "plt.close()\n",
    "print(\"[OK]  Plot -> ./SHAP_RANKING_TOP10.PNG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cda211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]  Per-k metrics CSV -> ./AUC_ACC_PR_REC_F1_PER_K.CSV\n",
      "[OK]  Plot -> ./AUC_VS_NUM_FEATURES.PNG\n",
      "[OK]  Plot -> ./CONFUSION_MATRIX_BESTK.PNG\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7 : Per-k metrics CSV + AUC plot + Confusion Matrix at best k =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, confusion_matrix,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "RANK_LABELED = \"SHAP_FEATURE_RANKING_LABELED.CSV\"\n",
    "RANK_RAW     = \"SHAP_FEATURE_RANKING.CSV\"\n",
    "OUT_CSV      = \"AUC_ACC_PR_REC_F1_PER_K.CSV\"   # すべてのkの指標（pooled）\n",
    "\n",
    "# ---------- 1) SHAPランキングを読み込み（labeled優先） ----------\n",
    "if os.path.exists(RANK_LABELED):\n",
    "    rank_df = pd.read_csv(RANK_LABELED, encoding=\"utf-8-sig\", index_col=0)\n",
    "elif os.path.exists(RANK_RAW):\n",
    "    rank_df = pd.read_csv(RANK_RAW, encoding=\"utf-8-sig\", index_col=0)\n",
    "else:\n",
    "    raise FileNotFoundError(\"SHAP_FEATURE_RANKING_LABELED.CSV / SHAP_FEATURE_RANKING.CSV が見つかりません。\")\n",
    "\n",
    "rank_df = rank_df.sort_values(\"mean_abs\", ascending=False)\n",
    "feature_order = [f for f in rank_df.index if f in df.columns]\n",
    "if len(feature_order) == 0:\n",
    "    raise RuntimeError(\"ランキングの特徴量名が df の列と一致していません。\")\n",
    "\n",
    "# ---------- 2) 学習行列の選択（スケーリング済みがあれば優先） ----------\n",
    "if \"X_scaled_all\" in globals():\n",
    "    X_source = X_scaled_all.copy()\n",
    "    y_source = y_all.copy()\n",
    "    g_source = groups.copy()\n",
    "else:\n",
    "    # スケーリング未使用のとき：ML窓を定義して取り出す\n",
    "    ML_START, ML_END = 1500, 2100\n",
    "    df_ml = df[(df[\"epoch_start\"] >= ML_START) & (df[\"epoch_start\"] < ML_END)].copy()\n",
    "    if \"subject_id\" not in df_ml.columns:\n",
    "        df_ml[\"subject_id\"] = \"SINGLE\"\n",
    "    X_source = df_ml[feature_cols].copy()\n",
    "    y_source = (df_ml[label_col] >= 2).astype(int)\n",
    "    g_source = df_ml[group_col]\n",
    "\n",
    "# 列順をランキング順に固定 + 型\n",
    "X_source = X_source.reindex(columns=[c for c in feature_order if c in X_source.columns]).astype(np.float32)\n",
    "\n",
    "# ---------- 3) k を全探索（多 → 少）し、各 k で pooled metrics を算出 ----------\n",
    "ks = list(range(len(feature_order), 0, -1))\n",
    "rows = []\n",
    "preds_by_k = {}   # k -> (y_true_pooled, y_pred_pooled, proba_pooled)  可視化/CM用\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "for k in ks:\n",
    "    feats = feature_order[:k]\n",
    "    X = X_source[feats]\n",
    "\n",
    "    y_true_all, y_pred_all, proba_all = [], [], []\n",
    "\n",
    "    for tr_idx, te_idx in logo.split(X, y_source, g_source):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y_source.iloc[tr_idx], y_source.iloc[te_idx]\n",
    "\n",
    "        # 既存の決定論設定版トレーナ（Cell 3の関数）を使用\n",
    "        model = fit_xgb_classifier(X_tr, y_tr)\n",
    "\n",
    "        proba = model.predict_proba(X_te.astype(np.float32))[:, 1]\n",
    "        y_pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "        y_true_all.append(y_te.values)\n",
    "        y_pred_all.append(y_pred)\n",
    "        proba_all.append(proba)\n",
    "\n",
    "    # ---- pooled結合 ----\n",
    "    y_true_k = np.concatenate(y_true_all) if y_true_all else np.array([])\n",
    "    y_pred_k = np.concatenate(y_pred_all) if y_pred_all else np.array([])\n",
    "    proba_k  = np.concatenate(proba_all)  if proba_all  else np.array([])\n",
    "\n",
    "    # ---- AUC（pooled）----\n",
    "    if y_true_k.size > 0 and len(np.unique(y_true_k)) == 2:\n",
    "        auc_val = float(roc_auc_score(y_true_k, proba_k))\n",
    "    else:\n",
    "        auc_val = float(\"nan\")\n",
    "\n",
    "    # ---- 二値指標（pooled, 閾値0.5）----\n",
    "    if y_true_k.size > 0:\n",
    "        acc = float(accuracy_score(y_true_k, y_pred_k))\n",
    "        pr  = float(precision_score(y_true_k, y_pred_k, zero_division=0))\n",
    "        rec = float(recall_score(y_true_k, y_pred_k, zero_division=0))\n",
    "        f1  = float(f1_score(y_true_k, y_pred_k, zero_division=0))\n",
    "        n_pos_all = int((y_true_k == 1).sum())\n",
    "        n_neg_all = int((y_true_k == 0).sum())\n",
    "    else:\n",
    "        acc = pr = rec = f1 = float(\"nan\")\n",
    "        n_pos_all = n_neg_all = 0\n",
    "\n",
    "    rows.append({\n",
    "        \"k\": int(k),\n",
    "        \"auc\": auc_val,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": pr,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"n_pos\": n_pos_all,\n",
    "        \"n_neg\": n_neg_all\n",
    "    })\n",
    "    preds_by_k[k] = (y_true_k, y_pred_k, proba_k)\n",
    "\n",
    "# ---------- 4) CSV保存（降順：特徴量が多い→少ない） ----------\n",
    "metrics_per_k = pd.DataFrame(rows).sort_values(\"k\", ascending=False)\n",
    "metrics_per_k.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK]  Per-k metrics CSV -> ./{OUT_CSV}\")\n",
    "\n",
    "# ---------- 5) AUC 図：AUC vs Number of Features（最大点＝赤い太丸、ラベルに k を表示） ----------\n",
    "auc_values = metrics_per_k.set_index(\"k\").loc[ks, \"auc\"].values  # ks順で配列化\n",
    "valid_mask = ~np.isnan(auc_values)\n",
    "if valid_mask.any():\n",
    "    best_idx_in_valid = np.argmax(auc_values[valid_mask])\n",
    "    best_k = np.array(ks)[np.where(valid_mask)[0][best_idx_in_valid]]\n",
    "    best_auc = auc_values[valid_mask][best_idx_in_valid]\n",
    "else:\n",
    "    best_k, best_auc = ks[0], np.nan  # AUCが全NaNの場合のフォールバック\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(ks, auc_values, marker='o')\n",
    "if not np.isnan(best_auc):\n",
    "    plt.scatter([best_k], [best_auc], s=160, color=\"red\", edgecolors=\"none\",\n",
    "                label=f\"Max AUC = {best_auc:.3f} (k={best_k})\", zorder=5)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.xlabel(\"Number of Features\", fontsize=24)\n",
    "plt.ylabel(\"ROC AUC (pooled)\", fontsize=24)\n",
    "plt.title(\"AUC vs Number of Features\", fontsize=24)\n",
    "if not np.isnan(best_auc):\n",
    "    plt.legend(fontsize=20)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"AUC_VS_NUM_FEATURES.PNG\", dpi=300)\n",
    "plt.close()\n",
    "print(\"[OK]  Plot -> ./AUC_VS_NUM_FEATURES.PNG\")\n",
    "\n",
    "# ---------- 6) 最大AUC時の混同行列（pooledの0.5閾値） ----------\n",
    "if not np.isnan(best_auc):\n",
    "    y_true_best, y_pred_best, _ = preds_by_k[int(best_k)]\n",
    "    if y_true_best.size > 0:\n",
    "        cm = confusion_matrix(y_true_best, y_pred_best, labels=[0, 1])\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cm, cmap=\"Blues\")\n",
    "        for (i, j), val in np.ndenumerate(cm):\n",
    "            plt.text(j, i, f\"{val:d}\", ha=\"center\", va=\"center\",\n",
    "                     fontsize=20, color=(\"white\" if val > cm.max()/2 else \"black\"))\n",
    "        plt.xticks([0, 1], [\"Pred 0\", \"Pred 1\"], fontsize=20)\n",
    "        plt.yticks([0, 1], [\"True 0\", \"True 1\"], fontsize=20)\n",
    "        plt.xlabel(\"Predicted\", fontsize=24)\n",
    "        plt.ylabel(\"True\", fontsize=24)\n",
    "        plt.title(f\"Confusion Matrix at Best AUC (k={best_k})\", fontsize=24)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"CONFUSION_MATRIX_BESTK.PNG\", dpi=300)\n",
    "        plt.close()\n",
    "        print(\"[OK]  Plot -> ./CONFUSION_MATRIX_BESTK.PNG\")\n",
    "    else:\n",
    "        print(\"⚠️ サンプルが空のため、混同行列は作成しませんでした。\")\n",
    "else:\n",
    "    print(\"⚠️ どの k でも AUC を計算できなかったため、混同行列の作成はスキップしました。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
