{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 0: 環境設定（全セル共通で利用）=====\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib           # 追加\n",
    "import matplotlib.backends  # 追加（←これがポイント）\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ------------------------\n",
    "# 実験スイッチ（Notebook全体で共有）\n",
    "# ------------------------\n",
    "FMS_THRESHOLD: int = 1           # FMS >= 1 を陽性ラベルとみなす\n",
    "EPOCH_LEN: int = 30               # 30 / 60 / 120 のいずれか\n",
    "MODEL_BACKEND: str = \"xgb\"        # \"xgb\" / \"rf\" / \"svm\"\n",
    "USE_MSSQ_FEATURE: bool = False\n",
    "USE_VIMSSQ_FEATURE: bool = False  # True: VIMSSQ を含める\n",
    "SEED_BASE: int = 20251101\n",
    "TOP_SUBSET_K: int = 15           # subset探索で使う上位特徴数\n",
    "\n",
    "# ---- MSSQ / VIMSSQ の High / Low 閾値 ----\n",
    "MSSQ_THRESHOLD_FIXED: float   = 11.0   # 例：MSSQ >= 20 を High\n",
    "VIMSSQ_THRESHOLD_FIXED: float = 3   # 例：VIMSSQ >= 15 を High\n",
    "\n",
    "# ---- グループ分けの基準（FMS推移プロットなどで使用）----\n",
    "# \"MSSQ\" または \"VIMSSQ\" を指定\n",
    "GROUPING_BASIS_FOR_PLOTS: str = \"VIMSSQ\"\n",
    "\n",
    "\n",
    "if EPOCH_LEN not in (30, 60, 120):\n",
    "    raise ValueError(\"EPOCH_LEN は 30/60/120 から選択してください。\")\n",
    "\n",
    "# ---------------- 設定（等間隔グリッド＋近傍再探索） ----------------\n",
    "COARSE_STEPS = 101      # 0.0〜1.0 を等間隔\n",
    "FINE_STEPS   = 101      # 近傍再探索の細かさ\n",
    "FINE_MARGIN  = 0.01      # 近傍幅（±0.01）\n",
    "VERBOSE      = True\n",
    "\n",
    "# ------------------------\n",
    "# ファイル入出力ルート\n",
    "# ------------------------\n",
    "BASE_INPUT_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "BASE_ANALYSIS_DIR = os.path.join(BASE_INPUT_DIR, \"ANALYSIS\")\n",
    "OUT_DIR = os.path.join(BASE_ANALYSIS_DIR, \"機械学習(SHAP_REF法)\", f\"RFE_閾値FMS{FMS_THRESHOLD}_k={TOP_SUBSET_K}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def outpath(filename: str) -> str:\n",
    "    return os.path.join(OUT_DIR, filename)\n",
    "\n",
    "print(f\"[OUT_DIR] {OUT_DIR}  |  EPOCH_LEN={EPOCH_LEN}s\")\n",
    "\n",
    "# ------------------------\n",
    "# 対象被験者・時間窓\n",
    "# ------------------------\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "BASELINE_EPOCH = 1770               # ベースライン行（必須）\n",
    "ML_START, ML_END = 1800, 2400       # 学習に使う epoch_start 範囲 [start, end)\n",
    "\n",
    "# ------------------------\n",
    "# 描画スタイル\n",
    "# ------------------------\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"font.size\": 20, \"axes.titlesize\": 26, \"axes.labelsize\": 22,\n",
    "    \"xtick.labelsize\": 20, \"ytick.labelsize\": 20, \"legend.fontsize\": 20,\n",
    "})\n",
    "\n",
    "# ------------------------\n",
    "# FMS二値化ヘルパ\n",
    "# ------------------------\n",
    "def binarize_fms(series: pd.Series, threshold: Optional[int] = None) -> pd.Series:\n",
    "    th = FMS_THRESHOLD if threshold is None else int(threshold)\n",
    "    return (series >= th).astype(int)\n",
    "\n",
    "# ------------------------\n",
    "# モデルレジストリ\n",
    "# ------------------------\n",
    "ModelBuilder = Callable[..., Any]\n",
    "MODEL_REGISTRY: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "def register_backend(name: str, params: Dict[str, Any], builder: ModelBuilder) -> None:\n",
    "    MODEL_REGISTRY[name] = {\"params\": params, \"builder\": builder}\n",
    "\n",
    "def _build_xgb(params: Dict[str, Any], *, scale_pos_weight: Optional[float] = None):\n",
    "    cfg = params.copy()\n",
    "    if scale_pos_weight is not None:\n",
    "        cfg[\"scale_pos_weight\"] = float(scale_pos_weight)\n",
    "    return xgb.XGBClassifier(**cfg)\n",
    "\n",
    "def _build_rf(params: Dict[str, Any], **_):\n",
    "    return RandomForestClassifier(**params)\n",
    "\n",
    "def _build_svm(params: Dict[str, Any], **_):\n",
    "    return SVC(**params)\n",
    "\n",
    "XGB_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=100,\n",
    "    eval_metric=\"logloss\",\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    n_jobs=1,\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cpu\",\n",
    "    seed=0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "XGB_PARAMS: Dict[str, Any] = dict(\n",
    "    # ← 元のまま\n",
    "    n_estimators=100,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=1,\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cpu\",\n",
    "    seed=0,\n",
    "    random_state=0,\n",
    "\n",
    "    # ---- 学習率（少しだけ低く） ----\n",
    "    learning_rate=0.1,      # デフォルト0.3 → 0.1 にして一歩ずつ学習\n",
    "\n",
    "    # ---- 木の複雑さ（軽く制限） ----\n",
    "    max_depth=3,            # デフォルト6 → 3（浅めの木に）\n",
    "    min_child_weight=5,     # デフォルト1 → 5（少数サンプルでの分割を禁止気味に）\n",
    "    gamma=0.5,              # デフォルト0 → 0.5（ショボい分割は切り捨て）\n",
    "\n",
    "    # ---- サブサンプリング（軽く正則化） ----\n",
    "    subsample=0.8,          # 1.0 → 0.8（各木が見るデータを8割に）\n",
    "    colsample_bytree=0.6,   # 1.0 → 0.6（各木が見る特徴量を6割に）\n",
    "\n",
    "    # ---- L2 / L1 正則化（本命） ----\n",
    "    reg_lambda=20.0,         # L2：1 → 5 に強化\n",
    "    reg_alpha=2,          # L1：0 → 0.5 で“自動特徴選択”を少し効かせる\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "RF_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=100,      # 論文：決定木100本\n",
    "    max_features=1,        # 論文：max feature of one\n",
    "\n",
    "    # 以下は論文に記載がないので，ほぼデフォルト＋再現性用\n",
    "    bootstrap=True,        # scikit-learn のデフォルト\n",
    "    random_state=SEED_BASE,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "\n",
    "SVM_PARAMS: Dict[str, Any] = dict(\n",
    "    C=1.0,\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"scale\",\n",
    "    probability=True,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED_BASE,\n",
    ")\n",
    "\n",
    "register_backend(\"xgb\", XGB_PARAMS, _build_xgb)\n",
    "register_backend(\"rf\",  RF_PARAMS,  _build_rf)\n",
    "register_backend(\"svm\", SVM_PARAMS, _build_svm)\n",
    "\n",
    "def set_model_backend(name: str) -> None:\n",
    "    name = name.lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録: {list(MODEL_REGISTRY.keys())}\")\n",
    "    global MODEL_BACKEND\n",
    "    MODEL_BACKEND = name\n",
    "\n",
    "def build_estimator(\n",
    "    backend: Optional[str] = None,\n",
    "    *,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    name = (backend or MODEL_BACKEND).lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録。\")\n",
    "    base = MODEL_REGISTRY[name][\"params\"].copy()\n",
    "    if overrides:\n",
    "        base.update(overrides)\n",
    "    builder = MODEL_REGISTRY[name][\"builder\"]\n",
    "    return builder(base, scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "def fit_estimator(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    model = build_estimator(\n",
    "        backend=backend, scale_pos_weight=scale_pos_weight, overrides=overrides\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def predict_positive_score(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return np.asarray(model.decision_function(X), dtype=float)\n",
    "    return model.predict(X).astype(float)\n",
    "\n",
    "MODEL_ID = MODEL_BACKEND.upper()\n",
    "print(f\"[INFO] MODEL_BACKEND={MODEL_ID} / SEED={SEED_BASE} / backends={list(MODEL_REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: データ準備（CSV読込 → EPOCH合成 → SUBJECT_META → 行列出力）=====\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --------------------------------------------\n",
    "# ① 30秒EPOCH CSVの読み込み・検証\n",
    "# --------------------------------------------\n",
    "def subject_csv_path(sid: str) -> str:\n",
    "    path = os.path.join(BASE_INPUT_DIR, sid, \"EPOCH\", f\"{sid}_epoch.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"[Cell1] CSV missing for subject {sid}: {path}\")\n",
    "    return path\n",
    "\n",
    "dfs = []\n",
    "for sid in SUBJECT_IDS:\n",
    "    df = pd.read_csv(subject_csv_path(sid))\n",
    "    if df.shape[1] < 4:\n",
    "        raise ValueError(f\"[Cell1] {sid}: 列数が不足（>=4 必須）\")\n",
    "    df = df.copy()\n",
    "    # 4列目以降の列名を文字列化（数値列名対策）\n",
    "    df.columns = list(df.columns[:3]) + [str(c) for c in df.columns[3:]]\n",
    "    c1, c2, c3 = df.columns[:3]\n",
    "    df = df.rename(columns={c1: \"epoch_start\", c2: \"epoch_end\", c3: \"FMS\"})\n",
    "    df[\"epoch_start\"] = pd.to_numeric(df[\"epoch_start\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"epoch_end\"]   = pd.to_numeric(df[\"epoch_end\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"FMS\"]         = pd.to_numeric(df[\"FMS\"],         errors=\"coerce\").astype(\"Int64\")\n",
    "    if df[[\"epoch_start\",\"epoch_end\",\"FMS\"]].isna().any().any():\n",
    "        raise ValueError(f\"[Cell1] {sid}: epoch_start/epoch_end/FMS に NaN\")\n",
    "    df.insert(0, \"subject_id\", sid)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 除外する特徴量（周波数領域など）\n",
    "exclude_feats = {\"HF_power\", \"LF_power\", \"LF_HF_ratio\"}\n",
    "feature_cols_all = [\n",
    "    c for c in combined_raw.columns\n",
    "    if c not in {\"subject_id\",\"epoch_start\",\"epoch_end\",\"FMS\"} and c not in exclude_feats\n",
    "]\n",
    "if not feature_cols_all:\n",
    "    raise RuntimeError(\"[Cell1] 特徴量列が0です。列名や除外設定を確認してください。\")\n",
    "\n",
    "print(f\"[Cell1] Loaded subjects={len(SUBJECT_IDS)}, rows={len(combined_raw)}, \"\n",
    "      f\"features(after drop)={len(feature_cols_all)}\")\n",
    "\n",
    "# ★ デバッグ(1): 生理特徴量のリストをプリント\n",
    "print(\"[Cell1] Physiological feature columns (feature_cols_all):\")\n",
    "for col in feature_cols_all:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ② EPOCH_LEN 秒への合成 + baseline差分 + ラベル生成\n",
    "# --------------------------------------------\n",
    "if (ML_END - ML_START) % EPOCH_LEN != 0:\n",
    "    raise ValueError(f\"[Cell1] ML window {ML_END-ML_START} が EPOCH_LEN={EPOCH_LEN} で割り切れません。\")\n",
    "\n",
    "rows_per_bin = EPOCH_LEN // 30\n",
    "df_out_list = []\n",
    "\n",
    "# ★ デバッグ用ディレクトリ\n",
    "DEBUG_DIR = os.path.join(OUT_DIR, \"Cell1_デバッグ\")\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "for sid, sdf in combined_raw.groupby(\"subject_id\", sort=False):\n",
    "    # baseline 行（1770）の行を1行だけ取る\n",
    "    base_row = sdf.loc[sdf[\"epoch_start\"] == BASELINE_EPOCH]\n",
    "    if len(base_row) != 1:\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline epoch_start=={BASELINE_EPOCH} が見つからない\")\n",
    "    base_vals = base_row[feature_cols_all].astype(float).iloc[0]\n",
    "    if base_vals.isna().any():\n",
    "        raise ValueError(f\"[Cell1] {sid}: baselineにNaN -> {base_vals.index[base_vals.isna()].tolist()}\")\n",
    "\n",
    "    # 学習に使う時間窓だけ抽出\n",
    "    sdf_ml = sdf[(sdf[\"epoch_start\"] >= ML_START) & (sdf[\"epoch_start\"] < ML_END)].copy()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: ML window [{ML_START},{ML_END}) が空です。\")\n",
    "\n",
    "    # 30秒epochを EPOCH_LEN 秒にまとめるためのbin\n",
    "    sdf_ml[\"bin_start\"] = ML_START + ((sdf_ml[\"epoch_start\"] - ML_START) // EPOCH_LEN) * EPOCH_LEN\n",
    "    sdf_ml[\"bin_end\"]   = sdf_ml[\"bin_start\"] + EPOCH_LEN\n",
    "\n",
    "    # 行数が揃っている bin のみ採用\n",
    "    bin_counts = sdf_ml.groupby([\"bin_start\",\"bin_end\"]).size()\n",
    "    complete_bins = bin_counts[bin_counts == rows_per_bin].index\n",
    "    sdf_ml = sdf_ml.set_index([\"bin_start\",\"bin_end\"]).loc[complete_bins].reset_index()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: EPOCH_LEN={EPOCH_LEN} で完全なbinが無い\")\n",
    "\n",
    "    # 各 bin で平均を取る（FMS も平均）\n",
    "    agg_dict = {c: \"mean\" for c in feature_cols_all}\n",
    "    agg_dict[\"FMS\"] = \"mean\"\n",
    "    g = sdf_ml.groupby([\"subject_id\",\"bin_start\",\"bin_end\"], as_index=False).agg(agg_dict)\n",
    "\n",
    "    # baseline 差分（生理特徴量のみ）\n",
    "    g_features = g[feature_cols_all].astype(float) - base_vals.values\n",
    "    if g_features.isna().any().any():\n",
    "        bad = g_features.columns[g_features.isna().any()].tolist()\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline差分後にNaN -> {bad}\")\n",
    "\n",
    "    # 出力用に整形\n",
    "    g_out = pd.concat([g[[\"subject_id\",\"bin_start\",\"bin_end\",\"FMS\"]], g_features], axis=1)\n",
    "    g_out = g_out.rename(columns={\"bin_start\":\"epoch_start\",\"bin_end\":\"epoch_end\"})\n",
    "    g_out[\"label\"] = binarize_fms(g_out[\"FMS\"])\n",
    "    g_out = g_out[[\"subject_id\",\"epoch_start\",\"epoch_end\",\"FMS\",\"label\"] + feature_cols_all]\n",
    "\n",
    "    # ★ デバッグ(2): この被験者のベースライン差分後データをCSVに保存\n",
    "    debug_path = os.path.join(DEBUG_DIR, f\"Cell1_debug_{sid}_E{EPOCH_LEN}s.csv\")\n",
    "    g_out.to_csv(debug_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell1-DEBUG] Saved baseline-diff data for subject {sid} -> {debug_path}\")\n",
    "\n",
    "    df_out_list.append(g_out)\n",
    "\n",
    "df_ml_epoch = pd.concat(df_out_list, ignore_index=True)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ③ SUBJECT_META & MSSQ / VIMSSQ group（被験者属性読み込み）\n",
    "# --------------------------------------------\n",
    "CANDIDATE_SCORE_PATHS = [\n",
    "    \"/mnt/data/summary_scores.xlsx\",\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"機械学習\", \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_INPUT_DIR, \"summary_scores.xlsx\"),\n",
    "]\n",
    "score_path = next((p for p in CANDIDATE_SCORE_PATHS if os.path.exists(p)), None)\n",
    "if score_path is None:\n",
    "    raise FileNotFoundError(\"[Cell1] summary_scores.xlsx が見つかりません。\")\n",
    "meta_raw = pd.read_excel(score_path, sheet_name=\"Summary\")\n",
    "\n",
    "required = [\"ID\", \"MSSQ\", \"VIMSSQ\"]\n",
    "missing = [c for c in required if c not in meta_raw.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"[Cell1] summary_scores.xlsx に必須列がありません -> {missing}\")\n",
    "\n",
    "meta = meta_raw[required].copy()\n",
    "meta[\"ID\"] = (\n",
    "    meta[\"ID\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "for c in [\"MSSQ\", \"VIMSSQ\"]:\n",
    "    meta[c] = pd.to_numeric(meta[c], errors=\"raise\")\n",
    "\n",
    "sid_set = set(map(str, SUBJECT_IDS))\n",
    "meta = meta[meta[\"ID\"].isin(sid_set)].copy()\n",
    "if meta[\"ID\"].duplicated().any():\n",
    "    raise ValueError(f\"[Cell1] ID 重複 -> {meta.loc[meta['ID'].duplicated(), 'ID'].tolist()}\")\n",
    "\n",
    "# ★ ここを拡張：MSSQ_group と VIMSSQ_group を両方作る\n",
    "meta[\"MSSQ_group\"]   = np.where(meta[\"MSSQ\"]   >= MSSQ_THRESHOLD_FIXED,   \"High\", \"Low\")\n",
    "meta[\"VIMSSQ_group\"] = np.where(meta[\"VIMSSQ\"] >= VIMSSQ_THRESHOLD_FIXED, \"High\", \"Low\")\n",
    "\n",
    "SUBJECT_META = (\n",
    "    meta.rename(columns={\"ID\": \"subject_id\"})\n",
    "        .set_index(\"subject_id\")[[\"MSSQ\", \"VIMSSQ\", \"MSSQ_group\", \"VIMSSQ_group\"]]\n",
    "        .copy()\n",
    ")\n",
    "\n",
    "SUBJECT_META.to_csv(outpath(\"subject_meta.csv\"), encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell1] SUBJECT_META saved -> {outpath('subject_meta.csv')} (source='{score_path}')\")\n",
    "\n",
    "# ---- ここから：MSSQ / VIMSSQ をフラグに応じて特徴量に追加 ----\n",
    "trait_cols_to_use = []\n",
    "if USE_MSSQ_FEATURE:\n",
    "    trait_cols_to_use.append(\"MSSQ\")\n",
    "if USE_VIMSSQ_FEATURE:\n",
    "    trait_cols_to_use.append(\"VIMSSQ\")\n",
    "\n",
    "if trait_cols_to_use:\n",
    "    merge_cols = [\"subject_id\"] + trait_cols_to_use\n",
    "    df_ml_epoch = df_ml_epoch.merge(\n",
    "        SUBJECT_META.reset_index()[merge_cols],\n",
    "        on=\"subject_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    if df_ml_epoch[trait_cols_to_use].isna().any().any():\n",
    "        bad_sids = df_ml_epoch.loc[\n",
    "            df_ml_epoch[trait_cols_to_use].isna().any(axis=1), \"subject_id\"\n",
    "        ].unique().tolist()\n",
    "        raise ValueError(f\"[Cell1] MSSQ/VIMSSQ が欠損の subject_id があります -> {bad_sids}\")\n",
    "\n",
    "    print(\"[Cell1] df_ml_epoch with trait features:\")\n",
    "    print(df_ml_epoch[[\"subject_id\"] + trait_cols_to_use].drop_duplicates().head())\n",
    "else:\n",
    "    print(\"[Cell1] Trait features (MSSQ/VIMSSQ) are disabled by flags.\")\n",
    "\n",
    "# 生理特徴量 + （オプションの）MSSQ/VIMSSQ をまとめた最終的な特徴量リスト\n",
    "feature_cols_full = feature_cols_all + trait_cols_to_use\n",
    "\n",
    "print(f\"[Cell1] Final feature columns (n={len(feature_cols_full)}):\")\n",
    "for col in feature_cols_full:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "\n",
    "# ---- ここから：MSSQ / VIMSSQ をフラグに応じて特徴量に追加 ----\n",
    "trait_cols_to_use = []\n",
    "if USE_MSSQ_FEATURE:\n",
    "    trait_cols_to_use.append(\"MSSQ\")\n",
    "if USE_VIMSSQ_FEATURE:\n",
    "    trait_cols_to_use.append(\"VIMSSQ\")\n",
    "\n",
    "if trait_cols_to_use:\n",
    "    # 必要な列だけを df_ml_epoch にマージ\n",
    "    merge_cols = [\"subject_id\"] + trait_cols_to_use\n",
    "    df_ml_epoch = df_ml_epoch.merge(\n",
    "        SUBJECT_META.reset_index()[merge_cols],\n",
    "        on=\"subject_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    if df_ml_epoch[trait_cols_to_use].isna().any().any():\n",
    "        bad_sids = df_ml_epoch.loc[\n",
    "            df_ml_epoch[trait_cols_to_use].isna().any(axis=1), \"subject_id\"\n",
    "        ].unique().tolist()\n",
    "        raise ValueError(f\"[Cell1] MSSQ/VIMSSQ が欠損の subject_id があります -> {bad_sids}\")\n",
    "\n",
    "    print(\"[Cell1] df_ml_epoch with trait features:\")\n",
    "    print(df_ml_epoch[[\"subject_id\"] + trait_cols_to_use].drop_duplicates().head())\n",
    "else:\n",
    "    print(\"[Cell1] Trait features (MSSQ/VIMSSQ) are disabled by flags.\")\n",
    "\n",
    "# 生理特徴量 + （オプションの）MSSQ/VIMSSQ をまとめた最終的な特徴量リスト\n",
    "feature_cols_full = feature_cols_all + trait_cols_to_use\n",
    "\n",
    "# デバッグ: 最終的な特徴量リストをプリント\n",
    "print(f\"[Cell1] Final feature columns (n={len(feature_cols_full)}):\")\n",
    "for col in feature_cols_full:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ④ 学習行列＆行列保存\n",
    "# --------------------------------------------\n",
    "fname_raw = f\"ML_DATA_DELTA_{EPOCH_LEN}S_RAW.CSV\"\n",
    "df_ml_epoch.to_csv(outpath(fname_raw), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "X_all = df_ml_epoch[feature_cols_full].copy().astype(float)\n",
    "y_all = df_ml_epoch[\"label\"].copy().astype(int)\n",
    "groups = df_ml_epoch[\"subject_id\"].copy()\n",
    "\n",
    "X_all.to_csv(outpath(f\"X_RAW_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "X_all.to_csv(outpath(f\"X_SCALED_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")  # 木系でスケーリング不要\n",
    "pd.DataFrame({\"subject_id\": groups, \"label\": y_all, \"FMS_mean\": df_ml_epoch[\"FMS\"]}).to_csv(\n",
    "    outpath(f\"Y_AND_GROUPS_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(f\"[Cell1] Saved -> {outpath(fname_raw)} / X_RAW_ALL / X_SCALED_ALL / Y_AND_GROUPS\")\n",
    "print(f\"[Cell1] Matrices ready: X_all={X_all.shape}, y_all={y_all.shape}, SUBJECT_META={SUBJECT_META.shape}\")\n",
    "print(f\"[Cell1] n_features(physio)={len(feature_cols_all)}, \"\n",
    "      f\"+ traits({len(trait_cols_to_use)}) -> {len(feature_cols_full)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1A: MSSQ / VIMSSQ 群別の FMS 推移プロット =====\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 出力ディレクトリ\n",
    "FMS_PLOT_DIR = os.path.join(OUT_DIR, \"Cell1A_FMS_trajectory\")\n",
    "os.makedirs(FMS_PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# 描画スタイル（ユーザー規約）\n",
    "LW = 1.5\n",
    "FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "\n",
    "COLOR_HIGH = \"red\"\n",
    "COLOR_LOW  = \"blue\"\n",
    "\n",
    "\n",
    "def _prepare_fms_long_use_epoch_end():\n",
    "    \"\"\"\n",
    "    df_ml_epoch から FMS 時系列を取り出し、\n",
    "    ML_START からの経過時間（分）を epoch_end 基準で付与した長データを返す。\n",
    "    \"\"\"\n",
    "    required_cols = {\"subject_id\", \"epoch_end\", \"FMS\"}\n",
    "    missing = required_cols - set(df_ml_epoch.columns)\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell1A] df_ml_epoch に必須列がありません -> {missing}\")\n",
    "\n",
    "    df = df_ml_epoch[[\"subject_id\", \"epoch_end\", \"FMS\"]].copy()\n",
    "\n",
    "    # ML_START からの経過時間（秒 → 分）を epoch_end 基準で計算\n",
    "    df[\"t_min\"] = (df[\"epoch_end\"] - ML_START).astype(float) / 60.0\n",
    "    return df\n",
    "\n",
    "\n",
    "def _plot_fms_by_group_min_axis(group_col: str, title_prefix: str, save_name: str):\n",
    "    \"\"\"\n",
    "    group_col: \"MSSQ_group\" または \"VIMSSQ_group\"\n",
    "    title_prefix: 図タイトルのプレフィックス（\"MSSQ group\" など）\n",
    "    save_name: 保存ファイル名\n",
    "    \"\"\"\n",
    "    if group_col not in SUBJECT_META.columns:\n",
    "        raise KeyError(\n",
    "            f\"[Cell1A] SUBJECT_META に {group_col} 列がありません。\"\n",
    "            \"Cell1 の SUBJECT_META 作成部を確認してください。\"\n",
    "        )\n",
    "\n",
    "    # 群に属する被験者 ID をプリント\n",
    "    for level in [\"High\", \"Low\"]:\n",
    "        sids = SUBJECT_META.index[SUBJECT_META[group_col] == level].tolist()\n",
    "        print(f\"[Cell1A] {group_col} = {level}: subjects = {sorted(sids)}\")\n",
    "\n",
    "    # FMS 長データに group_col をマージ（epoch_end 基準）\n",
    "    df_long = _prepare_fms_long_use_epoch_end()\n",
    "    df_long = df_long.merge(\n",
    "        SUBJECT_META.reset_index()[[\"subject_id\", group_col]],\n",
    "        on=\"subject_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    if df_long[group_col].isna().any():\n",
    "        bad = df_long.loc[df_long[group_col].isna(), \"subject_id\"].unique().tolist()\n",
    "        raise ValueError(f\"[Cell1A] {group_col} が欠損の subject_id があります -> {bad}\")\n",
    "\n",
    "    # group × t_min ごとに FMS の平均・標準偏差\n",
    "    agg = (\n",
    "        df_long\n",
    "        .groupby([group_col, \"t_min\"])[\"FMS\"]\n",
    "        .agg([\"mean\", \"std\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "    agg[\"std\"] = agg[\"std\"].fillna(0.0)\n",
    "\n",
    "    # プロット\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for level, color in [(\"High\", COLOR_HIGH), (\"Low\", COLOR_LOW)]:\n",
    "        sub = agg[agg[group_col] == level].sort_values(\"t_min\")\n",
    "        if sub.empty:\n",
    "            print(f\"[Cell1A] 注意: {group_col}={level} のデータがありません。\")\n",
    "            continue\n",
    "\n",
    "        t = sub[\"t_min\"].values  # [分] 単位\n",
    "        m = sub[\"mean\"].values\n",
    "        s = sub[\"std\"].values\n",
    "\n",
    "        # 平均（太線）\n",
    "        ax.plot(t, m, label=f\"{level} (mean)\", linewidth=LW * 2.0, color=color)\n",
    "        # ±1 SD バンド\n",
    "        ax.fill_between(t, m - s, m + s, alpha=0.2, color=color, linewidth=0)\n",
    "\n",
    "    # ----- 軸設定 -----\n",
    "\n",
    "    # 横軸：0〜10分（または ML 窓長に応じて自動）\n",
    "    duration_min = (ML_END - ML_START) / 60.0\n",
    "    # 安全側で 0〜duration_min、整数目盛（0,1,2,...）\n",
    "    max_tick = int(np.floor(duration_min))\n",
    "    xticks = np.arange(0, max_tick + 1, 1)\n",
    "    ax.set_xlim(0.0, duration_min)\n",
    "    ax.set_xticks(xticks)\n",
    "    # ラベルは整数表示（0,1,2,...）\n",
    "    ax.set_xlabel(\"Time [min]\", fontsize=FS_LABEL)\n",
    "\n",
    "    # 縦軸：FMS 0〜4、整数目盛\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_yticks([0, 1, 2, 3, 4])\n",
    "    ax.set_ylabel(\"FMS\", fontsize=FS_LABEL)\n",
    "\n",
    "    # タイトル\n",
    "    ax.set_title(f\"{title_prefix}別 FMS 推移\", fontsize=FS_TITLE)\n",
    "\n",
    "    # グリッド\n",
    "    ax.grid(True)\n",
    "\n",
    "    # 3分 と 6分30秒 に縦の点線\n",
    "    ax.axvline(3.0,  linestyle=\"--\", linewidth=LW, color=\"gray\")\n",
    "    ax.axvline(6.5,  linestyle=\"--\", linewidth=LW, color=\"gray\")\n",
    "\n",
    "    # 目盛フォント\n",
    "    ax.tick_params(axis=\"both\", labelsize=FS_TICK)\n",
    "\n",
    "    # 凡例\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_path = os.path.join(FMS_PLOT_DIR, save_name)\n",
    "    fig.savefig(save_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[Cell1A] Saved FMS trajectory plot -> {save_path}\")\n",
    "\n",
    "\n",
    "# ---- 基準切り替え：MSSQ / VIMSSQ ----\n",
    "basis = GROUPING_BASIS_FOR_PLOTS.upper()\n",
    "\n",
    "if basis == \"MSSQ\":\n",
    "    group_col    = \"MSSQ_group\"\n",
    "    title_prefix = \"MSSQ group\"\n",
    "    save_name    = f\"FMS_MSSQ_group_E{EPOCH_LEN}s.png\"\n",
    "\n",
    "elif basis == \"VIMSSQ\":\n",
    "    group_col    = \"VIMSSQ_group\"\n",
    "    title_prefix = \"VIMSSQ group\"\n",
    "    save_name    = f\"FMS_VIMSSQ_group_E{EPOCH_LEN}s.png\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"[Cell1A] GROUPING_BASIS_FOR_PLOTS は 'MSSQ' か 'VIMSSQ' を指定してください \"\n",
    "        f\"（現在: {GROUPING_BASIS_FOR_PLOTS}）\"\n",
    "    )\n",
    "\n",
    "print(f\"[Cell1A] GROUPING_BASIS_FOR_PLOTS = {GROUPING_BASIS_FOR_PLOTS} で FMS 推移を描画します。\")\n",
    "\n",
    "_plot_fms_by_group_min_axis(\n",
    "    group_col=group_col,\n",
    "    title_prefix=title_prefix,\n",
    "    save_name=save_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45007fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2: モデリング共通ヘルパ（fit / SHAP / 評価）=====\n",
    "\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 学習ラッパー（Cell0のレジストリAPIを利用）\n",
    "# --------------------------------------------\n",
    "def fit_classifier(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cell0 の fit_estimator を直接包む薄いラッパ。\n",
    "    - SHAP/評価セルから backend を差し替えたい場合のみ backend / overrides を指定する。\n",
    "    \"\"\"\n",
    "    if \"fit_estimator\" not in globals():\n",
    "        raise RuntimeError(\"[Cell2] fit_estimator が未定義です。Cell0 を先に実行してください。\")\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    return fit_estimator(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        backend=backend,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# TreeSHAP ベースの特徴重要度算出\n",
    "# --------------------------------------------\n",
    "def compute_train_shap_abs_mean(model, X_ref: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    学習データ X_ref 上での平均絶対SHAP値（降順）。\n",
    "    - XGB/RF 等の木モデルを想定（TreeSHAP）。\n",
    "    - SVM など非対応モデルでは ValueError を送出する。\n",
    "    \"\"\"\n",
    "    X_ref = X_ref.astype(np.float32, copy=False)\n",
    "\n",
    "    # 背景データ（最大128行）\n",
    "    bg_n = min(128, len(X_ref))\n",
    "    X_bg = X_ref.sample(n=bg_n, random_state=SEED_BASE) if bg_n >= 2 else X_ref\n",
    "\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            data=X_bg,\n",
    "            model_output=\"probability\",\n",
    "            feature_perturbation=\"interventional\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "    except Exception:\n",
    "        # probability指定が非対応な場合に raw へフォールバック\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            model_output=\"raw\",\n",
    "            feature_perturbation=\"tree_path_dependent\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "\n",
    "    # shap_values の戻り値形状を統一（2D: n_samples × n_features）\n",
    "    classes = getattr(model, \"classes_\", None)\n",
    "    pos_idx = int(np.where(classes == 1)[0][0]) if classes is not None and 1 in list(classes) else -1\n",
    "\n",
    "    if isinstance(sv_any, list):\n",
    "        sv = sv_any[pos_idx]\n",
    "    else:\n",
    "        sv = getattr(sv_any, \"values\", sv_any)\n",
    "        sv = np.asarray(sv)\n",
    "        if sv.ndim == 3:\n",
    "            sv = sv[..., pos_idx]\n",
    "        elif sv.ndim == 1:\n",
    "            sv = sv.reshape(-1, 1)\n",
    "\n",
    "    if sv.shape[1] != X_ref.shape[1]:\n",
    "        raise RuntimeError(\n",
    "            f\"[Cell2] SHAP shape mismatch: sv.shape={sv.shape}, X_ref.shape={X_ref.shape}\"\n",
    "        )\n",
    "\n",
    "    abs_mean = np.mean(np.abs(sv), axis=0)\n",
    "    return pd.Series(abs_mean, index=X_ref.columns, name=\"mean_abs\").sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 評価ユーティリティ\n",
    "# --------------------------------------------\n",
    "def _is_probability_like(scores: np.ndarray) -> bool:\n",
    "    return np.isfinite(scores).all() and 0.0 <= scores.min() and scores.max() <= 1.0\n",
    "\n",
    "\n",
    "def evaluate_fold(model, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - ROC AUC: 2クラス時のみ。\n",
    "    - Accuracy: 確率なら 0.5、スコアなら 0.0 を閾値とする（詳細な最適化は別セル）。\n",
    "    \"\"\"\n",
    "    X_test = X_test.astype(np.float32, copy=False)\n",
    "    scores = predict_positive_score(model, X_test)\n",
    "\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, scores)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "\n",
    "    thr = 0.5 if _is_probability_like(scores) else 0.0\n",
    "    pred = (scores >= thr).astype(int)\n",
    "    acc = accuracy_score(y_test.astype(int), pred)\n",
    "\n",
    "    return {\"roc_auc\": float(roc_auc), \"accuracy\": float(acc)}\n",
    "\n",
    "\n",
    "print(\"[Cell2] Modeling helpers ready (fit_classifier / compute_train_shap_abs_mean / evaluate_fold)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6751dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-pre: 高相関特徴の事前除去 =====\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "required = [\"X_all\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-pre] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "CORR_THRESHOLD = 0.70\n",
    "MIN_VARIANCE = 1e-8\n",
    "FEATURE_LIST_PATH = outpath(\"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "X_num = X_all.select_dtypes(include=[np.number]).copy()\n",
    "if X_num.empty:\n",
    "    raise RuntimeError(\"[Cell3A-pre] 数値列がありません。\")\n",
    "\n",
    "var = X_num.var(axis=0, ddof=1).fillna(0.0)\n",
    "valid_cols = var[var > MIN_VARIANCE].index.tolist()\n",
    "if not valid_cols:\n",
    "    raise RuntimeError(\"[Cell3A-pre] 分散がほぼゼロのため使用可能な列がありません。\")\n",
    "\n",
    "# 優先順は X_all の列順を維持（先に出てきた方を優先して残す）\n",
    "priority = [c for c in X_all.columns if c in valid_cols]\n",
    "X_use = X_num[priority]\n",
    "\n",
    "corr = X_use.corr(method=\"pearson\").abs()\n",
    "keep = []\n",
    "dropped = []\n",
    "\n",
    "for col in priority:\n",
    "    conflict = None\n",
    "    for kept in keep:\n",
    "        if corr.loc[col, kept] >= CORR_THRESHOLD:\n",
    "            conflict = kept\n",
    "            break\n",
    "    if conflict is None:\n",
    "        # まだどの kept とも高相関でない → 代表として残す\n",
    "        keep.append(col)\n",
    "    else:\n",
    "        # すでに keep に入っている代表 (conflict) を残し、後から出てきた col を除去\n",
    "        dropped.append({\n",
    "            \"feature\": col,           # ← 今回「除去した側」\n",
    "            \"representative\": conflict,  # ← 残した側（代表）\n",
    "            \"abs_corr\": float(corr.loc[col, conflict]),\n",
    "        })\n",
    "\n",
    "payload = {\n",
    "    \"keep\": keep,\n",
    "    \"dropped\": dropped,\n",
    "    \"threshold\": CORR_THRESHOLD,\n",
    "    \"total_columns\": len(priority),\n",
    "}\n",
    "with open(FEATURE_LIST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[Cell3A-pre] keep={len(keep)} / total={len(priority)}, dropped={len(dropped)}\")\n",
    "print(f\"[Cell3A-pre] JSON -> {FEATURE_LIST_PATH}\")\n",
    "\n",
    "# ---- ログ出力：どっちを KEEP, どっちを DROP したか ----\n",
    "if dropped:\n",
    "    print(f\"[Cell3A-pre] |r| >= {CORR_THRESHOLD:.2f} のペア（後から出てきた方を DROP）：\")\n",
    "    for d in dropped:\n",
    "        print(\n",
    "            f\"  [KEEP] {d['representative']}  \"\n",
    "            f\"[DROP] {d['feature']}  \"\n",
    "            f\"(abs_corr={d['abs_corr']:.3f})\"\n",
    "        )\n",
    "else:\n",
    "    print(\"[Cell3A-pre] 高相関による除去はありませんでした。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a061c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A: SHAPベース順位付け（LOSO, TreeSHAP） =====\n",
    "RUN_CELL_3A_SHAP = True  # False にするとこのセルは実質スキップ\n",
    "\n",
    "if RUN_CELL_3A_SHAP:\n",
    "\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.backends\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    required = [\n",
    "        \"X_all\", \"y_all\", \"groups\",\n",
    "        \"fit_classifier\", \"evaluate_fold\",\n",
    "        \"outpath\", \"compute_train_shap_abs_mean\"\n",
    "    ]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-SHAP-RANK] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    # RFE_BACKEND に相当（TreeSHAP 対応の木モデル）\n",
    "    SHAP_BACKEND = \"xgb\"\n",
    "\n",
    "    RFE_STEP = 1          # 互換性のため残すが未使用\n",
    "    RFE_MIN_FEATURES = 1  # 互換性のため残すが未使用\n",
    "    FEATURE_LIST_PATH = outpath(\"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "    # --- 特徴量プールの決定（相関事前除去の結果があれば利用） ---\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        feature_pool = [c for c in keep_payload.get(\"keep\", []) if c in X_all.columns]\n",
    "        print(f\"[Cell3A-SHAP-RANK] correlation-pruned features loaded ({len(feature_pool)} cols)\")\n",
    "    else:\n",
    "        feature_pool = list(X_all.columns)\n",
    "        print(\"[Cell3A-SHAP-RANK] correlation-pruned list not found. Using all columns.\")\n",
    "\n",
    "    if not feature_pool:\n",
    "        raise RuntimeError(\"[Cell3A-SHAP-RANK] feature_pool が空です。Cell3A-pre の結果を確認してください。\")\n",
    "\n",
    "    X_source = X_all[feature_pool].copy()\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    ranking_frames = []\n",
    "    metrics_rows = []\n",
    "\n",
    "    # --- LOSO ループ ---\n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_source, y_all, groups), start=1):\n",
    "        X_tr = X_source.iloc[tr_idx].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_source.iloc[te_idx].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(f\"[Cell3A-SHAP-RANK] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "        # --- 木モデルで学習（TreeSHAP 対応） ---\n",
    "        model = fit_classifier(X_tr, y_tr, backend=SHAP_BACKEND)\n",
    "\n",
    "        # --- TreeSHAP による mean(|SHAP|) を学習データで計算 ---\n",
    "        shap_mean = compute_train_shap_abs_mean(model, X_tr)  # index: feature, values: mean_abs（降順ソート済み）\n",
    "\n",
    "        # 元の列順で揃えておく（fold 間でインデックス整合のため）\n",
    "        shap_mean = shap_mean.reindex(X_tr.columns)\n",
    "\n",
    "        # --- Fold 内で 1,2,3,... の順位化（大きいほど重要 → rank=1 が最重要） ---\n",
    "        ranks = shap_mean.rank(ascending=False, method=\"min\").astype(int)\n",
    "        ranks.name = f\"fold{fold_id}\"\n",
    "        ranking_frames.append(ranks)\n",
    "\n",
    "        # --- Fold ごとの性能評価 ---\n",
    "        metrics = evaluate_fold(model, X_te, y_te)\n",
    "        metrics.update({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "        })\n",
    "        metrics_rows.append(metrics)\n",
    "\n",
    "        preview = shap_mean.sort_values(ascending=False).head(5).index.tolist()\n",
    "        print(f\"[Cell3A-SHAP-RANK] fold{fold_id}: ranked {X_tr.shape[1]} features (top5={preview})\")\n",
    "\n",
    "    # --- fold 間の順位集約 ---\n",
    "    rfe_rank = pd.concat(ranking_frames, axis=1)  # index: feature, columns: fold1, fold2, ...\n",
    "\n",
    "    # 平均順位・中央値順位（小さいほど重要）\n",
    "    rfe_rank[\"rank_mean\"] = rfe_rank.mean(axis=1)\n",
    "    rfe_rank[\"rank_median\"] = rfe_rank.median(axis=1)\n",
    "\n",
    "    # 平均順位が小さい順にソート\n",
    "    rfe_rank = rfe_rank.sort_values(\"rank_mean\")\n",
    "\n",
    "    rank_path = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "    rfe_rank.to_csv(rank_path, encoding=\"utf-8-sig\")\n",
    "    rfe_rank.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "    pd.DataFrame(metrics_rows).to_csv(outpath(\"LOSO_METRICS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] Saved ranking -> {rank_path}\")\n",
    "\n",
    "    # --- 可視化（全特徴）---\n",
    "    plt.figure(figsize=(10, max(5, len(rfe_rank)//3)))\n",
    "    plt.barh(rfe_rank.index[::-1], rfe_rank[\"rank_mean\"][::-1])\n",
    "    plt.xlabel(\"Average rank (lower=better)\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(\"SHAP-based Feature Ranking (All)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_RANKING_ALL.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # --- 上位 TOP_K のみ ---\n",
    "    TOP_K = 8\n",
    "    topk = rfe_rank.head(TOP_K).iloc[::-1]\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = plt.gca()\n",
    "    ax.barh(topk.index, topk[\"rank_mean\"])\n",
    "    ax.set_xlabel(\"Average rank (lower=better)\")\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "    ax.set_title(f\"Top-{TOP_K} SHAP-based Feature Ranking\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] 図を保存 -> {outpath('SHAP_TOP8_RANKING.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A: RFE特徴量ランキング（LOSO） =====\n",
    "\n",
    "RUN_CELL_3A_RFE = True  # False にするとこのセルは実質スキップ\n",
    "\n",
    "if RUN_CELL_3A_RFE:\n",
    "\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    import matplotlib\n",
    "    import matplotlib.backends\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "    required = [\"X_all\", \"y_all\", \"groups\", \"build_estimator\", \"fit_classifier\", \"evaluate_fold\", \"outpath\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-RFE] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    RFE_BACKEND = \"xgb\"   # RFE では XGB 固定\n",
    "    RFE_STEP = 1          # 1 本ずつ削除\n",
    "    RFE_MIN_FEATURES = 1  # 最低残す特徴数\n",
    "    FEATURE_LIST_PATH = outpath(\"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        feature_pool = [c for c in keep_payload.get(\"keep\", []) if c in X_all.columns]\n",
    "        print(f\"[Cell3A-RFE] correlation-pruned features loaded ({len(feature_pool)} cols)\")\n",
    "    else:\n",
    "        feature_pool = list(X_all.columns)\n",
    "        print(\"[Cell3A-RFE] correlation-pruned list not found. Using all columns.\")\n",
    "\n",
    "    if not feature_pool:\n",
    "        raise RuntimeError(\"[Cell3A-RFE] feature_pool が空です。Cell3A-pre の結果を確認してください。\")\n",
    "\n",
    "    X_source = X_all[feature_pool].copy()\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    ranking_frames = []\n",
    "    metrics_rows = []\n",
    "\n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_source, y_all, groups), start=1):\n",
    "        X_tr = X_source.iloc[tr_idx].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_source.iloc[te_idx].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(f\"[Cell3A-RFE] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "        base_estimator = build_estimator(backend=RFE_BACKEND)\n",
    "        selector = RFE(\n",
    "            estimator=base_estimator,\n",
    "            step=max(1, int(RFE_STEP)),\n",
    "            n_features_to_select=max(1, int(RFE_MIN_FEATURES)),\n",
    "        )\n",
    "        selector.fit(X_tr, y_tr)\n",
    "\n",
    "        ranks = pd.Series(selector.ranking_, index=X_tr.columns, name=f\"fold{fold_id}\")\n",
    "        ranking_frames.append(ranks)\n",
    "\n",
    "        selected_cols = list(X_tr.columns[selector.support_])\n",
    "        model = fit_classifier(X_tr[selected_cols], y_tr, backend=RFE_BACKEND)\n",
    "        metrics = evaluate_fold(model, X_te[selected_cols], y_te)\n",
    "        metrics.update({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "        })\n",
    "        metrics_rows.append(metrics)\n",
    "\n",
    "        preview = selected_cols[:5]\n",
    "        print(f\"[Cell3A-RFE] fold{fold_id}: selected {len(selected_cols)} features (preview={preview})\")\n",
    "\n",
    "    rfe_rank = pd.concat(ranking_frames, axis=1)\n",
    "    rfe_rank[\"rank_mean\"] = rfe_rank.mean(axis=1)\n",
    "    rfe_rank[\"rank_median\"] = rfe_rank.median(axis=1)\n",
    "    rfe_rank = rfe_rank.sort_values(\"rank_mean\")\n",
    "\n",
    "    rank_path = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "    rfe_rank.to_csv(rank_path, encoding=\"utf-8-sig\")\n",
    "    rfe_rank.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "    pd.DataFrame(metrics_rows).to_csv(outpath(\"LOSO_METRICS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-RFE] Saved ranking -> {rank_path}\")\n",
    "\n",
    "    plt.figure(figsize=(10, max(5, len(rfe_rank)//3)))\n",
    "    plt.barh(rfe_rank.index[::-1], rfe_rank[\"rank_mean\"][::-1])\n",
    "    plt.xlabel(\"Average rank (lower=better)\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(\"RFE Feature Ranking (All)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_RANKING_ALL.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    TOP_K = 8\n",
    "    topk = rfe_rank.head(TOP_K).iloc[::-1]\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = plt.gca()\n",
    "    ax.barh(topk.index, topk[\"rank_mean\"])\n",
    "    ax.set_xlabel(\"Average rank (lower=better)\")\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "    ax.set_title(f\"Top-{TOP_K} RFE Feature Ranking\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell3A-RFE] 図を保存 -> {outpath('SHAP_TOP8_RANKING.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-Subset: 全特徴の累積トップkでROC-AUC評価 =====\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "required = [\"X_all\", \"y_all\", \"groups\",\n",
    "            \"fit_classifier\", \"predict_positive_score\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-Subset] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "# 旧コードとの互換用：TOP_SUBSET_K があればそれを使い，なければ total_feats で上書き予定\n",
    "TOP_SUBSET_K = int(globals().get(\"TOP_SUBSET_K\", 0))\n",
    "\n",
    "# ---- SHAPランキングの読み込み ----\n",
    "rank_csv = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "if not os.path.exists(rank_csv):\n",
    "    raise FileNotFoundError(\"[Cell3A-Subset] SHAP_FEATURE_RANKING.CSV がありません。Cell3A を実行してください。\")\n",
    "\n",
    "rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "\n",
    "# rank_mean（小さいほど重要） > mean_abs（大きいほど重要） > index の優先順位で並べ替え\n",
    "if \"rank_mean\" in rank_df.columns:\n",
    "    feature_order = rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "elif \"mean_abs\" in rank_df.columns:\n",
    "    feature_order = rank_df.sort_values(\"mean_abs\", ascending=False).index.tolist()\n",
    "else:\n",
    "    feature_order = list(rank_df.index)\n",
    "\n",
    "# X_all に存在する列だけに制限（＝相関除去後＋実際にある列）\n",
    "feature_order = [f for f in feature_order if f in X_all.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[Cell3A-Subset] ランキングに該当する特徴が X_all に存在しません。\")\n",
    "\n",
    "# 相関除去後の「全特徴数」\n",
    "total_feats = len(feature_order)\n",
    "top_features = feature_order  # ここでは全部使う\n",
    "print(f\"[Cell3A-Subset] Using ALL correlation-pruned features ({total_feats}):\")\n",
    "print(top_features)\n",
    "\n",
    "# TOP_SUBSET_K が未設定 or 大きすぎる場合は、全特徴数に合わせる\n",
    "if TOP_SUBSET_K <= 0 or TOP_SUBSET_K > total_feats:\n",
    "    TOP_SUBSET_K = total_feats\n",
    "globals()[\"TOP_SUBSET_K\"] = TOP_SUBSET_K \n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "results = []\n",
    "\n",
    "# ---- 上位k個の累積セットで評価（k=1..total_feats）----\n",
    "for k in range(1, total_feats + 1):\n",
    "    feats = top_features[:k]\n",
    "    y_true_all = []\n",
    "    y_score_all = []\n",
    "\n",
    "    for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "        X_tr = X_all.iloc[tr_idx][feats].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_all.iloc[te_idx][feats].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        # 学習側が単一クラスならこのfoldはスキップ\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            continue\n",
    "\n",
    "        model = fit_classifier(X_tr, y_tr)\n",
    "        proba = predict_positive_score(model, X_te)\n",
    "\n",
    "        y_true_all.append(y_te)\n",
    "        y_score_all.append(proba)\n",
    "\n",
    "    if not y_true_all:\n",
    "        auc = float(\"nan\")\n",
    "    else:\n",
    "        y_true = np.concatenate(y_true_all)\n",
    "        y_score = np.concatenate(y_score_all)\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            auc = float(\"nan\")\n",
    "        else:\n",
    "            auc = float(roc_auc_score(y_true, y_score))\n",
    "\n",
    "    results.append({\n",
    "        \"size\": k,              # 使用した特徴量数（k）\n",
    "        \"features\": feats,      # 使った特徴量のリスト（上位k個）\n",
    "        \"auc\": auc,\n",
    "    })\n",
    "    print(f\"[Cell3A-Subset] k={k}/{total_feats}, AUC={auc:.4f}, features={feats}\")\n",
    "\n",
    "# ---- 結果の整形・保存 ----\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(\"size\")  # k昇順\n",
    "\n",
    "results_df[\"features_str\"] = results_df[\"features\"].apply(lambda lst: \",\".join(lst))\n",
    "\n",
    "# 1) このセル独自のファイル（ALLK_*）\n",
    "subset_csv_name_allk = \"ALLK_TOPORDER_AUC.csv\"\n",
    "subset_path_allk = outpath(subset_csv_name_allk)\n",
    "results_df[[\"size\", \"features_str\", \"auc\"]].to_csv(subset_path_allk, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell3A-Subset] 保存 (ALLK) -> {subset_path_allk}\")\n",
    "\n",
    "# 最良のk（AUC最大、同点なら小さいk）を取得\n",
    "best_row = results_df.sort_values([\"auc\", \"size\"], ascending=[False, True]).iloc[0]\n",
    "print(f\"[Cell3A-Subset] best k={int(best_row['size'])}, auc={best_row['auc']:.4f}, features={best_row['features']}\")\n",
    "\n",
    "# 2) このセル独自の JSON（ALLK_SUBSET_BEST.json）\n",
    "best_json_name_allk = \"ALLK_SUBSET_BEST.json\"\n",
    "with open(outpath(best_json_name_allk), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"size\": int(best_row[\"size\"]),\n",
    "        \"auc\": float(best_row[\"auc\"]),\n",
    "        \"features\": best_row[\"features\"],\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[Cell3A-Subset] BEST (ALLK) -> {outpath(best_json_name_allk)}\")\n",
    "\n",
    "# 3) 旧「組合せ探索版」と同じ名前・形式でも保存（互換用）\n",
    "#    - TOP{TOP_SUBSET_K}_SUBSET_AUC.csv\n",
    "#    - TOP{TOP_SUBSET_K}_SUBSET_BEST.json\n",
    "subset_csv_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_AUC.csv\"\n",
    "subset_path_compat = outpath(subset_csv_name_compat)\n",
    "results_df[[\"size\", \"features_str\", \"auc\"]].to_csv(subset_path_compat, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell3A-Subset] 互換CSV -> {subset_path_compat}\")\n",
    "\n",
    "best_json_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST.json\"\n",
    "with open(outpath(best_json_name_compat), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"size\": int(best_row[\"size\"]),\n",
    "        \"auc\": float(best_row[\"auc\"]),\n",
    "        \"features\": best_row[\"features\"],\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[Cell3A-Subset] 互換BEST JSON -> {outpath(best_json_name_compat)}\")\n",
    "\n",
    "# グローバル変数も旧仕様に合わせて更新\n",
    "globals()[\"BEST_SUBSET_FEATURES\"] = best_row[\"features\"]\n",
    "globals()[\"BEST_SUBSET_K\"] = len(best_row[\"features\"])\n",
    "\n",
    "# ---- グラフ描画：横軸=特徴量数k（右ほど少ない）, 縦軸=ROC-AUC (0.5〜1.0) ----\n",
    "FS_TITLE, FS_LABEL, FS_TICK = 30, 24, 20\n",
    "LW = 1.5\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(results_df[\"size\"], results_df[\"auc\"], marker=\"o\", linewidth=LW)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(\"Number of features (k)\", fontsize=FS_LABEL)\n",
    "ax.set_ylabel(\"ROC-AUC (pooled LOSO)\", fontsize=FS_LABEL)\n",
    "ax.set_title(\"ROC-AUC vs. number of features (all correlation-pruned features)\", fontsize=FS_TITLE)\n",
    "\n",
    "# 横軸：左が k = total_feats（全部）、右が k = 1（1特徴だけ）\n",
    "ax.set_xlim(total_feats, 1)\n",
    "ax.set_xticks(range(1, total_feats + 1))\n",
    "\n",
    "# 縦軸の範囲を 0.5〜1 に固定\n",
    "ax.set_ylim(0, 1.0)\n",
    "\n",
    "ax.tick_params(axis=\"x\", labelsize=FS_TICK)\n",
    "ax.tick_params(axis=\"y\", labelsize=FS_TICK)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"ALLK_TOPORDER_AUC.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"[Cell3A-Subset] 図を保存 -> {outpath('ALLK_TOPORDER_AUC.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ===== Cell 3A-Subset-dummy: 全特徴を TOP{TOP_SUBSET_K}_SUBSET_BEST.json に書き込むだけ =====\n",
    "RUN_CELL_3A_ALL_FEATURE= True\n",
    "\n",
    "\n",
    "if RUN_CELL_3A_ALL_FEATURE:\n",
    "\n",
    "    RUN_CELL_3A_SHAP\n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    required = [\"X_all\", \"outpath\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-Subset-dummy] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    # ---- SHAPランキングの読み込み ----\n",
    "    rank_csv = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "    if not os.path.exists(rank_csv):\n",
    "        raise FileNotFoundError(\"[Cell3A-Subset-dummy] SHAP_FEATURE_RANKING.CSV がありません。Cell3A を実行してください。\")\n",
    "\n",
    "    rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "\n",
    "    # rank_mean（小さいほど重要） > mean_abs（大きいほど重要） > index の優先順位で並べ替え\n",
    "    if \"rank_mean\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "    elif \"mean_abs\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"mean_abs\", ascending=False).index.tolist()\n",
    "    else:\n",
    "        feature_order = list(rank_df.index)\n",
    "\n",
    "    # X_all に存在する列だけに制限（＝相関除去後＋実際にある列）\n",
    "    feature_order = [f for f in feature_order if f in X_all.columns]\n",
    "    if not feature_order:\n",
    "        raise RuntimeError(\"[Cell3A-Subset-dummy] ランキングに該当する特徴が X_all に存在しません。\")\n",
    "\n",
    "    total_feats = len(feature_order)\n",
    "    top_features = feature_order  # ★ 全特徴そのまま\n",
    "\n",
    "    print(f\"[Cell3A-Subset-dummy] Using ALL correlation-pruned features ({total_feats}):\")\n",
    "    print(top_features)\n",
    "\n",
    "    # TOP_SUBSET_K を「全特徴数」に強制セット\n",
    "    TOP_SUBSET_K = total_feats\n",
    "    globals()[\"TOP_SUBSET_K\"] = TOP_SUBSET_K\n",
    "\n",
    "    # ---- JSONを書き出し（構造は既存セルと互換）----\n",
    "    best_json_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST.json\"\n",
    "    best_json_path = outpath(best_json_name_compat)\n",
    "\n",
    "    payload = {\n",
    "        \"size\": int(TOP_SUBSET_K),\n",
    "        # AUC はここでは計算しないので NaN または None で埋める\n",
    "        # 既存コードと同じく float('nan') を使っておく\n",
    "        \"auc\": float(\"nan\"),\n",
    "        \"features\": top_features,\n",
    "    }\n",
    "\n",
    "    with open(best_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[Cell3A-Subset-dummy] 全特徴を書き込み -> {best_json_path}\")\n",
    "\n",
    "    # 互換用にグローバル変数も更新\n",
    "    globals()[\"BEST_SUBSET_FEATURES\"] = top_features\n",
    "    globals()[\"BEST_SUBSET_K\"] = TOP_SUBSET_K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ===== Cell 3A-Subset: Top-k 組合せ探索 =====\n",
    "\n",
    "RUN_CELL_3A_Topk_FEATURE=True\n",
    "\n",
    "if RUN_CELL_3A_Topk_FEATURE:\n",
    "    import os\n",
    "    from itertools import combinations\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "    required = [\"X_all\", \"y_all\", \"groups\", \"fit_classifier\", \"predict_positive_score\", \"outpath\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-Subset] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    TOP_SUBSET_K = int(globals().get(\"TOP_SUBSET_K\", 15))\n",
    "    if TOP_SUBSET_K <= 0:\n",
    "        raise ValueError(\"TOP_SUBSET_K must be positive\")\n",
    "\n",
    "    rank_csv = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "    if not os.path.exists(rank_csv):\n",
    "        raise FileNotFoundError(\"[Cell3A-Subset] SHAP_FEATURE_RANKING.CSV がありません。Cell3A を実行してください。\")\n",
    "\n",
    "    rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "    if \"rank_mean\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "    elif \"mean_abs\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"mean_abs\", ascending=False).index.tolist()\n",
    "    else:\n",
    "        feature_order = list(rank_df.index)\n",
    "\n",
    "    feature_order = [f for f in feature_order if f in X_all.columns]\n",
    "    if not feature_order:\n",
    "        raise RuntimeError(\"[Cell3A-Subset] ランキングに該当する特徴が X_all に存在しません。\")\n",
    "\n",
    "    limit = min(TOP_SUBSET_K, len(feature_order))\n",
    "    top_features = feature_order[:limit]\n",
    "    print(f\"[Cell3A-Subset] Top features ({len(top_features)}): {top_features}\")\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    results = []\n",
    "\n",
    "    for r in range(1, len(top_features) + 1):\n",
    "        for comb in combinations(top_features, r):\n",
    "            feats = list(comb)\n",
    "            y_true_all = []\n",
    "            y_score_all = []\n",
    "            for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "                X_tr = X_all.iloc[tr_idx][feats].astype(np.float32)\n",
    "                y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "                X_te = X_all.iloc[te_idx][feats].astype(np.float32)\n",
    "                y_te = y_all.iloc[te_idx].astype(int)\n",
    "                if len(np.unique(y_tr)) < 2:\n",
    "                    continue\n",
    "                model = fit_classifier(X_tr, y_tr)\n",
    "                proba = predict_positive_score(model, X_te)\n",
    "                y_true_all.append(y_te)\n",
    "                y_score_all.append(proba)\n",
    "            if not y_true_all:\n",
    "                auc = float(\"nan\")\n",
    "            else:\n",
    "                y_true = np.concatenate(y_true_all)\n",
    "                y_score = np.concatenate(y_score_all)\n",
    "                if len(np.unique(y_true)) < 2:\n",
    "                    auc = float(\"nan\")\n",
    "                else:\n",
    "                    auc = float(roc_auc_score(y_true, y_score))\n",
    "            results.append({\n",
    "                \"size\": r,\n",
    "                \"features\": feats,\n",
    "                \"auc\": auc,\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values([\"auc\", \"size\"], ascending=[False, True])\n",
    "    results_df[\"features_str\"] = results_df[\"features\"].apply(lambda lst: \",\".join(lst))\n",
    "    subset_csv_name = f\"TOP{TOP_SUBSET_K}_SUBSET_AUC.csv\"\n",
    "    subset_path = outpath(subset_csv_name)\n",
    "    results_df[[\"size\", \"features_str\", \"auc\"]].to_csv(subset_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    print(f\"[Cell3A-Subset] best size={int(best_row['size'])}, auc={best_row['auc']:.4f}, features={best_row['features']}\")\n",
    "\n",
    "    best_json_name = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST.json\"\n",
    "    with open(outpath(best_json_name), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"size\": int(best_row[\"size\"]),\n",
    "            \"auc\": float(best_row[\"auc\"]),\n",
    "            \"features\": best_row[\"features\"],\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    globals()[\"BEST_SUBSET_FEATURES\"] = best_row[\"features\"]\n",
    "    globals()[\"BEST_SUBSET_K\"] = len(best_row[\"features\"])\n",
    "\n",
    "    print(f\"[Cell3A-Subset] 保存 -> {subset_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f952a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-Subset: グループ別BEST1×4評価 =====\n",
    "RUN_CELL_3A_GROUPBEST_FEATURE= True\n",
    "\n",
    "if RUN_CELL_3A_GROUPBEST_FEATURE:\n",
    "    # ここに従来の処理をそのままインデントして置く\n",
    "    # 例:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    required = [\"X_all\", \"y_all\", \"groups\",\n",
    "                \"fit_classifier\", \"compute_train_shap_abs_mean\",\n",
    "                \"evaluate_fold\", \"outpath\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-SHAP] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    SHAP_BACKEND = \"xgb\"  # SHAP評価用の学習器\n",
    "    FEATURE_LIST_PATH = outpath(\"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "    # ---------- 特徴量プールの決定（相関除去後リストがあればそれを使う） ----------\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        feature_pool = [c for c in keep_payload.get(\"keep\", []) if c in X_all.columns]\n",
    "        print(f\"[Cell3A-SHAP] correlation-pruned features loaded ({len(feature_pool)} cols)\")\n",
    "    else:\n",
    "        feature_pool = [c for c in X_all.columns if c not in (\"Epoch_start\", \"Epoch_end\", \"FMS\")]\n",
    "        print(\"[Cell3A-SHAP] correlation-pruned list not found. Using all columns (except time/FMS).\")\n",
    "\n",
    "    if not feature_pool:\n",
    "        raise RuntimeError(\"[Cell3A-SHAP] feature_pool が空です。Cell3A-pre の結果を確認してください。\")\n",
    "\n",
    "    X_source = X_all[feature_pool].astype(np.float32).copy()\n",
    "    y_source = y_all.astype(int).copy()\n",
    "\n",
    "    # ---------- 全データ一発学習 → SHAPランキング ----------\n",
    "    print(f\"[Cell3A-SHAP] start SHAP ranking (n_samples={len(X_source)}, n_features={X_source.shape[1]})\")\n",
    "    model_full = fit_classifier(X_source, y_source, backend=SHAP_BACKEND)\n",
    "\n",
    "    # ここで compute_train_shap_abs_mean は\n",
    "    #   pd.Series( index=特徴名, 値=abs(SHAP)の平均 )\n",
    "    # を返す想定\n",
    "    mean_abs = compute_train_shap_abs_mean(model_full, X_source)\n",
    "    if not isinstance(mean_abs, pd.Series):\n",
    "        raise RuntimeError(\"[Cell3A-SHAP] compute_train_shap_abs_mean は pd.Series を返す必要があります。\")\n",
    "\n",
    "    rank_df = pd.DataFrame({\n",
    "        \"mean_abs\": mean_abs.astype(float)\n",
    "    })\n",
    "    rank_df = rank_df.sort_values(\"mean_abs\", ascending=False)\n",
    "\n",
    "    rank_path = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "    rank_df.to_csv(rank_path, encoding=\"utf-8-sig\")\n",
    "    rank_df.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-SHAP] Saved SHAP ranking -> {rank_path}\")\n",
    "\n",
    "    # ---------- おまけ：全特徴での LOSO 評価（従来の LOSO_METRICS.CSV を互換出力） ----------\n",
    "    logo = LeaveOneGroupOut()\n",
    "    metrics_rows = []\n",
    "\n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_source, y_source, groups), start=1):\n",
    "        X_tr = X_source.iloc[tr_idx]\n",
    "        y_tr = y_source.iloc[tr_idx]\n",
    "        X_te = X_source.iloc[te_idx]\n",
    "        y_te = y_source.iloc[te_idx]\n",
    "\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(f\"[Cell3A-SHAP] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "        model = fit_classifier(X_tr, y_tr, backend=SHAP_BACKEND)\n",
    "        m = evaluate_fold(model, X_te, y_te)\n",
    "        m.update({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "        })\n",
    "        metrics_rows.append(m)\n",
    "\n",
    "    if metrics_rows:\n",
    "        pd.DataFrame(metrics_rows).to_csv(outpath(\"LOSO_METRICS.CSV\"),\n",
    "                                        index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[Cell3A-SHAP] Saved LOSO metrics -> {outpath('LOSO_METRICS.CSV')}\")\n",
    "\n",
    "    # ---------- 可視化（全特徴 / Top-8） ----------\n",
    "    plt.figure(figsize=(10, max(5, len(rank_df) // 3)))\n",
    "    plt.barh(rank_df.index[::-1], rank_df[\"mean_abs\"][::-1], linewidth=1.5)\n",
    "    plt.xlabel(\"Mean |SHAP value|\", fontsize=24)\n",
    "    plt.ylabel(\"Feature\", fontsize=24)\n",
    "    plt.title(\"SHAP Feature Ranking (All)\", fontsize=30)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_RANKING_ALL.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    TOP_K = 8\n",
    "    topk = rank_df.head(TOP_K).iloc[::-1]\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = plt.gca()\n",
    "    ax.barh(topk.index, topk[\"mean_abs\"], linewidth=1.5)\n",
    "    ax.set_xlabel(\"Mean |SHAP value|\", fontsize=24)\n",
    "    ax.set_ylabel(\"Feature\", fontsize=24)\n",
    "    ax.set_title(f\"Top-{TOP_K} SHAP Feature Ranking\", fontsize=30)\n",
    "    ax.tick_params(axis=\"both\", labelsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell3A-SHAP] 図を保存 -> {outpath('SHAP_TOP8_RANKING.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae819ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-SHAP: 全特徴の SHAP 可視化 =====\n",
    "import os\n",
    "import json\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "required = [\"X_all\", \"y_all\", \"fit_classifier\", \"outpath\", \"SEED_BASE\", \"OUT_DIR\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-SHAP] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "# ---- SHAPランキング順の特徴リスト取得 ----\n",
    "rank_csv = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "if os.path.exists(rank_csv):\n",
    "    rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "    if \"rank_mean\" in rank_df.columns:\n",
    "        # ★ foldごとの順位平均 rank_mean があればそれで並べ替え（小さいほど重要）\n",
    "        feature_order = rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "    elif \"mean_abs\" in rank_df.columns:\n",
    "        # 古い形式などで rank_mean がない場合だけ mean_abs でフォールバック\n",
    "        feature_order = rank_df.sort_values(\"mean_abs\", ascending=False).index.tolist()\n",
    "    else:\n",
    "        feature_order = list(rank_df.index)\n",
    "else:\n",
    "    # ランキングがなければ列順のまま\n",
    "    feature_order = list(X_all.columns)\n",
    "\n",
    "# X_all に存在する列だけに絞る（相関除去後の全特徴）\n",
    "feature_order = [f for f in feature_order if f in X_all.columns]\n",
    "\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[Cell3A-SHAP] 可視化対象の特徴がありません。\")\n",
    "\n",
    "# ---- Top-K の設定（summary plot 1枚目用）----\n",
    "TOP_K = int(globals().get(\"TOP_SUBSET_K\", 15))\n",
    "TOP_K = max(1, min(TOP_K, len(feature_order)))  # 1〜特徴数の範囲にクリップ\n",
    "\n",
    "# ---- SHAP計算用データ（相関除去後の全特徴）----\n",
    "# ★ features_for_model の順番 = SHAP_FEATURE_RANKING.CSV の rank_mean 順\n",
    "features_for_model = feature_order\n",
    "X_shap = X_all[features_for_model].astype(np.float32)\n",
    "y_shap = y_all.astype(int)\n",
    "\n",
    "print(f\"[Cell3A-SHAP] samples={X_shap.shape[0]}, features={X_shap.shape[1]} (ALL correlation-pruned features)\")\n",
    "\n",
    "# ---- モデル学習 ----\n",
    "model = fit_classifier(X_shap, y_shap)\n",
    "\n",
    "# ---- TreeExplainer で SHAP 計算 ----\n",
    "background = shap.sample(X_shap, min(256, len(X_shap)), random_state=SEED_BASE)\n",
    "explainer = shap.TreeExplainer(\n",
    "    model,\n",
    "    data=background,\n",
    "    model_output=\"probability\",\n",
    "    feature_perturbation=\"interventional\",\n",
    ")\n",
    "\n",
    "shap_values_any = explainer.shap_values(X_shap)\n",
    "if isinstance(shap_values_any, list):\n",
    "    # 2値分類などで [neg, pos] みたいなリストになる場合は「陽性クラス(1)」を採用\n",
    "    if hasattr(model, \"classes_\") and 1 in list(model.classes_):\n",
    "        class_idx = list(model.classes_).index(1)\n",
    "    else:\n",
    "        class_idx = -1  # 最後のクラス\n",
    "    shap_values = shap_values_any[class_idx]\n",
    "else:\n",
    "    shap_values = shap_values_any\n",
    "\n",
    "shap_values = np.asarray(shap_values)\n",
    "if shap_values.ndim == 3:\n",
    "    # (n_samples, n_features, n_classes) → 陽性クラスを取り出す\n",
    "    if hasattr(model, \"classes_\") and 1 in list(model.classes_):\n",
    "        pos_idx = list(model.classes_).index(1)\n",
    "    else:\n",
    "        pos_idx = -1\n",
    "    shap_values = shap_values[:, :, pos_idx]\n",
    "elif shap_values.ndim == 1:\n",
    "    shap_values = shap_values.reshape(-1, 1)\n",
    "\n",
    "if shap_values.shape[1] != X_shap.shape[1]:\n",
    "    raise RuntimeError(f\"[Cell3A-SHAP] shap_values 形状が一致しません: {shap_values.shape} vs {X_shap.shape}\")\n",
    "\n",
    "print(\"[Cell3A-SHAP] SHAP 値を計算しました。\")\n",
    "\n",
    "# ---- ベストサブセットの読み込み（★TOPkのJSONだけ見る）----\n",
    "best_subset_features = []\n",
    "try:\n",
    "    # まず TOP{TOP_SUBSET_K}_SUBSET_BEST.json を見る（例：TOP15_SUBSET_BEST.json）\n",
    "    topk_json = outpath(f\"TOP{TOP_K}_SUBSET_BEST.json\")\n",
    "    if os.path.exists(topk_json):\n",
    "        subset_json = topk_json\n",
    "    else:\n",
    "        # フォールバックで TOP10_SUBSET_BEST.json だけ見る\n",
    "        fallback_json = outpath(\"TOP10_SUBSET_BEST.json\")\n",
    "        subset_json = fallback_json if os.path.exists(fallback_json) else None\n",
    "\n",
    "    if subset_json is not None:\n",
    "        with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            info = json.load(f)\n",
    "        # 今回使っているfeatures_for_model内で共通するものだけ\n",
    "        best_subset_features = [f for f in info.get(\"features\", []) if f in features_for_model]\n",
    "        print(f\"[Cell3A-SHAP] subset features for highlight (from {os.path.basename(subset_json)}):\")\n",
    "        print(f\"  {best_subset_features}\")\n",
    "    else:\n",
    "        print(\"[Cell3A-SHAP] subset JSON が見つからなかったため、ハイライトなし。\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[Cell3A-SHAP][WARN] subset読み込み失敗: {e}\")\n",
    "    best_subset_features = []\n",
    "\n",
    "# ---- 出力ディレクトリ ----\n",
    "shap_dir = os.path.join(OUT_DIR, \"SHAP\")\n",
    "os.makedirs(shap_dir, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Summary plot（Top-K）  ★ fold順位平均の順番で表示（sort=False）\n",
    "# =============================================================================\n",
    "plt.figure()\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_shap,\n",
    "    show=False,\n",
    "    plot_type=\"dot\",\n",
    "    max_display=TOP_K,   # 先頭 TOP_K 特徴だけ\n",
    "    sort=False,          # ★ SHAP側での再ソートを無効化 → features_for_model の順を使う\n",
    ")\n",
    "ax = plt.gca()\n",
    "for label in ax.get_yticklabels():\n",
    "    if label.get_text() in best_subset_features:\n",
    "        label.set_color(\"red\")\n",
    "plt.tight_layout()\n",
    "summary_top_path = os.path.join(shap_dir, f\"SHAP_SUMMARY_TOP{TOP_K}.png\")\n",
    "plt.savefig(summary_top_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"[Cell3A-SHAP] Summary plot (Top-{TOP_K}, rank_mean順) -> {summary_top_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Summary plot（全特徴：ランキング順の全てを表示） ★こちらも sort=False\n",
    "# =============================================================================\n",
    "plt.figure()\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_shap,\n",
    "    show=False,\n",
    "    plot_type=\"dot\",\n",
    "    max_display=X_shap.shape[1],  # 全特徴を表示\n",
    "    sort=False,                   # ★ features_for_model（=rank_mean順）のまま\n",
    ")\n",
    "ax = plt.gca()\n",
    "for label in ax.get_yticklabels():\n",
    "    if label.get_text() in best_subset_features:\n",
    "        label.set_color(\"red\")\n",
    "plt.tight_layout()\n",
    "summary_all_path = os.path.join(shap_dir, f\"SHAP_SUMMARY_ALL.png\")\n",
    "plt.savefig(summary_all_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"[Cell3A-SHAP] Summary plot (ALL features, rank_mean順) -> {summary_all_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 各特徴ごとの dependence plot（全特徴分）\n",
    "# =============================================================================\n",
    "for feature in features_for_model:\n",
    "    plt.figure()\n",
    "    shap.dependence_plot(\n",
    "        feature,\n",
    "        shap_values,\n",
    "        X_shap,\n",
    "        show=False,\n",
    "        interaction_index=None,   # 一変数の関係だけを描画\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    safe_name = feature.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    dep_path = os.path.join(shap_dir, f\"SHAP_DEP_{safe_name}.png\")\n",
    "    plt.savefig(dep_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[Cell3A-SHAP] Dependence plot -> {dep_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4A: ROC曲線（TOPサブセット） =====\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "\n",
    "\n",
    "subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "preferred = [subset_primary, \"TOP10_SUBSET_BEST.json\"]\n",
    "subset_json = None\n",
    "for name in preferred:\n",
    "    cand = outpath(name)\n",
    "    if os.path.exists(cand):\n",
    "        subset_json = cand\n",
    "        break\n",
    "if subset_json is None:\n",
    "    raise FileNotFoundError(\"[Cell4A] TOP*_SUBSET_BEST.json がありません。Cell3A-Subset を実行してください。\")\n",
    "\n",
    "with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    subset_info = json.load(f)\n",
    "\n",
    "best_features = subset_info.get(\"features\", [])\n",
    "if not best_features:\n",
    "    raise RuntimeError(\"[Cell4A] JSON 内に features がありません。\")\n",
    "\n",
    "# X_all に存在するものだけに絞る\n",
    "best_features = [f for f in best_features if f in X_all.columns]\n",
    "if not best_features:\n",
    "    raise RuntimeError(\"[Cell4A] X_all に存在する特徴がありません。\")\n",
    "\n",
    "# ★ オプションで MSSQ / VIMSSQ を追加 ★\n",
    "extra_traits = []\n",
    "if globals().get(\"USE_MSSQ_FEATURE\", False) and \"MSSQ\" in X_all.columns:\n",
    "    extra_traits.append(\"MSSQ\")\n",
    "if globals().get(\"USE_VIMSSQ_FEATURE\", False) and \"VIMSSQ\" in X_all.columns:\n",
    "    extra_traits.append(\"VIMSSQ\")\n",
    "\n",
    "extra_traits = [f for f in extra_traits if f not in best_features]\n",
    "if extra_traits:\n",
    "    print(f\"[Cell4A] 追加で使用する属性特徴: {extra_traits}\")\n",
    "    best_features = best_features + extra_traits\n",
    "# ★ ここまで ★\n",
    "\n",
    "best_k = len(best_features)\n",
    "\n",
    "# もともとの表示\n",
    "print(f\"[Cell4A] 使用特徴 ({best_k}) from {os.path.basename(subset_json)}: {best_features}\")\n",
    "\n",
    "# ★ デバッグ用: 実際に使う最終特徴一覧を明示的にプリント ★\n",
    "print(\"[Cell4A][DEBUG] 実際にモデルに渡した特徴量リスト:\")\n",
    "for i, f_name in enumerate(best_features, start=1):\n",
    "    print(f\"  {i:2d}: {f_name}\")\n",
    "# ★ ここまで ★\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "y_true_all, proba_all, subj_all = [], [], []\n",
    "for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "    X_tr = X_all.iloc[tr_idx][best_features].astype(np.float32)\n",
    "    y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "    X_te = X_all.iloc[te_idx][best_features].astype(np.float32)\n",
    "    y_te = y_all.iloc[te_idx].astype(int)\n",
    "    if len(np.unique(y_tr)) < 2:\n",
    "        continue\n",
    "    model = fit_classifier(X_tr, y_tr)\n",
    "    proba = predict_positive_score(model, X_te)\n",
    "    y_true_all.append(y_te)\n",
    "    proba_all.append(proba)\n",
    "    subj_all.append(groups.iloc[te_idx].values)\n",
    "\n",
    "if not y_true_all:\n",
    "    raise RuntimeError(\"[Cell4A] 評価に必要な fold が得られませんでした。\")\n",
    "\n",
    "y_pool = np.concatenate(y_true_all)\n",
    "s_pool = np.concatenate(proba_all)\n",
    "subj_pool = np.concatenate(subj_all)\n",
    "if len(np.unique(y_pool)) < 2:\n",
    "    raise RuntimeError(\"[Cell4A] 真値が単一クラスのため ROC-AUC を計算できません。\")\n",
    "\n",
    "auc_obs = float(roc_auc_score(y_pool, s_pool))\n",
    "\n",
    "rng = np.random.default_rng(20251101)\n",
    "df_pool = pd.DataFrame({\"subject\": subj_pool, \"y_true\": y_pool, \"y_score\": s_pool})\n",
    "subj_ids = df_pool[\"subject\"].unique()\n",
    "auc_boot = []\n",
    "for _ in range(2000):\n",
    "    sampled = rng.choice(subj_ids, size=len(subj_ids), replace=True)\n",
    "    df_boot = pd.concat([df_pool[df_pool[\"subject\"] == sid] for sid in sampled], ignore_index=True)\n",
    "    if df_boot[\"y_true\"].nunique() < 2:\n",
    "        continue\n",
    "    auc_boot.append(float(roc_auc_score(df_boot[\"y_true\"], df_boot[\"y_score\"])))\n",
    "if auc_boot:\n",
    "    ci_low = float(np.quantile(auc_boot, 0.025))\n",
    "    ci_high = float(np.quantile(auc_boot, 0.975))\n",
    "else:\n",
    "    ci_low = ci_high = float(\"nan\")\n",
    "\n",
    "pd.DataFrame([{\"k\": best_k, \"auc\": auc_obs, \"ci_low\": ci_low, \"ci_high\": ci_high}]).to_csv(\n",
    "    outpath(\"AUC_K_CI.csv\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[Cell4A] AUC={auc_obs:.4f} (95% CI [{ci_low:.4f}, {ci_high:.4f}])\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_pool, s_pool)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_obs:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Chance\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Best Subset)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"AUC_K_CI.png\"), dpi=300)\n",
    "plt.close()\n",
    "print(f\"[Cell4A] ROC 図を保存 -> {outpath('AUC_K_CI.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ced576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Inner LOSO folds builder =====\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def choose_inner_folds_loso(train_subject_ids: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    外側LOSOで得た “学習側の被験者ID” リストを受け取り、\n",
    "    1名ずつ検証に回す inner-LOSO のfoldリスト（[[sid1], [sid2], ...]）を返す。\n",
    "    \"\"\"\n",
    "    if not isinstance(train_subject_ids, (list, tuple)):\n",
    "        raise RuntimeError(\"[inner folds] train_subject_ids は list/tuple を想定しています。\")\n",
    "    uniq = list(pd.unique(pd.Series([str(sid) for sid in train_subject_ids])))\n",
    "    if len(uniq) == 0:\n",
    "        raise RuntimeError(\"[inner folds] train_subject_ids が空です。\")\n",
    "    uniq_sorted = sorted(uniq, key=lambda x: (len(x), x))\n",
    "    folds = [[sid] for sid in uniq_sorted]\n",
    "    print(f\"[inner folds] {len(folds)} splits -> val subjects = {', '.join(uniq_sorted)}\")\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5A (FINAL, F1専用): inner-LOSO τ最適化（連結val）→ outer予測・評価 =====\n",
    "# 前提: Cell1〜4 実行済み（X_all, y_all, groups, SUBJECT_META, choose_inner_folds_loso 等）\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ---------------- 出力ディレクトリ: Cell5_F1 配下に集約 ----------------\n",
    "CELL5_DIR = os.path.join(OUT_DIR, \"Cell5_F1\")\n",
    "os.makedirs(CELL5_DIR, exist_ok=True)\n",
    "\n",
    "def cell5_out(filename: str) -> str:\n",
    "    \"\"\"Cell5_F1 配下への汎用出力パス\"\"\"\n",
    "    path = os.path.join(CELL5_DIR, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ---------------- GROUP_AWARE 出力先（F1） ----------------\n",
    "MODE_TAG = \"F1\"\n",
    "GROUP_AWARE_DIR = os.path.join(CELL5_DIR, \"GROUP_AWARE\", MODE_TAG)\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_out(filename: str) -> str:\n",
    "    path = os.path.join(GROUP_AWARE_DIR, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ---------------- 群分け基準（MSSQ / VIMSSQ 切り替え） ----------------\n",
    "# 優先順位:\n",
    "#  1) GROUPING_BASIS_FOR_FAIRNESS があればそれを使う\n",
    "#  2) なければ GROUPING_BASIS_FOR_PLOTS を流用\n",
    "#  3) どちらもなければ MSSQ をデフォルト\n",
    "GROUPING_BASIS_FOR_FAIRNESS = globals().get(\n",
    "    \"GROUPING_BASIS_FOR_FAIRNESS\",\n",
    "    globals().get(\"GROUPING_BASIS_FOR_PLOTS\", \"MSSQ\")\n",
    ")\n",
    "basis = str(GROUPING_BASIS_FOR_FAIRNESS).upper()\n",
    "\n",
    "if basis == \"MSSQ\":\n",
    "    GROUP_COL_NAME = \"MSSQ_group\"\n",
    "elif basis == \"VIMSSQ\":\n",
    "    GROUP_COL_NAME = \"VIMSSQ_group\"\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"[Cell5A] GROUPING_BASIS_FOR_FAIRNESS は 'MSSQ' か 'VIMSSQ' を指定してください（今: {GROUPING_BASIS_FOR_FAIRNESS}）\"\n",
    "    )\n",
    "\n",
    "print(f\"[Cell5A] GROUPING_BASIS_FOR_FAIRNESS = {GROUPING_BASIS_FOR_FAIRNESS} -> group_col = {GROUP_COL_NAME}\")\n",
    "\n",
    "# ---------------- 基本チェック ----------------\n",
    "req = [\"X_all\",\"y_all\",\"groups\",\"SUBJECT_META\",\n",
    "       \"choose_inner_folds_loso\",\"fit_classifier\",\"predict_positive_score\",\"outpath\"]\n",
    "missing = [v for v in req if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell5A] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "if GROUP_COL_NAME not in SUBJECT_META.columns and GROUP_COL_NAME not in SUBJECT_META.index.names:\n",
    "    raise RuntimeError(f\"[Cell5A] SUBJECT_META に {GROUP_COL_NAME} 列（または index）が必要である．\")\n",
    "\n",
    "# ---------------- 入力整形 ----------------\n",
    "X_base = X_all.astype(np.float32)\n",
    "y_base = y_all.astype(int)\n",
    "g_base = groups.astype(str)\n",
    "\n",
    "# 群ラベル（MSSQ_group / VIMSSQ_group）マッピング → High / Low に正規化\n",
    "if \"subject_id\" in SUBJECT_META.columns:\n",
    "    mapper = SUBJECT_META.set_index(\"subject_id\")[GROUP_COL_NAME].astype(str).to_dict()\n",
    "else:\n",
    "    # index が subject_id になっている想定\n",
    "    mapper = SUBJECT_META[GROUP_COL_NAME].astype(str).to_dict()\n",
    "\n",
    "fair_groups_raw = g_base.map(mapper)\n",
    "\n",
    "if fair_groups_raw.isna().any():\n",
    "    raise RuntimeError(\n",
    "        f\"[Cell5A] {GROUP_COL_NAME} 未割当ID: {sorted(set(g_base[fair_groups_raw.isna()]))}\"\n",
    "    )\n",
    "\n",
    "fair_groups = (fair_groups_raw\n",
    "               .astype(str)\n",
    "               .str.strip()\n",
    "               .str.lower()\n",
    "               .map({\"high\": \"High\", \"low\": \"Low\"}))\n",
    "\n",
    "if fair_groups.isna().any():\n",
    "    # High/Low 以外のラベルが混じっているケース\n",
    "    bad_labels = sorted(set(fair_groups_raw[~fair_groups_raw.isin(\n",
    "        [\"High\",\"Low\",\"high\",\"low\"]\n",
    "    )]))\n",
    "    raise RuntimeError(\n",
    "        f\"[Cell5A] {GROUP_COL_NAME} に 'High'/'Low' 以外のラベルが含まれている: {bad_labels}\"\n",
    "    )\n",
    "\n",
    "# ===== デバッグ: 群分類のプリント =====\n",
    "if \"subject_id\" in SUBJECT_META.columns:\n",
    "    dbg_meta = SUBJECT_META[[\"subject_id\", GROUP_COL_NAME]].copy()\n",
    "else:\n",
    "    # index 名が 'subject_id' である前提（そうでない場合は適宜修正）\n",
    "    dbg_meta = SUBJECT_META.reset_index()[[\"subject_id\", GROUP_COL_NAME]].copy()\n",
    "\n",
    "dbg_meta = dbg_meta.sort_values([GROUP_COL_NAME, \"subject_id\"])\n",
    "\n",
    "print(f\"\\n[Cell5A-DEBUG] SUBJECT_META における {GROUP_COL_NAME} 分類一覧\")\n",
    "print(dbg_meta.to_string(index=False))\n",
    "\n",
    "print(f\"\\n[Cell5A-DEBUG] {GROUP_COL_NAME} 件数 (SUBJECT_META 基準):\",\n",
    "      dbg_meta[GROUP_COL_NAME].value_counts().to_dict())\n",
    "\n",
    "print(\"\\n[Cell5A-DEBUG] fair_groups 件数 (エポックベース):\",\n",
    "      fair_groups.value_counts().to_dict())\n",
    "\n",
    "# ---------------- 特徴選抜（Cell3A-Subset の JSON のみ使用） ----------------\n",
    "subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "subset_candidates = [subset_primary, \"TOP10_SUBSET_BEST.json\"]\n",
    "subset_json = None\n",
    "for name in subset_candidates:\n",
    "    cand = outpath(name)\n",
    "    if os.path.exists(cand):\n",
    "        subset_json = cand\n",
    "        break\n",
    "\n",
    "if subset_json is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"[Cell5A] TOP*_SUBSET_BEST.json が見つからない．Cell3A-Subset を実行すること．\"\n",
    "    )\n",
    "\n",
    "with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    subset_info = json.load(f)\n",
    "\n",
    "raw_features = subset_info.get(\"features\", [])\n",
    "if not raw_features:\n",
    "    raise RuntimeError(f\"[Cell5A] JSON 内に 'features' が空である -> {os.path.basename(subset_json)}\")\n",
    "\n",
    "# X_all に実在する特徴だけに絞る\n",
    "feature_order = [f for f in raw_features if f in X_base.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\n",
    "        f\"[Cell5A] JSON の features が X_all に1つも存在しない: {raw_features}\"\n",
    "    )\n",
    "\n",
    "# Cell4 と同様に MSSQ / VIMSSQ を任意で追加\n",
    "extra_traits = []\n",
    "if globals().get(\"USE_MSSQ_FEATURE\", False) and \"MSSQ\" in X_base.columns:\n",
    "    extra_traits.append(\"MSSQ\")\n",
    "if globals().get(\"USE_VIMSSQ_FEATURE\", False) and \"VIMSSQ\" in X_base.columns:\n",
    "    extra_traits.append(\"VIMSSQ\")\n",
    "\n",
    "# 既に feature_order に入っていないものだけ追加\n",
    "extra_traits = [f for f in extra_traits if f not in feature_order]\n",
    "\n",
    "feats_k = feature_order + extra_traits\n",
    "best_k = len(feats_k)\n",
    "\n",
    "print(f\"[Cell5A] Using subset features from {os.path.basename(subset_json)}\")\n",
    "print(f\"[Cell5A] JSON features (base) k={len(feature_order)}: {feature_order}\")\n",
    "if extra_traits:\n",
    "    print(f\"[Cell5A] 追加で使用する属性特徴: {extra_traits}\")\n",
    "print(f\"[Cell5A] 最終的に使用する特徴数 = {best_k}\")\n",
    "\n",
    "X_k = X_base[feats_k]\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"[DBG] 使用特徴数: {len(feats_k)}\")\n",
    "    print(\"[DBG] 使用特徴一覧 (feats_k):\")\n",
    "    for i, f_name in enumerate(feats_k, start=1):\n",
    "        print(f\"  {i:2d}: {f_name}\")\n",
    "\n",
    "# ---------------- 指標・評価ユーティリティ（F1専用） ----------------\n",
    "def _conf_from_preds(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    TN, FP, FN, TP = skm.confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _f1_from_conf(TP, FP, FN, TN) -> float:\n",
    "    TP = float(TP); FP = float(FP); FN = float(FN)\n",
    "    denom = (2*TP + FP + FN)\n",
    "    return (2*TP / denom) if denom > 0 else 0.0\n",
    "\n",
    "def _f1_binary(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    TP, FP, FN, TN = _conf_from_preds(y_true, y_pred)\n",
    "    return _f1_from_conf(TP, FP, FN, TN)\n",
    "\n",
    "def _grid(l, r, steps):\n",
    "    l = float(max(0.0, l)); r = float(min(1.0, r))\n",
    "    if l > r: l, r = r, l\n",
    "    return np.linspace(l, r, int(steps), dtype=float)\n",
    "\n",
    "# ---------------- τ最適化（Single / WG-F1 / Group-F1） ----------------\n",
    "def _single_tau_opt(scores: np.ndarray, y: np.ndarray):\n",
    "    # coarse\n",
    "    cands = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    f1_vec = []\n",
    "    for t in cands:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        f1_vec.append(_f1_binary(y, yhat))\n",
    "    f1_vec = np.asarray(f1_vec)\n",
    "    idx = int(np.nanargmax(f1_vec)); tau0 = float(cands[idx]); best0 = float(f1_vec[idx])\n",
    "\n",
    "    # fine around tau0\n",
    "    left  = max(0.0, tau0 - FINE_MARGIN)\n",
    "    right = min(1.0, tau0 + FINE_MARGIN)\n",
    "    cands2 = _grid(left, right, FINE_STEPS)\n",
    "    f1_vec2 = []\n",
    "    for t in cands2:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        f1_vec2.append(_f1_binary(y, yhat))\n",
    "    f1_vec2 = np.asarray(f1_vec2)\n",
    "    idx2 = int(np.nanargmax(f1_vec2)); tau = float(cands2[idx2]); best = float(f1_vec2[idx2])\n",
    "\n",
    "    return {\"tau\": tau, \"F1_val\": best, \"tau_coarse\": tau0, \"F1_coarse\": best0}\n",
    "\n",
    "def _wg_f1_opt_joint(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    \"\"\"\n",
    "    Fair-MinF1（最悪群F1の最大化）を 2Dグリッド（τH×τL）で探索する．\n",
    "    戻り値:\n",
    "      {\"tauH\",\"tauL\",\"F1_H_val\",\"F1_L_val\",\"WG_F1_val\",\"F1_pooled_val\"}\n",
    "    \"\"\"\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[WG-F1] 連結valに High/Low の両群が必要（どちらかが0件）\")\n",
    "\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "\n",
    "    def _f1_vec(s, yy, cands):\n",
    "        f1 = np.empty_like(cands)\n",
    "        for i, t in enumerate(cands):\n",
    "            yhat = (s >= t).astype(int)\n",
    "            f1[i] = _f1_binary(yy, yhat)\n",
    "        return f1\n",
    "\n",
    "    f1H = _f1_vec(sH, yH, candH)\n",
    "    f1L = _f1_vec(sL, yL, candL)\n",
    "\n",
    "    best = {\"WG\": -np.inf, \"pooled\": -np.inf, \"tH\": 0.5, \"tL\": 0.5, \"F1H\": 0.0, \"F1L\": 0.0}\n",
    "    for i, tH in enumerate(candH):\n",
    "        wg_row = np.minimum(f1H[i], f1L)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL[j]).astype(int)\n",
    "        F1_pooled = _f1_binary(\n",
    "            np.concatenate([yH, yL]),\n",
    "            np.concatenate([yhatH, yhatL])\n",
    "        )\n",
    "\n",
    "        cand = {\"WG\": WG, \"pooled\": float(F1_pooled),\n",
    "                \"tH\": float(tH), \"tL\": float(candL[j]),\n",
    "                \"F1H\": float(f1H[i]), \"F1L\": float(f1L[j])}\n",
    "\n",
    "        def _is_better(cur, new):\n",
    "            if new[\"WG\"] > cur[\"WG\"]: return True\n",
    "            if new[\"WG\"] < cur[\"WG\"]: return False\n",
    "            if new[\"pooled\"] > cur[\"pooled\"]: return True\n",
    "            if new[\"pooled\"] < cur[\"pooled\"]: return False\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) < abs(cur[\"tH\"]-cur[\"tL\"]): return True\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) > abs(cur[\"tH\"]-cur[\"tL\"]): return False\n",
    "            if (new[\"tH\"], new[\"tL\"]) < (cur[\"tH\"], cur[\"tL\"]): return True\n",
    "            return False\n",
    "\n",
    "        if _is_better(best, cand):\n",
    "            best = cand\n",
    "\n",
    "    # fine (box refine)\n",
    "    lH = max(0.0, best[\"tH\"] - FINE_MARGIN); rH = min(1.0, best[\"tH\"] + FINE_MARGIN)\n",
    "    lL = max(0.0, best[\"tL\"] - FINE_MARGIN); rL = min(1.0, best[\"tL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS)\n",
    "    candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "\n",
    "    f1H2 = _f1_vec(sH, yH, candH2)\n",
    "    f1L2 = _f1_vec(sL, yL, candL2)\n",
    "\n",
    "    best2 = dict(best)\n",
    "    for i, tH in enumerate(candH2):\n",
    "        wg_row = np.minimum(f1H2[i], f1L2)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL2[j]).astype(int)\n",
    "        F1_pooled = _f1_binary(\n",
    "            np.concatenate([yH, yL]),\n",
    "            np.concatenate([yhatH, yhatL])\n",
    "        )\n",
    "        cand = {\"WG\": WG, \"pooled\": float(F1_pooled),\n",
    "                \"tH\": float(tH), \"tL\": float(candL2[j]),\n",
    "                \"F1H\": float(f1H2[i]), \"F1L\": float(f1L2[j])}\n",
    "\n",
    "        if (cand[\"WG\"] > best2[\"WG\"] or\n",
    "            (cand[\"WG\"] == best2[\"WG\"] and (\n",
    "                cand[\"pooled\"] > best2[\"pooled\"] or\n",
    "                (cand[\"pooled\"] == best2[\"pooled\"] and (\n",
    "                    abs(cand[\"tH\"]-cand[\"tL\"]) < abs(best2[\"tH\"]-best2[\"tL\"]) or\n",
    "                    (abs(cand[\"tH\"]-cand[\"tL\"]) == abs(best2[\"tH\"]-best2[\"tL\"]) and\n",
    "                     (cand[\"tH\"], cand[\"tL\"]) < (best2[\"tH\"], best2[\"tL\"]))\n",
    "                ))\n",
    "            ))):\n",
    "            best2 = cand\n",
    "\n",
    "    return {\n",
    "        \"tauH\": best2[\"tH\"], \"tauL\": best2[\"tL\"],\n",
    "        \"F1_H_val\": best2[\"F1H\"], \"F1_L_val\": best2[\"F1L\"],\n",
    "        \"WG_F1_val\": best2[\"WG\"], \"F1_pooled_val\": best2[\"pooled\"],\n",
    "    }\n",
    "\n",
    "def _group_f1_opt(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    # 2Dグリッド（τH×τL）で pooled F1 最大化（coarse→fine）\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[Group-F1] 連結valに High/Low の両群が必要（どちらかが0件）\")\n",
    "\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "\n",
    "    def _pooled_f1_for(tH, tL):\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= tL).astype(int)\n",
    "        y_true = np.concatenate([yH, yL])\n",
    "        y_pred = np.concatenate([yhatH, yhatL])\n",
    "        return _f1_binary(y_true, y_pred)\n",
    "\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    best = {\"F1_val\": -np.inf, \"tauH\": None, \"tauL\": None}\n",
    "    for tH in candH:\n",
    "        f1_vec = np.empty_like(candL)\n",
    "        for j, tL in enumerate(candL):\n",
    "            f1_vec[j] = _pooled_f1_for(tH, tL)\n",
    "        jmax = int(np.nanargmax(f1_vec))\n",
    "        if float(f1_vec[jmax]) > best[\"F1_val\"]:\n",
    "            best.update({\"F1_val\": float(f1_vec[jmax]), \"tauH\": float(tH), \"tauL\": float(candL[jmax])})\n",
    "\n",
    "    # fine\n",
    "    lH = max(0.0, best[\"tauH\"] - FINE_MARGIN); rH = min(1.0, best[\"tauH\"] + FINE_MARGIN)\n",
    "    lL = max(0.0, best[\"tauL\"] - FINE_MARGIN); rL = min(1.0, best[\"tauL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS); candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "\n",
    "    best2 = dict(best)\n",
    "    for tH in candH2:\n",
    "        f1_vec2 = np.empty_like(candL2)\n",
    "        for j, tL in enumerate(candL2):\n",
    "            f1_vec2[j] = _pooled_f1_for(tH, tL)\n",
    "        jmax = int(np.nanargmax(f1_vec2))\n",
    "        if float(f1_vec2[jmax]) > best2[\"F1_val\"]:\n",
    "            best2.update({\"F1_val\": float(f1_vec2[jmax]), \"tauH\": float(tH), \"tauL\": float(candL2[jmax])})\n",
    "\n",
    "    return best2  # {\"tauH\",\"tauL\",\"F1_val\"}\n",
    "\n",
    "\n",
    "# ---------------- outer LOSO with inner concatenation ----------------\n",
    "logo_outer = LeaveOneGroupOut()\n",
    "rows, pred_rows = [], []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo_outer.split(X_k, y_base.values, g_base.values), start=1):\n",
    "    train_mask = pd.Series(False, index=g_base.index); train_mask.iloc[tr_idx] = True\n",
    "    test_mask  = pd.Series(False, index=g_base.index);  test_mask.iloc[te_idx]  = True\n",
    "    test_sid = g_base.iloc[te_idx].iloc[0]\n",
    "\n",
    "    # ===== inner folds =====\n",
    "    inner_ids   = sorted(g_base[train_mask].unique())\n",
    "    inner_folds = choose_inner_folds_loso(inner_ids)\n",
    "    if VERBOSE:\n",
    "        print(f\"[inner folds] {len(inner_folds)} splits -> val subjects = {', '.join(inner_ids)}\")\n",
    "\n",
    "    # 連結valの器\n",
    "    val_scores_all, val_y_all, val_grp_all = [], [], []\n",
    "\n",
    "    # --- inner train に両群が居るか（群別τの前提） ---\n",
    "    inner_train_groups = fair_groups[train_mask]\n",
    "    if not ((\"High\" in set(inner_train_groups)) and (\"Low\" in set(inner_train_groups))):\n",
    "        raise RuntimeError(f\"[Cell5A] fold{fold_id}: inner-train に両群(High/Low)が必要（群別τの前提）\")\n",
    "\n",
    "    # ===== inner: train→val 予測の連結 =====\n",
    "    for inner_val in inner_folds:\n",
    "        val_mask  = g_base.isin(inner_val) & train_mask\n",
    "        trn_mask  = train_mask & (~val_mask)\n",
    "        if not trn_mask.any() or not val_mask.any():\n",
    "            continue\n",
    "\n",
    "        X_tr, y_tr = X_k[trn_mask], y_base[trn_mask]\n",
    "        X_vl, y_vl = X_k[val_mask], y_base[val_mask]\n",
    "        grp_vl     = fair_groups[val_mask].to_numpy()\n",
    "\n",
    "        # inner 学習\n",
    "        model_inner = fit_classifier(X_tr, y_tr)\n",
    "        sc_vl = predict_positive_score(model_inner, X_vl).astype(float)\n",
    "\n",
    "        # 連結（生の val 予測）\n",
    "        val_scores_all.append(sc_vl)\n",
    "        val_y_all.append(y_vl.to_numpy())\n",
    "        val_grp_all.append(grp_vl)\n",
    "\n",
    "        if VERBOSE:\n",
    "            nH = int((grp_vl==\"High\").sum()); nL = int((grp_vl==\"Low\").sum())\n",
    "            print(f\"[DBG] fold{fold_id}: inner_val size={len(y_vl):3d}  High={nH} Low={nL}\")\n",
    "\n",
    "    # 連結\n",
    "    if len(val_scores_all) == 0:\n",
    "        raise RuntimeError(f\"[Cell5A] fold{fold_id}: inner validation が空である．\")\n",
    "    s_val = np.concatenate(val_scores_all)\n",
    "    y_val = np.concatenate(val_y_all)\n",
    "    g_val = np.concatenate(val_grp_all)\n",
    "\n",
    "    # ===== 連結valで一度だけ τ を最適化（F1） =====\n",
    "    res_single = _single_tau_opt(s_val, y_val)\n",
    "    tau_single = float(res_single[\"tau\"])\n",
    "\n",
    "    res_wg = _wg_f1_opt_joint(s_val, y_val, g_val)\n",
    "    tauH_wg, tauL_wg = float(res_wg[\"tauH\"]), float(res_wg[\"tauL\"])\n",
    "\n",
    "    res_group = _group_f1_opt(s_val, y_val, g_val)\n",
    "    tauH_grp, tauL_grp = float(res_group[\"tauH\"]), float(res_group[\"tauL\"])\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"[DBG] fold{fold_id}: tau_single={tau_single:.6f} | \"\n",
    "              f\"Group-F1(tH,tL)=({tauH_grp:.6f},{tauL_grp:.6f}) | \"\n",
    "              f\"WG-F1(tH,tL)=({tauH_wg:.6f},{tauL_wg:.6f})\")\n",
    "\n",
    "    # ===== outer test =====\n",
    "    X_tr_o, y_tr_o = X_k[train_mask], y_base[train_mask]\n",
    "    X_te_o, y_te_o = X_k[test_mask],  y_base[test_mask]\n",
    "    grp_te         = fair_groups[test_mask].to_numpy()\n",
    "\n",
    "    model_outer = fit_classifier(X_tr_o, y_tr_o)\n",
    "    sc_te = predict_positive_score(model_outer, X_te_o).astype(float)\n",
    "\n",
    "    y_true_te = y_te_o.to_numpy()\n",
    "    yhat_single   = (sc_te >= tau_single).astype(int)\n",
    "    yhat_groupF1  = (sc_te >= np.where(grp_te==\"High\", tauH_grp, tauL_grp)).astype(int)\n",
    "    yhat_wgF1     = (sc_te >= np.where(grp_te==\"High\", tauH_wg,  tauL_wg )).astype(int)\n",
    "\n",
    "    F1_single = _f1_binary(y_true_te, yhat_single)\n",
    "    F1_group  = _f1_binary(y_true_te, yhat_groupF1)\n",
    "    F1_wg     = _f1_binary(y_true_te, yhat_wgF1)\n",
    "\n",
    "    # Fold別 TP/FP/FN/TN（3アルゴリズム）\n",
    "    TP_s, FP_s, FN_s, TN_s = _conf_from_preds(y_true_te, yhat_single)\n",
    "    TP_g, FP_g, FN_g, TN_g = _conf_from_preds(y_true_te, yhat_groupF1)\n",
    "    TP_w, FP_w, FN_w, TN_w = _conf_from_preds(y_true_te, yhat_wgF1)\n",
    "\n",
    "    if VERBOSE:\n",
    "        same_grp = bool(np.array_equal(yhat_single, yhat_groupF1))\n",
    "        same_wg  = bool(np.array_equal(yhat_single, yhat_wgF1))\n",
    "        nH_te = int((grp_te==\"High\").sum()); nL_te = int((grp_te==\"Low\").sum())\n",
    "        print(f\"[DBG] fold{fold_id}: TEST High={nH_te} Low={nL_te} | \"\n",
    "              f\"Single==Group-F1: {same_grp}  Single==WG-F1: {same_wg} | \"\n",
    "              f\"F1(single, group, wg)=({F1_single:.3f}, {F1_group:.3f}, {F1_wg:.3f})\")\n",
    "\n",
    "    rows.append({\n",
    "        \"fold_id\": int(fold_id),\n",
    "        \"test_id\": str(test_sid),\n",
    "        \"best_k\": int(best_k),\n",
    "        \"tau_single\": float(tau_single),\n",
    "        \"tau_high_GroupF1\": float(tauH_grp), \"tau_low_GroupF1\": float(tauL_grp),\n",
    "        \"tau_high_WGF1\": float(tauH_wg),     \"tau_low_WGF1\": float(tauL_wg),\n",
    "        \"F1_single\": float(F1_single),\n",
    "        \"F1_group\":  float(F1_group),\n",
    "        \"F1_group_WG\": float(F1_wg),\n",
    "        \"TP_single\": int(TP_s), \"FP_single\": int(FP_s),\n",
    "        \"FN_single\": int(FN_s), \"TN_single\": int(TN_s),\n",
    "        \"TP_group\": int(TP_g),  \"FP_group\": int(FP_g),\n",
    "        \"FN_group\": int(FN_g),  \"TN_group\": int(TN_g),\n",
    "        \"TP_group_WG\": int(TP_w), \"FP_group_WG\": int(FP_w),\n",
    "        \"FN_group_WG\": int(FN_w), \"TN_group_WG\": int(TN_w),\n",
    "        \"n_test\": int(len(y_te_o)),\n",
    "    })\n",
    "\n",
    "    # 予測詳細（F1専用命名）\n",
    "    for yy, ss, gg, ys, yg, yw in zip(y_te_o, sc_te, grp_te, yhat_single, yhat_groupF1, yhat_wgF1):\n",
    "        pred_rows.append({\n",
    "            \"fold_id\": int(fold_id),\n",
    "            \"test_id\": str(test_sid),\n",
    "            \"y_true\": int(yy),\n",
    "            \"proba\": float(ss),\n",
    "            \"group\": str(gg),\n",
    "            \"y_pred_single\": int(ys),\n",
    "            \"y_pred_group_F1\": int(yg),\n",
    "            \"y_pred_group_WG\": int(yw),\n",
    "        })\n",
    "    \n",
    "    print(f\"[DBG] fold{fold_id}: WG-joint val -> \"\n",
    "          f\"F1_H={res_wg['F1_H_val']:.3f}, F1_L={res_wg['F1_L_val']:.3f}, \"\n",
    "          f\"WG(F1)={res_wg['WG_F1_val']:.3f}, pooled(F1)={res_wg['F1_pooled_val']:.3f}, \"\n",
    "          f\"(tH,tL)=({tauH_wg:.4f},{tauL_wg:.4f})\")\n",
    "\n",
    "# ---------------- 出力（fold別・全予測・サマリ） ----------------\n",
    "df_fold = pd.DataFrame(rows)\n",
    "df_pred = pd.DataFrame(pred_rows)\n",
    "\n",
    "df_fold.to_csv(groupaware_out(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\"),\n",
    "               index=False, encoding=\"utf-8-sig\")\n",
    "df_pred.to_csv(groupaware_out(\"GROUP_AWARE_PREDICTIONS.CSV\"),\n",
    "               index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 参考: プールAUC（確率は共通なので1つ）\n",
    "auc_pool = float(roc_auc_score(df_pred[\"y_true\"].to_numpy(),\n",
    "                               df_pred[\"proba\"].to_numpy()))\n",
    "\n",
    "def _pooled_f1(col):\n",
    "    y_true = df_pred[\"y_true\"].to_numpy()\n",
    "    yhat   = df_pred[col].to_numpy().astype(int)\n",
    "    return _f1_binary(y_true, yhat)\n",
    "\n",
    "summary = {\n",
    "    \"best_k\": int(best_k),\n",
    "    \"AUC_pooled\": auc_pool,\n",
    "    \"F1_pooled_single\": _pooled_f1(\"y_pred_single\"),\n",
    "    \"F1_pooled_group\":  _pooled_f1(\"y_pred_group_F1\"),\n",
    "    \"F1_pooled_group_WG\": _pooled_f1(\"y_pred_group_WG\"),\n",
    "    \"metric\": \"F1\",\n",
    "    \"n_samples\": int(len(df_pred)),\n",
    "    \"n_pos\": int((df_pred[\"y_true\"]==1).sum()),\n",
    "    \"n_neg\": int((df_pred[\"y_true\"]==0).sum()),\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(groupaware_out(\"GROUP_AWARE_SUMMARY.CSV\"),\n",
    "                               index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell5A] Done: outer folds={len(df_fold)}, pooled AUC={auc_pool:.3f}\")\n",
    "print(f\"[Cell5A] Summary: {summary}\")\n",
    "\n",
    "# ---------------- Fold別 F1+TP/FP/FN/TN CSV（3アルゴリズム別） ----------------\n",
    "# Single τ\n",
    "df_single = df_fold[[\n",
    "    \"fold_id\",\"test_id\",\"n_test\",\n",
    "    \"TP_single\",\"FP_single\",\"FN_single\",\"TN_single\",\"F1_single\"\n",
    "]].rename(columns={\n",
    "    \"TP_single\":\"TP\", \"FP_single\":\"FP\",\n",
    "    \"FN_single\":\"FN\", \"TN_single\":\"TN\",\n",
    "    \"F1_single\":\"F1\"\n",
    "})\n",
    "df_single.to_csv(groupaware_out(\"F1_BY_FOLD_SINGLE.CSV\"),\n",
    "                 index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Group-F1 τ\n",
    "df_group = df_fold[[\n",
    "    \"fold_id\",\"test_id\",\"n_test\",\n",
    "    \"TP_group\",\"FP_group\",\"FN_group\",\"TN_group\",\"F1_group\"\n",
    "]].rename(columns={\n",
    "    \"TP_group\":\"TP\", \"FP_group\":\"FP\",\n",
    "    \"FN_group\":\"FN\", \"TN_group\":\"TN\",\n",
    "    \"F1_group\":\"F1\"\n",
    "})\n",
    "df_group.to_csv(groupaware_out(\"F1_BY_FOLD_GROUPF1.CSV\"),\n",
    "                index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# WG-F1 τ\n",
    "df_wg = df_fold[[\n",
    "    \"fold_id\",\"test_id\",\"n_test\",\n",
    "    \"TP_group_WG\",\"FP_group_WG\",\"FN_group_WG\",\"TN_group_WG\",\"F1_group_WG\"\n",
    "]].rename(columns={\n",
    "    \"TP_group_WG\":\"TP\", \"FP_group_WG\":\"FP\",\n",
    "    \"FN_group_WG\":\"FN\", \"TN_group_WG\":\"TN\",\n",
    "    \"F1_group_WG\":\"F1\"\n",
    "})\n",
    "df_wg.to_csv(groupaware_out(\"F1_BY_FOLD_WGF1.CSV\"),\n",
    "             index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[Cell5A] Fold-wise F1 & confusion counts saved -> F1_BY_FOLD_*.CSV\")\n",
    "\n",
    "# ---------------- 混同行列プロット（F1専用ラベル） ----------------\n",
    "def _draw_cm_on_ax(ax, cm, title):\n",
    "    im = ax.imshow(cm, cmap=\"Blues\", vmin=0, vmax=max(cm.max(),1))\n",
    "    labels = np.array([[\"TN\",\"FP\"],[\"FN\",\"TP\"]])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = int(cm[i,j])\n",
    "            color = \"white\" if val > 0.6*im.get_clim()[1] else \"black\"\n",
    "            ax.text(j, i, f\"{labels[i,j]}\\n{val}\", ha=\"center\", va=\"center\",\n",
    "                    fontsize=18, fontweight=\"bold\", color=color)\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Pred:0\",\"Pred:1\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"True:0\",\"True:1\"], rotation=90, va=\"center\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(False)\n",
    "\n",
    "def _draw_cm(cm, title, path):\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    _draw_cm_on_ax(ax, cm, title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# プール全体の混同行列（3モード）\n",
    "y_pool = df_pred[\"y_true\"].to_numpy()\n",
    "cm_single = skm.confusion_matrix(y_pool, df_pred[\"y_pred_single\"],    labels=[0,1])\n",
    "cm_group  = skm.confusion_matrix(y_pool, df_pred[\"y_pred_group_F1\"],  labels=[0,1])\n",
    "cm_wg     = skm.confusion_matrix(y_pool, df_pred[\"y_pred_group_WG\"],  labels=[0,1])\n",
    "\n",
    "# 単品 PNG\n",
    "_draw_cm(cm_single,\n",
    "         f\"Cell5A Single τ (F1={summary['F1_pooled_single']:.3f})\",\n",
    "         cell5_out(\"CONFMAT_CELL5A_SINGLE(F1).png\"))\n",
    "_draw_cm(cm_group,\n",
    "         f\"Cell5A Group-F1 τ (F1={summary['F1_pooled_group']:.3f})\",\n",
    "         cell5_out(\"CONFMAT_CELL5A_GROUP(F1).png\"))\n",
    "_draw_cm(cm_wg,\n",
    "         f\"Cell5A WG-F1 τ (F1={summary['F1_pooled_group_WG']:.3f})\",\n",
    "         cell5_out(\"CONFMAT_CELL5A_WG(F1).png\"))\n",
    "\n",
    "# （１）3モード（Single / Group-F1 / WG-F1）の混同行列を横並び\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "_draw_cm_on_ax(axes[0], cm_single,\n",
    "               f\"Single τ (F1={summary['F1_pooled_single']:.3f})\")\n",
    "_draw_cm_on_ax(axes[1], cm_group,\n",
    "               f\"Group-F1 τ (F1={summary['F1_pooled_group']:.3f})\")\n",
    "_draw_cm_on_ax(axes[2], cm_wg,\n",
    "               f\"WG-F1 τ (F1={summary['F1_pooled_group_WG']:.3f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cell5_out(\"CONFMAT_CELL5A_3MODES(F1).png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# （２）Group-F1 τ: 全体・Highのみ・Lowのみの3つを横並び\n",
    "maskH = (df_pred[\"group\"] == \"High\")\n",
    "maskL = (df_pred[\"group\"] == \"Low\")\n",
    "\n",
    "cm_grp_pool = cm_group\n",
    "cm_grp_H    = skm.confusion_matrix(\n",
    "    df_pred.loc[maskH, \"y_true\"].to_numpy(),\n",
    "    df_pred.loc[maskH, \"y_pred_group_F1\"].to_numpy(),\n",
    "    labels=[0,1]\n",
    ")\n",
    "cm_grp_L    = skm.confusion_matrix(\n",
    "    df_pred.loc[maskL, \"y_true\"].to_numpy(),\n",
    "    df_pred.loc[maskL, \"y_pred_group_F1\"].to_numpy(),\n",
    "    labels=[0,1]\n",
    ")\n",
    "\n",
    "F1_grp_pool = _f1_binary(df_pred[\"y_true\"].to_numpy(),\n",
    "                         df_pred[\"y_pred_group_F1\"].to_numpy())\n",
    "F1_grp_H    = _f1_binary(df_pred.loc[maskH, \"y_true\"].to_numpy(),\n",
    "                         df_pred.loc[maskH, \"y_pred_group_F1\"].to_numpy())\n",
    "F1_grp_L    = _f1_binary(df_pred.loc[maskL, \"y_true\"].to_numpy(),\n",
    "                         df_pred.loc[maskL, \"y_pred_group_F1\"].to_numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "_draw_cm_on_ax(axes[0], cm_grp_pool,\n",
    "               f\"Group-F1 τ / All (F1={F1_grp_pool:.3f})\")\n",
    "_draw_cm_on_ax(axes[1], cm_grp_H,\n",
    "               f\"Group-F1 τ / High (F1={F1_grp_H:.3f})\")\n",
    "_draw_cm_on_ax(axes[2], cm_grp_L,\n",
    "               f\"Group-F1 τ / Low (F1={F1_grp_L:.3f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cell5_out(\"CONFMAT_CELL5A_GROUP(F1)_HIGHLOW.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# （３）WG-F1 τ: 全体・Highのみ・Lowのみの3つを横並び\n",
    "cm_wg_pool = cm_wg\n",
    "cm_wg_H    = skm.confusion_matrix(\n",
    "    df_pred.loc[maskH, \"y_true\"].to_numpy(),\n",
    "    df_pred.loc[maskH, \"y_pred_group_WG\"].to_numpy(),\n",
    "    labels=[0,1]\n",
    ")\n",
    "cm_wg_L    = skm.confusion_matrix(\n",
    "    df_pred.loc[maskL, \"y_true\"].to_numpy(),\n",
    "    df_pred.loc[maskL, \"y_pred_group_WG\"].to_numpy(),\n",
    "    labels=[0,1]\n",
    ")\n",
    "\n",
    "F1_wg_pool = _f1_binary(df_pred[\"y_true\"].to_numpy(),\n",
    "                        df_pred[\"y_pred_group_WG\"].to_numpy())\n",
    "F1_wg_H    = _f1_binary(df_pred.loc[maskH, \"y_true\"].to_numpy(),\n",
    "                        df_pred.loc[maskH, \"y_pred_group_WG\"].to_numpy())\n",
    "F1_wg_L    = _f1_binary(df_pred.loc[maskL, \"y_true\"].to_numpy(),\n",
    "                        df_pred.loc[maskL, \"y_pred_group_WG\"].to_numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "_draw_cm_on_ax(axes[0], cm_wg_pool,\n",
    "               f\"WG-F1 τ / All (F1={F1_wg_pool:.3f})\")\n",
    "_draw_cm_on_ax(axes[1], cm_wg_H,\n",
    "               f\"WG-F1 τ / High (F1={F1_wg_H:.3f})\")\n",
    "_draw_cm_on_ax(axes[2], cm_wg_L,\n",
    "               f\"WG-F1 τ / Low (F1={F1_wg_L:.3f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cell5_out(\"CONFMAT_CELL5A_WG(F1)_HIGHLOW.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"[Cell5A] Confusion matrices saved in Cell5_F1 -> CONFMAT_CELL5A_*.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5B++: 確率スコア分布（OVERALL/群別/各Fold）＋ τ の中央値/IQR 帯・±表記 =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 設定 ----------\n",
    "BINS = 40\n",
    "LW = 1.5\n",
    "FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "\n",
    "# 色指定（ユーザ指定）\n",
    "COLOR_SICK = \"red\"   # True:Sick\n",
    "COLOR_NON  = \"blue\"  # True:Non-Sick\n",
    "\n",
    "# 線色（しきい値）\n",
    "COLOR_SINGLE = \"black\"\n",
    "COLOR_GROUP  = \"green\"\n",
    "COLOR_WG     = \"purple\"\n",
    "\n",
    "# 出力フォルダ（新規作成）\n",
    "RUN_ROOT = os.path.dirname(outpath(\"__dummy__\"))\n",
    "IMG_DIR  = os.path.join(RUN_ROOT, \"PROBA_DIST_F1\")\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "# ファイルパス\n",
    "SAVE_OVERALL_SvG   = os.path.join(IMG_DIR, \"OVERALL_SvGroup.png\")\n",
    "SAVE_OVERALL_SvWG  = os.path.join(IMG_DIR, \"OVERALL_SvWG.png\")\n",
    "SAVE_BYGROUP_SvG   = os.path.join(IMG_DIR, \"BYGROUP_SvGroup.png\")\n",
    "SAVE_BYGROUP_SvWG  = os.path.join(IMG_DIR, \"BYGROUP_SvWG.png\")\n",
    "\n",
    "# ---------- 入力 ----------\n",
    "GROUP_AWARE_DIR = os.path.join(OUT_DIR,\"Cell5_F1\",\"GROUP_AWARE\", \"F1\")\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_path(filename: str) -> str:\n",
    "    return os.path.join(GROUP_AWARE_DIR, filename)\n",
    "\n",
    "pred_path = groupaware_path(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "fold_path = groupaware_path(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "if not (os.path.exists(pred_path) and os.path.exists(fold_path)):\n",
    "    raise FileNotFoundError(\"[Cell5B++] 必要CSVが見つからない（F1版 Cell 5A を先に実行）\")\n",
    "\n",
    "df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- モード自動判定（F1 or BA） ----------\n",
    "cols_f1 = {\"high\":\"tau_high_GroupF1\", \"low\":\"tau_low_GroupF1\", \"wgh\":\"tau_high_WGF1\", \"wgl\":\"tau_low_WGF1\"}\n",
    "cols_ba = {\"high\":\"tau_high_GroupBA\", \"low\":\"tau_low_GroupBA\", \"wgh\":\"tau_high_WGBA\", \"wgl\":\"tau_low_WGBA\"}\n",
    "\n",
    "if all(c in df_fold.columns for c in [cols_f1[\"high\"], cols_f1[\"low\"]]):\n",
    "    mode = \"F1\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_f1[\"high\"], cols_f1[\"low\"], cols_f1[\"wgh\"], cols_f1[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_F1\"\n",
    "elif all(c in df_fold.columns for c in [cols_ba[\"high\"], cols_ba[\"low\"]]):\n",
    "    mode = \"BA\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_ba[\"high\"], cols_ba[\"low\"], cols_ba[\"wgh\"], cols_ba[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_BA\"\n",
    "else:\n",
    "    raise RuntimeError(\"[Cell5B++] しきい値列が見つからない（F1/BAどちらかのCell 5Aの出力が必要）\")\n",
    "\n",
    "# ---------- 集約: 中央値/IQR（Q1〜Q3） ----------\n",
    "def _qstats(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.size == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan  # med, q1, q3, half_iqr\n",
    "    med = float(np.nanmedian(s))\n",
    "    q1, q3 = np.nanpercentile(s, [25, 75])\n",
    "    half = float((q3 - q1)/2.0)\n",
    "    return float(med), float(q1), float(q3), half\n",
    "\n",
    "tau_single_med, tau_single_q1, tau_single_q3, tau_single_half = _qstats(df_fold[\"tau_single\"])\n",
    "tau_high_med,   tau_high_q1,   tau_high_q3,   tau_high_half   = _qstats(df_fold[c_high])\n",
    "tau_low_med,    tau_low_q1,    tau_low_q3,    tau_low_half    = _qstats(df_fold[c_low])\n",
    "tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, tau_high_wg_half = _qstats(df_fold[c_wgh]) if c_wgh in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  tau_low_wg_half  = _qstats(df_fold[c_wgl]) if c_wgl in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "# ---------- データ分解 ----------\n",
    "proba = pd.to_numeric(df_pred[\"proba\"], errors=\"coerce\").values\n",
    "ytrue = pd.to_numeric(df_pred[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "grp   = df_pred[\"group\"].astype(str).str.strip()\n",
    "\n",
    "p_sick = proba[ytrue == 1]   # True:Sick\n",
    "p_non  = proba[ytrue == 0]   # True:Non-Sick\n",
    "n_sick, n_non = len(p_sick), len(p_non)\n",
    "\n",
    "maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "\n",
    "# ---------- ユーティリティ ----------\n",
    "def _style_axes(ax, title=None):\n",
    "    if title: ax.set_title(title, fontsize=FS_TITLE)\n",
    "    ax.set_xlabel(\"Predicted probability\", fontsize=FS_LABEL)\n",
    "    ax.set_ylabel(\"Density\", fontsize=FS_LABEL)\n",
    "    ax.tick_params(axis=\"both\", labelsize=FS_TICK)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "def _hist_overall(ax):\n",
    "    ax.hist(p_sick, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={n_sick})\", color=COLOR_SICK)\n",
    "    ax.hist(p_non,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={n_non})\", color=COLOR_NON)\n",
    "\n",
    "def _hist_bygroup(axes):\n",
    "    # High\n",
    "    p_sick_H = proba[(ytrue==1) & maskH]; p_non_H = proba[(ytrue==0) & maskH]\n",
    "    axes[0].hist(p_sick_H, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_H)})\", color=COLOR_SICK)\n",
    "    axes[0].hist(p_non_H,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_H)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[0], \"High group\")\n",
    "    # Low\n",
    "    p_sick_L = proba[(ytrue==1) & maskL]; p_non_L = proba[(ytrue==0) & maskL]\n",
    "    axes[1].hist(p_sick_L, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_L)})\", color=COLOR_SICK)\n",
    "    axes[1].hist(p_non_L,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_L)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[1], \"Low group\")\n",
    "\n",
    "def _vline_with_iqr(ax, x_med, q1, q3, color, ls, label_core):\n",
    "    if np.isfinite(x_med):\n",
    "        ax.axvline(x_med, color=color, linestyle=ls, linewidth=LW,\n",
    "                   label=f\"{label_core} = {x_med:.3f} ± {(q3-q1)/2:.3f}\" if (np.isfinite(q1) and np.isfinite(q3)) else f\"{label_core} = {x_med:.3f}\")\n",
    "    if np.isfinite(q1) and np.isfinite(q3):\n",
    "        ax.axvspan(q1, q3, color=color, alpha=0.12)\n",
    "\n",
    "# ---------- 1) OVERALL: Single vs Group（中央値・IQR帯・±表記） ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_med,   tau_high_q1,   tau_high_q3,   COLOR_GROUP,  \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(ax, tau_low_med,    tau_low_q1,    tau_low_q3,    COLOR_GROUP,  \"--\", f\"τ_low_{mode}\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs Group [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvG}\")\n",
    "\n",
    "# ---------- 2) OVERALL: Single vs WG（中央値・IQR帯・±表記） ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(ax, tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs WG [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvWG}\")\n",
    "\n",
    "# ---------- 3) BY_GROUP: Single vs Group（各群パネルに該当の IQR帯） ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "# Single は両段に表示\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "# High 段に High の Group τ、Low 段に Low の Group τ\n",
    "_vline_with_iqr(axes[0], tau_high_med, tau_high_q1, tau_high_q3, COLOR_GROUP, \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(axes[1], tau_low_med,  tau_low_q1,  tau_low_q3,  COLOR_GROUP, \"--\", f\"τ_low_{mode}\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvG}\")\n",
    "\n",
    "# ---------- 4) BY_GROUP: Single vs WG（各群パネルに該当の IQR帯） ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[0], tau_high_wg, tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(axes[1], tau_low_wg,  tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvWG}\")\n",
    "\n",
    "# ---------- 5) Fold単位：OVERALL の確率分布と各Foldの τ（Single vs Group / Single vs WG） ----------\n",
    "#   - 各Foldのテストサンプルのみでヒストを作図\n",
    "#   - しきい値は「そのFold行」の値を直接表示（±は不要／IQR帯は使わない）\n",
    "for _, row in df_fold.iterrows():\n",
    "    fid = int(row[\"fold_id\"]) if \"fold_id\" in row else None\n",
    "    test_id = str(row.get(\"test_id\", f\"fold{fid}\"))\n",
    "    sub = df_pred[df_pred[\"fold_id\"] == fid] if \"fold_id\" in df_pred.columns and fid is not None else df_pred.copy()\n",
    "\n",
    "    p = pd.to_numeric(sub[\"proba\"], errors=\"coerce\").values\n",
    "    yt = pd.to_numeric(sub[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "    p_s, p_n = p[yt==1], p[yt==0]\n",
    "\n",
    "    # 値（このFoldの τ）\n",
    "    t_single = float(row[\"tau_single\"])\n",
    "    t_high   = float(row[c_high]) if c_high in row else np.nan\n",
    "    t_low    = float(row[c_low])  if c_low  in row else np.nan\n",
    "    t_high_w = float(row[c_wgh])  if c_wgh  in row else np.nan\n",
    "    t_low_w  = float(row[c_wgl])  if c_wgl  in row else np.nan\n",
    "\n",
    "    # 出力パス\n",
    "    p_sg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvGroup.png\")\n",
    "    p_wg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvWG.png\")\n",
    "\n",
    "    # Single vs Group\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high):   ax.axvline(t_high,   color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_high_{mode} = {t_high:.3f}\")\n",
    "    if np.isfinite(t_low):    ax.axvline(t_low,    color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_low_{mode}  = {t_low:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs Group [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_sg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_sg}\")\n",
    "\n",
    "    # Single vs WG\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high_w): ax.axvline(t_high_w, color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_high_WG = {t_high_w:.3f}\")\n",
    "    if np.isfinite(t_low_w):  ax.axvline(t_low_w,  color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_low_WG  = {t_low_w:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs WG [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_wg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_wg}\")\n",
    "\n",
    "print(f\"[Cell5B++] All images saved in: {IMG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5A (FINAL, BA専用): inner-LOSO τ最適化（連結val）→ outer予測・評価 =====\n",
    "# 前提: Cell1〜4 実行済み（X_all, y_all, groups, SUBJECT_META, choose_inner_folds_loso 等）\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- 出力ディレクトリ: Cell5_BA 配下に集約 ----------------\n",
    "CELL5_DIR = os.path.join(OUT_DIR, \"Cell5_BA\")\n",
    "os.makedirs(CELL5_DIR, exist_ok=True)\n",
    "\n",
    "def cell5_out(filename: str) -> str:\n",
    "    \"\"\"Cell5_BA 配下への汎用出力パス\"\"\"\n",
    "    path = os.path.join(CELL5_DIR, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ---------------- GROUP_AWARE 出力先（BA） ----------------\n",
    "MODE_TAG = \"BA\"\n",
    "GROUP_AWARE_DIR = os.path.join(CELL5_DIR, \"GROUP_AWARE\", MODE_TAG)\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_out(filename: str) -> str:\n",
    "    path = os.path.join(GROUP_AWARE_DIR, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ---------------- 群分け基準（MSSQ / VIMSSQ 切り替え） ----------------\n",
    "# 優先順位:\n",
    "#  1) GROUPING_BASIS_FOR_FAIRNESS があればそれを使う\n",
    "#  2) なければ GROUPING_BASIS_FOR_PLOTS を流用\n",
    "#  3) どちらもなければ MSSQ をデフォルト\n",
    "GROUPING_BASIS_FOR_FAIRNESS = globals().get(\n",
    "    \"GROUPING_BASIS_FOR_FAIRNESS\",\n",
    "    globals().get(\"GROUPING_BASIS_FOR_PLOTS\", \"MSSQ\")\n",
    ")\n",
    "basis = str(GROUPING_BASIS_FOR_FAIRNESS).upper()\n",
    "\n",
    "if basis == \"MSSQ\":\n",
    "    GROUP_COL_NAME = \"MSSQ_group\"\n",
    "elif basis == \"VIMSSQ\":\n",
    "    GROUP_COL_NAME = \"VIMSSQ_group\"\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"[Cell5A-BA] GROUPING_BASIS_FOR_FAIRNESS は 'MSSQ' か 'VIMSSQ' を指定してください（今: {GROUPING_BASIS_FOR_FAIRNESS}）\"\n",
    "    )\n",
    "\n",
    "print(f\"[Cell5A-BA] GROUPING_BASIS_FOR_FAIRNESS = {GROUPING_BASIS_FOR_FAIRNESS} -> group_col = {GROUP_COL_NAME}\")\n",
    "\n",
    "# ---------------- 基本チェック ----------------\n",
    "req = [\"X_all\",\"y_all\",\"groups\",\"SUBJECT_META\",\n",
    "       \"choose_inner_folds_loso\",\"fit_classifier\",\"predict_positive_score\",\"outpath\"]\n",
    "missing = [v for v in req if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell5A-BA] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "if GROUP_COL_NAME not in SUBJECT_META.columns and GROUP_COL_NAME not in SUBJECT_META.index.names:\n",
    "    raise RuntimeError(f\"[Cell5A-BA] SUBJECT_META に {GROUP_COL_NAME} 列（または index）が必要である．\")\n",
    "\n",
    "# ---------------- 入力整形 ----------------\n",
    "X_base = X_all.astype(np.float32)\n",
    "y_base = y_all.astype(int)\n",
    "g_base = groups.astype(str)\n",
    "\n",
    "# 群ラベル（MSSQ_group / VIMSSQ_group）マッピング → High / Low に正規化\n",
    "if \"subject_id\" in SUBJECT_META.columns:\n",
    "    mapper = SUBJECT_META.set_index(\"subject_id\")[GROUP_COL_NAME].astype(str).to_dict()\n",
    "else:\n",
    "    # index が subject_id になっている想定\n",
    "    mapper = SUBJECT_META[GROUP_COL_NAME].astype(str).to_dict()\n",
    "\n",
    "fair_groups_raw = g_base.map(mapper)\n",
    "\n",
    "if fair_groups_raw.isna().any():\n",
    "    raise RuntimeError(\n",
    "        f\"[Cell5A-BA] {GROUP_COL_NAME} 未割当ID: {sorted(set(g_base[fair_groups_raw.isna()]))}\"\n",
    "    )\n",
    "\n",
    "fair_groups = (fair_groups_raw\n",
    "               .astype(str)\n",
    "               .str.strip()\n",
    "               .str.lower()\n",
    "               .map({\"high\": \"High\", \"low\": \"Low\"}))\n",
    "\n",
    "if fair_groups.isna().any():\n",
    "    # High/Low 以外のラベルが混じっているケース\n",
    "    bad_labels = sorted(set(fair_groups_raw[~fair_groups_raw.isin(\n",
    "        [\"High\",\"Low\",\"high\",\"low\"]\n",
    "    )]))\n",
    "    raise RuntimeError(\n",
    "        f\"[Cell5A-BA] {GROUP_COL_NAME} に 'High'/'Low' 以外のラベルが含まれている: {bad_labels}\"\n",
    "    )\n",
    "\n",
    "# ===== デバッグ: 群分類のプリント =====\n",
    "if \"subject_id\" in SUBJECT_META.columns:\n",
    "    dbg_meta = SUBJECT_META[[\"subject_id\", GROUP_COL_NAME]].copy()\n",
    "else:\n",
    "    # index 名が 'subject_id' である前提（そうでない場合は適宜修正）\n",
    "    dbg_meta = SUBJECT_META.reset_index()[[\"subject_id\", GROUP_COL_NAME]].copy()\n",
    "\n",
    "dbg_meta = dbg_meta.sort_values([GROUP_COL_NAME, \"subject_id\"])\n",
    "\n",
    "print(f\"\\n[Cell5A-BA-DEBUG] SUBJECT_META における {GROUP_COL_NAME} 分類一覧\")\n",
    "print(dbg_meta.to_string(index=False))\n",
    "\n",
    "print(f\"\\n[Cell5A-BA-DEBUG] {GROUP_COL_NAME} 件数 (SUBJECT_META 基準):\",\n",
    "      dbg_meta[GROUP_COL_NAME].value_counts().to_dict())\n",
    "\n",
    "print(\"\\n[Cell5A-BA-DEBUG] fair_groups 件数 (エポックベース):\",\n",
    "      fair_groups.value_counts().to_dict())\n",
    "\n",
    "# ---------------- 特徴選抜（Cell3A-Subset の JSON のみ使用＋MSSQ/VIMSSQオプション） ----------------\n",
    "subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "subset_candidates = [subset_primary, \"TOP10_SUBSET_BEST.json\"]\n",
    "subset_json = None\n",
    "for name in subset_candidates:\n",
    "    cand = outpath(name)\n",
    "    if os.path.exists(cand):\n",
    "        subset_json = cand\n",
    "        break\n",
    "\n",
    "if subset_json is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"[Cell5A-BA] TOP*_SUBSET_BEST.json が見つからない．Cell3A-Subset を実行すること．\"\n",
    "    )\n",
    "\n",
    "with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    subset_info = json.load(f)\n",
    "\n",
    "raw_features = subset_info.get(\"features\", [])\n",
    "if not raw_features:\n",
    "    raise RuntimeError(f\"[Cell5A-BA] JSON 内に 'features' が空です -> {os.path.basename(subset_json)}\")\n",
    "\n",
    "# X_all に実在する特徴だけに絞る\n",
    "feature_order = [f for f in raw_features if f in X_base.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\n",
    "        f\"[Cell5A-BA] JSON の features が X_all に1つも存在しません: {raw_features}\"\n",
    "    )\n",
    "\n",
    "# MSSQ / VIMSSQ を任意で追加\n",
    "extra_traits = []\n",
    "if globals().get(\"USE_MSSQ_FEATURE\", False) and \"MSSQ\" in X_base.columns:\n",
    "    extra_traits.append(\"MSSQ\")\n",
    "if globals().get(\"USE_VIMSSQ_FEATURE\", False) and \"VIMSSQ\" in X_base.columns:\n",
    "    extra_traits.append(\"VIMSSQ\")\n",
    "\n",
    "# 既に含まれていないものだけ追加\n",
    "extra_traits = [f for f in extra_traits if f not in feature_order]\n",
    "\n",
    "feats_k = feature_order + extra_traits\n",
    "best_k = len(feats_k)\n",
    "\n",
    "print(f\"[Cell5A-BA] Using subset features from {os.path.basename(subset_json)}\")\n",
    "print(f\"[Cell5A-BA] JSON features (base) k={len(feature_order)}: {feature_order}\")\n",
    "if extra_traits:\n",
    "    print(f\"[Cell5A-BA] 追加で使用する属性特徴: {extra_traits}\")\n",
    "print(f\"[Cell5A-BA] 最終的に使用する特徴数 = {best_k}\")\n",
    "\n",
    "X_k = X_base[feats_k]\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"[DBG-BA] 使用特徴数: {len(feats_k)}\")\n",
    "    print(\"[DBG-BA] 使用特徴一覧 (feats_k):\")\n",
    "    for i, f_name in enumerate(feats_k, start=1):\n",
    "        print(f\"  {i:2d}: {f_name}\")\n",
    "\n",
    "# ---------------- 指標・評価ユーティリティ（BA専用） ----------------\n",
    "def _conf_from_preds(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    TN, FP, FN, TP = skm.confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _ba_from_conf(TP, FP, FN, TN) -> float:\n",
    "    TP = float(TP); FP = float(FP); FN = float(FN); TN = float(TN)\n",
    "    tpr = TP / (TP + FN) if (TP + FN) > 0 else 0.0  # Sensitivity/TPR\n",
    "    tnr = TN / (TN + FP) if (TN + FP) > 0 else 0.0  # Specificity/TNR\n",
    "    return 0.5 * (tpr + tnr)\n",
    "\n",
    "def _ba_binary(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    TP, FP, FN, TN = _conf_from_preds(y_true, y_pred)\n",
    "    return _ba_from_conf(TP, FP, FN, TN)\n",
    "\n",
    "def _grid(l, r, steps):\n",
    "    l = float(max(0.0, l)); r = float(min(1.0, r))\n",
    "    if l > r: l, r = r, l\n",
    "    return np.linspace(l, r, int(steps), dtype=float)\n",
    "\n",
    "# ---------------- τ最適化（Single / WG-BA / Group-BA） ----------------\n",
    "def _single_tau_opt(scores: np.ndarray, y: np.ndarray):\n",
    "    # coarse\n",
    "    cands = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    ba_vec = []\n",
    "    for t in cands:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        ba_vec.append(_ba_binary(y, yhat))\n",
    "    ba_vec = np.asarray(ba_vec)\n",
    "    idx = int(np.nanargmax(ba_vec)); tau0 = float(cands[idx]); best0 = float(ba_vec[idx])\n",
    "\n",
    "    # fine around tau0\n",
    "    left  = max(0.0, tau0 - FINE_MARGIN)\n",
    "    right = min(1.0, tau0 + FINE_MARGIN)\n",
    "    cands2 = _grid(left, right, FINE_STEPS)\n",
    "    ba_vec2 = []\n",
    "    for t in cands2:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        ba_vec2.append(_ba_binary(y, yhat))\n",
    "    ba_vec2 = np.asarray(ba_vec2)\n",
    "    idx2 = int(np.nanargmax(ba_vec2)); tau = float(cands2[idx2]); best = float(ba_vec2[idx2])\n",
    "\n",
    "    return {\"tau\": tau, \"BA_val\": best, \"tau_coarse\": tau0, \"BA_coarse\": best0}\n",
    "\n",
    "def _wg_ba_opt_joint(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    \"\"\"\n",
    "    Fair-MinBA（最悪群BAの最大化）を 2Dグリッド（τH×τL）で探索する．\n",
    "    戻り値: {\"tauH\",\"tauL\",\"BA_H_val\",\"BA_L_val\",\"WG_BA_val\",\"BA_pooled_val\"}\n",
    "    \"\"\"\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[WG-BA] 連結valに High/Low の両群が必要（どちらかが0件）\")\n",
    "\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "\n",
    "    def _ba_vec(s, yy, cands):\n",
    "        ba = np.empty_like(cands)\n",
    "        for i, t in enumerate(cands):\n",
    "            yhat = (s >= t).astype(int)\n",
    "            ba[i] = _ba_binary(yy, yhat)\n",
    "        return ba\n",
    "\n",
    "    baH = _ba_vec(sH, yH, candH)\n",
    "    baL = _ba_vec(sL, yL, candL)\n",
    "\n",
    "    best = {\"WG\": -np.inf, \"pooled\": -np.inf, \"tH\": 0.5, \"tL\": 0.5, \"BAH\": 0.0, \"BAL\": 0.0}\n",
    "    for i, tH in enumerate(candH):\n",
    "        wg_row = np.minimum(baH[i], baL)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL[j]).astype(int)\n",
    "        BA_pooled = _ba_binary(\n",
    "            np.concatenate([yH, yL]),\n",
    "            np.concatenate([yhatH, yhatL])\n",
    "        )\n",
    "\n",
    "        cand = {\"WG\": WG, \"pooled\": float(BA_pooled),\n",
    "                \"tH\": float(tH), \"tL\": float(candL[j]),\n",
    "                \"BAH\": float(baH[i]), \"BAL\": float(baL[j])}\n",
    "\n",
    "        # tie-break: pooled BA → |τH-τL| 小 → (τH,τL) 辞書順小\n",
    "        def _is_better(cur, new):\n",
    "            if new[\"WG\"] > cur[\"WG\"]: return True\n",
    "            if new[\"WG\"] < cur[\"WG\"]: return False\n",
    "            if new[\"pooled\"] > cur[\"pooled\"]: return True\n",
    "            if new[\"pooled\"] < cur[\"pooled\"]: return False\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) < abs(cur[\"tH\"]-cur[\"tL\"]): return True\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) > abs(cur[\"tH\"]-cur[\"tL\"]): return False\n",
    "            if (new[\"tH\"], new[\"tL\"]) < (cur[\"tH\"], cur[\"tL\"]): return True\n",
    "            return False\n",
    "\n",
    "        if _is_better(best, cand):\n",
    "            best = cand\n",
    "\n",
    "    # fine (box refine)\n",
    "    lH = max(0.0, best[\"tH\"] - FINE_MARGIN); rH = min(1.0, best[\"tH\"] + FINE_MARGIN)\n",
    "    lL = max(0.0, best[\"tL\"] - FINE_MARGIN); rL = min(1.0, best[\"tL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS)\n",
    "    candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "\n",
    "    baH2 = _ba_vec(sH, yH, candH2)\n",
    "    baL2 = _ba_vec(sL, yL, candL2)\n",
    "\n",
    "    best2 = dict(best)\n",
    "    for i, tH in enumerate(candH2):\n",
    "        wg_row = np.minimum(baH2[i], baL2)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL2[j]).astype(int)\n",
    "        BA_pooled = _ba_binary(\n",
    "            np.concatenate([yH, yL]),\n",
    "            np.concatenate([yhatH, yhatL])\n",
    "        )\n",
    "        cand = {\"WG\": WG, \"pooled\": float(BA_pooled),\n",
    "                \"tH\": float(tH), \"tL\": float(candL2[j]),\n",
    "                \"BAH\": float(baH2[i]), \"BAL\": float(baL2[j])}\n",
    "\n",
    "        if (cand[\"WG\"] > best2[\"WG\"] or\n",
    "            (cand[\"WG\"] == best2[\"WG\"] and (\n",
    "                cand[\"pooled\"] > best2[\"pooled\"] or\n",
    "                (cand[\"pooled\"] == best2[\"pooled\"] and (\n",
    "                    abs(cand[\"tH\"]-cand[\"tL\"]) < abs(best2[\"tH\"]-best2[\"tL\"]) or\n",
    "                    (abs(cand[\"tH\"]-cand[\"tL\"]) == abs(best2[\"tH\"]-best2[\"tL\"]) and\n",
    "                     (cand[\"tH\"], cand[\"tL\"]) < (best2[\"tH\"], best2[\"tL\"]))\n",
    "                ))\n",
    "            ))):\n",
    "            best2 = cand\n",
    "\n",
    "    return {\n",
    "        \"tauH\": best2[\"tH\"], \"tauL\": best2[\"tL\"],\n",
    "        \"BA_H_val\": best2[\"BAH\"], \"BA_L_val\": best2[\"BAL\"],\n",
    "        \"WG_BA_val\": best2[\"WG\"], \"BA_pooled_val\": best2[\"pooled\"],\n",
    "    }\n",
    "\n",
    "def _group_ba_opt(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    # 2Dグリッド（τH×τL）で pooled BA 最大化（coarse→fine）\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[Group-BA] 連結valに High/Low の両群が必要（どちらかが0件）\")\n",
    "\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "\n",
    "    def _pooled_ba_for(tH, tL):\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= tL).astype(int)\n",
    "        y_true = np.concatenate([yH, yL])\n",
    "        y_pred = np.concatenate([yhatH, yhatL])\n",
    "        return _ba_binary(y_true, y_pred)\n",
    "\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    best = {\"BA_val\": -np.inf, \"tauH\": None, \"tauL\": None}\n",
    "    for tH in candH:\n",
    "        ba_vec = np.empty_like(candL)\n",
    "        for j, tL in enumerate(candL):\n",
    "            ba_vec[j] = _pooled_ba_for(tH, tL)\n",
    "        jmax = int(np.nanargmax(ba_vec))\n",
    "        if float(ba_vec[jmax]) > best[\"BA_val\"]:\n",
    "            best.update({\"BA_val\": float(ba_vec[jmax]), \"tauH\": float(tH), \"tauL\": float(candL[jmax])})\n",
    "\n",
    "    # fine\n",
    "    lH = max(0.0, best[\"tauH\"] - FINE_MARGIN); rH = min(1.0, best[\"tauH\"] + FINE_MARGIN)\n",
    "    lL = max(0.0, best[\"tauL\"] - FINE_MARGIN); rL = min(1.0, best[\"tauL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS); candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "\n",
    "    best2 = dict(best)\n",
    "    for tH in candH2:\n",
    "        ba_vec2 = np.empty_like(candL2)\n",
    "        for j, tL in enumerate(candL2):\n",
    "            ba_vec2[j] = _pooled_ba_for(tH, tL)\n",
    "        jmax = int(np.nanargmax(ba_vec2))\n",
    "        if float(ba_vec2[jmax]) > best2[\"BA_val\"]:\n",
    "            best2.update({\"BA_val\": float(ba_vec2[jmax]), \"tauH\": float(tH), \"tauL\": float(candL2[jmax])})\n",
    "\n",
    "    return best2  # {\"tauH\",\"tauL\",\"BA_val\"}\n",
    "\n",
    "# ---------------- outer LOSO with inner concatenation ----------------\n",
    "logo_outer = LeaveOneGroupOut()\n",
    "rows, pred_rows = [], []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo_outer.split(X_k, y_base.values, g_base.values), start=1):\n",
    "    train_mask = pd.Series(False, index=g_base.index); train_mask.iloc[tr_idx] = True\n",
    "    test_mask  = pd.Series(False, index=g_base.index);  test_mask.iloc[te_idx]  = True\n",
    "    test_sid = g_base.iloc[te_idx].iloc[0]\n",
    "\n",
    "    # ===== inner folds =====\n",
    "    inner_ids   = sorted(g_base[train_mask].unique())\n",
    "    inner_folds = choose_inner_folds_loso(inner_ids)\n",
    "    if VERBOSE:\n",
    "        print(f\"[inner folds-BA] {len(inner_folds)} splits -> val subjects = {', '.join(inner_ids)}\")\n",
    "\n",
    "    # 連結valの器\n",
    "    val_scores_all, val_y_all, val_grp_all = [], [], []\n",
    "\n",
    "    # --- inner train に両群が居るか（群別τの前提） ---\n",
    "    inner_train_groups = fair_groups[train_mask]\n",
    "    if not ((\"High\" in set(inner_train_groups)) and (\"Low\" in set(inner_train_groups))):\n",
    "        raise RuntimeError(f\"[Cell5A-BA] fold{fold_id}: inner-train に両群(High/Low)が必要（群別τの前提）\")\n",
    "\n",
    "    # ===== inner: train→val 予測の連結 =====\n",
    "    for inner_val in inner_folds:\n",
    "        val_mask  = g_base.isin(inner_val) & train_mask\n",
    "        trn_mask  = train_mask & (~val_mask)\n",
    "        if not trn_mask.any() or not val_mask.any():\n",
    "            continue\n",
    "\n",
    "        X_tr, y_tr = X_k[trn_mask], y_base[trn_mask]\n",
    "        X_vl, y_vl = X_k[val_mask], y_base[val_mask]\n",
    "        grp_vl     = fair_groups[val_mask].to_numpy()\n",
    "\n",
    "        # inner 学習\n",
    "        model_inner = fit_classifier(X_tr, y_tr)\n",
    "        sc_vl = predict_positive_score(model_inner, X_vl).astype(float)\n",
    "\n",
    "        # 連結（生の val 予測）\n",
    "        val_scores_all.append(sc_vl)\n",
    "        val_y_all.append(y_vl.to_numpy())\n",
    "        val_grp_all.append(grp_vl)\n",
    "\n",
    "        if VERBOSE:\n",
    "            nH = int((grp_vl==\"High\").sum()); nL = int((grp_vl==\"Low\").sum())\n",
    "            print(f\"[DBG-BA] fold{fold_id}: inner_val size={len(y_vl):3d}  High={nH} Low={nL}\")\n",
    "\n",
    "    # 連結\n",
    "    if len(val_scores_all) == 0:\n",
    "        raise RuntimeError(f\"[Cell5A-BA] fold{fold_id}: inner validation が空である．\")\n",
    "    s_val = np.concatenate(val_scores_all)\n",
    "    y_val = np.concatenate(val_y_all)\n",
    "    g_val = np.concatenate(val_grp_all)\n",
    "\n",
    "    # ===== 連結valで一度だけ τ を最適化（BA） =====\n",
    "    res_single = _single_tau_opt(s_val, y_val)\n",
    "    tau_single = float(res_single[\"tau\"])\n",
    "\n",
    "    res_wg = _wg_ba_opt_joint(s_val, y_val, g_val)\n",
    "    tauH_wg, tauL_wg = float(res_wg[\"tauH\"]), float(res_wg[\"tauL\"])\n",
    "\n",
    "    res_group = _group_ba_opt(s_val, y_val, g_val)\n",
    "    tauH_grp, tauL_grp = float(res_group[\"tauH\"]), float(res_group[\"tauL\"])\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"[DBG-BA] fold{fold_id}: tau_single={tau_single:.6f} | \"\n",
    "              f\"Group-BA(tH,tL)=({tauH_grp:.6f},{tauL_grp:.6f}) | \"\n",
    "              f\"WG-BA(tH,tL)=({tauH_wg:.6f},{tauL_wg:.6f})\")\n",
    "\n",
    "    # ===== outer test =====\n",
    "    X_tr_o, y_tr_o = X_k[train_mask], y_base[train_mask]\n",
    "    X_te_o, y_te_o = X_k[test_mask],  y_base[test_mask]\n",
    "    grp_te         = fair_groups[test_mask].to_numpy()\n",
    "\n",
    "    model_outer = fit_classifier(X_tr_o, y_tr_o)\n",
    "    sc_te = predict_positive_score(model_outer, X_te_o).astype(float)\n",
    "\n",
    "    y_true_te = y_te_o.to_numpy()\n",
    "    yhat_single = (sc_te >= tau_single).astype(int)\n",
    "    yhat_groupBA = (sc_te >= np.where(grp_te==\"High\", tauH_grp, tauL_grp)).astype(int)\n",
    "    yhat_wgBA    = (sc_te >= np.where(grp_te==\"High\", tauH_wg,  tauL_wg )).astype(int)\n",
    "\n",
    "    BA_single = _ba_binary(y_true_te, yhat_single)\n",
    "    BA_group  = _ba_binary(y_true_te, yhat_groupBA)\n",
    "    BA_wg     = _ba_binary(y_true_te, yhat_wgBA)\n",
    "\n",
    "    # Fold別 TP/FP/FN/TN（3アルゴリズム）\n",
    "    TP_s, FP_s, FN_s, TN_s = _conf_from_preds(y_true_te, yhat_single)\n",
    "    TP_g, FP_g, FN_g, TN_g = _conf_from_preds(y_true_te, yhat_groupBA)\n",
    "    TP_w, FP_w, FN_w, TN_w = _conf_from_preds(y_true_te, yhat_wgBA)\n",
    "\n",
    "    if VERBOSE:\n",
    "        same_grp = bool(np.array_equal(yhat_single, yhat_groupBA))\n",
    "        same_wg  = bool(np.array_equal(yhat_single, yhat_wgBA))\n",
    "        nH_te = int((grp_te==\"High\").sum()); nL_te = int((grp_te==\"Low\").sum())\n",
    "        print(f\"[DBG-BA] fold{fold_id}: TEST High={nH_te} Low={nL_te} | \"\n",
    "              f\"Single==Group-BA: {same_grp}  Single==WG-BA: {same_wg} | \"\n",
    "              f\"BA(single, group, wg)=({BA_single:.3f}, {BA_group:.3f}, {BA_wg:.3f})\")\n",
    "\n",
    "    rows.append({\n",
    "        \"fold_id\": int(fold_id),\n",
    "        \"test_id\": str(test_sid),\n",
    "        \"best_k\": int(best_k),\n",
    "        \"tau_single\": float(tau_single),\n",
    "        \"tau_high_GroupBA\": float(tauH_grp), \"tau_low_GroupBA\": float(tauL_grp),\n",
    "        \"tau_high_WGBA\": float(tauH_wg),     \"tau_low_WGBA\": float(tauL_wg),\n",
    "        \"BA_single\": float(BA_single),\n",
    "        \"BA_group\":  float(BA_group),\n",
    "        \"BA_group_WG\": float(BA_wg),\n",
    "        \"TP_single\": int(TP_s), \"FP_single\": int(FP_s),\n",
    "        \"FN_single\": int(FN_s), \"TN_single\": int(TN_s),\n",
    "        \"TP_group\": int(TP_g),  \"FP_group\": int(FP_g),\n",
    "        \"FN_group\": int(FN_g),  \"TN_group\": int(TN_g),\n",
    "        \"TP_group_WG\": int(TP_w), \"FP_group_WG\": int(FP_w),\n",
    "        \"FN_group_WG\": int(FN_w), \"TN_group_WG\": int(TN_w),\n",
    "        \"n_test\": int(len(y_te_o)),\n",
    "    })\n",
    "\n",
    "    # 予測詳細（BA専用命名）\n",
    "    for yy, ss, gg, ys, yg, yw in zip(y_te_o, sc_te, grp_te, yhat_single, yhat_groupBA, yhat_wgBA):\n",
    "        pred_rows.append({\n",
    "            \"fold_id\": int(fold_id),\n",
    "            \"test_id\": str(test_sid),\n",
    "            \"y_true\": int(yy),\n",
    "            \"proba\": float(ss),\n",
    "            \"group\": str(gg),\n",
    "            \"y_pred_single\": int(ys),\n",
    "            \"y_pred_group_BA\": int(yg),\n",
    "            \"y_pred_group_WG\": int(yw),\n",
    "        })\n",
    "    \n",
    "    print(f\"[DBG-BA] fold{fold_id}: WG-joint val -> \"\n",
    "          f\"BA_H={res_wg['BA_H_val']:.3f}, BA_L={res_wg['BA_L_val']:.3f}, \"\n",
    "          f\"WG(BA)={res_wg['WG_BA_val']:.3f}, pooled(BA)={res_wg['BA_pooled_val']:.3f}, \"\n",
    "          f\"(tH,tL)=({tauH_wg:.4f},{tauL_wg:.4f})\")\n",
    "\n",
    "# ---------------- 出力（fold別・全予測・サマリ） ----------------\n",
    "df_fold = pd.DataFrame(rows)\n",
    "df_pred = pd.DataFrame(pred_rows)\n",
    "\n",
    "df_fold.to_csv(groupaware_out(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\"),\n",
    "               index=False, encoding=\"utf-8-sig\")\n",
    "df_pred.to_csv(groupaware_out(\"GROUP_AWARE_PREDICTIONS.CSV\"),\n",
    "               index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 参考: プールAUC（確率は共通なので1つ）\n",
    "auc_pool = float(roc_auc_score(df_pred[\"y_true\"].to_numpy(),\n",
    "                               df_pred[\"proba\"].to_numpy()))\n",
    "\n",
    "def _pooled_ba(col):\n",
    "    y_true = df_pred[\"y_true\"].to_numpy()\n",
    "    yhat   = df_pred[col].to_numpy().astype(int)\n",
    "    return _ba_binary(y_true, yhat)\n",
    "\n",
    "summary = {\n",
    "    \"best_k\": int(best_k),\n",
    "    \"AUC_pooled\": auc_pool,\n",
    "    \"BA_pooled_single\": _pooled_ba(\"y_pred_single\"),\n",
    "    \"BA_pooled_group\":  _pooled_ba(\"y_pred_group_BA\"),\n",
    "    \"BA_pooled_group_WG\": _pooled_ba(\"y_pred_group_WG\"),\n",
    "    \"metric\": \"BA\",\n",
    "    \"n_samples\": int(len(df_pred)),\n",
    "    \"n_pos\": int((df_pred[\"y_true\"]==1).sum()),\n",
    "    \"n_neg\": int((df_pred[\"y_true\"]==0).sum()),\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(groupaware_out(\"GROUP_AWARE_SUMMARY.CSV\"),\n",
    "                               index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell5A-BA] Done: outer folds={len(df_fold)}, pooled AUC={auc_pool:.3f}\")\n",
    "print(f\"[Cell5A-BA] Summary: {summary}\")\n",
    "\n",
    "# ---------------- Fold別 BA+TP/FP/FN/TN CSV（3アルゴリズム別） ----------------\n",
    "# Single τ\n",
    "df_single = df_fold[[\n",
    "    \"fold_id\",\"test_id\",\"n_test\",\n",
    "    \"TP_single\",\"FP_single\",\"FN_single\",\"TN_single\",\"BA_single\"\n",
    "]].rename(columns={\n",
    "    \"TP_single\":\"TP\", \"FP_single\":\"FP\",\n",
    "    \"FN_single\":\"FN\", \"TN_single\":\"TN\",\n",
    "    \"BA_single\":\"BA\"\n",
    "})\n",
    "df_single.to_csv(groupaware_out(\"BA_BY_FOLD_SINGLE.CSV\"),\n",
    "                 index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Group-BA τ\n",
    "df_group = df_fold[[\n",
    "    \"fold_id\",\"test_id\",\"n_test\",\n",
    "    \"TP_group\",\"FP_group\",\"FN_group\",\"TN_group\",\"BA_group\"\n",
    "]].rename(columns={\n",
    "    \"TP_group\":\"TP\", \"FP_group\":\"FP\",\n",
    "    \"FN_group\":\"FN\", \"TN_group\":\"TN\",\n",
    "    \"BA_group\":\"BA\"\n",
    "})\n",
    "df_group.to_csv(groupaware_out(\"BA_BY_FOLD_GROUPBA.CSV\"),\n",
    "                index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# WG-BA τ\n",
    "df_wg = df_fold[[\n",
    "    \"fold_id\",\"test_id\",\"n_test\",\n",
    "    \"TP_group_WG\",\"FP_group_WG\",\"FN_group_WG\",\"TN_group_WG\",\"BA_group_WG\"\n",
    "]].rename(columns={\n",
    "    \"TP_group_WG\":\"TP\", \"FP_group_WG\":\"FP\",\n",
    "    \"FN_group_WG\":\"FN\", \"TN_group_WG\":\"TN\",\n",
    "    \"BA_group_WG\":\"BA\"\n",
    "})\n",
    "df_wg.to_csv(groupaware_out(\"BA_BY_FOLD_WGBA.CSV\"),\n",
    "             index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[Cell5A-BA] Fold-wise BA & confusion counts saved -> BA_BY_FOLD_*.CSV\")\n",
    "\n",
    "# ---------------- 混同行列プロット（BA専用タイトル） ----------------\n",
    "def _draw_cm_on_ax(ax, cm, title):\n",
    "    im = ax.imshow(cm, cmap=\"Blues\", vmin=0, vmax=max(cm.max(),1))\n",
    "    labels = np.array([[\"TN\",\"FP\"],[\"FN\",\"TP\"]])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = int(cm[i,j])\n",
    "            color = \"white\" if val > 0.6*im.get_clim()[1] else \"black\"\n",
    "            ax.text(j, i, f\"{labels[i,j]}\\n{val}\", ha=\"center\", va=\"center\",\n",
    "                    fontsize=18, fontweight=\"bold\", color=color)\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Pred:0\",\"Pred:1\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"True:0\",\"True:1\"], rotation=90, va=\"center\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(False)\n",
    "\n",
    "def _draw_cm(cm, title, path):\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    _draw_cm_on_ax(ax, cm, title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# プール全体の混同行列（3モード）\n",
    "y_pool = df_pred[\"y_true\"].to_numpy()\n",
    "cm_single = skm.confusion_matrix(y_pool, df_pred[\"y_pred_single\"],    labels=[0,1])\n",
    "cm_group  = skm.confusion_matrix(y_pool, df_pred[\"y_pred_group_BA\"],  labels=[0,1])\n",
    "cm_wg     = skm.confusion_matrix(y_pool, df_pred[\"y_pred_group_WG\"],  labels=[0,1])\n",
    "\n",
    "# 単品 PNG\n",
    "_draw_cm(cm_single,\n",
    "         f\"Cell5A Single τ (BA={summary['BA_pooled_single']:.3f})\",\n",
    "         cell5_out(\"CONFMAT_CELL5A_SINGLE(BA).png\"))\n",
    "_draw_cm(cm_group,\n",
    "         f\"Cell5A Group-BA τ (BA={summary['BA_pooled_group']:.3f})\",\n",
    "         cell5_out(\"CONFMAT_CELL5A_GROUP(BA).png\"))\n",
    "_draw_cm(cm_wg,\n",
    "         f\"Cell5A WG-BA τ (BA={summary['BA_pooled_group_WG']:.3f})\",\n",
    "         cell5_out(\"CONFMAT_CELL5A_WG(BA).png\"))\n",
    "\n",
    "# （１）3モード（Single / Group-BA / WG-BA）の混同行列を横並び\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "_draw_cm_on_ax(axes[0], cm_single,\n",
    "               f\"Single τ (BA={summary['BA_pooled_single']:.3f})\")\n",
    "_draw_cm_on_ax(axes[1], cm_group,\n",
    "               f\"Group-BA τ (BA={summary['BA_pooled_group']:.3f})\")\n",
    "_draw_cm_on_ax(axes[2], cm_wg,\n",
    "               f\"WG-BA τ (BA={summary['BA_pooled_group_WG']:.3f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cell5_out(\"CONFMAT_CELL5A_3MODES(BA).png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# （２）Group-BA τ: 全体・Highのみ・Lowのみの3つを横並び\n",
    "maskH = (df_pred[\"group\"] == \"High\")\n",
    "maskL = (df_pred[\"group\"] == \"Low\")\n",
    "\n",
    "cm_grp_pool = cm_group\n",
    "cm_grp_H    = skm.confusion_matrix(\n",
    "    df_pred.loc[maskH, \"y_true\"].to_numpy(),\n",
    "    df_pred.loc[maskH, \"y_pred_group_BA\"].to_numpy(),\n",
    "    labels=[0,1]\n",
    ")\n",
    "cm_grp_L    = skm.confusion_matrix(\n",
    "    df_pred.loc[maskL, \"y_true\"].to_numpy(),\n",
    "    df_pred.loc[maskL, \"y_pred_group_BA\"].to_numpy(),\n",
    "    labels=[0,1]\n",
    ")\n",
    "\n",
    "BA_grp_pool = _ba_binary(df_pred[\"y_true\"].to_numpy(),\n",
    "                         df_pred[\"y_pred_group_BA\"].to_numpy())\n",
    "BA_grp_H    = _ba_binary(df_pred.loc[maskH, \"y_true\"].to_numpy(),\n",
    "                         df_pred.loc[maskH, \"y_pred_group_BA\"].to_numpy())\n",
    "BA_grp_L    = _ba_binary(df_pred.loc[maskL, \"y_true\"].to_numpy(),\n",
    "                         df_pred.loc[maskL, \"y_pred_group_BA\"].to_numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "_draw_cm_on_ax(axes[0], cm_grp_pool,\n",
    "               f\"Group-BA τ / All (BA={BA_grp_pool:.3f})\")\n",
    "_draw_cm_on_ax(axes[1], cm_grp_H,\n",
    "               f\"Group-BA τ / High (BA={BA_grp_H:.3f})\")\n",
    "_draw_cm_on_ax(axes[2], cm_grp_L,\n",
    "               f\"Group-BA τ / Low (BA={BA_grp_L:.3f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cell5_out(\"CONFMAT_CELL5A_GROUP(BA)_HIGHLOW.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# （３）WG-BA τ: 全体・Highのみ・Lowのみの3つを横並び\n",
    "cm_wg_pool = cm_wg\n",
    "cm_wg_H    = skm.confusion_matrix(\n",
    "    df_pred.loc[maskH, \"y_true\"].to_numpy(),\n",
    "    df_pred.loc[maskH, \"y_pred_group_WG\"].to_numpy(),\n",
    "    labels=[0,1]\n",
    ")\n",
    "cm_wg_L    = skm.confusion_matrix(\n",
    "    df_pred.loc[maskL, \"y_true\"].to_numpy(),\n",
    "    df_pred.loc[maskL, \"y_pred_group_WG\"].to_numpy(),\n",
    "    labels=[0,1]\n",
    ")\n",
    "\n",
    "BA_wg_pool = _ba_binary(df_pred[\"y_true\"].to_numpy(),\n",
    "                        df_pred[\"y_pred_group_WG\"].to_numpy())\n",
    "BA_wg_H    = _ba_binary(df_pred.loc[maskH, \"y_true\"].to_numpy(),\n",
    "                        df_pred.loc[maskH, \"y_pred_group_WG\"].to_numpy())\n",
    "BA_wg_L    = _ba_binary(df_pred.loc[maskL, \"y_true\"].to_numpy(),\n",
    "                        df_pred.loc[maskL, \"y_pred_group_WG\"].to_numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "_draw_cm_on_ax(axes[0], cm_wg_pool,\n",
    "               f\"WG-BA τ / All (BA={BA_wg_pool:.3f})\")\n",
    "_draw_cm_on_ax(axes[1], cm_wg_H,\n",
    "               f\"WG-BA τ / High (BA={BA_wg_H:.3f})\")\n",
    "_draw_cm_on_ax(axes[2], cm_wg_L,\n",
    "               f\"WG-BA τ / Low (BA={BA_wg_L:.3f})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(cell5_out(\"CONFMAT_CELL5A_WG(BA)_HIGHLOW.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"[Cell5A-BA] Confusion matrices saved in Cell5_BA -> CONFMAT_CELL5A_*.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6eb4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5B++: 確率スコア分布（OVERALL/群別/各Fold）＋ τ の中央値/IQR 帯・±表記 =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 設定 ----------\n",
    "BINS = 40\n",
    "LW = 1.5\n",
    "FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "\n",
    "# 色指定（ユーザ指定）\n",
    "COLOR_SICK = \"red\"   # True:Sick\n",
    "COLOR_NON  = \"blue\"  # True:Non-Sick\n",
    "\n",
    "# 線色（しきい値）\n",
    "COLOR_SINGLE = \"black\"\n",
    "COLOR_GROUP  = \"green\"\n",
    "COLOR_WG     = \"purple\"\n",
    "\n",
    "# 出力フォルダ（新規作成）\n",
    "RUN_ROOT = os.path.dirname(outpath(\"__dummy__\"))\n",
    "IMG_DIR  = os.path.join(RUN_ROOT, \"PROBA_DIST_BA\")\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "# ファイルパス\n",
    "SAVE_OVERALL_SvG   = os.path.join(IMG_DIR, \"OVERALL_SvGroup.png\")\n",
    "SAVE_OVERALL_SvWG  = os.path.join(IMG_DIR, \"OVERALL_SvWG.png\")\n",
    "SAVE_BYGROUP_SvG   = os.path.join(IMG_DIR, \"BYGROUP_SvGroup.png\")\n",
    "SAVE_BYGROUP_SvWG  = os.path.join(IMG_DIR, \"BYGROUP_SvWG.png\")\n",
    "\n",
    "# ---------- 入力 ----------\n",
    "GROUP_AWARE_DIR = os.path.join(OUT_DIR, \"GROUP_AWARE\", \"BA\")\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_path(filename: str) -> str:\n",
    "    return os.path.join(GROUP_AWARE_DIR, filename)\n",
    "\n",
    "pred_path = groupaware_path(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "fold_path = groupaware_path(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "if not (os.path.exists(pred_path) and os.path.exists(fold_path)):\n",
    "    raise FileNotFoundError(\"[Cell5B++] 必要CSVが見つからない（BA版 Cell 5A を先に実行）\")\n",
    "\n",
    "df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- モード自動判定（F1 or BA） ----------\n",
    "cols_f1 = {\"high\":\"tau_high_GroupF1\", \"low\":\"tau_low_GroupF1\", \"wgh\":\"tau_high_WGF1\", \"wgl\":\"tau_low_WGF1\"}\n",
    "cols_ba = {\"high\":\"tau_high_GroupBA\", \"low\":\"tau_low_GroupBA\", \"wgh\":\"tau_high_WGBA\", \"wgl\":\"tau_low_WGBA\"}\n",
    "\n",
    "if all(c in df_fold.columns for c in [cols_f1[\"high\"], cols_f1[\"low\"]]):\n",
    "    mode = \"F1\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_f1[\"high\"], cols_f1[\"low\"], cols_f1[\"wgh\"], cols_f1[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_F1\"\n",
    "elif all(c in df_fold.columns for c in [cols_ba[\"high\"], cols_ba[\"low\"]]):\n",
    "    mode = \"BA\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_ba[\"high\"], cols_ba[\"low\"], cols_ba[\"wgh\"], cols_ba[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_BA\"\n",
    "else:\n",
    "    raise RuntimeError(\"[Cell5B++] しきい値列が見つからない（F1/BAどちらかのCell 5Aの出力が必要）\")\n",
    "\n",
    "# ---------- 集約: 中央値/IQR（Q1〜Q3） ----------\n",
    "def _qstats(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.size == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan  # med, q1, q3, half_iqr\n",
    "    med = float(np.nanmedian(s))\n",
    "    q1, q3 = np.nanpercentile(s, [25, 75])\n",
    "    half = float((q3 - q1)/2.0)\n",
    "    return float(med), float(q1), float(q3), half\n",
    "\n",
    "tau_single_med, tau_single_q1, tau_single_q3, tau_single_half = _qstats(df_fold[\"tau_single\"])\n",
    "tau_high_med,   tau_high_q1,   tau_high_q3,   tau_high_half   = _qstats(df_fold[c_high])\n",
    "tau_low_med,    tau_low_q1,    tau_low_q3,    tau_low_half    = _qstats(df_fold[c_low])\n",
    "tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, tau_high_wg_half = _qstats(df_fold[c_wgh]) if c_wgh in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  tau_low_wg_half  = _qstats(df_fold[c_wgl]) if c_wgl in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "# ---------- データ分解 ----------\n",
    "proba = pd.to_numeric(df_pred[\"proba\"], errors=\"coerce\").values\n",
    "ytrue = pd.to_numeric(df_pred[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "grp   = df_pred[\"group\"].astype(str).str.strip()\n",
    "\n",
    "p_sick = proba[ytrue == 1]   # True:Sick\n",
    "p_non  = proba[ytrue == 0]   # True:Non-Sick\n",
    "n_sick, n_non = len(p_sick), len(p_non)\n",
    "\n",
    "maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "\n",
    "# ---------- ユーティリティ ----------\n",
    "def _style_axes(ax, title=None):\n",
    "    if title: ax.set_title(title, fontsize=FS_TITLE)\n",
    "    ax.set_xlabel(\"Predicted probability\", fontsize=FS_LABEL)\n",
    "    ax.set_ylabel(\"Density\", fontsize=FS_LABEL)\n",
    "    ax.tick_params(axis=\"both\", labelsize=FS_TICK)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "def _hist_overall(ax):\n",
    "    ax.hist(p_sick, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={n_sick})\", color=COLOR_SICK)\n",
    "    ax.hist(p_non,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={n_non})\", color=COLOR_NON)\n",
    "\n",
    "def _hist_bygroup(axes):\n",
    "    # High\n",
    "    p_sick_H = proba[(ytrue==1) & maskH]; p_non_H = proba[(ytrue==0) & maskH]\n",
    "    axes[0].hist(p_sick_H, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_H)})\", color=COLOR_SICK)\n",
    "    axes[0].hist(p_non_H,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_H)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[0], \"High group\")\n",
    "    # Low\n",
    "    p_sick_L = proba[(ytrue==1) & maskL]; p_non_L = proba[(ytrue==0) & maskL]\n",
    "    axes[1].hist(p_sick_L, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_L)})\", color=COLOR_SICK)\n",
    "    axes[1].hist(p_non_L,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_L)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[1], \"Low group\")\n",
    "\n",
    "def _vline_with_iqr(ax, x_med, q1, q3, color, ls, label_core):\n",
    "    if np.isfinite(x_med):\n",
    "        ax.axvline(x_med, color=color, linestyle=ls, linewidth=LW,\n",
    "                   label=f\"{label_core} = {x_med:.3f} ± {(q3-q1)/2:.3f}\" if (np.isfinite(q1) and np.isfinite(q3)) else f\"{label_core} = {x_med:.3f}\")\n",
    "    if np.isfinite(q1) and np.isfinite(q3):\n",
    "        ax.axvspan(q1, q3, color=color, alpha=0.12)\n",
    "\n",
    "# ---------- 1) OVERALL: Single vs Group（中央値・IQR帯・±表記） ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_med,   tau_high_q1,   tau_high_q3,   COLOR_GROUP,  \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(ax, tau_low_med,    tau_low_q1,    tau_low_q3,    COLOR_GROUP,  \"--\", f\"τ_low_{mode}\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs Group [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvG}\")\n",
    "\n",
    "# ---------- 2) OVERALL: Single vs WG（中央値・IQR帯・±表記） ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(ax, tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs WG [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvWG}\")\n",
    "\n",
    "# ---------- 3) BY_GROUP: Single vs Group（各群パネルに該当の IQR帯） ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "# Single は両段に表示\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "# High 段に High の Group τ、Low 段に Low の Group τ\n",
    "_vline_with_iqr(axes[0], tau_high_med, tau_high_q1, tau_high_q3, COLOR_GROUP, \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(axes[1], tau_low_med,  tau_low_q1,  tau_low_q3,  COLOR_GROUP, \"--\", f\"τ_low_{mode}\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvG}\")\n",
    "\n",
    "# ---------- 4) BY_GROUP: Single vs WG（各群パネルに該当の IQR帯） ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[0], tau_high_wg, tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(axes[1], tau_low_wg,  tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvWG}\")\n",
    "\n",
    "# ---------- 5) Fold単位：OVERALL の確率分布と各Foldの τ（Single vs Group / Single vs WG） ----------\n",
    "#   - 各Foldのテストサンプルのみでヒストを作図\n",
    "#   - しきい値は「そのFold行」の値を直接表示（±は不要／IQR帯は使わない）\n",
    "for _, row in df_fold.iterrows():\n",
    "    fid = int(row[\"fold_id\"]) if \"fold_id\" in row else None\n",
    "    test_id = str(row.get(\"test_id\", f\"fold{fid}\"))\n",
    "    sub = df_pred[df_pred[\"fold_id\"] == fid] if \"fold_id\" in df_pred.columns and fid is not None else df_pred.copy()\n",
    "\n",
    "    p = pd.to_numeric(sub[\"proba\"], errors=\"coerce\").values\n",
    "    yt = pd.to_numeric(sub[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "    p_s, p_n = p[yt==1], p[yt==0]\n",
    "\n",
    "    # 値（このFoldの τ）\n",
    "    t_single = float(row[\"tau_single\"])\n",
    "    t_high   = float(row[c_high]) if c_high in row else np.nan\n",
    "    t_low    = float(row[c_low])  if c_low  in row else np.nan\n",
    "    t_high_w = float(row[c_wgh])  if c_wgh  in row else np.nan\n",
    "    t_low_w  = float(row[c_wgl])  if c_wgl  in row else np.nan\n",
    "\n",
    "    # 出力パス\n",
    "    p_sg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvGroup.png\")\n",
    "    p_wg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvWG.png\")\n",
    "\n",
    "    # Single vs Group\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high):   ax.axvline(t_high,   color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_high_{mode} = {t_high:.3f}\")\n",
    "    if np.isfinite(t_low):    ax.axvline(t_low,    color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_low_{mode}  = {t_low:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs Group [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_sg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_sg}\")\n",
    "\n",
    "    # Single vs WG\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high_w): ax.axvline(t_high_w, color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_high_WG = {t_high_w:.3f}\")\n",
    "    if np.isfinite(t_low_w):  ax.axvline(t_low_w,  color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_low_WG  = {t_low_w:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs WG [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_wg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_wg}\")\n",
    "\n",
    "print(f\"[Cell5B++] All images saved in: {IMG_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
