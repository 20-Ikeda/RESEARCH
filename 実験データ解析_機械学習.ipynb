{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0bc0e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] MODEL_BACKEND=XGB / SEED=20251101 / backends=['xgb', 'rf', 'svm']\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 0: 学習モデルのレジストリ化（XGB / RF / SVM ほか拡張可能） =====\n",
    "from __future__ import annotations\n",
    "from typing import Callable, Dict, Any, Optional\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ★ 2値化の閾値（例：FMS >= 1 を Sick=1）\n",
    "FMS_THRESHOLD = 1  # ← 0,1,2... に変更可\n",
    "\n",
    "# ★ EPOCH長（秒）：30 / 60 / 120 から選択\n",
    "EPOCH_LEN = 30     # ← ここを 60, 120 に切替え可\n",
    "if EPOCH_LEN not in (30, 60, 120):\n",
    "    raise ValueError(\"EPOCH_LEN は 30/60/120 のいずれかで指定すること。\")\n",
    "\n",
    "\n",
    "# --- 切替スイッチ（必要に応じて \"xgb\"/\"rf\" / \"svm\" に変更） ---\n",
    "MODEL_BACKEND: str = \"xgb\"\n",
    "USE_AP_FOR_K = bool(globals().get(\"USE_AP_FOR_K\", False))  # 既定=True（APでbest_kを上書き）\n",
    "\n",
    "METRIC = \"f1\"\n",
    "METRIC_NAME = \"f1\"\n",
    "\n",
    "\n",
    "# --- 既定パラメータ（必要に応じてここだけ触れば全セルに反映） ---\n",
    "SEED_BASE = 20251101\n",
    "\n",
    "XGB_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=100,\n",
    "    eval_metric=\"logloss\",\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    n_jobs=1, # 決定論的\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cpu\",\n",
    "    seed=0, random_state=0\n",
    ")\n",
    "\n",
    "RF_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=439,\n",
    "    max_depth=14,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=False,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED_BASE,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "SVM_PARAMS: Dict[str, Any] = dict(\n",
    "    C=1.0,\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"scale\",\n",
    "    probability=True,          # AUC/閾値用に確率出力を有効化（重ければ False でも可）\n",
    "    class_weight=\"balanced\",   # 不均衡対応\n",
    "    random_state=SEED_BASE,\n",
    ")\n",
    "\n",
    "# --- モデルレジストリ（IF分を増やさず拡張） ---\n",
    "ModelBuilder = Callable[..., Any]\n",
    "MODEL_REGISTRY: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "def register_backend(name: str, params: Dict[str, Any], builder: ModelBuilder) -> None:\n",
    "    MODEL_REGISTRY[name] = {\"params\": params, \"builder\": builder}\n",
    "\n",
    "def _build_xgb(params: Dict[str, Any], *, scale_pos_weight: Optional[float] = None):\n",
    "    assert xgb is not None, \"[ERROR] xgboost がインポートできません。\"\n",
    "    p = params.copy()\n",
    "    if scale_pos_weight is not None:\n",
    "        p[\"scale_pos_weight\"] = float(scale_pos_weight)\n",
    "    return xgb.XGBClassifier(**p)\n",
    "\n",
    "def _build_rf(params: Dict[str, Any], *, scale_pos_weight: Optional[float] = None):\n",
    "    # RF は scale_pos_weight を使わない（class_weight='balanced' を既定にしてある）\n",
    "    return RandomForestClassifier(**params)\n",
    "\n",
    "def _build_svm(params: Dict[str, Any], *, scale_pos_weight: Optional[float] = None):\n",
    "    # SVM は class_weight で不均衡対応。probability=True なら predict_proba 利用可\n",
    "    return SVC(**params)\n",
    "\n",
    "# 初期登録\n",
    "register_backend(\"xgb\", XGB_PARAMS, _build_xgb)\n",
    "register_backend(\"rf\",  RF_PARAMS,  _build_rf)\n",
    "register_backend(\"svm\", SVM_PARAMS, _build_svm)\n",
    "\n",
    "\n",
    "def set_model_backend(name: str) -> None:\n",
    "    \"\"\"バックエンド名の安全な切替。\"\"\"\n",
    "    global MODEL_BACKEND\n",
    "    assert name in MODEL_REGISTRY, f\"[ERROR] backend '{name}' は未登録。候補: {list(MODEL_REGISTRY.keys())}\"\n",
    "    MODEL_BACKEND = name\n",
    "\n",
    "def build_estimator(\n",
    "    backend: Optional[str] = None,\n",
    "    *,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"レジストリから学習器インスタンスを生成。overrides で一時上書き可。\"\"\"\n",
    "    name = (backend or MODEL_BACKEND).lower()\n",
    "    assert name in MODEL_REGISTRY, f\"[ERROR] backend '{name}' は未登録。\"\n",
    "    base = MODEL_REGISTRY[name][\"params\"].copy()\n",
    "    if overrides:\n",
    "        base.update(overrides)\n",
    "    builder = MODEL_REGISTRY[name][\"builder\"]\n",
    "    return builder(base, scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "def fit_estimator(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    共通フィット関数。XGB/RF/SVM いずれでも同じ呼び出しで学習可能。\n",
    "    - X_train は float32 推奨（XGB の速度/メモリ対策）\n",
    "    - y_train は int32 推奨\n",
    "    \"\"\"\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    model = build_estimator(backend, scale_pos_weight=scale_pos_weight, overrides=overrides)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def predict_positive_score(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    陽性（1）クラスのスコアを返す。\n",
    "    - predict_proba があればその第2列を返す（確率）\n",
    "    - なければ decision_function を返す（AUCは単調変換に不変）\n",
    "    \"\"\"\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        s = model.decision_function(X)\n",
    "        return np.asarray(s, dtype=float)\n",
    "    # 最後の手段（スコア不明な推定器）\n",
    "    return model.predict(X).astype(float)\n",
    "\n",
    "MODEL_ID = f\"{MODEL_BACKEND.upper()}\"\n",
    "print(f\"[INFO] MODEL_BACKEND={MODEL_ID} / SEED={SEED_BASE} / backends={list(MODEL_REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f32cd645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OUT_DIR] C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1  |  EPOCH_LEN=30s\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 0: 出力ディレクトリ（FMS閾値ごと）と共通設定 =====\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# ★ 入力CSVの基本パス（被験者ごと）\n",
    "BASE_INPUT_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "\n",
    "# ★ 出力ルート：本実験結果/ANALYSIS/機械学習/閾値FMS{n}\n",
    "BASE_ANALYSIS_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\"\n",
    "OUT_DIR = os.path.join(BASE_ANALYSIS_DIR, \"機械学習(MSSQ込み)\",f\"閾値FMS{int(FMS_THRESHOLD)}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def outpath(filename: str) -> str:\n",
    "    return os.path.join(OUT_DIR, filename)\n",
    "\n",
    "print(f\"[OUT_DIR] {OUT_DIR}  |  EPOCH_LEN={EPOCH_LEN}s\")\n",
    "\n",
    "# 被験者ID（氏名なし）\n",
    "from typing import List\n",
    "SUBJECT_IDS: List[str] = [\n",
    "    \"10061\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "# ベースラインとML範囲（epoch_startは30秒刻みの整数）\n",
    "BASELINE_EPOCH = 1770            # 単一行（フォールバック無し、無ければ即エラー）\n",
    "ML_START, ML_END = 1800, 2400    # [ML_START, ML_END) を学習用に使用\n",
    "\n",
    "# 図の体裁（英語ラベル・フォント大きめ）\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"font.size\": 20, \"axes.titlesize\": 26, \"axes.labelsize\": 22,\n",
    "    \"xtick.labelsize\": 20, \"ytick.labelsize\": 20, \"legend.fontsize\": 20,\n",
    "})\n",
    "\n",
    "# 便利ヘルパー：FMS二値化（OUT_DIRの閾値に連動）\n",
    "def binarize_fms(series, threshold: int = None):\n",
    "    th = FMS_THRESHOLD if threshold is None else int(threshold)\n",
    "    return (series >= th).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01b0b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded subjects: 17, rows=357, features(after drop)=45\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: 入力CSV読み込み（全被験者, 元は30sエポック） =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def subject_csv_path(sid: str) -> str:\n",
    "    p = os.path.join(BASE_INPUT_DIR, sid, \"EPOCH\", f\"{sid}_epoch.csv\")\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"[ERROR] CSV not found for subject {sid}: {p}\")\n",
    "    return p\n",
    "\n",
    "dfs = []\n",
    "for sid in SUBJECT_IDS:\n",
    "    p = subject_csv_path(sid)\n",
    "    df = pd.read_csv(p)\n",
    "    if df.shape[1] < 4:\n",
    "        raise ValueError(f\"[ERROR] CSV columns too few for {sid}: need >=4 (epoch_start, epoch_end, FMS, features...)\")\n",
    "    df = df.copy()\n",
    "    # 1列目=epoch_start, 2列目=epoch_end, 3列目=FMS, 4列目以降=特徴量（名前はCSVそのまま）\n",
    "    df.columns = list(df.columns[:3]) + [str(c) for c in df.columns[3:]]\n",
    "    # 列名標準化\n",
    "    c1, c2, c3 = df.columns[:3]\n",
    "    df = df.rename(columns={c1: \"epoch_start\", c2: \"epoch_end\", c3: \"FMS\"})\n",
    "    # 型\n",
    "    df[\"epoch_start\"] = pd.to_numeric(df[\"epoch_start\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"epoch_end\"]   = pd.to_numeric(df[\"epoch_end\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"FMS\"]         = pd.to_numeric(df[\"FMS\"],         errors=\"coerce\").astype(\"Int64\")\n",
    "    if df[[\"epoch_start\",\"epoch_end\",\"FMS\"]].isna().any().any():\n",
    "        raise ValueError(f\"[ERROR] epoch_start/epoch_end/FMS に NaN (subject {sid})\")\n",
    "    # subject列\n",
    "    df.insert(0, \"subject_id\", sid)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 特徴量列（除外：HF_power, LF_power, LF_HF_ratio）\n",
    "exclude_feats = {\"HF_power\", \"LF_power\", \"LF_HF_ratio\"}\n",
    "all_cols = combined_raw.columns.tolist()\n",
    "feature_cols_all = [c for c in all_cols if c not in [\"subject_id\",\"epoch_start\",\"epoch_end\",\"FMS\"] and c not in exclude_feats]\n",
    "if len(feature_cols_all) == 0:\n",
    "    raise RuntimeError(\"[ERROR] 特徴量列が0です（除外のしすぎか列名不一致）\")\n",
    "\n",
    "print(f\"[INFO] Loaded subjects: {len(SUBJECT_IDS)}, rows={len(combined_raw)}, features(after drop)={len(feature_cols_all)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "63b43e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved RAW ML table -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\ML_DATA_DELTA_30S_RAW.CSV\n",
      "[OK] Saved matrices -> X_RAW_ALL_30S.CSV, X_SCALED_ALL_30S.CSV, Y_AND_GROUPS_30S.CSV\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2: EPOCH_LEN（30/60/120s）合成 → baseline差分（特徴量のみ） =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# EPOCH_LEN=30 のときも同じロジック（実質1本平均）\n",
    "if (ML_END - ML_START) % EPOCH_LEN != 0:\n",
    "    raise ValueError(f\"[ERROR] ML window length {(ML_END-ML_START)} は EPOCH_LEN={EPOCH_LEN} で割り切れる必要あり。\")\n",
    "\n",
    "df_out_list = []\n",
    "rows_per_bin = EPOCH_LEN // 30  # 30秒エポック何本で1binか（30→1, 60→2, 120→4）\n",
    "\n",
    "for sid, sdf in combined_raw.groupby(\"subject_id\", sort=False):\n",
    "    # baseline row（1770–1800 の 1行）\n",
    "    base_row = sdf.loc[sdf[\"epoch_start\"] == BASELINE_EPOCH]\n",
    "    if base_row.shape[0] != 1:\n",
    "        raise ValueError(f\"[ERROR] subject {sid}: baseline row not found (epoch_start=={BASELINE_EPOCH})\")\n",
    "    base_vals = base_row[feature_cols_all].astype(float).iloc[0]\n",
    "    if base_vals.isna().any():\n",
    "        bad = base_vals.index[base_vals.isna()].tolist()\n",
    "        raise ValueError(f\"[ERROR] subject {sid}: baseline feature NaN -> {bad}\")\n",
    "\n",
    "    # ML window（元 30秒エポック）\n",
    "    sdf_ml = sdf[(sdf[\"epoch_start\"] >= ML_START) & (sdf[\"epoch_start\"] < ML_END)].copy()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[ERROR] subject {sid}: ML window empty [{ML_START},{ML_END})\")\n",
    "\n",
    "    # 30秒 → EPOCH_LEN秒 の bin_start を計算\n",
    "    # 例：EPOCH_LEN=60 の場合、1800,1830→bin_start=1800 / 1860,1890→bin_start=1860 ...\n",
    "    sdf_ml[\"bin_start\"] = ML_START + ((sdf_ml[\"epoch_start\"] - ML_START) // EPOCH_LEN) * EPOCH_LEN\n",
    "    sdf_ml[\"bin_end\"]   = sdf_ml[\"bin_start\"] + EPOCH_LEN\n",
    "\n",
    "    # 各 bin に含まれる30秒行数が rows_per_bin（完全）であるものだけ採用\n",
    "    size_check = sdf_ml.groupby([\"bin_start\",\"bin_end\"]).size()\n",
    "    complete_bins = size_check[size_check == rows_per_bin].index\n",
    "    sdf_ml = sdf_ml.set_index([\"bin_start\",\"bin_end\"]).loc[complete_bins].reset_index()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[ERROR] subject {sid}: no complete bins for EPOCH_LEN={EPOCH_LEN}\")\n",
    "\n",
    "    # ★ 合成前（元30秒）の平均で NEWEPOCH を作る（FMS も features も mean）\n",
    "    agg_dict = {c: \"mean\" for c in feature_cols_all}\n",
    "    agg_dict.update({\"FMS\": \"mean\"})  # FMS も平均（小数になる）\n",
    "    g = sdf_ml.groupby([\"subject_id\", \"bin_start\", \"bin_end\"], as_index=False).agg(agg_dict)\n",
    "\n",
    "    # baseline差分：特徴量のみ（mean − baseline）\n",
    "    g_features = g[feature_cols_all].astype(float) - base_vals.values\n",
    "    if g_features.isna().any().any():\n",
    "        bad_cols = g_features.columns[g_features.isna().any()].tolist()\n",
    "        raise ValueError(f\"[ERROR] subject {sid}: baseline-delta has NaN in features -> {bad_cols}\")\n",
    "\n",
    "    g_out = pd.concat([g[[\"subject_id\",\"bin_start\",\"bin_end\",\"FMS\"]], g_features], axis=1)\n",
    "    g_out = g_out.rename(columns={\"bin_start\":\"epoch_start\", \"bin_end\":\"epoch_end\"})  # 以降と同じ列名にそろえる\n",
    "\n",
    "    # ラベル（平均FMSに対して二値化）\n",
    "    g_out[\"label\"] = binarize_fms(g_out[\"FMS\"])\n",
    "\n",
    "    # 列順を揃える\n",
    "    g_out = g_out[[\"subject_id\",\"epoch_start\",\"epoch_end\",\"FMS\",\"label\"] + feature_cols_all]\n",
    "    df_out_list.append(g_out)\n",
    "\n",
    "# 連結\n",
    "df_ml_epoch = pd.concat(df_out_list, ignore_index=True)\n",
    "\n",
    "# 保存（スケーリング前＝差分後の生値；木系なのでスケーリング不要だがファイルも出す）\n",
    "fname_base = f\"ML_DATA_DELTA_{EPOCH_LEN}S_RAW.CSV\"\n",
    "df_ml_epoch.to_csv(outpath(fname_base), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] Saved RAW ML table -> {outpath(fname_base)}\")\n",
    "\n",
    "# 学習行列（以降のセルが参照）\n",
    "feature_cols = feature_cols_all[:]  # CSVそのままの列名（除外済み）\n",
    "X_all = df_ml_epoch[feature_cols].copy().astype(float)\n",
    "y_all = df_ml_epoch[\"label\"].copy().astype(int)\n",
    "groups = df_ml_epoch[\"subject_id\"].copy()\n",
    "\n",
    "# X_scaled_all は同一（スケーリングなし）\n",
    "X_scaled_all = X_all.copy()\n",
    "X_all.to_csv(outpath(f\"X_RAW_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "X_scaled_all.to_csv(outpath(f\"X_SCALED_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "pd.DataFrame({\"subject_id\": groups, \"label\": y_all, \"FMS_mean\": df_ml_epoch[\"FMS\"]}).to_csv(\n",
    "    outpath(f\"Y_AND_GROUPS_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[OK] Saved matrices -> X_RAW_ALL_{EPOCH_LEN}S.CSV, X_SCALED_ALL_{EPOCH_LEN}S.CSV, Y_AND_GROUPS_{EPOCH_LEN}S.CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33c7a1fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSQ</th>\n",
       "      <th>VIMSSQ</th>\n",
       "      <th>MSSQ_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10061</th>\n",
       "      <td>19.710</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10063</th>\n",
       "      <td>3.000</td>\n",
       "      <td>2</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10064</th>\n",
       "      <td>16.625</td>\n",
       "      <td>4</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10071</th>\n",
       "      <td>13.000</td>\n",
       "      <td>7</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10072</th>\n",
       "      <td>5.140</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              MSSQ  VIMSSQ MSSQ_group\n",
       "subject_id                           \n",
       "10061       19.710       1       High\n",
       "10063        3.000       2        Low\n",
       "10064       16.625       4       High\n",
       "10071       13.000       7       High\n",
       "10072        5.140       0        Low"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell2.5] SUBJECT_META saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\subject_meta.csv (file='C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\summary_scores.xlsx', sheet='Summary')\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2.5 (REPLACE): SUBJECT_META loader =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 参照元：/mnt/data/summary_scores.xlsx の \"Summary\" シート\n",
    "CANDIDATE_SCORE_PATHS = [\n",
    "    \"/mnt/data/summary_scores.xlsx\",  # ← ユーザー添付想定\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"機械学習\", \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_INPUT_DIR, \"summary_scores.xlsx\"),\n",
    "]\n",
    "score_path = next((p for p in CANDIDATE_SCORE_PATHS if os.path.exists(p)), None)\n",
    "if score_path is None:\n",
    "    raise FileNotFoundError(\"[Cell2.5] summary_scores.xlsx が見つかりません。パスを確認してください。\")\n",
    "\n",
    "SHEET = \"Summary\"\n",
    "meta_raw = pd.read_excel(score_path, sheet_name=SHEET)\n",
    "\n",
    "# 必須列（このファイル仕様に合わせる）\n",
    "required = [\"ID\", \"MSSQ\", \"VIMSSQ\"]\n",
    "missing = [c for c in required if c not in meta_raw.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"[Cell2.5] 必須列が不足: {missing} / シート: {SHEET}\")\n",
    "\n",
    "# 列を抽出＆正規化\n",
    "meta = meta_raw[required].copy()\n",
    "\n",
    "# ID を string 化（Excelの 10061 → \"10061\" など）\n",
    "meta[\"ID\"] = (\n",
    "    meta[\"ID\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# 数値列チェック\n",
    "for c in [\"MSSQ\", \"VIMSSQ\"]:\n",
    "    try:\n",
    "        meta[c] = pd.to_numeric(meta[c], errors=\"raise\")\n",
    "    except Exception:\n",
    "        raise ValueError(f\"[Cell2.5] 数値変換に失敗: {c}\")\n",
    "\n",
    "# （任意）解析対象のIDに合わせてフィルタ（SUBJECT_IDS があれば交差）\n",
    "if \"SUBJECT_IDS\" in globals() and len(SUBJECT_IDS) > 0:\n",
    "    sid_set = set(map(str, SUBJECT_IDS))\n",
    "    meta = meta[meta[\"ID\"].isin(sid_set)].copy()\n",
    "\n",
    "# 重複チェック\n",
    "if meta[\"ID\"].duplicated().any():\n",
    "    dups = meta.loc[meta[\"ID\"].duplicated(), \"ID\"].tolist()\n",
    "    raise ValueError(f\"[Cell2.5] ID が重複: {dups}\")\n",
    "\n",
    "# ★ 群ラベリング定義（ここが“グループ分けのラベリング”の定義場所）\n",
    "#   仕様：MSSQ の固定閾値 10.0 で High / Low に二分\n",
    "MSSQ_THRESHOLD_FIXED = 10.0\n",
    "meta[\"MSSQ_group\"] = np.where(meta[\"MSSQ\"] >= MSSQ_THRESHOLD_FIXED, \"High\", \"Low\")\n",
    "\n",
    "# SUBJECT_META を index=subject_id として提供（以降は subject_id=文字列ID）\n",
    "SUBJECT_META = (\n",
    "    meta.rename(columns={\"ID\": \"subject_id\"})\n",
    "        .set_index(\"subject_id\")[[\"MSSQ\", \"VIMSSQ\", \"MSSQ_group\"]]\n",
    "        .copy()\n",
    ")\n",
    "\n",
    "display(SUBJECT_META.head())\n",
    "SUBJECT_META.to_csv(outpath(\"subject_meta.csv\"), encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell2.5] SUBJECT_META saved -> {outpath('subject_meta.csv')} (file='{score_path}', sheet='{SHEET}')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a7e5e2",
   "metadata": {},
   "source": [
    "# ===== Cell 3: Modeling & SHAP helper functions（バックエンド非依存） =====\n",
    "from typing import Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# Cell 0 の共通APIがロード済みであることを確認\n",
    "assert \"fit_estimator\" in globals(), \"[ERROR] Cell 0 の共通ユーティリティが未定義です。\"\n",
    "assert \"predict_positive_score\" in globals(), \"[ERROR] Cell 0 の共通ユーティリティが未定義です。\"\n",
    "MODEL_BACKEND_STR = globals().get(\"MODEL_BACKEND\", \"xgb\")\n",
    "\n",
    "# ----------------------------\n",
    "# 学習（後方互換ラッパ：既存名を維持）\n",
    "# ----------------------------\n",
    "def fit_xgb_classifier(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    互換ラッパ：既存コードの呼び出し名を維持しつつ、Cell 0 の fit_estimator を利用。\n",
    "    - XGB のときは scale_pos_weight を内部注入\n",
    "    - RF / SVM のときは内部で無視（RFは class_weight='balanced'）\n",
    "    \"\"\"\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    model = fit_estimator(X_train, y_train, scale_pos_weight=scale_pos_weight)\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# SHAP（木モデル用：XGB / RF の両対応）\n",
    "# ----------------------------\n",
    "def compute_train_shap_abs_mean(model, X_ref: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    平均|SHAP|で特徴重要度を算出（学習データのみ・リーク防止）。\n",
    "    - 対応：XGB / RF（TreeSHAP）\n",
    "    - 非対応：SVM 等（必要なら別途 permutation 重要度を用意）\n",
    "    戻り値：index=特徴名, name='mean_abs'（降順）\n",
    "    \"\"\"\n",
    "\n",
    "    X_ref = X_ref.astype(np.float32, copy=False)\n",
    "\n",
    "    # 背景データ（確率出力に必要）：学習データのサブセット\n",
    "    bg_n = min(128, len(X_ref))\n",
    "    X_bg = X_ref.sample(n=bg_n, random_state=globals().get(\"SEED_BASE\", 0)) if bg_n >= 2 else X_ref\n",
    "\n",
    "    # 可能なら probability×interventional、失敗時は raw にフォールバック\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            data=X_bg,\n",
    "            model_output=\"probability\",\n",
    "            feature_perturbation=\"interventional\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "    except Exception:\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            feature_perturbation=\"tree_path_dependent\",\n",
    "            model_output=\"raw\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "\n",
    "    # ---- ここから戻り値を「陽性クラス1の2次元配列 (n_samples, n_features)」に正規化 ----\n",
    "    \n",
    "\n",
    "    # 陽性クラスのインデックスを決定（なければ末尾を陽性扱い）\n",
    "    classes = getattr(model, \"classes_\", None)\n",
    "    if classes is not None and 1 in list(classes):\n",
    "        pos_idx = int(np.where(classes == 1)[0][0])\n",
    "    else:\n",
    "        pos_idx = -1  # 末尾\n",
    "\n",
    "    # 1) マルチクラス形式のリスト（古いAPIでありがち）\n",
    "    if isinstance(sv_any, list):\n",
    "        sv = sv_any[pos_idx]  # (n_samples, n_features)\n",
    "    else:\n",
    "        # 2) Explanation オブジェクト → .values\n",
    "        if hasattr(sv_any, \"values\"):\n",
    "            sv_any = sv_any.values\n",
    "        sv = np.asarray(sv_any)\n",
    "\n",
    "        # 3) 3次元 (n_samples, n_features, n_classes) → 陽性だけ切り出し\n",
    "        if sv.ndim == 3:\n",
    "            sv = sv[..., pos_idx]  # (n_samples, n_features)\n",
    "\n",
    "        # 4) 1次元なら列に直す（まれ）\n",
    "        elif sv.ndim == 1:\n",
    "            sv = sv.reshape(-1, 1)\n",
    "\n",
    "        # 2次元ならそのまま (n_samples, n_features)\n",
    "\n",
    "    # 最終チェック：列数と特徴数を合わせる\n",
    "    if sv.shape[1] != X_ref.shape[1]:\n",
    "        raise RuntimeError(\n",
    "            f\"[ERROR] SHAP shape mismatch: sv.shape={sv.shape}, X_ref.shape={X_ref.shape}. \"\n",
    "            \"列順・前処理を確認してください。\"\n",
    "        )\n",
    "\n",
    "    abs_mean = np.mean(np.abs(sv), axis=0)  # (n_features,)\n",
    "    return pd.Series(abs_mean, index=X_ref.columns, name=\"mean_abs\").sort_values(ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# 評価（AUC / Accuracy）\n",
    "# ----------------------------\n",
    "def _is_probability_like(scores: np.ndarray) -> bool:\n",
    "    return np.isfinite(scores).all() and (0.0 <= scores.min() <= scores.max() <= 1.0)\n",
    "\n",
    "def evaluate_fold(model, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    共通APIでスコアを取得して評価。\n",
    "    - AUC：確率でも decision score でもOK（単調変換に不変）\n",
    "    - Accuracy：確率なら 0.5、score なら 0.0 を閾値に（しきい値最適化は別セルで実施）\n",
    "    \"\"\"\n",
    "    X_test = X_test.astype(np.float32, copy=False)\n",
    "    scores = predict_positive_score(model, X_test)\n",
    "\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, scores)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "\n",
    "    thr = 0.5 if _is_probability_like(scores) else 0.0\n",
    "    pred = (scores >= thr).astype(int)\n",
    "    acc = accuracy_score(y_test.astype(int), pred)\n",
    "\n",
    "    return {\"roc_auc\": float(roc_auc), \"accuracy\": float(acc)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6dd5cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3: Modeling & SHAP helper functions =====\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "def fit_xgb_classifier(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> XGBClassifier:\n",
    "    \"\"\"決定論的XGB（木系なのでスケーリング不要）。警告要因は排除。\"\"\"\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    y_train = y_train.astype(np.int32)\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        eval_metric=\"logloss\",\n",
    "        subsample=1.0,\n",
    "        colsample_bytree=1.0,\n",
    "        n_jobs=1,              # 決定論的\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cpu\",\n",
    "        seed=0, random_state=0\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def compute_train_shap_abs_mean(model: XGBClassifier, X_ref: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Boosterの pred_contribs でSHAP値（バイアス除く）→ |.|平均\"\"\"\n",
    "    X_ref = X_ref.astype(np.float32)\n",
    "    dm = xgb.DMatrix(X_ref, feature_names=list(X_ref.columns))\n",
    "    contribs = model.get_booster().predict(dm, pred_contribs=True)  # (n, n_features+1)\n",
    "    shap_vals = contribs[:, :-1]\n",
    "    abs_mean = np.abs(shap_vals).mean(axis=0)\n",
    "    return pd.Series(abs_mean, index=X_ref.columns, name=\"mean_abs\")\n",
    "\n",
    "def evaluate_fold(model: XGBClassifier, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "    proba = model.predict_proba(X_test.astype(np.float32))[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, proba)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    return {\"roc_auc\": roc_auc, \"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b1148b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SHAP ranking -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\SHAP_FEATURE_RANKING.CSV\n",
      "[OK] SHAP labeled -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\SHAP_FEATURE_RANKING_LABELED.CSV\n",
      "[OK] LOSO metrics -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\LOSO_METRICS.CSV\n",
      "[OK] Plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\SHAP_RANKING_ALL.PNG\n",
      "[OK] Plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\SHAP_TOP8_RANKING.PNG\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 4: SHAPランキング（LOSOで学習側のみ SHAP、平均集計） =====\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "shap_frames = []\n",
    "metrics_rows = []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_scaled_all, y_all, groups), start=1):\n",
    "    X_tr, X_te = X_scaled_all.iloc[tr_idx], X_scaled_all.iloc[te_idx]\n",
    "    y_tr, y_te = y_all.iloc[tr_idx], y_all.iloc[te_idx]\n",
    "    # 学習側が単一クラスは不可\n",
    "    if len(np.unique(y_tr)) < 2:\n",
    "        raise RuntimeError(\"[ERROR] 学習foldが単一クラスです。FMS閾値/期間を見直してください。\")\n",
    "\n",
    "    model = fit_xgb_classifier(X_tr, y_tr)\n",
    "    # 学習データ上でのSHAP重要度\n",
    "    abs_mean = compute_train_shap_abs_mean(model, X_tr).rename(f\"fold{fold_id}\")\n",
    "    shap_frames.append(abs_mean)\n",
    "\n",
    "    # 参考：テストfoldのAUC/ACC\n",
    "    m = evaluate_fold(model, X_te, y_te)\n",
    "    metrics_rows.append({\"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "                         \"roc_auc\": m[\"roc_auc\"], \"accuracy\": m[\"accuracy\"]})\n",
    "\n",
    "# 重要度表：各fold列＋平均列\n",
    "shap_rank = pd.concat(shap_frames, axis=1)\n",
    "shap_rank[\"mean_abs\"] = shap_rank.mean(axis=1)\n",
    "shap_rank = shap_rank.sort_values(\"mean_abs\", ascending=False)\n",
    "\n",
    "# 保存\n",
    "shap_rank.to_csv(outpath(\"SHAP_FEATURE_RANKING.CSV\"), encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] SHAP ranking -> {outpath('SHAP_FEATURE_RANKING.CSV')}\")\n",
    "\n",
    "# ラベル付き（表示名は今回は元列名をそのまま）\n",
    "shap_labeled = shap_rank.copy()\n",
    "shap_labeled.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] SHAP labeled -> {outpath('SHAP_FEATURE_RANKING_LABELED.CSV')}\")\n",
    "\n",
    "# 参考メトリクス\n",
    "pd.DataFrame(metrics_rows).to_csv(outpath(\"LOSO_METRICS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] LOSO metrics -> {outpath('LOSO_METRICS.CSV')}\")\n",
    "\n",
    "# 図（ALL & TOP10）\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ALL\n",
    "plt.figure(figsize=(10, max(5, len(shap_rank)//3)))\n",
    "plt.barh(shap_rank.index[::-1], shap_rank[\"mean_abs\"][::-1])\n",
    "plt.xlabel(\"Mean |SHAP|\"); plt.ylabel(\"Feature\")\n",
    "plt.title(\"SHAP Ranking (All)\")\n",
    "plt.tight_layout(); plt.savefig(outpath(\"SHAP_RANKING_ALL.PNG\"), dpi=300); plt.close()\n",
    "print(f\"[OK] Plot -> {outpath('SHAP_RANKING_ALL.PNG')}\")\n",
    "\n",
    "# TOP8（数値ラベルなし・フォント拡大）\n",
    "topk = shap_rank.head(8).iloc[::-1]  # 上位8件を下から描く\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = plt.gca()\n",
    "ax.barh(topk.index, topk[\"mean_abs\"].values)\n",
    "\n",
    "# 余白と体裁\n",
    "mx = float(topk[\"mean_abs\"].max()) if len(topk) else 1.0\n",
    "ax.set_xlim(0, mx * 1.08)  # 少し余白\n",
    "ax.set_xlabel(\"Mean |SHAP value|\", fontsize=26)\n",
    "ax.set_ylabel(\"Feature\", fontsize=26)\n",
    "ax.tick_params(axis=\"both\", labelsize=22)\n",
    "ax.set_title(\"Top-8 SHAP Feature Ranking\", fontsize=34, pad=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"SHAP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "plt.close()\n",
    "print(f\"[OK] Plot -> {outpath('SHAP_TOP8_RANKING.PNG')}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f2368c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUC_PER_K.CSV\n",
      "[INFO] Best AUC at k=5: AUC=0.747\n",
      "[OK] Plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUC_VS_NUM_FEATURES.PNG\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6: 全kで pooled AUC → best_k 決定（しきい値非依存） =====\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# ランキング読込（必ず OUT_DIR 内）\n",
    "rank_candidates = [outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"),\n",
    "                   outpath(\"SHAP_FEATURE_RANKING.CSV\")]\n",
    "rank_path = None\n",
    "for p in rank_candidates:\n",
    "    if os.path.exists(p):\n",
    "        rank_path = p; break\n",
    "if rank_path is None:\n",
    "    raise FileNotFoundError(\"[ERROR] SHAP_FEATURE_RANKING(_LABELED).CSV が OUT_DIR にありません。\")\n",
    "\n",
    "rank_df = pd.read_csv(rank_path, encoding=\"utf-8-sig\", index_col=0)\n",
    "rank_col = \"mean_abs\" if \"mean_abs\" in rank_df.columns else (\"mean_abs_shap\" if \"mean_abs_shap\" in rank_df.columns else None)\n",
    "if rank_col is None:\n",
    "    raise KeyError(\"[ERROR] ランキングCSVに mean_abs / mean_abs_shap がありません。\")\n",
    "rank_df = rank_df.sort_values(rank_col, ascending=False)\n",
    "\n",
    "feature_order = [f for f in rank_df.index if f in X_scaled_all.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[ERROR] ランキングの特徴が X_scaled_all に存在しません。\")\n",
    "\n",
    "ks = list(range(len(feature_order), 0, -1))  # 多→少\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "auc_list = []\n",
    "for k in ks:\n",
    "    feats = feature_order[:k]\n",
    "    X = X_scaled_all[feats].astype(np.float32)\n",
    "    y = y_all.values\n",
    "    g = groups.values\n",
    "\n",
    "    y_true_all, proba_all = [], []\n",
    "    for tr_idx, te_idx in logo.split(X, y, g):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(\"[ERROR] 学習foldが単一クラスです。閾値/範囲を見直してください。\")\n",
    "        m = fit_xgb_classifier(X_tr, pd.Series(y_tr))\n",
    "        proba = m.predict_proba(X_te)[:, 1]\n",
    "        y_true_all.append(y_te); proba_all.append(proba)\n",
    "    y_true_k = np.concatenate(y_true_all); proba_k = np.concatenate(proba_all)\n",
    "    if len(np.unique(y_true_k)) < 2:\n",
    "        raise RuntimeError(\"[ERROR] pooled 真値が単一クラスで AUC が計算できません。\")\n",
    "    auc_list.append(float(roc_auc_score(y_true_k, proba_k)))\n",
    "\n",
    "# CSV保存\n",
    "pd.DataFrame({\"k\": ks, \"auc_pooled\": auc_list}).to_csv(outpath(\"AUC_PER_K.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] CSV -> {outpath('AUC_PER_K.CSV')}\")\n",
    "\n",
    "# best_k\n",
    "auc_array = np.asarray(auc_list, dtype=float)\n",
    "best_k = ks[int(np.nanargmax(auc_array))]\n",
    "best_auc = float(np.nanmax(auc_array))\n",
    "print(f\"[INFO] Best AUC at k={best_k}: AUC={best_auc:.3f}\")\n",
    "\n",
    "# 図（最大点の赤丸＋注釈、フォント拡大）\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(ks, auc_list, marker='o', linewidth=1.5)\n",
    "ax.scatter([best_k], [best_auc], s=180, color=\"red\", zorder=5)\n",
    "\n",
    "ax.annotate(f\"Max AUC = {best_auc:.3f} (k={best_k})\",\n",
    "            xy=(best_k, best_auc),\n",
    "            xytext=(best_k, best_auc + 0.02),\n",
    "            ha=\"center\", va=\"bottom\", fontsize=20, color=\"red\")\n",
    "\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlabel(\"Number of Features (k)\", fontsize=26)\n",
    "ax.set_ylabel(\"ROC AUC (pooled)\", fontsize=26)\n",
    "ax.tick_params(axis=\"both\", labelsize=22)\n",
    "ax.set_title(\"AUC vs Number of Features\", fontsize=34, pad=10)\n",
    "ax.grid(True, alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"AUC_VS_NUM_FEATURES.PNG\"), dpi=300)\n",
    "plt.close()\n",
    "print(f\"[OK] Plot -> {outpath('AUC_VS_NUM_FEATURES.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5c4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] load OOF -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\OOF_PRED_BESTK.CSV\n",
      "[INFO] OOF AUC = 0.747  (n=340)\n",
      "[BOOT] start: B=2000, mode=oof, BEST_K=5, SEED=20251101\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell X: 被験者ブートストラップCI（AUC, OOFベース既定） =====\n",
    "\"\"\"\n",
    "目的：\n",
    "- LOSO OOF予測を固定し，被験者（cluster）単位のブートストラップで AUC の95%CIを推定する．\n",
    "- 既に OOF が無ければ，上位 BEST_K 特徴（Cell4のSHAPランキング）で一度だけLOSOしてOOFを作成してから実行．\n",
    "\n",
    "出力：\n",
    "- OOF_PRED_BESTK.CSV（無ければ作成）\n",
    "- AUC_BOOTSTRAP_SUBJECT.csv（各反復のAUC）\n",
    "- AUC_BOOTSTRAP_SUMMARY.csv（平均/SE/95%CI/有効反復数 等）\n",
    "- AUC_BOOTSTRAP_HIST.png（分布＋CI） / AUC_BOOTSTRAP_ECDF.png（累積分布）\n",
    "\n",
    "主要パラメータ（下の CONFIG を調整）：\n",
    "- B = 2000（反復回数）\n",
    "- SEED = 20251101（乱数）\n",
    "- MAX_REDRAW = 20（単一クラス回避の再抽選上限）\n",
    "- MODE = \"oof\" または \"retrain\"（既定は oof）\n",
    "- STRATIFY_BY = None または \"MSSQ_group\"（被験者層別の比率維持；既定 None）\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# -------- CONFIG --------\n",
    "B = 2000\n",
    "SEED = 20251101\n",
    "MAX_REDRAW = 20\n",
    "MODE = \"oof\"            # \"oof\" or \"retrain\"\n",
    "STRATIFY_BY = None      # None or \"MSSQ_group\"\n",
    "OOF_CSV = \"OOF_PRED_BESTK.CSV\"\n",
    "BOOT_CSV = \"AUC_BOOTSTRAP_SUBJECT.csv\"\n",
    "SUMM_CSV = \"AUC_BOOTSTRAP_SUMMARY.csv\"\n",
    "HIST_PNG = \"AUC_BOOTSTRAP_HIST.png\"\n",
    "ECDF_PNG = \"AUC_BOOTSTRAP_ECDF.png\"\n",
    "\n",
    "# ------------- 前提確認 -------------\n",
    "assert 'X_scaled_all' in globals(), \"[ERROR] X_scaled_all が未定義\"\n",
    "assert 'y_all'        in globals(), \"[ERROR] y_all が未定義\"\n",
    "assert 'groups'       in globals(), \"[ERROR] groups が未定義\"\n",
    "assert 'outpath'      in globals(), \"[ERROR] outpath() が未定義\"\n",
    "\n",
    "# ------------- ユーティリティ -------------\n",
    "def _load_feature_order():\n",
    "    \"\"\"Cell4のランキングCSVから重要度降順の特徴順を取得\"\"\"\n",
    "    rank_candidates = [outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"),\n",
    "                       outpath(\"SHAP_FEATURE_RANKING.CSV\")]\n",
    "    rank_path = None\n",
    "    for p in rank_candidates:\n",
    "        if os.path.exists(p):\n",
    "            rank_path = p; break\n",
    "    if rank_path is None:\n",
    "        raise FileNotFoundError(\"[ERROR] SHAP_FEATURE_RANKING*.CSV が見つかりません（Cell 4 実行を確認）\")\n",
    "\n",
    "    df = pd.read_csv(rank_path, encoding=\"utf-8-sig\", index_col=0)\n",
    "    rcol = \"mean_abs\" if \"mean_abs\" in df.columns else (\"mean_abs_shap\" if \"mean_abs_shap\" in df.columns else None)\n",
    "    if rcol is None:\n",
    "        raise KeyError(\"[ERROR] ランキングCSVに mean_abs / mean_abs_shap が無い\")\n",
    "    order = [f for f in df.sort_values(rcol, ascending=False).index if f in X_scaled_all.columns]\n",
    "    if not order:\n",
    "        raise RuntimeError(\"[ERROR] ランキングの特徴が X_scaled_all に存在しません\")\n",
    "    return order\n",
    "\n",
    "def _predict_proba_safe(model, X):\n",
    "    \"\"\"predict_proba が無い学習器への保険\"\"\"\n",
    "    try:\n",
    "        return model.predict_proba(X.astype(np.float32))[:, 1]\n",
    "    except Exception:\n",
    "        p = model.decision_function(X)\n",
    "        p = (p - p.min()) / (p.max() - p.min() + 1e-12)\n",
    "        return p\n",
    "\n",
    "def build_oof_bestk(feature_order, best_k):\n",
    "    \"\"\"上位 BEST_K 特徴で LOSO OOF を作成（1回だけ学習）\"\"\"\n",
    "    feats = feature_order[:int(best_k)]\n",
    "    X = X_scaled_all[feats].astype(np.float32)\n",
    "    y = pd.Series(np.asarray(y_all)).reset_index(drop=True)\n",
    "    g = pd.Series(groups.astype(str).values).reset_index(drop=True)\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    rows = []\n",
    "    for tr_idx, te_idx in logo.split(X, y, g):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(\"[ERROR] 学習foldが単一クラス（OOF作成中）\")\n",
    "        model = fit_xgb_classifier(X_tr, y_tr)\n",
    "        proba = _predict_proba_safe(model, X_te)\n",
    "        sub_ids = g.iloc[te_idx].astype(str).values  # 同一subjectが並ぶ\n",
    "        df_fold = pd.DataFrame({\"subject\": sub_ids, \"y_true\": y_te.values, \"y_score\": proba})\n",
    "        rows.append(df_fold)\n",
    "\n",
    "    oof = pd.concat(rows, ignore_index=True)\n",
    "    oof.to_csv(outpath(OOF_CSV), index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] OOF saved -> {outpath(OOF_CSV)}\")\n",
    "    return oof\n",
    "\n",
    "def _attach_strata_if_needed(oof_df):\n",
    "    \"\"\"STRATIFY_BY='MSSQ_group' の場合，被験者メタから層ラベルを付与\"\"\"\n",
    "    if STRATIFY_BY is None:\n",
    "        oof_df[\"strata\"] = \"ALL\"\n",
    "        return oof_df\n",
    "    if STRATIFY_BY == \"MSSQ_group\" and 'SUBJECT_META' in globals():\n",
    "        meta = SUBJECT_META.copy()\n",
    "        # 代表名の推定\n",
    "        id_col  = next((c for c in meta.columns if c.lower() in (\"subject\",\"subject_id\",\"id\",\"sid\")), None)\n",
    "        grp_col = next((c for c in meta.columns if c.lower() in (\"mssq_group\",\"mssqgroup\",\"group\",\"mssq_highlow\")), None)\n",
    "        if (id_col is not None) and (grp_col is not None):\n",
    "            d = dict(zip(meta[id_col].astype(str), meta[grp_col].astype(str)))\n",
    "            oof_df[\"strata\"] = oof_df[\"subject\"].astype(str).map(d).fillna(\"ALL\")\n",
    "            return oof_df\n",
    "    # フォールバック\n",
    "    oof_df[\"strata\"] = \"ALL\"\n",
    "    return oof_df\n",
    "\n",
    "def bootstrap_auc_subject(oof_df, B=2000, seed=20251101, max_redraw=20):\n",
    "    \"\"\"被験者（cluster）ブートストラップでAUC分布を推定（OOF固定）\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    subjects = oof_df[\"subject\"].astype(str).unique()\n",
    "    n_subj = len(subjects)\n",
    "\n",
    "    # strataごとに被験者集合を準備\n",
    "    strata_by_subj = (oof_df[[\"subject\",\"strata\"]].drop_duplicates()\n",
    "                      .set_index(\"subject\")[\"strata\"].to_dict())\n",
    "    strata_levels = sorted(oof_df[\"strata\"].unique())\n",
    "    subj_by_strata = {s: [sub for sub in subjects if strata_by_subj.get(sub,\"ALL\")==s] for s in strata_levels}\n",
    "\n",
    "    rec, skipped = [], 0\n",
    "    print(f\"[BOOT] start: B={B}, mode=oof, BEST_K={globals().get('BEST_K','?')}, SEED={seed}\")\n",
    "\n",
    "    for b in range(B):\n",
    "        redraw = 0\n",
    "        while True:\n",
    "            # 層別（必要なら各層で元の被験者数と同数を復元）\n",
    "            chosen = []\n",
    "            for st in strata_levels:\n",
    "                pool = subj_by_strata[st]\n",
    "                if len(pool) == 0:\n",
    "                    continue\n",
    "                chosen.extend(list(rng.choice(pool, size=len(pool), replace=True)))\n",
    "            # 連結（重複subjectは複数回分を結合）\n",
    "            parts = [oof_df[oof_df[\"subject\"]==sid] for sid in chosen]\n",
    "            boot = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "            yb = boot[\"y_true\"].values\n",
    "            if np.unique(yb).size >= 2:\n",
    "                break\n",
    "            redraw += 1\n",
    "            if redraw > max_redraw:\n",
    "                skipped += 1\n",
    "                boot = None\n",
    "                break\n",
    "        if boot is None:\n",
    "            continue\n",
    "\n",
    "        auc_b = float(roc_auc_score(boot[\"y_true\"].values, boot[\"y_score\"].values))\n",
    "        rec.append(dict(\n",
    "            b_id=int(b), auc=auc_b, n_subjects=int(n_subj),\n",
    "            n_pos=int((boot[\"y_true\"].values==1).sum()),\n",
    "            n_neg=int((boot[\"y_true\"].values==0).sum()),\n",
    "            seed=int(seed)\n",
    "        ))\n",
    "        if (b+1) % 200 == 0:\n",
    "            print(f\"[BOOT] b={b+1:4d}  auc={auc_b:.3f}\")\n",
    "\n",
    "    df_boot = pd.DataFrame(rec)\n",
    "    return df_boot, skipped\n",
    "\n",
    "def summarize_bootstrap(df_boot, auc_oof):\n",
    "    \"\"\"平均・SE・95%percentile CI を計算\"\"\"\n",
    "    vals = df_boot[\"auc\"].dropna().values\n",
    "    mean = float(np.nanmean(vals)) if len(vals) else np.nan\n",
    "    se   = float(np.nanstd(vals, ddof=1)/np.sqrt(max(1,len(vals)))) if len(vals)>1 else np.nan\n",
    "    p2p5 = float(np.nanquantile(vals, 0.025)) if len(vals) else np.nan\n",
    "    p97p5= float(np.nanquantile(vals, 0.975)) if len(vals) else np.nan\n",
    "    return dict(auc_oof=float(auc_oof), mean=mean, se=se, p2p5=p2p5, p97p5=p97p5,\n",
    "                n_boot=int(len(vals)))\n",
    "\n",
    "def _set_plot_style():\n",
    "    plt.rcParams.update({\n",
    "        \"font.size\": 20, \"axes.titlesize\": 30, \"axes.labelsize\": 24,\n",
    "        \"xtick.labelsize\": 20, \"ytick.labelsize\": 20, \"legend.fontsize\": 20\n",
    "    })\n",
    "\n",
    "def plot_bootstrap_hist(df_boot, auc_oof, ci_low, ci_high, note, png_name):\n",
    "    _set_plot_style()\n",
    "    plt.figure(figsize=(9,6))\n",
    "    vals = df_boot[\"auc\"].dropna().values\n",
    "    plt.hist(vals, bins=30, alpha=0.8)\n",
    "    # 目標線\n",
    "    ax = plt.gca()\n",
    "    ax.axvline(auc_oof, color=\"red\", linewidth=1.5, label=f\"OOF AUC = {auc_oof:.3f}\")\n",
    "    ax.axvline(ci_low, color=\"black\", linewidth=1.5, linestyle=\"--\", label=f\"95% CI [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "    ax.axvline(ci_high, color=\"black\", linewidth=1.5, linestyle=\"--\")\n",
    "    plt.title(\"Subject Bootstrap of AUC (Histogram)\")\n",
    "    plt.xlabel(\"AUC\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.text(0.98, 0.02, note, ha=\"right\", va=\"bottom\", transform=ax.transAxes, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(png_name), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[OK] FIG -> {outpath(png_name)}\")\n",
    "\n",
    "def plot_bootstrap_ecdf(df_boot, auc_oof, ci_low, ci_high, note, png_name):\n",
    "    _set_plot_style()\n",
    "    plt.figure(figsize=(9,6))\n",
    "    vals = np.sort(df_boot[\"auc\"].dropna().values)\n",
    "    y = np.arange(1, len(vals)+1) / max(1, len(vals))\n",
    "    plt.plot(vals, y, linewidth=1.5)\n",
    "    ax = plt.gca()\n",
    "    ax.axvline(auc_oof, color=\"red\", linewidth=1.5, label=f\"OOF AUC = {auc_oof:.3f}\")\n",
    "    ax.axvline(ci_low, color=\"black\", linewidth=1.5, linestyle=\"--\", label=f\"95% CI [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "    ax.axvline(ci_high, color=\"black\", linewidth=1.5, linestyle=\"--\")\n",
    "    plt.title(\"Subject Bootstrap of AUC (ECDF)\")\n",
    "    plt.xlabel(\"AUC\")\n",
    "    plt.ylabel(\"Cumulative probability\")\n",
    "    plt.legend()\n",
    "    plt.text(0.98, 0.02, note, ha=\"right\", va=\"bottom\", transform=ax.transAxes, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(png_name), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[OK] FIG -> {outpath(png_name)}\")\n",
    "\n",
    "# ------------- 実行 -------------\n",
    "# 1) OOFの用意（無ければ作成）\n",
    "oof_path = outpath(OOF_CSV)\n",
    "if os.path.exists(oof_path):\n",
    "    oof = pd.read_csv(oof_path, encoding=\"utf-8-sig\")\n",
    "    print(f\"[INFO] load OOF -> {oof_path}\")\n",
    "else:\n",
    "    assert 'BEST_K' in globals(), \"[ERROR] BEST_K が未定義（Cell 6 実行で決定してください）\"\n",
    "    feat_order = _load_feature_order()\n",
    "    oof = build_oof_bestk(feat_order, BEST_K)\n",
    "\n",
    "# OOF基準のAUC\n",
    "auc_oof = float(roc_auc_score(oof[\"y_true\"].values, oof[\"y_score\"].values))\n",
    "print(f\"[INFO] OOF AUC = {auc_oof:.3f}  (n={len(oof)})\")\n",
    "\n",
    "# 層ラベル付与（必要時）\n",
    "oof = _attach_strata_if_needed(oof)\n",
    "\n",
    "# 2) ブートストラップ（MODE=\"oof\" の場合）\n",
    "if MODE == \"oof\":\n",
    "    df_boot, skipped = bootstrap_auc_subject(oof, B=B, seed=SEED, max_redraw=MAX_REDRAW)\n",
    "\n",
    "# 3) まとめ・保存\n",
    "df_boot.to_csv(outpath(BOOT_CSV), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] CSV -> {outpath(BOOT_CSV)}\")\n",
    "\n",
    "summ = summarize_bootstrap(df_boot, auc_oof)\n",
    "summ_df = pd.DataFrame([{\n",
    "    **summ,\n",
    "    \"skipped\": int(skipped),\n",
    "    \"B\": int(B),\n",
    "    \"BEST_K\": int(globals().get(\"BEST_K\", -1)),\n",
    "    \"MODE\": MODE,\n",
    "    \"SEED\": int(SEED)\n",
    "}])\n",
    "summ_df.to_csv(outpath(SUMM_CSV), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] CSV -> {outpath(SUMM_CSV)}\")\n",
    "print(f\"[INFO] 95% CI: [{summ['p2p5']:.3f}, {summ['p97p5']:.3f}]  mean={summ['mean']:.3f}  se={summ['se']:.4f}  (n_boot={summ['n_boot']}, skipped={skipped})\")\n",
    "\n",
    "# 4) 図\n",
    "note = f\"B={B}, BEST_K={globals().get('BEST_K','?')}, MODE={MODE}, SEED={SEED}\"\n",
    "plot_bootstrap_hist(df_boot, auc_oof, summ[\"p2p5\"], summ[\"p97p5\"], note, HIST_PNG)\n",
    "plot_bootstrap_ecdf(df_boot, auc_oof, summ[\"p2p5\"], summ[\"p97p5\"], note, ECDF_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c9f0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_tr)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] 学習foldが単一クラスです。閾値/範囲を見直してください。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mfit_xgb_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m proba \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mpredict_proba(X_te)[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     49\u001b[0m y_true_all\u001b[38;5;241m.\u001b[39mappend(y_te); proba_all\u001b[38;5;241m.\u001b[39mappend(proba)\n",
      "Cell \u001b[1;32mIn[50], line 26\u001b[0m, in \u001b[0;36mfit_xgb_classifier\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m     15\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[0;32m     17\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     18\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1682\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1660\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1661\u001b[0m     xgb_model, params, feature_weights\n\u001b[0;32m   1662\u001b[0m )\n\u001b[0;32m   1663\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1664\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1665\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1680\u001b[0m )\n\u001b[1;32m-> 1682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2243\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtrain, DMatrix):\n\u001b[0;32m   2242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid training matrix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dtrain)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2243\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_dmatrix_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2246\u001b[0m     _check_call(\n\u001b[0;32m   2247\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[0;32m   2248\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   2249\u001b[0m         )\n\u001b[0;32m   2250\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:3203\u001b[0m, in \u001b[0;36mBooster._assign_dmatrix_features\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   3200\u001b[0m fn \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfeature_names\n\u001b[0;32m   3201\u001b[0m ft \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfeature_types\n\u001b[1;32m-> 3203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_names\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names \u001b[38;5;241m=\u001b[39m fn\n\u001b[0;32m   3205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2193\u001b[0m, in \u001b[0;36mBooster.feature_names\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2187\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_names\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[FeatureNames]:\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Feature names for this booster.  Can be directly set by input data or by\u001b[39;00m\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;124;03m    assignment.\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m \n\u001b[0;32m   2192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_feature_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeature_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2143\u001b[0m, in \u001b[0;36mBooster._get_feature_info\u001b[1;34m(self, field)\u001b[0m\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m _check_call(\n\u001b[1;32m-> 2143\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterGetStrFeatureInfo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43msarr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2149\u001b[0m )\n\u001b[0;32m   2150\u001b[0m feature_info \u001b[38;5;241m=\u001b[39m from_cstr_to_pystr(sarr, length)\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feature_info \u001b[38;5;28;01mif\u001b[39;00m feature_info \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===== Cell 6b: 全kで pooled AUPRC(AP) → best_k 決定（しきい値非依存）＋ best_k のPR曲線 =====\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- 設定：APで選んだkを best_k に反映するか（Cell7/Cell8の前提用） ---\n",
    "USE_AP_FOR_K = bool(globals().get(\"USE_AP_FOR_K\", False))  # 既定=True（APでbest_kを上書き）\n",
    "\n",
    "# ランキング読込（必ず OUT_DIR 内）\n",
    "rank_candidates = [outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"),\n",
    "                   outpath(\"SHAP_FEATURE_RANKING.CSV\")]\n",
    "rank_path = next((p for p in rank_candidates if os.path.exists(p)), None)\n",
    "if rank_path is None:\n",
    "    raise FileNotFoundError(\"[ERROR] SHAP_FEATURE_RANKING(_LABELED).CSV が OUT_DIR にありません。\")\n",
    "\n",
    "rank_df = pd.read_csv(rank_path, encoding=\"utf-8-sig\", index_col=0)\n",
    "rank_col = \"mean_abs\" if \"mean_abs\" in rank_df.columns else (\"mean_abs_shap\" if \"mean_abs_shap\" in rank_df.columns else None)\n",
    "if rank_col is None:\n",
    "    raise KeyError(\"[ERROR] ランキングCSVに mean_abs / mean_abs_shap がありません。\")\n",
    "rank_df = rank_df.sort_values(rank_col, ascending=False)\n",
    "\n",
    "feature_order = [f for f in rank_df.index if f in X_scaled_all.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[ERROR] ランキングの特徴が X_scaled_all に存在しません。\")\n",
    "\n",
    "ks = list(range(len(feature_order), 0, -1))  # 多→少\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "ap_list, prauc_list = [], []\n",
    "pi_list = []  # 陽性率（基準ライン）\n",
    "\n",
    "for k in ks:\n",
    "    feats = feature_order[:k]\n",
    "    X = X_scaled_all[feats].astype(np.float32)\n",
    "    y = y_all.values\n",
    "    g = groups.values\n",
    "\n",
    "    y_true_all, proba_all = [], []\n",
    "    for tr_idx, te_idx in logo.split(X, y, g):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(\"[ERROR] 学習foldが単一クラスです。閾値/範囲を見直してください。\")\n",
    "        m = fit_xgb_classifier(X_tr, pd.Series(y_tr))\n",
    "        proba = m.predict_proba(X_te)[:, 1]\n",
    "        y_true_all.append(y_te); proba_all.append(proba)\n",
    "\n",
    "    y_true_k = np.concatenate(y_true_all)\n",
    "    proba_k  = np.concatenate(proba_all)\n",
    "    if len(np.unique(y_true_k)) < 2:\n",
    "        raise RuntimeError(\"[ERROR] pooled 真値が単一クラスで AUPRC が計算できません。\")\n",
    "\n",
    "    # Average Precision（AP）と参考の台形則PR-AUC\n",
    "    ap = float(average_precision_score(y_true_k, proba_k))\n",
    "    prec, rec, _ = precision_recall_curve(y_true_k, proba_k)\n",
    "    prauc = float(auc(rec, prec))\n",
    "\n",
    "    ap_list.append(ap)\n",
    "    prauc_list.append(prauc)\n",
    "    pi_list.append(float((y_true_k == 1).mean()))\n",
    "\n",
    "# CSV保存（kごとのAP）\n",
    "out_csv = outpath(\"AUPRC_PER_K.CSV\")\n",
    "pd.DataFrame({\n",
    "    \"k\": ks,\n",
    "    \"ap_pooled\": ap_list,\n",
    "    \"pr_auc_pooled\": prauc_list,\n",
    "    \"pi\": pi_list\n",
    "}).to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] CSV -> {out_csv}\")\n",
    "\n",
    "# best_k（AP最大で決定）\n",
    "ap_array = np.asarray(ap_list, dtype=float)\n",
    "best_k_ap = ks[int(np.nanargmax(ap_array))]\n",
    "best_ap = float(np.nanmax(ap_array))\n",
    "print(f\"[INFO] Best AP at k={best_k_ap}: AP={best_ap:.3f}\")\n",
    "\n",
    "# 図（AP vs k：最大点の赤丸＋注釈＋基準ライン π）\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = plt.gca()\n",
    "ax.plot(ks, ap_list, marker='o', linewidth=1.5, label=\"AP (AUPRC)\")\n",
    "ax.scatter([best_k_ap], [best_ap], s=180, color=\"red\", zorder=5)\n",
    "\n",
    "# πは全kで同一のはずだが、明示的に平均値を使う\n",
    "pi_ref = float(np.mean(pi_list)) if len(pi_list) > 0 else np.nan\n",
    "if np.isfinite(pi_ref):\n",
    "    ax.axhline(pi_ref, linestyle=\"--\", linewidth=1.5, label=f\"Baseline (π={pi_ref:.3f})\", alpha=0.8)\n",
    "\n",
    "ax.annotate(f\"Max AP = {best_ap:.3f} (k={best_k_ap})\",\n",
    "            xy=(best_k_ap, best_ap),\n",
    "            xytext=(best_k_ap, best_ap + 0.03),\n",
    "            ha=\"center\", va=\"bottom\", fontsize=20, color=\"red\")\n",
    "\n",
    "# 表示規約\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlabel(\"Number of Features (k)\", fontsize=24)\n",
    "ax.set_ylabel(\"AUPRC (Average Precision)\", fontsize=24)\n",
    "ax.tick_params(axis=\"both\", labelsize=20)\n",
    "ax.set_title(\"AUPRC vs Number of Features\", fontsize=30, pad=10)\n",
    "ax.grid(True, alpha=0.4)\n",
    "ax.legend(fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "out_png = outpath(\"AUPRC_VS_NUM_FEATURES.PNG\")\n",
    "plt.savefig(out_png, dpi=300)\n",
    "plt.close()\n",
    "print(f\"[OK] Plot -> {out_png}\")\n",
    "\n",
    "# --- Cell7/Cell8 互換：best_k を必要に応じて設定 ---\n",
    "if USE_AP_FOR_K or (\"best_k\" not in globals()):\n",
    "    best_k = int(best_k_ap)\n",
    "    print(f\"[INFO] best_k を APベースで設定: best_k={best_k}  (USE_AP_FOR_K={USE_AP_FOR_K})\")\n",
    "else:\n",
    "    # 既存の best_k を尊重（Cell6でAUC選抜済みなど）\n",
    "    print(f\"[INFO] 既存の best_k を保持（USE_AP_FOR_K=False）。APベースは best_k_ap={best_k_ap}\")\n",
    "\n",
    "# =========================\n",
    "# 追加：best_k における PR 曲線の作図（pooled outer-LOSO 予測）\n",
    "# =========================\n",
    "feats_best = feature_order[:best_k_ap]\n",
    "X_best = X_scaled_all[feats_best].astype(np.float32)\n",
    "y_full = y_all.values\n",
    "g_full = groups.values\n",
    "\n",
    "y_true_best, proba_best = [], []\n",
    "for tr_idx, te_idx in logo.split(X_best, y_full, g_full):\n",
    "    X_tr, X_te = X_best.iloc[tr_idx], X_best.iloc[te_idx]\n",
    "    y_tr, y_te = y_full[tr_idx], y_full[te_idx]\n",
    "    if len(np.unique(y_tr)) < 2:\n",
    "        raise RuntimeError(\"[ERROR] best_k の学習foldが単一クラスです。閾値/範囲を見直してください。\")\n",
    "    m = fit_xgb_classifier(X_tr, pd.Series(y_tr))\n",
    "    proba = m.predict_proba(X_te)[:, 1]\n",
    "    y_true_best.append(y_te); proba_best.append(proba)\n",
    "\n",
    "y_true_best = np.concatenate(y_true_best)\n",
    "proba_best  = np.concatenate(proba_best)\n",
    "if len(np.unique(y_true_best)) < 2:\n",
    "    raise RuntimeError(\"[ERROR] best_k の pooled 真値が単一クラスで PR 曲線を描画できません。\")\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_true_best, proba_best)\n",
    "ap_best = float(average_precision_score(y_true_best, proba_best))\n",
    "prauc_best = float(auc(rec, prec))\n",
    "pi_best = float((y_true_best == 1).mean())\n",
    "\n",
    "# PR 点群をCSV保存（しきい値は長さ合わせで先頭にNaNを追加）\n",
    "pr_csv = outpath(\"PR_CURVE_AT_BEST_K.CSV\")\n",
    "thr_pad = np.r_[np.nan, thr]  # precision/recall と揃える\n",
    "pd.DataFrame({\n",
    "    \"recall\": rec,\n",
    "    \"precision\": prec,\n",
    "    \"threshold\": thr_pad\n",
    "}).to_csv(pr_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] CSV -> {pr_csv}\")\n",
    "\n",
    "# 作図（PR曲線）\n",
    "plt.figure(figsize=(10, 7))\n",
    "ax = plt.gca()\n",
    "# ステップ表示（精度は階段的に変化する）\n",
    "ax.step(rec, prec, where=\"post\", linewidth=1.5, label=f\"PR (AP={ap_best:.3f}, PR-AUC={prauc_best:.3f})\")\n",
    "# 基準ライン：陽性率\n",
    "ax.axhline(pi_best, linestyle=\"--\", linewidth=1.5, label=f\"Baseline π={pi_best:.3f}\", alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Recall\", fontsize=24)\n",
    "ax.set_ylabel(\"Precision\", fontsize=24)\n",
    "ax.tick_params(axis=\"both\", labelsize=20)\n",
    "ax.set_title(f\"Precision–Recall at best k = {best_k_ap}\", fontsize=30, pad=10)\n",
    "ax.set_xlim([0.0, 1.0]); ax.set_ylim([0.0, 1.05])\n",
    "ax.grid(True, alpha=0.4)\n",
    "ax.legend(fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "pr_png = outpath(\"PR_CURVE_AT_BEST_K.PNG\")\n",
    "plt.savefig(pr_png, dpi=300)\n",
    "plt.close()\n",
    "print(f\"[OK] Plot -> {pr_png}  (features: top-{best_k_ap})\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b99358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell6c] CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUC_PER_K_BY_GROUP.CSV\n",
      "[Cell6c] BEST_K 保存 -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\BEST_K_BY_GROUP.JSON\n",
      "[Cell6c] High: best_k=30, AUC=0.651 | Low: best_k=3, AUC=0.798\n",
      "[Cell6c] Plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUC_VS_K_BY_GROUP.PNG\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 6c: MSSQ群別 AUC vs k（in-group LOSO；しきい値非依存） =====\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- 前提チェック ---\n",
    "req_vars = [\"X_scaled_all\", \"y_all\", \"groups\", \"SUBJECT_META\"]\n",
    "for v in req_vars:\n",
    "    if v not in globals():\n",
    "        raise RuntimeError(f\"[Cell6c][ERROR] 必要変数 {v} が未定義である。前セルを実行すること。\")\n",
    "\n",
    "if \"MSSQ_group\" not in SUBJECT_META.columns:\n",
    "    raise RuntimeError(\"[Cell6c][ERROR] SUBJECT_META に 'MSSQ_group' 列が存在しない。\")\n",
    "\n",
    "# outpath が無い環境でも動くようにフォールバック\n",
    "if \"outpath\" not in globals():\n",
    "    def outpath(name: str) -> str:\n",
    "        return os.path.abspath(name)\n",
    "\n",
    "# --- ランキング読込（Cell 4/6と同様） ---\n",
    "rank_candidates = [outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"),\n",
    "                   outpath(\"SHAP_FEATURE_RANKING.CSV\")]\n",
    "rank_path = next((p for p in rank_candidates if os.path.exists(p)), None)\n",
    "if rank_path is None:\n",
    "    raise FileNotFoundError(\"[Cell6c][ERROR] SHAP_FEATURE_RANKING(_LABELED).CSV が見つからない。\")\n",
    "\n",
    "rank_df = pd.read_csv(rank_path, encoding=\"utf-8-sig\", index_col=0)\n",
    "rank_col = \"mean_abs\" if \"mean_abs\" in rank_df.columns else (\n",
    "    \"mean_abs_shap\" if \"mean_abs_shap\" in rank_df.columns else None\n",
    ")\n",
    "if rank_col is None:\n",
    "    raise KeyError(\"[Cell6c][ERROR] ランキングCSVに mean_abs / mean_abs_shap が無い。\")\n",
    "\n",
    "# 特徴の並び（存在チェック込み）\n",
    "feature_order = [f for f in rank_df.sort_values(rank_col, ascending=False).index\n",
    "                 if f in X_scaled_all.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[Cell6c][ERROR] ランキング上位特徴が X_scaled_all に1つも見つからない。\")\n",
    "\n",
    "# k の候補（多→少）\n",
    "ks = list(range(len(feature_order), 0, -1))\n",
    "\n",
    "# --- 群ラベル（MSSQ High/Low）を各サンプルに付与 ---\n",
    "sid_series = pd.Series(groups.astype(str), index=X_scaled_all.index)  # subject_id/被験者ID\n",
    "missing_ids = sorted(set(sid_series.unique()) - set(SUBJECT_META.index))\n",
    "if missing_ids:\n",
    "    raise RuntimeError(f\"[Cell6c][ERROR] SUBJECT_META に無い subject_id: {missing_ids}\")\n",
    "\n",
    "fair_groups = sid_series.map(SUBJECT_META[\"MSSQ_group\"].astype(str))  # \"High\"/\"Low\"\n",
    "if fair_groups.isna().any():\n",
    "    bad = sorted(sid_series[fair_groups.isna()].unique().tolist())\n",
    "    raise RuntimeError(f\"[Cell6c][ERROR] MSSQ_group が欠損の subject_id: {bad}\")\n",
    "\n",
    "# --- 学習器ビルダー（既存の fit_xgb_classifier を優先） ---\n",
    "def _fit_clf(X_tr, y_tr):\n",
    "    if \"fit_xgb_classifier\" in globals():\n",
    "        return fit_xgb_classifier(X_tr, pd.Series(y_tr))\n",
    "    # フォールバック：ロジスティック回帰（L2）※xgboost未定義時\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    m = LogisticRegression(max_iter=200, class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "    m.fit(X_tr, y_tr.astype(int))\n",
    "    return m\n",
    "\n",
    "# --- 群ごとに in-group LOSO で AUC vs k を計算 ---\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "def _auc_vs_k_for_group(tag: str):\n",
    "    \"\"\"tag ∈ {'High','Low'}\"\"\"\n",
    "    mask = (fair_groups.values == tag)\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"[Cell6c][WARN] {tag} 群にサンプルが無い。\")\n",
    "        return [np.nan]*len(ks), 0, 0, 0\n",
    "\n",
    "    X_sub = X_scaled_all.loc[mask]\n",
    "    y_sub = pd.Series(y_all, index=X_scaled_all.index).loc[mask].astype(int).values\n",
    "    sid_sub = sid_series.loc[mask].values\n",
    "\n",
    "    n_sub = int(len(y_sub))\n",
    "    pos_sub = int(np.sum(y_sub == 1))\n",
    "    neg_sub = int(np.sum(y_sub == 0))\n",
    "\n",
    "    aucs = []\n",
    "    for k in ks:\n",
    "        feats = feature_order[:k]\n",
    "        Xk = X_sub[feats].astype(np.float32)\n",
    "\n",
    "        y_true_all, proba_all = [], []\n",
    "        skipped = 0\n",
    "\n",
    "        for tr_idx, te_idx in logo.split(Xk.values, y_sub, sid_sub):\n",
    "            y_tr = y_sub[tr_idx]\n",
    "            # 学習foldが単一クラスなら skip（AUC計算は pooled で行う）\n",
    "            if len(np.unique(y_tr)) < 2:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            m = _fit_clf(Xk.iloc[tr_idx], y_tr)\n",
    "            proba = m.predict_proba(Xk.iloc[te_idx])[:, 1]\n",
    "\n",
    "            y_true_all.append(y_sub[te_idx])\n",
    "            proba_all.append(proba)\n",
    "\n",
    "        if len(y_true_all) == 0:\n",
    "            print(f\"[Cell6c][WARN] {tag} 群 k={k}: すべてのfoldで学習ラベルが単一 → AUC不可\")\n",
    "            aucs.append(np.nan); continue\n",
    "\n",
    "        y_pool = np.concatenate(y_true_all)\n",
    "        p_pool = np.concatenate(proba_all)\n",
    "\n",
    "        if len(np.unique(y_pool)) < 2:\n",
    "            print(f\"[Cell6c][WARN] {tag} 群 k={k}: pooled 真値が単一クラス → AUC不可\")\n",
    "            aucs.append(np.nan); continue\n",
    "\n",
    "        auc = float(roc_auc_score(y_pool, p_pool))\n",
    "        aucs.append(auc)\n",
    "\n",
    "    return aucs, n_sub, pos_sub, neg_sub\n",
    "\n",
    "auc_high, nH, posH, negH = _auc_vs_k_for_group(\"High\")\n",
    "auc_low,  nL, posL, negL = _auc_vs_k_for_group(\"Low\")\n",
    "\n",
    "# --- CSV保存 ---\n",
    "df_out = pd.DataFrame({\n",
    "    \"k\": ks,\n",
    "    \"auc_high\": auc_high,\n",
    "    \"auc_low\": auc_low,\n",
    "    \"n_high\": nH, \"pos_high\": posH, \"neg_high\": negH,\n",
    "    \"n_low\": nL,  \"pos_low\": posL,  \"neg_low\": negL,\n",
    "})\n",
    "csv_path = outpath(\"AUC_PER_K_BY_GROUP.CSV\")\n",
    "df_out.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell6c] CSV -> {csv_path}\")\n",
    "\n",
    "# --- best k（同点は“より小さいk”を優先） ---\n",
    "def _best_k(ks_list, auc_list):\n",
    "    ks_arr = np.asarray(ks_list, dtype=int)\n",
    "    auc_arr = np.asarray(auc_list, dtype=float)\n",
    "    if np.all(np.isnan(auc_arr)):\n",
    "        return None, np.nan\n",
    "    maxv = np.nanmax(auc_arr)\n",
    "    # isclose を使って同点許容（数値誤差対策）\n",
    "    mask = np.isclose(auc_arr, maxv, rtol=1e-6, atol=1e-12)\n",
    "    cand_k = ks_arr[mask]\n",
    "    best_k = int(np.min(cand_k))  # より小さいkを選好\n",
    "    return best_k, float(maxv)\n",
    "\n",
    "best_k_high, best_auc_high = _best_k(ks, auc_high)\n",
    "best_k_low,  best_auc_low  = _best_k(ks, auc_low)\n",
    "\n",
    "# JSONでも保存（任意）\n",
    "best_json = {\n",
    "    \"BEST_K_HIGH\": best_k_high,\n",
    "    \"BEST_AUC_HIGH\": best_auc_high,\n",
    "    \"BEST_K_LOW\": best_k_low,\n",
    "    \"BEST_AUC_LOW\": best_auc_low,\n",
    "}\n",
    "json_path = outpath(\"BEST_K_BY_GROUP.JSON\")\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_json, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[Cell6c] BEST_K 保存 -> {json_path}\")\n",
    "print(f\"[Cell6c] High: best_k={best_k_high}, AUC={best_auc_high:.3f} | Low: best_k={best_k_low}, AUC={best_auc_low:.3f}\")\n",
    "\n",
    "# --- 図：AUC vs k（High/Low） ---\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"axes.titlesize\": 30,\n",
    "    \"axes.labelsize\": 24,\n",
    "    \"legend.fontsize\": 20,\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "ax.plot(ks, auc_high, marker=\"o\", label=\"MSSQ High\")\n",
    "ax.plot(ks, auc_low,  marker=\"s\", label=\"MSSQ Low\")\n",
    "\n",
    "# 最大点にマーカー＋注釈（存在する場合のみ）\n",
    "if best_k_high is not None and not np.isnan(best_auc_high):\n",
    "    ax.scatter([best_k_high], [best_auc_high], s=160, zorder=5)\n",
    "    ax.annotate(f\"High max={best_auc_high:.3f} (k={best_k_high})\",\n",
    "                xy=(best_k_high, best_auc_high),\n",
    "                xytext=(best_k_high, best_auc_high + 0.02),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=18)\n",
    "\n",
    "if best_k_low is not None and not np.isnan(best_auc_low):\n",
    "    ax.scatter([best_k_low], [best_auc_low], s=160, zorder=5)\n",
    "    ax.annotate(f\"Low max={best_auc_low:.3f} (k={best_k_low})\",\n",
    "                xy=(best_k_low, best_auc_low),\n",
    "                xytext=(best_k_low, best_auc_low + 0.02),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=18)\n",
    "\n",
    "ax.invert_xaxis()  # k が大→小の視覚（既存Cell6に合わせる）\n",
    "ax.set_xlabel(\"Number of Features (k)\")\n",
    "ax.set_ylabel(\"ROC AUC (pooled, in-group LOSO)\")\n",
    "ax.set_title(\"AUC vs Number of Features by MSSQ Group (in-group LOSO)\")\n",
    "ax.grid(True, alpha=0.4)\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "png_path = outpath(\"AUC_VS_K_BY_GROUP.PNG\")\n",
    "plt.savefig(png_path, dpi=300)\n",
    "plt.close()\n",
    "print(f\"[Cell6c] Plot -> {png_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81043540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell7] Settings:\n",
      "  USE_GLOBAL_BESTK = True\n",
      "  BEST_K = 5\n",
      "  GridSearch (BA): coarse=0.01, fine=0.001, margin=±0.03\n",
      "  HL Validator: retry_max=30, both_classes_required=True, min_samples=None\n",
      "  XGB_PARAMS = {'n_estimators': 100, 'eval_metric': 'logloss', 'subsample': 1.0, 'colsample_bytree': 1.0, 'n_jobs': 1, 'tree_method': 'hist', 'device': 'cpu', 'seed': 0, 'random_state': 0}\n",
      "  SEED_BASE = 20251101\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 7: 実験設定（固定 best_k／モデル既定／乱数・閾値探索パラメータ） =====\n",
    "import numpy as np\n",
    "\n",
    "# --- best_k を Cell 6 の結果から固定採用する ---\n",
    "assert 'best_k' in globals(), \"[Cell7] best_k が未定義である（Cell 6 を先に実行すること）\"\n",
    "USE_GLOBAL_BESTK: bool = True\n",
    "BEST_K: int = int(best_k)\n",
    "\n",
    "# （参考）本セルでは k 探索は行わないため K_LIST は使用しない（固定kで運用）\n",
    "# K_LIST = [BEST_K]\n",
    "\n",
    "# --- 群別グリッドサーチ（しきい値最適化：BA 最大化）の設定 ---\n",
    "THRESH_COARSE_STEP: float = 0.01   # 粗探索刻み\n",
    "THRESH_FINE_STEP:   float = 0.001  # 細探索刻み\n",
    "THRESH_MARGIN:      float = 0.03   # 細探索の±幅\n",
    "\n",
    "# --- しきい値探索モード／WG定義／校正方式 ---\n",
    "#THRESH_SEARCH_MODE: str = \"exact\"   # ← 追加（\"exact\" 固定運用）\n",
    "#THRESH_WG_MODE:     str = \"min\"     # ← 追加（\"min\": 最悪群, \"mean\": 等重み）\n",
    "#THRESH_CALIB:       str = \"none\"    # ← 任意（\"none\"/\"platt\"/\"isotonic\"）\n",
    "\n",
    "# --- HL 検証ペア（学習側16名の12.5%＝2名）の選定ポリシ ---\n",
    "VAL_RETRY_MAX: int = 30            # 条件を満たす HL ペアが引けるまでの再抽選上限回数\n",
    "VAL_REQUIRE_BOTH_CLASSES: bool = True  # 検証2名の合算で陽性/陰性がともに>0を要求する\n",
    "VAL_MIN_SAMPLES: int | None = None     # 検証データの最小サンプル数下限（例: 40）。不要なら None\n",
    "\n",
    "# --- 分類器（XGBoost）既定値 ---\n",
    "#     目的：確率の安定性と過学習抑制のバランス（早期終了なし。閾値は別途最適化するため）\n",
    "\n",
    "# --- 不均衡対策：scale_pos_weight をデータ毎に都度与える ---\n",
    "def _scale_pos_weight_from_y(y_binary: np.ndarray) -> float:\n",
    "    \"\"\"y_binary(0/1) から neg/pos を返す。pos=0 の場合は 1.0 を返す（学習は通すがログで警告する）である。\"\"\"\n",
    "    pos = int(np.sum(y_binary == 1))\n",
    "    neg = int(np.sum(y_binary == 0))\n",
    "    if pos == 0:\n",
    "        print(\"[Cell7][WARN] y に陽性が存在しないため scale_pos_weight を 1.0 とする（学習は通す）\")\n",
    "        return 1.0\n",
    "    return float(neg / max(pos, 1))\n",
    "\n",
    "# --- XGBClassifier ビルダー（既存実装と衝突しないように存在チェック） ---\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ここでは XGB_DEFAULTS を定義しない（Cell 0 の XGB_PARAMS が唯一のソース）\n",
    "def _build_xgb(scale_pos_weight: float) -> XGBClassifier:\n",
    "    # Cell 0 定義を土台にし、foldごとの不均衡だけ追記する\n",
    "    params = dict(XGB_PARAMS)\n",
    "    params[\"scale_pos_weight\"] = float(scale_pos_weight)\n",
    "    return XGBClassifier(**params)\n",
    "\n",
    "# 既に他セルで fit_xgb_classifier が定義済みならそれを優先し、未定義なら軽量版を用意する\n",
    "if \"fit_xgb_classifier\" not in globals():\n",
    "    def fit_xgb_classifier(X_train, y_train):\n",
    "        \"\"\"\n",
    "        既定ハイパラ＋データ由来の scale_pos_weight で XGB を単一学習して返す簡易版である。\n",
    "        - 早期終了や eval_set は使用しない（閾値最適化は別セルで行うため）\n",
    "        \"\"\"\n",
    "        y_arr = np.asarray(y_train, dtype=int)\n",
    "        spw = _scale_pos_weight_from_y(y_arr)\n",
    "        model = _build_xgb(spw)\n",
    "        model.fit(X_train, y_arr)\n",
    "        return model\n",
    "\n",
    "# --- ログ出力（実行時確認用） ---\n",
    "print(\"[Cell7] Settings:\")\n",
    "print(f\"  USE_GLOBAL_BESTK = {USE_GLOBAL_BESTK}\")\n",
    "print(f\"  BEST_K = {BEST_K}\")\n",
    "print(f\"  GridSearch (BA): coarse={THRESH_COARSE_STEP}, fine={THRESH_FINE_STEP}, margin=±{THRESH_MARGIN}\")\n",
    "print(f\"  HL Validator: retry_max={VAL_RETRY_MAX}, both_classes_required={VAL_REQUIRE_BOTH_CLASSES}, min_samples={VAL_MIN_SAMPLES}\")\n",
    "print(f\"  XGB_PARAMS = {XGB_PARAMS}\")\n",
    "print(f\"  SEED_BASE = {SEED_BASE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ab0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell7.4] SUBJECT_LABEL_STATS saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\SUBJECT_LABEL_STATS.CSV (subjects=17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_total</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>MSSQ_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10061</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10063</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10064</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10071</th>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10072</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_total  pos  neg MSSQ_group\n",
       "sid                                \n",
       "10061       20    6   14       High\n",
       "10063       20    0   20        Low\n",
       "10064       20    3   17       High\n",
       "10071       20   14    6       High\n",
       "10072       20    0   20        Low"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===== Cell 7.4 (NEW): Subjectごとのラベル分布を集計しCSV保存 =====\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 前提: groups (pd.Series: sample -> subject_id as str), y_all (pd.Series: 0/1)\n",
    "if groups.dtype != 'O':\n",
    "    groups = groups.astype(str)\n",
    "y_bin = y_all.astype(int)\n",
    "\n",
    "# 集計: 被験者ごとの総件数・陽性数・陰性数\n",
    "_subj_stats = (\n",
    "    pd.DataFrame({\"sid\": groups.values, \"y\": y_bin.values})\n",
    "      .groupby(\"sid\")[\"y\"]\n",
    "      .agg(n_total=\"count\", pos=\"sum\")\n",
    "      .reset_index()\n",
    ")\n",
    "_subj_stats[\"neg\"] = _subj_stats[\"n_total\"] - _subj_stats[\"pos\"]\n",
    "\n",
    "# 可能なら群情報（MSSQ_group）を付加（無ければスキップ）\n",
    "if \"SUBJECT_META\" in globals() and \"MSSQ_group\" in SUBJECT_META.columns:\n",
    "    _subj_stats = _subj_stats.merge(\n",
    "        SUBJECT_META[[\"MSSQ_group\"]].reset_index().rename(columns={\"subject_id\":\"sid\"}),\n",
    "        on=\"sid\", how=\"left\"\n",
    "    )\n",
    "\n",
    "SUBJECT_LABEL_STATS = _subj_stats.set_index(\"sid\").sort_index()\n",
    "\n",
    "# 保存パスを決定\n",
    "csv_name = \"SUBJECT_LABEL_STATS.CSV\"\n",
    "try:\n",
    "    save_path = outpath(csv_name)  # 既存の outpath がある前提\n",
    "except NameError:\n",
    "    # outpath が未定義ならカレントに保存\n",
    "    save_path = os.path.join(os.getcwd(), csv_name)\n",
    "\n",
    "# CSV 保存（BOM付き）\n",
    "SUBJECT_LABEL_STATS.to_csv(save_path, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell7.4] SUBJECT_LABEL_STATS saved -> {save_path} (subjects={SUBJECT_LABEL_STATS.shape[0]})\")\n",
    "\n",
    "# 確認表示（先頭のみ）\n",
    "display(SUBJECT_LABEL_STATS.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 7.5 (REPLACE): inner-LOSO folds builder =====\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def choose_inner_folds_loso(train_subject_ids: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    外側LOSOの学習側 subject_id（16名）を受け取り、内側LOSOの検証者を1名ずつ回す\n",
    "    16分割のfoldリストを返す（[[sid1],[sid2],...,[sid16]]）。\n",
    "\n",
    "    厳格運用：想定から外れたら即 RuntimeError\n",
    "    \"\"\"\n",
    "    if not isinstance(train_subject_ids, (list, tuple)):\n",
    "        raise RuntimeError(\"[Cell7.5] train_subject_ids は list/tuple である必要がある。\")\n",
    "    # 文字列正規化\n",
    "    train_subject_ids = [str(sid) for sid in train_subject_ids]\n",
    "    # ユニーク・件数チェック\n",
    "    uniq = list(pd.unique(pd.Series(train_subject_ids)))\n",
    "    if len(uniq) != 16:\n",
    "        raise RuntimeError(f\"[Cell7.5] 学習側 subject 数が16ではない: {len(uniq)}\")\n",
    "    # 安定順序（昇順）\n",
    "    try:\n",
    "        uniq_sorted = sorted(uniq, key=lambda x: (len(x), x))\n",
    "    except Exception:\n",
    "        uniq_sorted = sorted(uniq)\n",
    "\n",
    "    folds = [[sid] for sid in uniq_sorted]  # 検証=1名ずつ\n",
    "    print(f\"[Cell7.5] inner-LOSO folds: {len(folds)} splits -> val subjects = {', '.join(uniq_sorted)}\")\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8f56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell8] outer 1: BA | Single τ=0.053 | Group-BA τ=(0.053,0.166) | WG-1D τ=(0.053,0.033) | WG-2D τ=(0.053,0.004)\n",
      "[Cell8] outer 2: BA | Single τ=0.007 | Group-BA τ=(0.003,0.991) | WG-1D τ=(0.189,0.009) | WG-2D τ=(0.189,0.026)\n",
      "[Cell8] outer 3: BA | Single τ=0.009 | Group-BA τ=(0.009,0.436) | WG-1D τ=(0.029,0.016) | WG-2D τ=(0.029,0.003)\n",
      "[Cell8] outer 4: BA | Single τ=0.012 | Group-BA τ=(0.001,0.031) | WG-1D τ=(0.059,0.009) | WG-2D τ=(0.059,0.002)\n",
      "[Cell8] outer 5: BA | Single τ=0.027 | Group-BA τ=(0.003,0.992) | WG-1D τ=(0.027,0.091) | WG-2D τ=(0.025,0.091)\n",
      "[Cell8] outer 6: BA | Single τ=0.026 | Group-BA τ=(0.022,0.206) | WG-1D τ=(0.026,0.031) | WG-2D τ=(0.026,0.149)\n",
      "[Cell8] outer 7: BA | Single τ=0.024 | Group-BA τ=(0.033,0.994) | WG-1D τ=(0.033,0.011) | WG-2D τ=(0.033,0.016)\n",
      "[Cell8] outer 8: BA | Single τ=0.044 | Group-BA τ=(0.010,0.090) | WG-1D τ=(0.119,0.067) | WG-2D τ=(0.119,0.138)\n",
      "[Cell8] outer 9: BA | Single τ=0.020 | Group-BA τ=(0.007,0.982) | WG-1D τ=(0.007,0.020) | WG-2D τ=(0.007,0.007)\n",
      "[Cell8] outer 10: BA | Single τ=0.021 | Group-BA τ=(0.006,0.018) | WG-1D τ=(0.021,0.018) | WG-2D τ=(0.021,0.096)\n",
      "[Cell8] outer 11: BA | Single τ=0.026 | Group-BA τ=(0.005,0.024) | WG-1D τ=(0.242,0.024) | WG-2D τ=(0.242,0.002)\n",
      "[Cell8] outer 12: BA | Single τ=0.011 | Group-BA τ=(0.011,0.017) | WG-1D τ=(0.011,0.009) | WG-2D τ=(0.011,0.073)\n",
      "[Cell8] outer 13: BA | Single τ=0.007 | Group-BA τ=(0.004,0.487) | WG-1D τ=(0.007,0.008) | WG-2D τ=(0.007,0.487)\n",
      "[Cell8] outer 14: BA | Single τ=0.013 | Group-BA τ=(0.007,0.042) | WG-1D τ=(0.013,0.037) | WG-2D τ=(0.013,0.188)\n",
      "[Cell8] outer 15: BA | Single τ=0.005 | Group-BA τ=(0.005,0.125) | WG-1D τ=(0.005,0.024) | WG-2D τ=(0.005,0.002)\n",
      "[Cell8] outer 16: BA | Single τ=0.048 | Group-BA τ=(0.008,0.048) | WG-1D τ=(0.011,0.048) | WG-2D τ=(0.011,0.038)\n",
      "[Cell8] outer 17: BA | Single τ=0.013 | Group-BA τ=(0.005,0.288) | WG-1D τ=(0.036,0.013) | WG-2D τ=(0.036,0.028)\n",
      "[Cell8] saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\GROUP_AWARE_THRESH_BY_FOLD.CSV (rows=17)\n",
      "[Cell8] saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\GROUP_AWARE_PREDICTIONS.CSV (rows=340)\n",
      "[OK] Pooled AUC = 0.747  (METRIC=BA, SEARCH=exact) -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\LOSO_METRICS.CSV\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8 (REPLACE): inner-LOSO で τ 最適化（Single/Group-GLOBAL/WG-1D/WG-2D）→ 外側LOSOで予測収集 → プールAUC/BA算出 =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# ---------------- 基本設定 ----------------\n",
    "METRIC = \"ba\"     # BA固定\n",
    "METRIC_NAME = \"BA\"\n",
    "THRESH_SEARCH_MODE = str(globals().get(\"THRESH_SEARCH_MODE\", \"exact\")).lower()\n",
    "\n",
    "COARSE_STEP = float(globals().get(\"THRESH_COARSE_STEP\", 0.01))\n",
    "FINE_STEP   = float(globals().get(\"THRESH_FINE_STEP\",   0.001))\n",
    "MARGIN      = float(globals().get(\"THRESH_MARGIN\",      0.03))\n",
    "\n",
    "# best_k の取得（Cell 6/7で決定済み）\n",
    "k_use = int(BEST_K)\n",
    "\n",
    "# MSSQ High/Low の取得（グループ情報）\n",
    "if not set(groups.unique()).issubset(set(SUBJECT_META.index)):\n",
    "    missing_ids = sorted(set(groups.unique()) - set(SUBJECT_META.index))\n",
    "    raise ValueError(f\"[Cell8] SUBJECT_META に無い subject_id: {missing_ids}\")\n",
    "fair_groups = groups.map(SUBJECT_META[\"MSSQ_group\"]).astype(str)\n",
    "\n",
    "# SHAPランキングから上位 k_use 特徴を抽出\n",
    "rank_candidates = [outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), outpath(\"SHAP_FEATURE_RANKING.CSV\")]\n",
    "rank_path = next((p for p in rank_candidates if os.path.exists(p)), None)\n",
    "if rank_path is None:\n",
    "    raise FileNotFoundError(\"[Cell8] SHAP_FEATURE_RANKING(_LABELED).CSV が見つからない（Cell 4/6 を先に）\")\n",
    "rank_df = pd.read_csv(rank_path, encoding=\"utf-8-sig\", index_col=0)\n",
    "rank_col = \"mean_abs\" if \"mean_abs\" in rank_df.columns else (\"mean_abs_shap\" if \"mean_abs_shap\" in rank_df.columns else None)\n",
    "if rank_col is None:\n",
    "    raise KeyError(\"[Cell8] ランキングCSVに mean_abs / mean_abs_shap 列が無い\")\n",
    "feature_order = [f for f in rank_df.sort_values(rank_col, ascending=False).index if f in X_scaled_all.columns]\n",
    "feats_k = feature_order[:k_use]\n",
    "if len(feats_k) < k_use:\n",
    "    print(f\"[Cell8][WARN] Xに存在しない特徴が含まれたため、実使用は {len(feats_k)} 列\")\n",
    "X_k = X_scaled_all[feats_k].astype(np.float32)\n",
    "\n",
    "# Series の整形\n",
    "y  = pd.Series(y_all.astype(int),  index=X_scaled_all.index)\n",
    "g  = pd.Series(groups.astype(str), index=X_scaled_all.index)\n",
    "fg = pd.Series(fair_groups.astype(str), index=X_scaled_all.index)\n",
    "\n",
    "# ---------------- ユーティリティ ----------------\n",
    "def _cumulative_conf_table(scores: np.ndarray, labels: np.ndarray):\n",
    "    order = np.argsort(-scores)\n",
    "    s = np.asarray(scores, float)[order]\n",
    "    yb = np.asarray(labels, int)[order]\n",
    "    pos = (yb == 1).astype(int)\n",
    "    neg = (yb == 0).astype(int)\n",
    "    cpos = np.cumsum(pos)\n",
    "    cneg = np.cumsum(neg)\n",
    "    return s, yb, cpos, cneg, int(pos.sum()), int(neg.sum())\n",
    "\n",
    "def _conf_from_threshold(sort_scores, sort_labels, cpos, cneg, P, N, tau: float):\n",
    "    k = int(np.searchsorted(-sort_scores, -tau, side=\"right\"))  # s >= tau を正\n",
    "    TP = int(cpos[k-1]) if k > 0 else 0\n",
    "    FP = int(cneg[k-1]) if k > 0 else 0\n",
    "    FN = int(P - TP)\n",
    "    TN = int(N - FP)\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _ba_from_conf(TP, FP, FN, TN):\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    return 0.5 * (TPR + TNR)\n",
    "\n",
    "def _f1_from_conf(TP, FP, FN):\n",
    "    denom = (2*TP + FP + FN)\n",
    "    return (2*TP / denom) if denom > 0 else 0.0\n",
    "\n",
    "def _score_from_conf(metric: str, TP, FP, FN, TN):\n",
    "    return _f1_from_conf(TP, FP, FN) if metric == \"f1\" else _ba_from_conf(TP, FP, FN, TN)\n",
    "\n",
    "def _score_from_preds(metric: str, y_true_bin: np.ndarray, y_pred_bin: np.ndarray) -> float:\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    return float(_score_from_conf(metric, TP, FP, FN, TN))\n",
    "\n",
    "def _conf_vectors_from_candidates(tab, taus: np.ndarray):\n",
    "    sort_scores, sort_labels, cpos, cneg, P, N = tab\n",
    "    k = np.searchsorted(-sort_scores, -taus, side=\"right\")  # vectorized\n",
    "    TP = np.where(k > 0, cpos[k-1], 0)\n",
    "    FP = np.where(k > 0, cneg[k-1], 0)\n",
    "    FN = P - TP\n",
    "    TN = N - FP\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _make_candidates(scores: np.ndarray) -> np.ndarray:\n",
    "    s = np.asarray(scores, float)\n",
    "    if s.size == 0:\n",
    "        return np.array([0.5], dtype=float)\n",
    "    uniq = np.unique(s)\n",
    "    hi = np.nextafter(float(uniq.max()), np.inf)\n",
    "    lo = np.nextafter(float(uniq.min()), -np.inf)\n",
    "    return np.concatenate([[hi], uniq[::-1], [lo]])\n",
    "\n",
    "def _best_tau_single_exact(scores: np.ndarray, labels: np.ndarray, metric: str) -> float:\n",
    "    tab = _cumulative_conf_table(np.asarray(scores,float), np.asarray(labels,int))\n",
    "    taus = _make_candidates(scores)\n",
    "    TP, FP, FN, TN = _conf_vectors_from_candidates(tab, taus)\n",
    "    if metric == \"f1\":\n",
    "        num = 2*TP; den = 2*TP + FP + FN\n",
    "        sc = np.where(den > 0, num/den, 0.0)\n",
    "    else:\n",
    "        TPR = np.where((TP+FN) > 0, TP/(TP+FN), 0.0)\n",
    "        TNR = np.where((TN+FP) > 0, TN/(TN+FP), 0.0)\n",
    "        sc = 0.5*(TPR+TNR)\n",
    "    tau_clip = np.clip(taus, 0.0, 1.0)\n",
    "    best = np.argmax(sc)\n",
    "    ties = np.where(np.isclose(sc, sc[best]))[0]\n",
    "    if ties.size > 1:\n",
    "        d = np.abs(tau_clip[ties] - 0.5)\n",
    "        j = np.argmin(d); cand = ties[np.isclose(d, d[j])]\n",
    "        best = cand[np.argmin(taus[cand])] if cand.size > 1 else cand[0]\n",
    "    return float(taus[best])\n",
    "\n",
    "def _best_tau_single(scores: np.ndarray, labels: np.ndarray,\n",
    "                     coarse: float, fine: float, margin: float,\n",
    "                     metric: str) -> float:\n",
    "    tab = _cumulative_conf_table(scores, labels)\n",
    "    def score_at(t):\n",
    "        TP, FP, FN, TN = _conf_from_threshold(*tab, t)\n",
    "        return _score_from_conf(metric, TP, FP, FN, TN)\n",
    "    best = (-1.0, 0.5)\n",
    "    for t in np.arange(0.0, 1.0 + 1e-12, coarse):\n",
    "        sc = score_at(t)\n",
    "        if sc > best[0]:\n",
    "            best = (float(sc), float(t))\n",
    "    t0 = best[1]\n",
    "    lo = max(0.0, t0 - margin); hi = min(1.0, t0 + margin)\n",
    "    score0, t_best = best[0], t0\n",
    "    for t in np.arange(lo, hi + 1e-12, fine):\n",
    "        sc = score_at(t)\n",
    "        if sc > score0 or (np.isclose(sc, score0) and (abs(t-0.5) < abs(t_best-0.5) or (abs(t-0.5) == abs(t_best-0.5) and t < t_best))):\n",
    "            score0, t_best = float(sc), float(t)\n",
    "    return float(t_best)\n",
    "\n",
    "def _grid_search_group_thresholds_exact(scores_H, labels_H, scores_L, labels_L, metric: str):\n",
    "    sH = np.asarray(scores_H, float); yH = np.asarray(labels_H, int)\n",
    "    sL = np.asarray(scores_L, float); yL = np.asarray(labels_L, int)\n",
    "\n",
    "    tabH = _cumulative_conf_table(sH, yH)\n",
    "    tabL = _cumulative_conf_table(sL, yL)\n",
    "    tausH = _make_candidates(sH)\n",
    "    tausL = _make_candidates(sL)\n",
    "\n",
    "    TP_H, FP_H, FN_H, TN_H = _conf_vectors_from_candidates(tabH, tausH)\n",
    "    TP_L, FP_L, FN_L, TN_L = _conf_vectors_from_candidates(tabL, tausL)\n",
    "\n",
    "    TP = TP_H[:, None] + TP_L[None, :]\n",
    "    FP = FP_H[:, None] + FP_L[None, :]\n",
    "    FN = FN_H[:, None] + FN_L[None, :]\n",
    "    TN = TN_H[:, None] + TN_L[None, :]\n",
    "\n",
    "    if metric == \"f1\":\n",
    "        num = 2*TP; den = 2*TP + FP + FN\n",
    "        S_all = np.where(den > 0, num/den, 0.0)\n",
    "    else:\n",
    "        TPR = np.where((TP+FN) > 0, TP/(TP+FN), 0.0)\n",
    "        TNR = np.where((TN+FP) > 0, TN/(TN+FP), 0.0)\n",
    "        S_all = 0.5*(TPR+TNR)\n",
    "\n",
    "    idx = np.unravel_index(np.argmax(S_all), S_all.shape)\n",
    "    best_val = S_all[idx]\n",
    "\n",
    "    # tie-break: 平均τが0.5に近い→さらに平均が小さい\n",
    "    avg = (np.clip(tausH,0,1)[:,None] + np.clip(tausL,0,1)[None,:]) / 2\n",
    "    ties = np.argwhere(np.isclose(S_all, best_val))\n",
    "    if ties.shape[0] > 1:\n",
    "        d = np.abs(avg[ties[:,0], ties[:,1]] - 0.5)\n",
    "        j = np.argmin(d); cand = ties[np.isclose(d, d[j])]\n",
    "        if cand.shape[0] > 1:\n",
    "            avgv = avg[cand[:,0], cand[:,1]]\n",
    "            idx = (int(cand[np.argmin(avgv),0]), int(cand[np.argmin(avgv),1]))\n",
    "        else:\n",
    "            idx = (int(cand[0,0]), int(cand[0,1]))\n",
    "\n",
    "    tauH_best = float(tausH[idx[0]])\n",
    "    tauL_best = float(tausL[idx[1]])\n",
    "    return {\"GLOBAL\": {\"score\": float(best_val), \"tauH\": tauH_best, \"tauL\": tauL_best}}\n",
    "\n",
    "def _best_tau_groupwise_exact(scores_H, labels_H, scores_L, labels_L, metric: str, wg_mode: str = \"min\"):\n",
    "    tauH = _best_tau_single_exact(scores_H, labels_H, metric)\n",
    "    tauL = _best_tau_single_exact(scores_L, labels_L, metric)\n",
    "    tabH = _cumulative_conf_table(np.asarray(scores_H,float), np.asarray(labels_H,int))\n",
    "    tabL = _cumulative_conf_table(np.asarray(scores_L,float), np.asarray(labels_L,int))\n",
    "    TP_H, FP_H, FN_H, TN_H = _conf_from_threshold(*tabH, tauH)\n",
    "    TP_L, FP_L, FN_L, TN_L = _conf_from_threshold(*tabL, tauL)\n",
    "    sH = _score_from_conf(metric, TP_H, FP_H, FN_H, TN_H)\n",
    "    sL = _score_from_conf(metric, TP_L, FP_L, FN_L, TN_L)\n",
    "    WG = min(sH, sL) if wg_mode == \"min\" else 0.5*(sH + sL)\n",
    "    return {\"tauH\": float(tauH), \"tauL\": float(tauL), \"score_H\": float(sH), \"score_L\": float(sL), \"WG\": float(WG)}\n",
    "\n",
    "# --- NEW: 2D 最悪群最適化（min-max） ---\n",
    "def _grid_search_group_thresholds_exact_minimax(scores_H, labels_H, scores_L, labels_L, metric: str):\n",
    "    \"\"\"\n",
    "    max_{τH,τL} min{ S_H(τH), S_L(τL) } を 2D で厳密探索。\n",
    "    タイブレーク: (1) |S_H-S_L| 最小 → (2) 平均τが0.5に近い → (3) 平均τが小さい。\n",
    "    \"\"\"\n",
    "    sH = np.asarray(scores_H, float); yH = np.asarray(labels_H, int)\n",
    "    sL = np.asarray(scores_L, float); yL = np.asarray(labels_L, int)\n",
    "\n",
    "    tabH = _cumulative_conf_table(sH, yH)\n",
    "    tabL = _cumulative_conf_table(sL, yL)\n",
    "    tausH = _make_candidates(sH)\n",
    "    tausL = _make_candidates(sL)\n",
    "\n",
    "    TP_H, FP_H, FN_H, TN_H = _conf_vectors_from_candidates(tabH, tausH)\n",
    "    TP_L, FP_L, FN_L, TN_L = _conf_vectors_from_candidates(tabL, tausL)\n",
    "\n",
    "    if metric == \"f1\":\n",
    "        numH, denH = 2*TP_H, 2*TP_H + FP_H + FN_H\n",
    "        numL, denL = 2*TP_L, 2*TP_L + FP_L + FN_L\n",
    "        S_H = np.where(denH > 0, numH/denH, 0.0)\n",
    "        S_L = np.where(denL > 0, numL/denL, 0.0)\n",
    "    else:\n",
    "        TPR_H = np.where((TP_H+FN_H) > 0, TP_H/(TP_H+FN_H), 0.0)\n",
    "        TNR_H = np.where((TN_H+FP_H) > 0, TN_H/(TN_H+FP_H), 0.0)\n",
    "        TPR_L = np.where((TP_L+FN_L) > 0, TP_L/(TP_L+FN_L), 0.0)\n",
    "        TNR_L = np.where((TN_L+FP_L) > 0, TN_L/(TN_L+FP_L), 0.0)\n",
    "        S_H = 0.5*(TPR_H+TNR_H)\n",
    "        S_L = 0.5*(TPR_L+TNR_L)\n",
    "\n",
    "    M = np.minimum(S_H[:, None], S_L[None, :])          # (mH, mL)\n",
    "    iH, iL = np.unravel_index(np.argmax(M), M.shape)\n",
    "    best_val = M[iH, iL]\n",
    "\n",
    "    ties = np.argwhere(np.isclose(M, best_val))\n",
    "    if ties.shape[0] > 1:\n",
    "        gap = np.abs(S_H[ties[:,0]] - S_L[ties[:,1]])\n",
    "        j = np.argmin(gap)\n",
    "        cand = ties[np.isclose(gap, gap[j])]\n",
    "        if cand.shape[0] > 1:\n",
    "            avg_tau = (np.clip(tausH,0,1)[cand[:,0]] + np.clip(tausL,0,1)[cand[:,1]])/2\n",
    "            j2 = np.argmin(np.abs(avg_tau - 0.5))\n",
    "            cand2 = cand[np.isclose(np.abs(avg_tau-0.5), np.abs(avg_tau-0.5)[j2])]\n",
    "            if cand2.shape[0] > 1:\n",
    "                avg_tau2 = (tausH[cand2[:,0]] + tausL[cand2[:,1]])/2\n",
    "                iH, iL = int(cand2[np.argmin(avg_tau2),0]), int(cand2[np.argmin(avg_tau2),1])\n",
    "            else:\n",
    "                iH, iL = int(cand2[0,0]), int(cand2[0,1])\n",
    "        else:\n",
    "            iH, iL = int(cand[0,0]), int(cand[0,1])\n",
    "\n",
    "    tauH_best = float(tausH[iH]); tauL_best = float(tausL[iL])\n",
    "    return {\"WG2D\": {\n",
    "        \"score\": float(best_val),\n",
    "        \"tauH\": tauH_best, \"tauL\": tauL_best,\n",
    "        \"score_H\": float(S_H[iH]), \"score_L\": float(S_L[iL]),\n",
    "    }}\n",
    "\n",
    "# ---------------- outer-LOSO：inner-LOSOで τ を決定し、外側テストへ適用 ----------------\n",
    "logo = LeaveOneGroupOut()\n",
    "rows, pred_rows = [], []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_k, y, g), start=1):\n",
    "    test_sids = pd.Index(g.iloc[te_idx]).unique().tolist()\n",
    "    if len(test_sids) != 1:\n",
    "        raise RuntimeError(f\"[Cell8] LOSO違反: fold={fold_id}, test={test_sids}\")\n",
    "    test_sid = test_sids[0]\n",
    "    train_sids = pd.Index(g.iloc[tr_idx]).unique().tolist()\n",
    "\n",
    "    # --- inner-LOSO: OOF作成 ---\n",
    "    oof_index = X_k.index[g.isin(train_sids)]\n",
    "    oof_proba = pd.Series(index=oof_index, dtype=float)\n",
    "    oof_true  = pd.Series(index=oof_index, dtype=int)\n",
    "    oof_group = pd.Series(index=oof_index, dtype=object)\n",
    "\n",
    "    for val_sid in train_sids:\n",
    "        val_mask   = (g == val_sid)\n",
    "        train_mask = g.isin([sid for sid in train_sids if sid != val_sid])\n",
    "        if train_mask.sum() == 0 or val_mask.sum() == 0:\n",
    "            raise RuntimeError(f\"[Cell8] inner-LOSO 空fold（val={val_sid})\")\n",
    "        model_inner = fit_xgb_classifier(X_k.loc[train_mask], y.loc[train_mask])\n",
    "        proba_val = model_inner.predict_proba(X_k.loc[val_mask])[:, 1]\n",
    "        idx = val_mask[val_mask].index\n",
    "        oof_proba.loc[idx] = proba_val\n",
    "        oof_true.loc[idx]  = y.loc[val_mask].values\n",
    "        oof_group.loc[idx] = fg.loc[val_mask].values\n",
    "\n",
    "    valid_idx = oof_proba.dropna().index\n",
    "    if len(valid_idx) == 0:\n",
    "        raise RuntimeError(f\"[Cell8] fold {fold_id}: OOFが空\")\n",
    "\n",
    "    # --- Single τ（属性完全無視） ---\n",
    "    if THRESH_SEARCH_MODE == \"exact\":\n",
    "        tau_single = _best_tau_single_exact(oof_proba.loc[valid_idx].to_numpy(),\n",
    "                                            oof_true.loc[valid_idx].to_numpy().astype(int), METRIC)\n",
    "    else:\n",
    "        tau_single = _best_tau_single(oof_proba.loc[valid_idx].to_numpy(),\n",
    "                                      oof_true.loc[valid_idx].to_numpy().astype(int),\n",
    "                                      coarse=COARSE_STEP, fine=FINE_STEP, margin=MARGIN, metric=METRIC)\n",
    "\n",
    "    # --- Group用 OOF の分割 ---\n",
    "    s_H = oof_proba.loc[valid_idx][oof_group.loc[valid_idx] == \"High\"].to_numpy()\n",
    "    y_H = oof_true.loc[valid_idx][oof_group.loc[valid_idx] == \"High\"].to_numpy().astype(int)\n",
    "    s_L = oof_proba.loc[valid_idx][oof_group.loc[valid_idx] == \"Low\"].to_numpy()\n",
    "    y_L = oof_true.loc[valid_idx][oof_group.loc[valid_idx] == \"Low\"].to_numpy().astype(int)\n",
    "    if len(s_H) == 0 or len(s_L) == 0:\n",
    "        raise RuntimeError(f\"[Cell8] fold {fold_id}: OOFにHigh/Lowが揃わない\")\n",
    "\n",
    "    # A) Group-GLOBAL（全体指標最大化）\n",
    "    gopt_global = _grid_search_group_thresholds_exact(s_H, y_H, s_L, y_L, metric=METRIC)[\"GLOBAL\"]\n",
    "    tauH_MAIN, tauL_MAIN = gopt_global[\"tauH\"], gopt_global[\"tauL\"]\n",
    "\n",
    "    # B) Group-WG（1D×2 独立最適；min/meanはTHRESH_WG_MODEで選択）\n",
    "    WG_MODE = str(globals().get(\"THRESH_WG_MODE\", \"min\")).lower()\n",
    "    if WG_MODE not in {\"min\", \"mean\"}:\n",
    "        WG_MODE = \"min\"\n",
    "    gopt_wg = _best_tau_groupwise_exact(s_H, y_H, s_L, y_L, metric=METRIC, wg_mode=WG_MODE)\n",
    "    tauH_WG1D, tauL_WG1D = gopt_wg[\"tauH\"], gopt_wg[\"tauL\"]\n",
    "\n",
    "    # C) Group-WG（2D min-max：max min{S_H,S_L}）\n",
    "    gopt_wg2d = _grid_search_group_thresholds_exact_minimax(s_H, y_H, s_L, y_L, metric=METRIC)[\"WG2D\"]\n",
    "    tauH_WG2D, tauL_WG2D = gopt_wg2d[\"tauH\"], gopt_wg2d[\"tauL\"]\n",
    "\n",
    "    print(f\"[Cell8] outer {fold_id}: {METRIC_NAME} | \"\n",
    "          f\"Single τ={tau_single:.3f} | Group-BA τ=({tauH_MAIN:.3f},{tauL_MAIN:.3f}) | \"\n",
    "          f\"WG-1D τ=({tauH_WG1D:.3f},{tauL_WG1D:.3f}) | WG-2D τ=({tauH_WG2D:.3f},{tauL_WG2D:.3f})\")\n",
    "\n",
    "    # --- 外側テスト：学習側で再学習 → テストへ適用 ---\n",
    "    model_full = fit_xgb_classifier(X_k.loc[g.isin(train_sids)], y.loc[g.isin(train_sids)])\n",
    "    te_mask  = (g == test_sid)\n",
    "    proba_te = model_full.predict_proba(X_k.loc[te_mask])[:, 1]\n",
    "    y_te     = y.loc[te_mask].to_numpy().astype(int)\n",
    "    grp_te   = fg.loc[te_mask].to_numpy()\n",
    "\n",
    "    # 2値化\n",
    "    y_pred_single     = (proba_te >= tau_single).astype(int)\n",
    "    y_pred_group_MAIN = (proba_te >= np.where(grp_te==\"High\", tauH_MAIN,  tauL_MAIN)).astype(int)\n",
    "    y_pred_group_WG1D = (proba_te >= np.where(grp_te==\"High\", tauH_WG1D, tauL_WG1D)).astype(int)\n",
    "    y_pred_group_WG2D = (proba_te >= np.where(grp_te==\"High\", tauH_WG2D, tauL_WG2D)).astype(int)\n",
    "\n",
    "    # fold内スコア\n",
    "    SCORE_single      = _score_from_preds(METRIC, y_te, y_pred_single)\n",
    "    SCORE_group_MAIN  = _score_from_preds(METRIC, y_te, y_pred_group_MAIN)\n",
    "    SCORE_group_WG1D  = _score_from_preds(METRIC, y_te, y_pred_group_WG1D)\n",
    "    SCORE_group_WG2D  = _score_from_preds(METRIC, y_te, y_pred_group_WG2D)\n",
    "    WG_single         = min(_score_from_preds(METRIC, y_te[grp_te==\"High\"], y_pred_single[grp_te==\"High\"]) if np.any(grp_te==\"High\") else np.nan,\n",
    "                            _score_from_preds(METRIC, y_te[grp_te==\"Low\"],  y_pred_single[grp_te==\"Low\"])  if np.any(grp_te==\"Low\")  else np.nan)\n",
    "\n",
    "    # 保存（BA固定）\n",
    "    rows.append({\n",
    "        \"fold_id\": fold_id, \"test_id\": test_sid, \"best_k\": k_use,\n",
    "        \"tau_single\": float(tau_single),\n",
    "        \"BA_single\": float(SCORE_single), \"WGBA_single\": float(WG_single) if WG_single == WG_single else np.nan,\n",
    "        \"tau_high_BA\": float(tauH_MAIN), \"tau_low_BA\": float(tauL_MAIN),\n",
    "        \"BA_group\": float(SCORE_group_MAIN),\n",
    "        \"tau_high_WG\": float(tauH_WG1D), \"tau_low_WG\": float(tauL_WG1D),\n",
    "        \"BA_group_WGopt\": float(SCORE_group_WG1D),\n",
    "        \"tau_high_WG2D\": float(tauH_WG2D), \"tau_low_WG2D\": float(tauL_WG2D),\n",
    "        \"BA_group_WG2Dopt\": float(SCORE_group_WG2D),\n",
    "        \"AUC_test\": np.nan, \"n_test\": int(te_mask.sum())\n",
    "    })\n",
    "\n",
    "    for yy, pp, gg_, ys, ym, yw1, yw2 in zip(y_te, proba_te, grp_te, y_pred_single, y_pred_group_MAIN, y_pred_group_WG1D, y_pred_group_WG2D):\n",
    "        pred_rows.append({\n",
    "            \"fold_id\": fold_id, \"test_id\": test_sid,\n",
    "            \"y_true\": int(yy), \"proba\": float(pp), \"group\": str(gg_),\n",
    "            \"y_pred_single\": int(ys),\n",
    "            \"y_pred_group_BA\": int(ym),\n",
    "            \"y_pred_group_WG\": int(yw1),\n",
    "            \"y_pred_group_WG2D\": int(yw2),\n",
    "        })\n",
    "\n",
    "# ---------------- 保存（fold別 / 予測明細） ----------------\n",
    "df_fold = pd.DataFrame(rows)\n",
    "df_pred = pd.DataFrame(pred_rows)\n",
    "df_fold.to_csv(outpath(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_pred.to_csv(outpath(\"GROUP_AWARE_PREDICTIONS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell8] saved -> {outpath('GROUP_AWARE_THRESH_BY_FOLD.CSV')} (rows={len(df_fold)})\")\n",
    "print(f\"[Cell8] saved -> {outpath('GROUP_AWARE_PREDICTIONS.CSV')} (rows={len(df_pred)})\")\n",
    "\n",
    "# ---------------- プールAUC / BA（Single / Group / WG1D / WG2D） ----------------\n",
    "y_pool = df_pred[\"y_true\"].to_numpy()\n",
    "s_pool = df_pred[\"proba\"].to_numpy()\n",
    "if len(np.unique(y_pool)) < 2:\n",
    "    raise RuntimeError(\"[Cell8] プール真値が単一クラスで AUC 不可（データやFMS閾値定義を確認）\")\n",
    "\n",
    "auc_pool = float(roc_auc_score(y_pool, s_pool))\n",
    "\n",
    "def _BA_from_cols(df, colname):\n",
    "    yb = df[\"y_true\"].to_numpy().astype(int)\n",
    "    yh = df[colname].to_numpy().astype(int)\n",
    "    return _score_from_preds(METRIC, yb, yh)\n",
    "\n",
    "BA_single = _BA_from_cols(df_pred, \"y_pred_single\")\n",
    "BA_gMain  = _BA_from_cols(df_pred, \"y_pred_group_BA\")\n",
    "BA_wg1d   = _BA_from_cols(df_pred, \"y_pred_group_WG\")\n",
    "BA_wg2d   = _BA_from_cols(df_pred, \"y_pred_group_WG2D\") if \"y_pred_group_WG2D\" in df_pred.columns else np.nan\n",
    "\n",
    "summary = {\n",
    "    \"best_k\": k_use,\n",
    "    \"AUC_pooled\": auc_pool,\n",
    "    \"BA_pooled_single\": float(BA_single),\n",
    "    \"BA_pooled_group_GLOBALopt\": float(BA_gMain),\n",
    "    \"BA_pooled_group_WG1Dopt\": float(BA_wg1d),\n",
    "    \"BA_pooled_group_WG2Dopt\": float(BA_wg2d) if BA_wg2d==BA_wg2d else np.nan,\n",
    "    \"metric\": METRIC_NAME,\n",
    "    \"n_folds\": int(len(df_fold)),\n",
    "    \"search_mode\": THRESH_SEARCH_MODE\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(outpath(\"LOSO_METRICS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] Pooled AUC = {auc_pool:.3f}  (METRIC={METRIC_NAME}, SEARCH={THRESH_SEARCH_MODE}) -> {outpath('LOSO_METRICS.CSV')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb8915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell8b] collected predictions: n=340, pos=101, neg=239\n",
      "[Cell8b] saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\PREDICTIONS_OUTERONLY.CSV\n",
      "[Cell8b] preds updated -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\PREDICTIONS_OUTERONLY.CSV\n",
      "[Cell8b] τ_single=0.019  BA_single=0.731\n",
      "[Cell8b] (τ_H,τ_L)_WG1D=(0.019,0.022)  WG1D-BA=0.735\n",
      "[Cell8b] (τ_H,τ_L)_WG2D=(0.019,0.004)  WG2D-BA=0.710\n",
      "[OK] saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\FINAL_THRESHOLDS_POSTHOC.CSV / C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\METRICS_POSTHOC.CSV\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8b (NEW/REPLACE): OUTER-ONLY → post-hoc Exact thresholding（Single / Group-BA / WG-1D / WG-2D） =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# ---------------- 基本設定 ----------------\n",
    "METRIC = \"ba\"     # BA固定\n",
    "METRIC_NAME = \"BA\"\n",
    "\n",
    "# best_k の取得\n",
    "k_use = int(BEST_K)\n",
    "\n",
    "# MSSQ High/Low の取得\n",
    "if not set(groups.unique()).issubset(set(SUBJECT_META.index)):\n",
    "    missing_ids = sorted(set(groups.unique()) - set(SUBJECT_META.index))\n",
    "    raise ValueError(f\"[Cell8b] SUBJECT_META に無い subject_id: {missing_ids}\")\n",
    "fair_groups = groups.map(SUBJECT_META[\"MSSQ_group\"]).astype(str)\n",
    "\n",
    "# SHAPランキングから上位 k_use 特徴を抽出\n",
    "rank_candidates = [outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), outpath(\"SHAP_FEATURE_RANKING.CSV\")]\n",
    "rank_path = next((p for p in rank_candidates if os.path.exists(p)), None)\n",
    "if rank_path is None:\n",
    "    raise FileNotFoundError(\"[Cell8b] SHAP_FEATURE_RANKING(_LABELED).CSV が見つからない（Cell 4/6 を先に）\")\n",
    "rank_df = pd.read_csv(rank_path, encoding=\"utf-8-sig\", index_col=0)\n",
    "rank_col = \"mean_abs\" if \"mean_abs\" in rank_df.columns else (\"mean_abs_shap\" if \"mean_abs_shap\" in rank_df.columns else None)\n",
    "if rank_col is None:\n",
    "    raise KeyError(\"[Cell8b] ランキングCSVに mean_abs / mean_abs_shap 列が無い\")\n",
    "feature_order = [f for f in rank_df.sort_values(rank_col, ascending=False).index if f in X_scaled_all.columns]\n",
    "feats_k = feature_order[:k_use]\n",
    "X_k = X_scaled_all[feats_k].astype(np.float32)\n",
    "\n",
    "# Series\n",
    "y  = pd.Series(y_all.astype(int),  index=X_scaled_all.index)\n",
    "g  = pd.Series(groups.astype(str), index=X_scaled_all.index)\n",
    "fg = pd.Series(fair_groups.astype(str), index=X_scaled_all.index)\n",
    "\n",
    "# --- 再利用ユーティリティ（Cell 8 と同一定義を前提） ---\n",
    "def _cumulative_conf_table(scores: np.ndarray, labels: np.ndarray):\n",
    "    order = np.argsort(-scores)\n",
    "    s = np.asarray(scores, float)[order]\n",
    "    yb = np.asarray(labels, int)[order]\n",
    "    pos = (yb == 1).astype(int)\n",
    "    neg = (yb == 0).astype(int)\n",
    "    cpos = np.cumsum(pos)\n",
    "    cneg = np.cumsum(neg)\n",
    "    return s, yb, cpos, cneg, int(pos.sum()), int(neg.sum())\n",
    "\n",
    "def _conf_vectors_from_candidates(tab, taus: np.ndarray):\n",
    "    sort_scores, sort_labels, cpos, cneg, P, N = tab\n",
    "    k = np.searchsorted(-sort_scores, -taus, side=\"right\")\n",
    "    TP = np.where(k > 0, cpos[k-1], 0); FP = np.where(k > 0, cneg[k-1], 0)\n",
    "    FN = P - TP; TN = N - FP\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _make_candidates(scores: np.ndarray) -> np.ndarray:\n",
    "    s = np.asarray(scores, float)\n",
    "    if s.size == 0: return np.array([0.5], dtype=float)\n",
    "    uniq = np.unique(s)\n",
    "    hi = np.nextafter(float(uniq.max()), np.inf)\n",
    "    lo = np.nextafter(float(uniq.min()), -np.inf)\n",
    "    return np.concatenate([[hi], uniq[::-1], [lo]])\n",
    "\n",
    "def _ba_from_conf(TP, FP, FN, TN):\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    return 0.5 * (TPR + TNR)\n",
    "\n",
    "def _f1_from_conf(TP, FP, FN):\n",
    "    denom = (2*TP + FP + FN)\n",
    "    return (2*TP / denom) if denom > 0 else 0.0\n",
    "\n",
    "def _score_from_conf(metric: str, TP, FP, FN, TN):\n",
    "    return _f1_from_conf(TP, FP, FN) if metric == \"f1\" else _ba_from_conf(TP, FP, FN, TN)\n",
    "\n",
    "def _score_from_preds(metric: str, y_true_bin: np.ndarray, y_pred_bin: np.ndarray) -> float:\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    return float(_score_from_conf(metric, TP, FP, FN, TN))\n",
    "\n",
    "def _best_tau_single_exact(scores: np.ndarray, labels: np.ndarray, metric: str) -> float:\n",
    "    tab = _cumulative_conf_table(np.asarray(scores,float), np.asarray(labels,int))\n",
    "    taus = _make_candidates(scores)\n",
    "    TP, FP, FN, TN = _conf_vectors_from_candidates(tab, taus)\n",
    "    if metric == \"f1\":\n",
    "        num = 2*TP; den = 2*TP + FP + FN\n",
    "        sc = np.where(den > 0, num/den, 0.0)\n",
    "    else:\n",
    "        TPR = np.where((TP+FN) > 0, TP/(TP+FN), 0.0)\n",
    "        TNR = np.where((TN+FP) > 0, TN/(TN+FP), 0.0)\n",
    "        sc = 0.5*(TPR+TNR)\n",
    "    tau_clip = np.clip(taus, 0.0, 1.0)\n",
    "    best = np.argmax(sc)\n",
    "    ties = np.where(np.isclose(sc, sc[best]))[0]\n",
    "    if ties.size > 1:\n",
    "        d = np.abs(tau_clip[ties] - 0.5)\n",
    "        j = np.argmin(d); cand = ties[np.isclose(d, d[j])]\n",
    "        best = cand[np.argmin(taus[cand])] if cand.size > 1 else cand[0]\n",
    "    return float(taus[best])\n",
    "\n",
    "def _grid_search_group_thresholds_exact_minimax(scores_H, labels_H, scores_L, labels_L, metric: str):\n",
    "    sH = np.asarray(scores_H, float); yH = np.asarray(labels_H, int)\n",
    "    sL = np.asarray(scores_L, float); yL = np.asarray(labels_L, int)\n",
    "    tabH = _cumulative_conf_table(sH, yH)\n",
    "    tabL = _cumulative_conf_table(sL, yL)\n",
    "    tausH = _make_candidates(sH)\n",
    "    tausL = _make_candidates(sL)\n",
    "    TP_H, FP_H, FN_H, TN_H = _conf_vectors_from_candidates(tabH, tausH)\n",
    "    TP_L, FP_L, FN_L, TN_L = _conf_vectors_from_candidates(tabL, tausL)\n",
    "    if metric == \"f1\":\n",
    "        numH, denH = 2*TP_H, 2*TP_H + FP_H + FN_H\n",
    "        numL, denL = 2*TP_L, 2*TP_L + FP_L + FN_L\n",
    "        S_H = np.where(denH > 0, numH/denH, 0.0)\n",
    "        S_L = np.where(denL > 0, numL/denL, 0.0)\n",
    "    else:\n",
    "        TPR_H = np.where((TP_H+FN_H) > 0, TP_H/(TP_H+FN_H), 0.0)\n",
    "        TNR_H = np.where((TN_H+FP_H) > 0, TN_H/(TN_H+FP_H), 0.0)\n",
    "        TPR_L = np.where((TP_L+FN_L) > 0, TP_L/(TP_L+FN_L), 0.0)\n",
    "        TNR_L = np.where((TN_L+FP_L) > 0, TN_L/(TN_L+FP_L), 0.0)\n",
    "        S_H = 0.5*(TPR_H+TNR_H); S_L = 0.5*(TPR_L+TNR_L)\n",
    "    M = np.minimum(S_H[:, None], S_L[None, :])\n",
    "    iH, iL = np.unravel_index(np.argmax(M), M.shape)\n",
    "    best_val = M[iH, iL]\n",
    "    ties = np.argwhere(np.isclose(M, best_val))\n",
    "    if ties.shape[0] > 1:\n",
    "        gap = np.abs(S_H[ties[:,0]] - S_L[ties[:,1]])\n",
    "        j = np.argmin(gap); cand = ties[np.isclose(gap, gap[j])]\n",
    "        if cand.shape[0] > 1:\n",
    "            avg_tau = (np.clip(tausH,0,1)[cand[:,0]] + np.clip(tausL,0,1)[cand[:,1]])/2\n",
    "            j2 = np.argmin(np.abs(avg_tau - 0.5))\n",
    "            cand2 = cand[np.isclose(np.abs(avg_tau-0.5), np.abs(avg_tau-0.5)[j2])]\n",
    "            if cand2.shape[0] > 1:\n",
    "                avg_tau2 = (tausH[cand2[:,0]] + tausL[cand2[:,1]])/2\n",
    "                iH, iL = int(cand2[np.argmin(avg_tau2),0]), int(cand2[np.argmin(avg_tau2),1])\n",
    "            else:\n",
    "                iH, iL = int(cand2[0,0]), int(cand2[0,1])\n",
    "        else:\n",
    "            iH, iL = int(cand[0,0]), int(cand[0,1])\n",
    "    tauH_best = float(tausH[iH]); tauL_best = float(tausL[iL])\n",
    "    return {\"WG2D\": {\"score\": float(best_val), \"tauH\": tauH_best, \"tauL\": tauL_best}}\n",
    "\n",
    "# ---------------- 外側LOSOのみ：予測を収集 ----------------\n",
    "logo = LeaveOneGroupOut()\n",
    "rows = []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_k, y, g), start=1):\n",
    "    test_sids = pd.Index(g.iloc[te_idx]).unique().tolist()\n",
    "    if len(test_sids) != 1:\n",
    "        raise RuntimeError(f\"[Cell8b] LOSO違反: fold={fold_id}, test={test_sids}\")\n",
    "    test_sid = test_sids[0]\n",
    "\n",
    "    model_full = fit_xgb_classifier(X_k.iloc[tr_idx], y.iloc[tr_idx])\n",
    "    proba_te = model_full.predict_proba(X_k.iloc[te_idx])[:, 1]\n",
    "    y_te     = y.iloc[te_idx].to_numpy().astype(int)\n",
    "    grp_te   = fg.iloc[te_idx].to_numpy()\n",
    "\n",
    "    for yy, pp, gg_ in zip(y_te, proba_te, grp_te):\n",
    "        rows.append({\"fold_id\": fold_id, \"test_id\": test_sid,\n",
    "                     \"y_true\": int(yy), \"proba\": float(pp), \"group\": str(gg_)})\n",
    "\n",
    "df_pred = pd.DataFrame(rows)\n",
    "if df_pred.empty:\n",
    "    raise RuntimeError(\"[Cell8b] 予測が空である\")\n",
    "df_pred.to_csv(outpath(\"PREDICTIONS_OUTERONLY.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell8b] collected predictions: n={len(df_pred)}, pos={(df_pred['y_true']==1).sum()}, neg={(df_pred['y_true']==0).sum()}\")\n",
    "print(f\"[Cell8b] saved -> {outpath('PREDICTIONS_OUTERONLY.CSV')}\")\n",
    "\n",
    "# ---------------- 後処理：プール上で Exact しきい値最適化 ----------------\n",
    "y_pool = df_pred[\"y_true\"].to_numpy().astype(int)\n",
    "s_pool = df_pred[\"proba\"].to_numpy().astype(float)\n",
    "g_pool = df_pred[\"group\"].to_numpy().astype(str)\n",
    "if len(np.unique(y_pool)) < 2:\n",
    "    raise RuntimeError(\"[Cell8b] プール真値が単一クラスで AUC/最適化不可（データやFMS閾値定義を確認）\")\n",
    "\n",
    "# Single（1D）\n",
    "tau_single = _best_tau_single_exact(s_pool, y_pool, METRIC)\n",
    "\n",
    "# Group（High/Low 抽出）\n",
    "maskH = (g_pool == \"High\"); maskL = (g_pool == \"Low\")\n",
    "if not (maskH.any() and maskL.any()):\n",
    "    raise RuntimeError(\"[Cell8b] High/Low のいずれかがプールに存在しない（Group系最適化不可）\")\n",
    "sH, yH = s_pool[maskH], y_pool[maskH]\n",
    "sL, yL = s_pool[maskL], y_pool[maskL]\n",
    "\n",
    "# Group-BA（2D 全体最適）\n",
    "gopt_global = _grid_search_group_thresholds_exact_minimax(sH, yH, sL, yL, metric=\"ba\")  # min-maxと同じ候補を流用可\n",
    "tauH_MAIN, tauL_MAIN = _best_tau_single_exact(sH, yH, METRIC), _best_tau_single_exact(sL, yL, METRIC)  # 参考：群内1D\n",
    "# ↑もし “全体BA最大化（2D合算）” を使いたい場合は Cell 8 と同様の _grid_search_group_thresholds_exact を別途呼んでもOK\n",
    "\n",
    "# Worst-Group（1D×2 独立最適）\n",
    "tauH_WG1D, tauL_WG1D = _best_tau_single_exact(sH, yH, METRIC), _best_tau_single_exact(sL, yL, METRIC)\n",
    "\n",
    "# Worst-Group（2D min-max）\n",
    "gopt_wg2d = _grid_search_group_thresholds_exact_minimax(sH, yH, sL, yL, metric=METRIC)[\"WG2D\"]\n",
    "tauH_WG2D, tauL_WG2D = gopt_wg2d[\"tauH\"], gopt_wg2d[\"tauL\"]\n",
    "\n",
    "# ---------------- 二値化・メトリクス算出 ----------------\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def _BA_from_preds(y_true_bin: np.ndarray, y_pred_bin: np.ndarray) -> float:\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    return _score_from_conf(METRIC, TP, FP, FN, TN)\n",
    "\n",
    "y_pred_single     = (s_pool >= tau_single).astype(int)\n",
    "y_pred_group_MAIN = (s_pool >= np.where(g_pool==\"High\", tauH_MAIN,  tauL_MAIN)).astype(int)\n",
    "y_pred_group_WG1D = (s_pool >= np.where(g_pool==\"High\", tauH_WG1D, tauL_WG1D)).astype(int)\n",
    "y_pred_group_WG2D = (s_pool >= np.where(g_pool==\"High\", tauH_WG2D, tauL_WG2D)).astype(int)\n",
    "\n",
    "BA_single = _BA_from_preds(y_pool, y_pred_single)\n",
    "BA_gMain  = _BA_from_preds(y_pool, y_pred_group_MAIN)\n",
    "BA_wg1d   = _BA_from_preds(y_pool, y_pred_group_WG1D)\n",
    "BA_wg2d   = _BA_from_preds(y_pool, y_pred_group_WG2D)\n",
    "AUC_pool  = float(roc_auc_score(y_pool, s_pool))\n",
    "\n",
    "# 予測列も保存\n",
    "df_pred[\"y_pred_single\"]     = y_pred_single\n",
    "df_pred[\"y_pred_group_BA\"]   = y_pred_group_MAIN\n",
    "df_pred[\"y_pred_group_WG\"]   = y_pred_group_WG1D\n",
    "df_pred[\"y_pred_group_WG2D\"] = y_pred_group_WG2D\n",
    "df_pred.to_csv(outpath(\"PREDICTIONS_OUTERONLY.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell8b] preds updated -> {outpath('PREDICTIONS_OUTERONLY.CSV')}\")\n",
    "\n",
    "# しきい値とプール評価の保存\n",
    "th_row = {\n",
    "    \"metric\": METRIC_NAME, \"search_mode\": \"exact_posthoc\", \"WG_mode\": \"min\",\n",
    "    \"tau_single\": float(tau_single),\n",
    "    \"tau_high_BA\": float(tauH_MAIN),  \"tau_low_BA\":  float(tauL_MAIN),\n",
    "    \"tau_high_WG\": float(tauH_WG1D),  \"tau_low_WG\":  float(tauL_WG1D),\n",
    "    \"tau_high_WG2D\": float(tauH_WG2D), \"tau_low_WG2D\": float(tauL_WG2D),\n",
    "}\n",
    "pd.DataFrame([th_row]).to_csv(outpath(\"FINAL_THRESHOLDS_POSTHOC.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "summary = {\n",
    "    \"best_k\": k_use,\n",
    "    \"AUC_pooled\": AUC_pool,\n",
    "    \"BA_pooled_single\": float(BA_single),\n",
    "    \"BA_pooled_group_GLOBALopt\": float(BA_gMain),\n",
    "    \"BA_pooled_group_WG1Dopt\": float(BA_wg1d),\n",
    "    \"BA_pooled_group_WG2Dopt\": float(BA_wg2d),\n",
    "    \"metric\": METRIC_NAME,\n",
    "    \"n_samples\": int(len(df_pred)),\n",
    "    \"n_pos\": int((df_pred[\"y_true\"]==1).sum()),\n",
    "    \"n_neg\": int((df_pred[\"y_true\"]==0).sum()),\n",
    "    \"note\": \"post-hoc tuned on pooled CV predictions (outer-only)\"\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(outpath(\"METRICS_POSTHOC.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[Cell8b] τ_single={tau_single:.3f}  BA_single={BA_single:.3f}\")\n",
    "print(f\"[Cell8b] (τ_H,τ_L)_WG1D=({tauH_WG1D:.3f},{tauL_WG1D:.3f})  WG1D-BA={BA_wg1d:.3f}\")\n",
    "print(f\"[Cell8b] (τ_H,τ_L)_WG2D=({tauH_WG2D:.3f},{tauL_WG2D:.3f})  WG2D-BA={BA_wg2d:.3f}\")\n",
    "print(f\"[OK] saved -> {outpath('FINAL_THRESHOLDS_POSTHOC.CSV')} / {outpath('METRICS_POSTHOC.CSV')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc42d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell8g] saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\SINGLE_THRESH_BY_GROUPK_FOLD.CSV (rows=17)\n",
      "[Cell8g] saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\PREDICTIONS_SINGLE_BY_GROUPK.CSV (rows=340)\n",
      "[OK] pooled AUC=0.630, BA(all)=0.675 -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\METRICS_SINGLE_BY_GROUPK.CSV\n",
      "[Cell8g] plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\CONFMAT_SINGLE_GROUPK_ALL.png\n",
      "[Cell8g] plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\CONFMAT_SINGLE_GROUPK_HIGH.png\n",
      "[Cell8g] plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\CONFMAT_SINGLE_GROUPK_LOW.png\n",
      "[DONE] METRIC=BA, SEARCH=exact, BEST_K_HIGH=30, BEST_K_LOW=3\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8g (BA-only): 群別ベストk × inner-LOSOでSingle(属性無視) τ最適化 → outer適用・集計・作図 =====\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# ---------------- 基本セットアップ ----------------\n",
    "METRIC_NAME = \"BA\"  # BA固定\n",
    "SEARCH_MODE = str(globals().get(\"THRESH_SEARCH_MODE\", \"exact\")).lower()\n",
    "COARSE_STEP = float(globals().get(\"THRESH_COARSE_STEP\", 0.01))\n",
    "FINE_STEP   = float(globals().get(\"THRESH_FINE_STEP\",   0.001))\n",
    "MARGIN      = float(globals().get(\"THRESH_MARGIN\",      0.03))\n",
    "\n",
    "# MSSQ High/Low\n",
    "if not set(groups.unique()).issubset(set(SUBJECT_META.index)):\n",
    "    missing_ids = sorted(set(groups.unique()) - set(SUBJECT_META.index))\n",
    "    raise ValueError(f\"[Cell8g] SUBJECT_META に無い subject_id: {missing_ids}\")\n",
    "fair_groups = groups.map(SUBJECT_META[\"MSSQ_group\"]).astype(str)\n",
    "\n",
    "# 群別 best_k の読み込み\n",
    "bestk_json = outpath(\"BEST_K_BY_GROUP.JSON\")\n",
    "if os.path.exists(bestk_json):\n",
    "    with open(bestk_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        _jk = json.load(f)\n",
    "    BEST_K_HIGH = int(_jk.get(\"BEST_K_HIGH\", int(globals().get(\"BEST_K\", 10))))\n",
    "    BEST_K_LOW  = int(_jk.get(\"BEST_K_LOW\",  int(globals().get(\"BEST_K\", 10))))\n",
    "else:\n",
    "    k_fallback = int(globals().get(\"BEST_K\", 10))\n",
    "    BEST_K_HIGH = k_fallback; BEST_K_LOW = k_fallback\n",
    "    print(f\"[Cell8g][WARN] BEST_K_BY_GROUP.JSON が見つからないため共通BEST_K={k_fallback}を両群で使用\")\n",
    "\n",
    "# SHAPランキング読み込み\n",
    "rank_candidates = [outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), outpath(\"SHAP_FEATURE_RANKING.CSV\")]\n",
    "rank_path = next((p for p in rank_candidates if os.path.exists(p)), None)\n",
    "if rank_path is None:\n",
    "    raise FileNotFoundError(\"[Cell8g] SHAP_FEATURE_RANKING(_LABELED).CSV が見つからない（Cell 4/6 を先に）\")\n",
    "rank_df = pd.read_csv(rank_path, encoding=\"utf-8-sig\", index_col=0)\n",
    "rank_col = \"mean_abs\" if \"mean_abs\" in rank_df.columns else (\"mean_abs_shap\" if \"mean_abs_shap\" in rank_df.columns else None)\n",
    "if rank_col is None:\n",
    "    raise KeyError(\"[Cell8g] ランキングCSVに mean_abs / mean_abs_shap 列が無い\")\n",
    "feature_order = [f for f in rank_df.sort_values(rank_col, ascending=False).index if f in X_scaled_all.columns]\n",
    "if len(feature_order) == 0:\n",
    "    raise RuntimeError(\"[Cell8g] ランキングの特徴が X_scaled_all に存在しません\")\n",
    "\n",
    "def _feats_for_k(k: int):\n",
    "    kk = max(1, min(int(k), len(feature_order)))\n",
    "    return feature_order[:kk]\n",
    "\n",
    "# --- BAユーティリティ ---\n",
    "def _cumulative_conf_table(scores: np.ndarray, labels: np.ndarray):\n",
    "    order = np.argsort(-scores)\n",
    "    s = np.asarray(scores, float)[order]\n",
    "    yb = np.asarray(labels, int)[order]\n",
    "    pos = (yb == 1).astype(int)\n",
    "    neg = (yb == 0).astype(int)\n",
    "    cpos = np.cumsum(pos)\n",
    "    cneg = np.cumsum(neg)\n",
    "    return s, yb, cpos, cneg, int(pos.sum()), int(neg.sum())\n",
    "\n",
    "def _conf_from_threshold(sort_scores, sort_labels, cpos, cneg, P, N, tau: float):\n",
    "    k = int(np.searchsorted(-sort_scores, -tau, side=\"right\"))  # s >= tau を陽性\n",
    "    TP = int(cpos[k-1]) if k > 0 else 0\n",
    "    FP = int(cneg[k-1]) if k > 0 else 0\n",
    "    FN = int(P - TP)\n",
    "    TN = int(N - FP)\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _ba_from_conf(TP, FP, FN, TN):\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    return 0.5 * (TPR + TNR)\n",
    "\n",
    "def _BA_from_preds(y_true_bin: np.ndarray, y_pred_bin: np.ndarray) -> float:\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    return float(_ba_from_conf(TP, FP, FN, TN))\n",
    "\n",
    "def _make_candidates(scores: np.ndarray) -> np.ndarray:\n",
    "    s = np.asarray(scores, float)\n",
    "    if s.size == 0:\n",
    "        return np.array([0.5], dtype=float)\n",
    "    uniq = np.unique(s)\n",
    "    hi = np.nextafter(float(uniq.max()), np.inf)\n",
    "    lo = np.nextafter(float(uniq.min()), -np.inf)\n",
    "    return np.concatenate([[hi], uniq[::-1], [lo]])\n",
    "\n",
    "def _conf_vectors_from_candidates(tab, taus: np.ndarray):\n",
    "    sort_scores, sort_labels, cpos, cneg, P, N = tab\n",
    "    k = np.searchsorted(-sort_scores, -taus, side=\"right\")\n",
    "    TP = np.where(k > 0, cpos[k-1], 0)\n",
    "    FP = np.where(k > 0, cneg[k-1], 0)\n",
    "    FN = P - TP\n",
    "    TN = N - FP\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _best_tau_single_exact(scores: np.ndarray, labels: np.ndarray) -> float:\n",
    "    tab = _cumulative_conf_table(np.asarray(scores,float), np.asarray(labels,int))\n",
    "    taus = _make_candidates(scores)\n",
    "    TP, FP, FN, TN = _conf_vectors_from_candidates(tab, taus)\n",
    "    TPR = np.where((TP+FN) > 0, TP/(TP+FN), 0.0)\n",
    "    TNR = np.where((TN+FP) > 0, TN/(TN+FP), 0.0)\n",
    "    S = 0.5*(TPR+TNR)\n",
    "    # tie-break: 0.5に近い→さらに小さいτを優先\n",
    "    tau_clip = np.clip(taus, 0.0, 1.0)\n",
    "    j = np.argmax(S)\n",
    "    ties = np.where(np.isclose(S, S[j]))[0]\n",
    "    if ties.size > 1:\n",
    "        d = np.abs(tau_clip[ties] - 0.5)\n",
    "        j2 = np.argmin(d); cand = ties[np.isclose(d, d[j2])]\n",
    "        j = cand[np.argmin(taus[cand])] if cand.size > 1 else cand[0]\n",
    "    return float(taus[j])\n",
    "\n",
    "def _best_tau_single_grid(scores: np.ndarray, labels: np.ndarray) -> float:\n",
    "    tab = _cumulative_conf_table(scores, labels)\n",
    "    def score_at(t):\n",
    "        TP, FP, FN, TN = _conf_from_threshold(*tab, t)\n",
    "        return _ba_from_conf(TP, FP, FN, TN)\n",
    "    best = (-1.0, 0.5)\n",
    "    for t in np.arange(0.0, 1.0 + 1e-12, COARSE_STEP):\n",
    "        sc = score_at(t)\n",
    "        if sc > best[0]:\n",
    "            best = (float(sc), float(t))\n",
    "    t0 = best[1]; lo = max(0.0, t0 - MARGIN); hi = min(1.0, t0 + MARGIN)\n",
    "    score0, t_best = best[0], t0\n",
    "    for t in np.arange(lo, hi + 1e-12, FINE_STEP):\n",
    "        sc = score_at(t)\n",
    "        if sc > score0 or (np.isclose(sc, score0) and (abs(t-0.5) < abs(t_best-0.5) or (abs(t-0.5) == abs(t_best-0.5) and t < t_best))):\n",
    "            score0, t_best = float(sc), float(t)\n",
    "    return float(t_best)\n",
    "\n",
    "def _pick_best_tau(scores, labels) -> float:\n",
    "    if len(scores) == 0 or len(np.unique(labels)) < 2:\n",
    "        print(\"[Cell8g][WARN] OOFが空または単一クラスのため τ=0.5 にフォールバック\")\n",
    "        return 0.5\n",
    "    if SEARCH_MODE == \"exact\":\n",
    "        return _best_tau_single_exact(scores, labels)\n",
    "    else:\n",
    "        return _best_tau_single_grid(scores, labels)\n",
    "\n",
    "# --- データ成形 ---\n",
    "X_index = X_scaled_all.index\n",
    "y = pd.Series(y_all.astype(int), index=X_index)\n",
    "g = pd.Series(groups.astype(str), index=X_index)\n",
    "fg = pd.Series(fair_groups.astype(str), index=X_index)\n",
    "\n",
    "# --- outer-LOSO 本体（Single τ × 群別k） ---\n",
    "logo = LeaveOneGroupOut()\n",
    "rows, pred_rows = [], []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_scaled_all, y, g), start=1):\n",
    "    te_sid = pd.Index(g.iloc[te_idx]).unique().tolist()\n",
    "    if len(te_sid) != 1:\n",
    "        raise RuntimeError(f\"[Cell8g] LOSO違反: fold={fold_id}, test={te_sid}\")\n",
    "    te_sid = te_sid[0]\n",
    "    te_tag = str(SUBJECT_META.loc[te_sid, \"MSSQ_group\"])\n",
    "    k_use = BEST_K_HIGH if te_tag == \"High\" else BEST_K_LOW\n",
    "\n",
    "    feats_k = _feats_for_k(k_use)\n",
    "    X_k = X_scaled_all[feats_k].astype(np.float32)\n",
    "    train_sids = pd.Index(g.iloc[tr_idx]).unique().tolist()\n",
    "\n",
    "    # ---- inner-LOSO: OOF作成（属性無視 Single τ 用） ----\n",
    "    oof_index = X_k.index[g.isin(train_sids)]\n",
    "    oof_proba = pd.Series(index=oof_index, dtype=float)\n",
    "    oof_true  = pd.Series(index=oof_index, dtype=int)\n",
    "\n",
    "    for val_sid in train_sids:\n",
    "        val_mask   = (g == val_sid)\n",
    "        train_mask = g.isin([sid for sid in train_sids if sid != val_sid])\n",
    "        if train_mask.sum() == 0 or val_mask.sum() == 0:\n",
    "            print(f\"[Cell8g][WARN] inner-LOSO 空fold（val={val_sid}) をskip\")\n",
    "            continue\n",
    "        model_inner = fit_xgb_classifier(X_k.loc[train_mask], y.loc[train_mask])\n",
    "        proba_val = model_inner.predict_proba(X_k.loc[val_mask])[:, 1]\n",
    "        idx = val_mask[val_mask].index\n",
    "        oof_proba.loc[idx] = proba_val\n",
    "        oof_true.loc[idx]  = y.loc[val_mask].values\n",
    "\n",
    "    valid_idx = oof_proba.dropna().index\n",
    "    tau_single = _pick_best_tau(oof_proba.loc[valid_idx].to_numpy(),\n",
    "                                oof_true.loc[valid_idx].to_numpy().astype(int))\n",
    "\n",
    "    # ---- outer適用 ----\n",
    "    model_full = fit_xgb_classifier(X_k.loc[g.isin(train_sids)], y.loc[g.isin(train_sids)])\n",
    "    te_mask  = (g == te_sid)\n",
    "    proba_te = model_full.predict_proba(X_k.loc[te_mask])[:, 1]\n",
    "    y_te     = y.loc[te_mask].to_numpy().astype(int)\n",
    "    grp_te   = fg.loc[te_mask].to_numpy()\n",
    "\n",
    "    y_pred_single = (proba_te >= tau_single).astype(int)\n",
    "    SCORE_single  = _BA_from_preds(y_te, y_pred_single)\n",
    "\n",
    "    rows.append({\n",
    "        \"fold_id\": fold_id, \"test_id\": te_sid, \"group\": te_tag,\n",
    "        \"k_use\": int(k_use), \"tau_single\": float(tau_single),\n",
    "        \"BA_single\": float(SCORE_single),\n",
    "        \"AUC_test\": np.nan, \"n_test\": int(te_mask.sum())\n",
    "    })\n",
    "    for yy, pp, gg_, ys in zip(y_te, proba_te, grp_te, y_pred_single):\n",
    "        pred_rows.append({\n",
    "            \"fold_id\": fold_id, \"test_id\": te_sid, \"group\": str(gg_),\n",
    "            \"y_true\": int(yy), \"proba\": float(pp), \"y_pred_single\": int(ys),\n",
    "            \"k_use\": int(k_use)\n",
    "        })\n",
    "\n",
    "# --- 保存（fold別 / 予測明細 / 集計） ---\n",
    "df_fold = pd.DataFrame(rows)\n",
    "df_pred = pd.DataFrame(pred_rows)\n",
    "df_fold.to_csv(outpath(\"SINGLE_THRESH_BY_GROUPK_FOLD.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_pred.to_csv(outpath(\"PREDICTIONS_SINGLE_BY_GROUPK.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell8g] saved -> {outpath('SINGLE_THRESH_BY_GROUPK_FOLD.CSV')} (rows={len(df_fold)})\")\n",
    "print(f\"[Cell8g] saved -> {outpath('PREDICTIONS_SINGLE_BY_GROUPK.CSV')} (rows={len(df_pred)})\")\n",
    "\n",
    "# pooled AUC と pooled BA（全体／群別）\n",
    "y_pool = df_pred[\"y_true\"].to_numpy()\n",
    "s_pool = df_pred[\"proba\"].to_numpy()\n",
    "if len(np.unique(y_pool)) < 2:\n",
    "    raise RuntimeError(\"[Cell8g] プール真値が単一クラスで AUC 不可（データやFMS閾値定義を確認）\")\n",
    "auc_pool = float(roc_auc_score(y_pool, s_pool))\n",
    "\n",
    "def _BA_from_cols(y_true, y_hat):\n",
    "    return _BA_from_preds(y_true.astype(int), y_hat.astype(int))\n",
    "\n",
    "BA_all = _BA_from_cols(df_pred[\"y_true\"].to_numpy(), df_pred[\"y_pred_single\"].to_numpy())\n",
    "BA_hi  = _BA_from_cols(df_pred.loc[df_pred[\"group\"]==\"High\",\"y_true\"].to_numpy(),\n",
    "                       df_pred.loc[df_pred[\"group\"]==\"High\",\"y_pred_single\"].to_numpy()) if (df_pred[\"group\"]==\"High\").any() else np.nan\n",
    "BA_lo  = _BA_from_cols(df_pred.loc[df_pred[\"group\"]==\"Low\",\"y_true\"].to_numpy(),\n",
    "                       df_pred.loc[df_pred[\"group\"]==\"Low\",\"y_pred_single\"].to_numpy()) if (df_pred[\"group\"]==\"Low\").any() else np.nan\n",
    "\n",
    "summary = {\n",
    "    \"metric\": METRIC_NAME, \"search_mode\": SEARCH_MODE,\n",
    "    \"BEST_K_HIGH\": int(BEST_K_HIGH), \"BEST_K_LOW\": int(BEST_K_LOW),\n",
    "    \"AUC_pooled\": auc_pool,\n",
    "    \"BA_pooled_all\": float(BA_all),\n",
    "    \"BA_pooled_high\": float(BA_hi) if BA_hi==BA_hi else np.nan,\n",
    "    \"BA_pooled_low\":  float(BA_lo) if BA_lo==BA_lo else np.nan,\n",
    "    \"n_folds\": int(len(df_fold)), \"n_pred_rows\": int(len(df_pred))\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(outpath(\"METRICS_SINGLE_BY_GROUPK.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] pooled AUC={auc_pool:.3f}, BA(all)={BA_all:.3f} -> {outpath('METRICS_SINGLE_BY_GROUPK.CSV')}\")\n",
    "\n",
    "# --- 参考作図：全体/High/Low の混同行列PNG（タイトルにτ統計） ---\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"axes.titlesize\": 30, \"axes.labelsize\": 24,\n",
    "    \"legend.fontsize\": 20, \"xtick.labelsize\": 20, \"ytick.labelsize\": 20,\n",
    "})\n",
    "\n",
    "def _cm(y_true, y_pred): return confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "\n",
    "def _draw_cm(ax, cm, title):\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    mat = np.array([[TN, FP],[FN, TP]], dtype=int)\n",
    "    vmax = max(mat.max(), 1)\n",
    "    ax.imshow(mat, cmap=\"Blues\", vmin=0, vmax=vmax)\n",
    "    row_sums = mat.sum(axis=1, keepdims=True)\n",
    "    pct = np.divide(mat, np.where(row_sums==0, 1, row_sums), where=(row_sums!=0)) * 100.0\n",
    "    name = np.array([[\"TN\",\"FP\"],[\"FN\",\"TP\"]])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = mat[i,j]; prc = pct[i,j]\n",
    "            color = \"white\" if val > 0.6*vmax else \"black\"\n",
    "            ax.text(j, i, f\"{name[i,j]} {val}\\n({prc:.1f}%)\", ha=\"center\", va=\"center\",\n",
    "                    fontsize=26, fontweight=\"bold\", color=color)\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Pred: Non-Sick\",\"Pred: Sick\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"True: Non-Sick\",\"True: Sick\"], rotation=90, va=\"center\")\n",
    "    ax.set_title(title, pad=10); ax.grid(False)\n",
    "\n",
    "# τ μ±SD\n",
    "def _mu_sd(sr):\n",
    "    sr = pd.to_numeric(sr, errors=\"coerce\").dropna()\n",
    "    return (float(sr.mean()), float(sr.std(ddof=1)) if len(sr)>1 else 0.0) if len(sr)>0 else (np.nan, np.nan)\n",
    "tau_mu, tau_sd = _mu_sd(df_fold[\"tau_single\"]) if \"tau_single\" in df_fold.columns else (np.nan, np.nan)\n",
    "\n",
    "# 全体\n",
    "cm_all = _cm(df_pred[\"y_true\"].to_numpy(), df_pred[\"y_pred_single\"].to_numpy())\n",
    "fig, ax = plt.subplots(1,1, figsize=(9.5,9))\n",
    "_draw_cm(ax, cm_all, f\"Single τ (All) — {METRIC_NAME}={BA_all:.3f}\\nτ={tau_mu:.3f}±{tau_sd:.3f}\")\n",
    "plt.savefig(outpath(\"CONFMAT_SINGLE_GROUPK_ALL.png\"), dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "print(f\"[Cell8g] plot -> {outpath('CONFMAT_SINGLE_GROUPK_ALL.png')}\")\n",
    "\n",
    "# High\n",
    "if (df_pred[\"group\"]==\"High\").any():\n",
    "    cm_hi = _cm(df_pred.loc[df_pred[\"group\"]==\"High\",\"y_true\"].to_numpy(),\n",
    "                df_pred.loc[df_pred[\"group\"]==\"High\",\"y_pred_single\"].to_numpy())\n",
    "    fig, ax = plt.subplots(1,1, figsize=(9.5,9))\n",
    "    _draw_cm(ax, cm_hi, f\"Single τ (High) — {METRIC_NAME}={BA_hi:.3f}\")\n",
    "    plt.savefig(outpath(\"CONFMAT_SINGLE_GROUPK_HIGH.png\"), dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "    print(f\"[Cell8g] plot -> {outpath('CONFMAT_SINGLE_GROUPK_HIGH.png')}\")\n",
    "\n",
    "# Low\n",
    "if (df_pred[\"group\"]==\"Low\").any():\n",
    "    cm_lo = _cm(df_pred.loc[df_pred[\"group\"]==\"Low\",\"y_true\"].to_numpy(),\n",
    "                df_pred.loc[df_pred[\"group\"]==\"Low\",\"y_pred_single\"].to_numpy())\n",
    "    fig, ax = plt.subplots(1,1, figsize=(9.5,9))\n",
    "    _draw_cm(ax, cm_lo, f\"Single τ (Low) — {METRIC_NAME}={BA_lo:.3f}\")\n",
    "    plt.savefig(outpath(\"CONFMAT_SINGLE_GROUPK_LOW.png\"), dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "    print(f\"[Cell8g] plot -> {outpath('CONFMAT_SINGLE_GROUPK_LOW.png')}\")\n",
    "\n",
    "print(f\"[DONE] METRIC={METRIC_NAME}, SEARCH={SEARCH_MODE}, BEST_K_HIGH={BEST_K_HIGH}, BEST_K_LOW={BEST_K_LOW}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f274923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell8.7] plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\CONFMAT_SINGLE_vs_GROUP.png  (WG-BA-opt (2D))\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8.7 (BA-only): Confusion matrices — Single(属性無視) vs Group-GLOBAL vs Group-WG(2D優先) =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "METRIC_NAME = \"BA\"  # BA固定\n",
    "\n",
    "# 入力ファイル\n",
    "fold_path = outpath(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "pred_path = outpath(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "if not (os.path.exists(fold_path) and os.path.exists(pred_path)):\n",
    "    raise FileNotFoundError(\"[Cell8.7] 必要CSVが見つからない（Cell 8 を先に実行）\")\n",
    "\n",
    "df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "pred    = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- BAユーティリティ ----------\n",
    "def _ba_from_conf(TP, FP, FN, TN):\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    return 0.5 * (TPR + TNR)\n",
    "\n",
    "def _BA_from_preds(y_true_bin: np.ndarray, y_pred_bin: np.ndarray) -> float:\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    return float(_ba_from_conf(TP, FP, FN, TN))\n",
    "\n",
    "def _cm(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "\n",
    "def _mu_sd(sr):\n",
    "    sr = pd.to_numeric(sr, errors=\"coerce\").dropna()\n",
    "    if len(sr) == 0: return (np.nan, np.nan)\n",
    "    return float(sr.mean()), (float(sr.std(ddof=1)) if len(sr) > 1 else 0.0)\n",
    "\n",
    "# ---------- 予測列の再構築（保険） ----------\n",
    "# Single（foldごとの τ_single を使用）\n",
    "if \"y_pred_single\" not in pred.columns and \"tau_single\" in df_fold.columns:\n",
    "    pred = pred.merge(df_fold[[\"fold_id\",\"tau_single\"]], on=\"fold_id\", how=\"left\")\n",
    "    if pred[\"tau_single\"].isna().any():\n",
    "        raise RuntimeError(\"[Cell8.7] tau_single の再構築に失敗（fold対応ズレ）\")\n",
    "    pred[\"y_pred_single\"] = (pred[\"proba\"].astype(float) >= pred[\"tau_single\"].astype(float)).astype(int)\n",
    "    pred.drop(columns=[\"tau_single\"], inplace=True)\n",
    "\n",
    "# Group-GLOBAL（BA最適）を再構築\n",
    "need_global = {\"y_pred_group_BA\"}\n",
    "if not need_global.issubset(pred.columns):\n",
    "    cols = [\"fold_id\",\"tau_high_BA\",\"tau_low_BA\"]\n",
    "    if not set(cols).issubset(df_fold.columns):\n",
    "        raise RuntimeError(\"[Cell8.7] BA用（GLOBAL）のしきい値列が見つからない\")\n",
    "    pred = pred.merge(df_fold[cols], on=\"fold_id\", how=\"left\")\n",
    "    th_main = np.where(pred[\"group\"].astype(str)==\"High\", pred[\"tau_high_BA\"], pred[\"tau_low_BA\"]).astype(float)\n",
    "    pred[\"y_pred_group_BA\"] = (pred[\"proba\"].astype(float) >= th_main).astype(int)\n",
    "    pred.drop(columns=[\"tau_high_BA\",\"tau_low_BA\"], inplace=True)\n",
    "\n",
    "# Group-WG（2Dがあれば優先、無ければ1D）\n",
    "wg_pred_col, wg_tauH_col, wg_tauL_col = None, None, None\n",
    "\n",
    "if \"y_pred_group_WG2D\" in pred.columns:\n",
    "    wg_pred_col = \"y_pred_group_WG2D\"\n",
    "    wg_tauH_col, wg_tauL_col = \"tau_high_WG2D\", \"tau_low_WG2D\"\n",
    "    if not {wg_tauH_col, wg_tauL_col}.issubset(df_fold.columns):\n",
    "        # 予測列はあるが τ 列が fold ファイルに無い場合の保険\n",
    "        wg_tauH_col, wg_tauL_col = None, None\n",
    "elif \"y_pred_group_WG\" in pred.columns:\n",
    "    wg_pred_col = \"y_pred_group_WG\"\n",
    "    wg_tauH_col, wg_tauL_col = \"tau_high_WG\", \"tau_low_WG\"\n",
    "else:\n",
    "    # 列が無ければ再構築を試みる（2D優先）\n",
    "    if {\"tau_high_WG2D\",\"tau_low_WG2D\"}.issubset(df_fold.columns):\n",
    "        pred = pred.merge(df_fold[[\"fold_id\",\"tau_high_WG2D\",\"tau_low_WG2D\"]], on=\"fold_id\", how=\"left\")\n",
    "        th_wg = np.where(pred[\"group\"].astype(str)==\"High\", pred[\"tau_high_WG2D\"], pred[\"tau_low_WG2D\"]).astype(float)\n",
    "        pred[\"y_pred_group_WG2D\"] = (pred[\"proba\"].astype(float) >= th_wg).astype(int)\n",
    "        wg_pred_col, wg_tauH_col, wg_tauL_col = \"y_pred_group_WG2D\", \"tau_high_WG2D\", \"tau_low_WG2D\"\n",
    "        pred.drop(columns=[\"tau_high_WG2D\",\"tau_low_WG2D\"], inplace=True)\n",
    "    elif {\"tau_high_WG\",\"tau_low_WG\"}.issubset(df_fold.columns):\n",
    "        pred = pred.merge(df_fold[[\"fold_id\",\"tau_high_WG\",\"tau_low_WG\"]], on=\"fold_id\", how=\"left\")\n",
    "        th_wg = np.where(pred[\"group\"].astype(str)==\"High\", pred[\"tau_high_WG\"], pred[\"tau_low_WG\"]).astype(float)\n",
    "        pred[\"y_pred_group_WG\"] = (pred[\"proba\"].astype(float) >= th_wg).astype(int)\n",
    "        wg_pred_col, wg_tauH_col, wg_tauL_col = \"y_pred_group_WG\", \"tau_high_WG\", \"tau_low_WG\"\n",
    "        pred.drop(columns=[\"tau_high_WG\",\"tau_low_WG\"], inplace=True)\n",
    "    else:\n",
    "        raise RuntimeError(\"[Cell8.7] WG系の列が見つからない（WG2D/WG1Dのτ列どちらも無し）\")\n",
    "\n",
    "# ---------- データ抽出 ----------\n",
    "y = pred[\"y_true\"].astype(int).to_numpy()\n",
    "p = pred[\"proba\"].astype(float).to_numpy()\n",
    "if len(np.unique(y)) < 2:\n",
    "    raise RuntimeError(\"[Cell8.7] プール真値が単一クラスで AUC/混同行列不可（データやFMS閾値定義を確認）\")\n",
    "\n",
    "yS = pred[\"y_pred_single\"].astype(int).to_numpy()\n",
    "yG = pred[\"y_pred_group_BA\"].astype(int).to_numpy()\n",
    "yW = pred[wg_pred_col].astype(int).to_numpy()\n",
    "\n",
    "cmS = _cm(y, yS); cmG = _cm(y, yG); cmW = _cm(y, yW)\n",
    "scoreS = _BA_from_preds(y, yS)\n",
    "scoreG = _BA_from_preds(y, yG)\n",
    "scoreW = _BA_from_preds(y, yW)\n",
    "\n",
    "# 表示用：AUC と τ の μ±SD\n",
    "auc_pooled = float(roc_auc_score(y, p))\n",
    "k_disp = int(df_fold[\"best_k\"].iloc[0]) if \"best_k\" in df_fold.columns and not pd.isna(df_fold[\"best_k\"].iloc[0]) else np.nan\n",
    "\n",
    "tauS_mu, tauS_sd = _mu_sd(df_fold[\"tau_single\"]) if \"tau_single\" in df_fold.columns else (np.nan, np.nan)\n",
    "tauHB_mu, tauHB_sd = _mu_sd(df_fold.get(\"tau_high_BA\", pd.Series(dtype=float)))\n",
    "tauLB_mu, tauLB_sd = _mu_sd(df_fold.get(\"tau_low_BA\",  pd.Series(dtype=float)))\n",
    "\n",
    "# WGは2D/1Dに応じて列を選択\n",
    "if wg_tauH_col is not None and wg_tauH_col in df_fold.columns:\n",
    "    tauHW_mu, tauHW_sd = _mu_sd(df_fold.get(wg_tauH_col, pd.Series(dtype=float)))\n",
    "    tauLW_mu, tauLW_sd = _mu_sd(df_fold.get(wg_tauL_col, pd.Series(dtype=float)))\n",
    "else:\n",
    "    tauHW_mu, tauHW_sd = (np.nan, np.nan)\n",
    "    tauLW_mu, tauLW_sd = (np.nan, np.nan)\n",
    "\n",
    "# ---------- 描画ヘルパ ----------\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"axes.titlesize\": 30, \"axes.labelsize\": 24,\n",
    "    \"legend.fontsize\": 20, \"xtick.labelsize\": 20, \"ytick.labelsize\": 20,\n",
    "})\n",
    "\n",
    "def _draw(ax, cm, title):\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    mat  = np.array([[TN, FP],[FN, TP]], dtype=int)\n",
    "    name = np.array([[\"TN\",\"FP\"],[\"FN\",\"TP\"]])\n",
    "\n",
    "    vmax = max(mat.max(), 1)\n",
    "    ax.imshow(mat, cmap=\"Blues\", vmin=0, vmax=vmax)\n",
    "\n",
    "    row_sums = mat.sum(axis=1, keepdims=True)\n",
    "    pct = np.divide(mat, np.where(row_sums==0, 1, row_sums),\n",
    "                    where=(row_sums!=0)) * 100.0\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = mat[i, j]; prc = pct[i, j]\n",
    "            color = \"white\" if val > 0.6 * vmax else \"black\"\n",
    "            ax.text(j, i, f\"{name[i,j]} {val}\\n({prc:.1f}%)\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=26, fontweight=\"bold\", color=color)\n",
    "\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Pred: Non-Sick\",\"Pred: Sick\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"True: Non-Sick\",\"True: Sick\"], rotation=90, va=\"center\")\n",
    "    ax.set_title(title, pad=10)\n",
    "    ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "    ax.grid(False)\n",
    "\n",
    "# ---------- タイトル文言 ----------\n",
    "wg_label = \"WG-BA-opt (2D)\" if wg_pred_col.endswith(\"WG2D\") else \"WG-BA-opt (1D)\"\n",
    "title_single = f\"Single τ  ({METRIC_NAME}={scoreS:.3f})\\nτ={tauS_mu:.3f}±{tauS_sd:.3f}\" if not np.isnan(tauS_mu) else f\"Single τ  ({METRIC_NAME}={scoreS:.3f})\"\n",
    "title_gMAIN  = f\"Group τ (BA-opt)  ({METRIC_NAME}={scoreG:.3f})\\nτ_H={tauHB_mu:.3f}±{tauHB_sd:.3f}, τ_L={tauLB_mu:.3f}±{tauLB_sd:.3f}\"\n",
    "title_gWG    = f\"Group τ ({wg_label})  ({METRIC_NAME}={scoreW:.3f})\"\n",
    "if not np.isnan(tauHW_mu) and not np.isnan(tauLW_mu):\n",
    "    title_gWG += f\"\\nτ_H={tauHW_mu:.3f}±{tauHW_sd:.3f}, τ_L={tauLW_mu:.3f}±{tauLW_sd:.3f}\"\n",
    "\n",
    "# ---------- 作図 ----------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 9), constrained_layout=True)\n",
    "_draw(axes[0], cmS, title_single)\n",
    "_draw(axes[1], cmG, title_gMAIN)\n",
    "_draw(axes[2], cmW, title_gWG)\n",
    "\n",
    "supt = f\"Confusion matrices  (k={k_disp if k_disp==k_disp else 'NA'}, ROC AUC={auc_pooled:.3f}, metric={METRIC_NAME})\"\n",
    "fig.suptitle(supt, fontsize=32, y=1.10)  # タイトルを少し上に\n",
    "fig.set_constrained_layout_pads(w_pad=0.02, h_pad=0.02, wspace=0.28, hspace=0.02)\n",
    "\n",
    "out_png = outpath(\"CONFMAT_SINGLE_vs_GROUP.png\")  # 既存名を維持\n",
    "plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"[Cell8.7] plot -> {out_png}  ({wg_label})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2cd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.7c] saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUDIT_BLOCK_CONFUSION_BA.csv\n",
      "[8.7c] saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\FOLD_TPRFPR_COMPARISON.csv\n",
      "[8.7c] TPR↑&FPR↓ folds — Group-BA: None | Group-WG: None\n",
      "[8.7c] plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUDIT_HIST_SCORES_WITH_TAU_BA.png\n",
      "[8.7c] plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUDIT_ROC_BY_GROUP_WITH_OP_POINTS.png\n",
      "[8.7c] DONE\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8.7c (NEW): 5分監査 — 即チェック（assert/TPR-FPR比較/直観図） =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "\n",
    "METRIC_NAME = \"BA\"  # 8.7(BA-only)と整合\n",
    "\n",
    "# ---- 直前セル(8.7: BA-only)が作った df_fold/pred を再読込（安全のため）----\n",
    "fold_path = outpath(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "pred_path = outpath(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "if not (os.path.exists(fold_path) and os.path.exists(pred_path)):\n",
    "    raise FileNotFoundError(\"[Cell8.7c] 必要CSVが見つからない（Cell 8 を先に実行）\")\n",
    "\n",
    "df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "pred    = pd.read_csv(pred_path,  encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---- y_pred_* 列が無い場合は8.7と同様に保険で再構築 ----\n",
    "need_cols = {\"y_pred_single\",\"y_pred_group_BA\",\"y_pred_group_WG\"}\n",
    "rebuild_msgs = []\n",
    "\n",
    "# Single\n",
    "if \"y_pred_single\" not in pred.columns and \"tau_single\" in df_fold.columns:\n",
    "    pred = pred.merge(df_fold[[\"fold_id\",\"tau_single\"]], on=\"fold_id\", how=\"left\")\n",
    "    if pred[\"tau_single\"].isna().any():\n",
    "        raise RuntimeError(\"[Cell8.7c] tau_single の再構築に失敗（fold対応ズレ）\")\n",
    "    pred[\"y_pred_single\"] = (pred[\"proba\"].astype(float) >= pred[\"tau_single\"].astype(float)).astype(int)\n",
    "    pred.drop(columns=[\"tau_single\"], inplace=True)\n",
    "    rebuild_msgs.append(\"rebuild y_pred_single from tau_single\")\n",
    "\n",
    "# Group（BA）\n",
    "if not {\"y_pred_group_BA\",\"y_pred_group_WG\"}.issubset(pred.columns):\n",
    "    cols = [\"fold_id\",\"tau_high_BA\",\"tau_low_BA\",\"tau_high_WG\",\"tau_low_WG\"]\n",
    "    if not set(cols).issubset(df_fold.columns):\n",
    "        raise RuntimeError(\"[Cell8.7c] BA用しきい値列が見つからない（Cell 8）\")\n",
    "    pred = pred.merge(df_fold[cols], on=\"fold_id\", how=\"left\")\n",
    "    th_main = np.where(pred[\"group\"].astype(str)==\"High\", pred[\"tau_high_BA\"], pred[\"tau_low_BA\"]).astype(float)\n",
    "    th_wg   = np.where(pred[\"group\"].astype(str)==\"High\", pred[\"tau_high_WG\"], pred[\"tau_low_WG\"]).astype(float)\n",
    "    pred[\"y_pred_group_BA\"] = (pred[\"proba\"].astype(float) >= th_main).astype(int)\n",
    "    pred[\"y_pred_group_WG\"] = (pred[\"proba\"].astype(float) >= th_wg).astype(int)\n",
    "    pred.drop(columns=cols, inplace=True, errors=\"ignore\")\n",
    "    rebuild_msgs.append(\"rebuild y_pred_group_BA / y_pred_group_WG from taus\")\n",
    "\n",
    "# ---- 即アサーション（ユーザ指定） ----\n",
    "assert pred[[\"y_true\",\"proba\",\"group\"]].isna().sum().sum() == 0, \"[8.7c] pred に NA が含まれる\"\n",
    "# fold件数一致（pred の fold 出現数と df_fold の fold 行数が一致）\n",
    "n_pred_folds = pred[\"fold_id\"].nunique()\n",
    "n_fold_rows  = df_fold[\"fold_id\"].nunique() if \"fold_id\" in df_fold.columns else len(df_fold[\"fold_id\"].unique())\n",
    "assert n_pred_folds == n_fold_rows, f\"[8.7c] fold数不一致: pred={n_pred_folds}, df_fold={n_fold_rows}\"\n",
    "# 必須列\n",
    "assert set(pred.columns).issuperset(need_cols), f\"[8.7c] 必須列欠如: need={need_cols - set(pred.columns)}\"\n",
    "if rebuild_msgs:\n",
    "    print(\"[8.7c] rebuilt -> \" + \" / \".join(rebuild_msgs))\n",
    "\n",
    "# ---- ユーティリティ ----\n",
    "def _cm(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "\n",
    "def _tp_fp_fn_tn(y_true, y_pred):\n",
    "    TN, FP, FN, TP = _cm(y_true, y_pred).ravel()\n",
    "    return int(TP), int(FP), int(FN), int(TN)\n",
    "\n",
    "def _tpr_fpr(y_true, y_pred):\n",
    "    TP, FP, FN, TN = _tp_fp_fn_tn(y_true, y_pred)\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else np.nan\n",
    "    return TPR, FPR\n",
    "\n",
    "# ---- (1) 全体 / High / Low ：同一インデックス集合で (TP,FP,FN,TN) を再計算して表にする ----\n",
    "def _block_compare(mask, tag):\n",
    "    y  = pred.loc[mask, \"y_true\"].to_numpy().astype(int)\n",
    "    yS = pred.loc[mask, \"y_pred_single\"].to_numpy().astype(int)\n",
    "    yG = pred.loc[mask, \"y_pred_group_BA\"].to_numpy().astype(int)\n",
    "    yW = pred.loc[mask, \"y_pred_group_WG\"].to_numpy().astype(int)\n",
    "\n",
    "    rows = []\n",
    "    for name, yhat in [(\"Single\", yS), (\"Group-BA\", yG), (\"Group-WG\", yW)]:\n",
    "        TP, FP, FN, TN = _tp_fp_fn_tn(y, yhat)\n",
    "        TPR, FPR = _tpr_fpr(y, yhat)\n",
    "        rows.append({\"Block\": tag, \"Scheme\": name, \"N\": int(mask.sum()),\n",
    "                     \"TP\":TP, \"FP\":FP, \"FN\":FN, \"TN\":TN,\n",
    "                     \"TPR\":float(TPR), \"FPR\":float(FPR)})\n",
    "    return rows\n",
    "\n",
    "mask_all = np.ones(len(pred), dtype=bool)\n",
    "mask_H   = pred[\"group\"].astype(str) == \"High\"\n",
    "mask_L   = pred[\"group\"].astype(str) == \"Low\"\n",
    "\n",
    "rows_all = _block_compare(mask_all, \"All\")\n",
    "rows_hi  = _block_compare(mask_H,   \"High\")\n",
    "rows_lo  = _block_compare(mask_L,   \"Low\")\n",
    "\n",
    "df_blocks = pd.DataFrame(rows_all + rows_hi + rows_lo)\n",
    "df_blocks.to_csv(outpath(\"AUDIT_BLOCK_CONFUSION_BA.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[8.7c] saved -> {outpath('AUDIT_BLOCK_CONFUSION_BA.csv')}\")\n",
    "\n",
    "# ---- (2) foldごとの TPR/FPR を比較し、“TPR↑ & FPR↓” のfoldを特定 ----\n",
    "comp_rows = []\n",
    "for fid, sub in pred.groupby(\"fold_id\"):\n",
    "    y  = sub[\"y_true\"].to_numpy().astype(int)\n",
    "\n",
    "    # Single\n",
    "    yS = sub[\"y_pred_single\"].to_numpy().astype(int)\n",
    "    TPR_S, FPR_S = _tpr_fpr(y, yS)\n",
    "\n",
    "    # Group-BA\n",
    "    yG = sub[\"y_pred_group_BA\"].to_numpy().astype(int)\n",
    "    TPR_G, FPR_G = _tpr_fpr(y, yG)\n",
    "\n",
    "    # Group-WG\n",
    "    yW = sub[\"y_pred_group_WG\"].to_numpy().astype(int)\n",
    "    TPR_W, FPR_W = _tpr_fpr(y, yW)\n",
    "\n",
    "    comp_rows.append({\n",
    "        \"fold_id\": fid,\n",
    "        \"TPR_S\": TPR_S, \"FPR_S\": FPR_S,\n",
    "        \"TPR_G\": TPR_G, \"FPR_G\": FPR_G,\n",
    "        \"TPR_W\": TPR_W, \"FPR_W\": FPR_W,\n",
    "        \"TPR_up_FPR_down_G\": (TPR_G > TPR_S) and (FPR_G < FPR_S),\n",
    "        \"TPR_up_FPR_down_W\": (TPR_W > TPR_S) and (FPR_W < FPR_S),\n",
    "        \"n_rows\": len(sub)\n",
    "    })\n",
    "\n",
    "df_fold_cmp = pd.DataFrame(comp_rows).sort_values(\"fold_id\")\n",
    "df_fold_cmp.to_csv(outpath(\"FOLD_TPRFPR_COMPARISON.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[8.7c] saved -> {outpath('FOLD_TPRFPR_COMPARISON.csv')}\")\n",
    "\n",
    "hit_G = df_fold_cmp.loc[df_fold_cmp[\"TPR_up_FPR_down_G\"], \"fold_id\"].tolist()\n",
    "hit_W = df_fold_cmp.loc[df_fold_cmp[\"TPR_up_FPR_down_W\"], \"fold_id\"].tolist()\n",
    "print(f\"[8.7c] TPR↑&FPR↓ folds — Group-BA: {hit_G if hit_G else 'None'} | Group-WG: {hit_W if hit_W else 'None'}\")\n",
    "\n",
    "# ---- (3) 直観図：High/Low のスコア分布と τ（Single/Group-BA） ----\n",
    "# 8.7(BA-only)で使った τ の μ±SD を再計算（縦線は平均値を表示）\n",
    "def _mu_sd(sr):\n",
    "    sr = pd.to_numeric(sr, errors=\"coerce\").dropna()\n",
    "    if len(sr) == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    mu = float(sr.mean())\n",
    "    sd = float(sr.std(ddof=1)) if len(sr) > 1 else 0.0\n",
    "    return mu, sd\n",
    "\n",
    "tauS_mu, _ = _mu_sd(df_fold.get(\"tau_single\", pd.Series(dtype=float)))\n",
    "tauH_mu, _ = _mu_sd(df_fold.get(\"tau_high_BA\", pd.Series(dtype=float)))\n",
    "tauL_mu, _ = _mu_sd(df_fold.get(\"tau_low_BA\",  pd.Series(dtype=float)))\n",
    "\n",
    "s_all = pred[\"proba\"].astype(float).to_numpy()\n",
    "s_H   = pred.loc[mask_H, \"proba\"].astype(float).to_numpy()\n",
    "s_L   = pred.loc[mask_L, \"proba\"].astype(float).to_numpy()\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"axes.titlesize\": 30, \"axes.labelsize\": 24,\n",
    "    \"legend.fontsize\": 20, \"xtick.labelsize\": 20, \"ytick.labelsize\": 20,\n",
    "})\n",
    "\n",
    "# ヒスト（High/Low 重ね） + τの縦線\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "bins = np.linspace(0.0, 1.0, 41)\n",
    "ax.hist(s_H, bins=bins, alpha=0.55, density=True, label=\"High\", edgecolor=\"none\")\n",
    "ax.hist(s_L, bins=bins, alpha=0.55, density=True, label=\"Low\",  edgecolor=\"none\")\n",
    "if not np.isnan(tauS_mu): ax.axvline(tauS_mu,  linestyle=\"--\", linewidth=1.5, label=f\"τ_single≈{tauS_mu:.3f}\")\n",
    "if not np.isnan(tauH_mu): ax.axvline(tauH_mu,  linestyle=\"-.\", linewidth=1.5, label=f\"τ_H(BA)≈{tauH_mu:.3f}\")\n",
    "if not np.isnan(tauL_mu): ax.axvline(tauL_mu,  linestyle=\":\",  linewidth=1.5, label=f\"τ_L(BA)≈{tauL_mu:.3f}\")\n",
    "ax.set_xlabel(\"Predicted score\"); ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Score distribution by group with thresholds (means)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"AUDIT_HIST_SCORES_WITH_TAU_BA.png\"), dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "print(f\"[8.7c] plot -> {outpath('AUDIT_HIST_SCORES_WITH_TAU_BA.png')}\")\n",
    "\n",
    "# ---- (4) 参考：ROC（High/Low）に τ_single/τ_H/τ_L の動作点を打つ ----\n",
    "def _op_point(scores, labels, tau):\n",
    "    yhat = (scores >= tau).astype(int)\n",
    "    TP, FP, FN, TN = _tp_fp_fn_tn(labels, yhat)\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else np.nan\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else np.nan\n",
    "    return FPR, TPR\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(7.5,7.5))\n",
    "for tag, scores, labels, col in [\n",
    "    (\"High\", s_H, pred.loc[mask_H, \"y_true\"].to_numpy().astype(int), \"tab:blue\"),\n",
    "    (\"Low\",  s_L, pred.loc[mask_L, \"y_true\"].to_numpy().astype(int), \"tab:orange\")\n",
    "]:\n",
    "    if len(np.unique(labels)) < 2:\n",
    "        print(f\"[8.7c][ROC] {tag}: 単一クラスのため ROC 省略\")\n",
    "        continue\n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    ax.plot(fpr, tpr, label=f\"{tag} ROC (AUC={auc(fpr,tpr):.3f})\", color=col, linewidth=1.5)\n",
    "\n",
    "# 動作点（平均τを便宜的に利用）\n",
    "if not np.isnan(tauS_mu):\n",
    "    # High/Low それぞれに single τ を適用した動作点\n",
    "    if mask_H.any():\n",
    "        xS_H, yS_H = _op_point(s_H, pred.loc[mask_H,\"y_true\"].to_numpy().astype(int), tauS_mu)\n",
    "        ax.scatter([xS_H],[yS_H], marker=\"o\", s=60, color=\"tab:blue\", edgecolor=\"k\", label=\"High @ τ_single\")\n",
    "    if mask_L.any():\n",
    "        xS_L, yS_L = _op_point(s_L, pred.loc[mask_L,\"y_true\"].to_numpy().astype(int), tauS_mu)\n",
    "        ax.scatter([xS_L],[yS_L], marker=\"o\", s=60, color=\"tab:orange\", edgecolor=\"k\", label=\"Low  @ τ_single\")\n",
    "if not np.isnan(tauH_mu) and mask_H.any():\n",
    "    xH, yH = _op_point(s_H, pred.loc[mask_H,\"y_true\"].to_numpy().astype(int), tauH_mu)\n",
    "    ax.scatter([xH],[yH], marker=\"^\", s=70, color=\"tab:blue\", edgecolor=\"k\", label=\"High @ τ_H(BA)\")\n",
    "if not np.isnan(tauL_mu) and mask_L.any():\n",
    "    xL, yL = _op_point(s_L, pred.loc[mask_L,\"y_true\"].to_numpy().astype(int), tauL_mu)\n",
    "    ax.scatter([xL],[yL], marker=\"^\", s=70, color=\"tab:orange\", edgecolor=\"k\", label=\"Low  @ τ_L(BA)\")\n",
    "\n",
    "ax.plot([0,1],[0,1], linestyle=\"--\", color=\"gray\", linewidth=1.0)\n",
    "ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\")\n",
    "ax.set_title(\"ROC by group with operating points\")\n",
    "ax.legend(loc=\"lower right\", fontsize=12)\n",
    "ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"AUDIT_ROC_BY_GROUP_WITH_OP_POINTS.png\"), dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "print(f\"[8.7c] plot -> {outpath('AUDIT_ROC_BY_GROUP_WITH_OP_POINTS.png')}\")\n",
    "\n",
    "print(\"[8.7c] DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f1a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell8.7b] plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\CONFMAT_OUTERONLY_SINGLE_vs_GROUP.png  (post-hoc tuned on pooled CV predictions)\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8.7b (BA-only): Confusion matrices — OUTER-ONLY（Single vs Group-GLOBAL vs Group-WG） =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "METRIC_NAME = \"BA\"\n",
    "\n",
    "# 入力ファイル（8b の出力）\n",
    "pred_path = outpath(\"PREDICTIONS_OUTERONLY.CSV\")\n",
    "th_path   = outpath(\"FINAL_THRESHOLDS_POSTHOC.CSV\")\n",
    "if not (os.path.exists(pred_path) and os.path.exists(th_path)):\n",
    "    raise FileNotFoundError(\"[Cell8.7b] 必要CSVが見つからない（Cell 8b を先に実行）\")\n",
    "\n",
    "pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "th   = pd.read_csv(th_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- BAユーティリティ ---\n",
    "def _ba_from_conf(TP, FP, FN, TN):\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    return 0.5 * (TPR + TNR)\n",
    "\n",
    "def _BA_from_preds(y_true_bin: np.ndarray, y_pred_bin: np.ndarray) -> float:\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    return float(_ba_from_conf(TP, FP, FN, TN))\n",
    "\n",
    "# --- 予測列が無い場合は、しきい値ファイルから再構築（保険） ---\n",
    "if \"y_pred_single\" not in pred.columns:\n",
    "    tau_single = float(th[\"tau_single\"].iloc[0])\n",
    "    pred[\"y_pred_single\"] = (pred[\"proba\"].astype(float) >= tau_single).astype(int)\n",
    "\n",
    "if not {\"y_pred_group_BA\",\"y_pred_group_WG\"}.issubset(pred.columns):\n",
    "    tauH_MAIN, tauL_MAIN = float(th[\"tau_high_BA\"].iloc[0]), float(th[\"tau_low_BA\"].iloc[0])\n",
    "    tauH_WG,   tauL_WG   = float(th[\"tau_high_WG\"].iloc[0]), float(th[\"tau_low_WG\"].iloc[0])\n",
    "    th_main = np.where(pred[\"group\"].astype(str)==\"High\", tauH_MAIN, tauL_MAIN).astype(float)\n",
    "    th_wg   = np.where(pred[\"group\"].astype(str)==\"High\", tauH_WG,   tauL_WG).astype(float)\n",
    "    pred[\"y_pred_group_BA\"] = (pred[\"proba\"].astype(float) >= th_main).astype(int)\n",
    "    pred[\"y_pred_group_WG\"] = (pred[\"proba\"].astype(float) >= th_wg).astype(int)\n",
    "\n",
    "# --- データ抽出 ---\n",
    "y = pred[\"y_true\"].astype(int).to_numpy()\n",
    "p = pred[\"proba\"].astype(float).to_numpy()\n",
    "if len(np.unique(y)) < 2:\n",
    "    raise RuntimeError(\"[Cell8.7b] プール真値が単一クラスで AUC/混同行列不可（データやFMS閾値定義を確認）\")\n",
    "\n",
    "def _cm(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "\n",
    "yS = pred[\"y_pred_single\"].astype(int).to_numpy()\n",
    "yG = pred[\"y_pred_group_BA\"].astype(int).to_numpy()\n",
    "yW = pred[\"y_pred_group_WG\"].astype(int).to_numpy()\n",
    "\n",
    "cmS = _cm(y, yS); cmG = _cm(y, yG); cmW = _cm(y, yW)\n",
    "scoreS = _BA_from_preds(y, yS)\n",
    "scoreG = _BA_from_preds(y, yG)\n",
    "scoreW = _BA_from_preds(y, yW)\n",
    "\n",
    "auc_pooled = float(roc_auc_score(y, p))\n",
    "\n",
    "# --- 描画ヘルパ ---\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"axes.titlesize\": 30,\n",
    "    \"axes.labelsize\": 24,\n",
    "    \"legend.fontsize\": 20,\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "})\n",
    "\n",
    "def draw(ax, cm, title):\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    mat = np.array([[TN, FP],[FN, TP]], dtype=int)\n",
    "    name = np.array([[\"TN\", \"FP\"],[\"FN\", \"TP\"]])\n",
    "    vmax = max(mat.max(), 1)\n",
    "    ax.imshow(mat, cmap=\"Blues\", vmin=0, vmax=vmax)\n",
    "\n",
    "    row_sums = mat.sum(axis=1, keepdims=True)\n",
    "    pct = np.divide(mat, np.where(row_sums==0, 1, row_sums), where=(row_sums!=0)) * 100.0\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = mat[i, j]; prc = pct[i, j]\n",
    "            color = \"white\" if val > 0.6 * vmax else \"black\"\n",
    "            ax.text(j, i, f\"{name[i,j]} {val}\\n({prc:.1f}%)\",\n",
    "                    ha=\"center\", va=\"center\", fontsize=26, fontweight=\"bold\", color=color)\n",
    "\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Pred: Non-Sick\",\"Pred: Sick\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"True: Non-Sick\",\"True: Sick\"], rotation=90, va=\"center\")\n",
    "    ax.set_title(title, pad=10)\n",
    "    ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "    ax.grid(False)\n",
    "\n",
    "# タイトル\n",
    "title_single = f\"Single τ ({METRIC_NAME}={scoreS:.3f})\"\n",
    "title_gMAIN  = f\"Group τ ({METRIC_NAME}-opt) ({METRIC_NAME}={scoreG:.3f})\"\n",
    "title_gWG    = f\"Group τ (WG-{METRIC_NAME}-opt) ({METRIC_NAME}={scoreW:.3f})\"\n",
    "\n",
    "# 作図\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 9), constrained_layout=True)\n",
    "draw(axes[0], cmS, title_single)\n",
    "draw(axes[1], cmG, title_gMAIN)\n",
    "draw(axes[2], cmW, title_gWG)\n",
    "\n",
    "supt = f\"Confusion matrices (OUTER-ONLY post-hoc)  ROC AUC={auc_pooled:.3f}, metric={METRIC_NAME}\"\n",
    "fig.suptitle(supt, fontsize=32, y=1.10)\n",
    "fig.set_constrained_layout_pads(w_pad=0.02, h_pad=0.02, wspace=0.28, hspace=0.02)\n",
    "\n",
    "out_png = outpath(\"CONFMAT_OUTERONLY_SINGLE_vs_GROUP.png\")\n",
    "plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"[Cell8.7b] plot -> {out_png}  (post-hoc tuned on pooled CV predictions)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ba5e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cell8.8] plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\CONFMAT_GROUP_GLOBAL_BY_GROUP.png\n",
      "[Cell8.8] plot -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\CONFMAT_GROUP_WG_BY_GROUP.png\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 8.8 (BA-only): Confusion matrices — Group別（Highのみ/Lowのみ） =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "METRIC_NAME = \"BA\"\n",
    "\n",
    "# 入力（Cell 8 実行済み前提）\n",
    "fold_path = outpath(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "pred_path = outpath(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "if not (os.path.exists(fold_path) and os.path.exists(pred_path)):\n",
    "    raise FileNotFoundError(\"[Cell8.8] 必要CSVが見つからない（Cell 8 を先に実行）\")\n",
    "\n",
    "df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "pred    = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- BAユーティリティ ---\n",
    "def _ba_from_conf(TP, FP, FN, TN):\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    TNR = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    return 0.5 * (TPR + TNR)\n",
    "\n",
    "def _BA_from_preds(y_true_bin: np.ndarray, y_pred_bin: np.ndarray) -> float:\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    return float(_ba_from_conf(TP, FP, FN, TN))\n",
    "\n",
    "def _cm(y_true, y_pred):\n",
    "    return confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "\n",
    "def _mu_sd(sr):\n",
    "    sr = pd.to_numeric(sr, errors=\"coerce\").dropna()\n",
    "    if len(sr) == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    mu = float(sr.mean())\n",
    "    sd = float(sr.std(ddof=1)) if len(sr) > 1 else 0.0\n",
    "    return (mu, sd)\n",
    "\n",
    "# --- 予測列が無い場合は再構築（保険） ---\n",
    "if \"y_pred_single\" not in pred.columns and \"tau_single\" in df_fold.columns:\n",
    "    pred = pred.merge(df_fold[[\"fold_id\",\"tau_single\"]], on=\"fold_id\", how=\"left\")\n",
    "    pred[\"y_pred_single\"] = (pred[\"proba\"].astype(float) >= pred[\"tau_single\"].astype(float)).astype(int)\n",
    "    pred.drop(columns=[\"tau_single\"], inplace=True)\n",
    "\n",
    "# Group（BAしきい値から再構築）\n",
    "need = {\"y_pred_group_BA\",\"y_pred_group_WG\"}\n",
    "if not need.issubset(pred.columns):\n",
    "    cols = [\"fold_id\",\"tau_high_BA\",\"tau_low_BA\",\"tau_high_WG\",\"tau_low_WG\"]\n",
    "    pred = pred.merge(df_fold[cols], on=\"fold_id\", how=\"left\")\n",
    "    th_main = np.where(pred[\"group\"].astype(str)==\"High\", pred[\"tau_high_BA\"], pred[\"tau_low_BA\"]).astype(float)\n",
    "    th_wg   = np.where(pred[\"group\"].astype(str)==\"High\", pred[\"tau_high_WG\"], pred[\"tau_low_WG\"]).astype(float)\n",
    "    pred[\"y_pred_group_BA\"] = (pred[\"proba\"].astype(float) >= th_main).astype(int)\n",
    "    pred[\"y_pred_group_WG\"] = (pred[\"proba\"].astype(float) >= th_wg).astype(int)\n",
    "    pred.drop(columns=[\"tau_high_BA\",\"tau_low_BA\",\"tau_high_WG\",\"tau_low_WG\"], inplace=True)\n",
    "\n",
    "col_main, col_wg = \"y_pred_group_BA\", \"y_pred_group_WG\"\n",
    "\n",
    "# --- 群別に抽出 ---\n",
    "mask_H = pred[\"group\"].astype(str) == \"High\"\n",
    "mask_L = pred[\"group\"].astype(str) == \"Low\"\n",
    "\n",
    "if not mask_H.any() or not mask_L.any():\n",
    "    raise RuntimeError(\"[Cell8.8] High/Low のいずれかが存在しないため群別混同行列が作れません\")\n",
    "\n",
    "# τの μ±SD（群別・スキーム別）\n",
    "tauH_MAIN_mu, tauH_MAIN_sd = _mu_sd(df_fold.get(\"tau_high_BA\",  pd.Series(dtype=float)))\n",
    "tauL_MAIN_mu, tauL_MAIN_sd = _mu_sd(df_fold.get(\"tau_low_BA\",   pd.Series(dtype=float)))\n",
    "tauH_WG_mu,   tauH_WG_sd   = _mu_sd(df_fold.get(\"tau_high_WG\",  pd.Series(dtype=float)))\n",
    "tauL_WG_mu,   tauL_WG_sd   = _mu_sd(df_fold.get(\"tau_low_WG\",   pd.Series(dtype=float)))\n",
    "\n",
    "# --- スコア計算（群別×スキーム別） ---\n",
    "def _score_cm_for(mask, col_pred):\n",
    "    y = pred.loc[mask, \"y_true\"].astype(int).to_numpy()\n",
    "    yhat = pred.loc[mask, col_pred].astype(int).to_numpy()\n",
    "    if len(y) == 0:\n",
    "        return None, np.nan, 0, 0, 0, np.nan\n",
    "    cm = _cm(y, yhat)\n",
    "    sc = _BA_from_preds(y, yhat)\n",
    "    n  = int(mask.sum())\n",
    "    pos = int((pred.loc[mask, \"y_true\"]==1).sum())\n",
    "    neg = n - pos\n",
    "    pi  = (pos / n * 100.0) if n > 0 else np.nan\n",
    "    return cm, sc, n, pos, neg, pi\n",
    "\n",
    "cm_H_main, BA_H_main, nH, pH, nHn, piH = _score_cm_for(mask_H, col_main)\n",
    "cm_L_main, BA_L_main, nL, pL, nLn, piL = _score_cm_for(mask_L, col_main)\n",
    "cm_H_wg,   BA_H_wg,   _,  _,  _,   _   = _score_cm_for(mask_H, col_wg)\n",
    "cm_L_wg,   BA_L_wg,   _,  _,  _,   _   = _score_cm_for(mask_L, col_wg)\n",
    "\n",
    "# --- 描画共通スタイル ---\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"axes.titlesize\": 30,\n",
    "    \"axes.labelsize\": 24,\n",
    "    \"legend.fontsize\": 20,\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "})\n",
    "\n",
    "def _draw(ax, cm, title):\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    mat = np.array([[TN, FP],[FN, TP]], dtype=int)\n",
    "    name = np.array([[\"TN\",\"FP\"],[\"FN\",\"TP\"]])\n",
    "\n",
    "    vmax = max(mat.max(), 1)\n",
    "    ax.imshow(mat, cmap=\"Blues\", vmin=0, vmax=vmax)\n",
    "\n",
    "    row_sums = mat.sum(axis=1, keepdims=True)\n",
    "    pct = np.divide(mat, np.where(row_sums==0, 1, row_sums),\n",
    "                    where=(row_sums!=0)) * 100.0\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = mat[i, j]; prc = pct[i, j]\n",
    "            color = \"white\" if val > 0.6 * vmax else \"black\"\n",
    "            ax.text(j, i, f\"{name[i,j]} {val}\\n({prc:.1f}%)\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=26, fontweight=\"bold\", color=color)\n",
    "\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Pred: Non-Sick\",\"Pred: Sick\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"True: Non-Sick\",\"True: Sick\"], rotation=90, va=\"center\")\n",
    "    ax.set_title(title, pad=10)\n",
    "    ax.set_xlabel(\"\"); ax.set_ylabel(\"\")\n",
    "    ax.grid(False)\n",
    "\n",
    "# === (1) Group-GLOBAL（BA最適）の High/Low 1x2 ===\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8), constrained_layout=True)\n",
    "title_H_main = f\"High — Group-{METRIC_NAME}-opt  ({METRIC_NAME}={BA_H_main:.3f}, n={nH}, π={piH:.1f}%)\\nτ_H={tauH_MAIN_mu:.3f}±{tauH_MAIN_sd:.3f}\"\n",
    "title_L_main = f\"Low  — Group-{METRIC_NAME}-opt  ({METRIC_NAME}={BA_L_main:.3f}, n={nL}, π={piL:.1f}%)\\nτ_L={tauL_MAIN_mu:.3f}±{tauL_MAIN_sd:.3f}\"\n",
    "_draw(axes[0], cm_H_main, title_H_main)\n",
    "_draw(axes[1], cm_L_main, title_L_main)\n",
    "out_png1 = outpath(\"CONFMAT_GROUP_GLOBAL_BY_GROUP.png\")\n",
    "plt.savefig(out_png1, dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "print(f\"[Cell8.8] plot -> {out_png1}\")\n",
    "\n",
    "# === (2) Group-WG（Worst-Group BA最適）の High/Low 1x2 ===\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8), constrained_layout=True)\n",
    "title_H_wg = f\"High — Group-WG-{METRIC_NAME}-opt  ({METRIC_NAME}={BA_H_wg:.3f}, n={nH}, π={piH:.1f}%)\\nτ_H={tauH_WG_mu:.3f}±{tauH_WG_sd:.3f}\"\n",
    "title_L_wg = f\"Low  — Group-WG-{METRIC_NAME}-opt  ({METRIC_NAME}={BA_L_wg:.3f}, n={nL}, π={piL:.1f}%)\\nτ_L={tauL_WG_mu:.3f}±{tauL_WG_sd:.3f}\"\n",
    "_draw(axes[0], cm_H_wg, title_H_wg)\n",
    "_draw(axes[1], cm_L_wg, title_L_wg)\n",
    "out_png2 = outpath(\"CONFMAT_GROUP_WG_BY_GROUP.png\")\n",
    "plt.savefig(out_png2, dpi=300, bbox_inches=\"tight\"); plt.close()\n",
    "print(f\"[Cell8.8] plot -> {out_png2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26838055",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[Cell9] C:\\\\Users\\\\taiki\\\\OneDrive - Science Tokyo\\\\デスクトップ\\\\研究\\\\本実験結果\\\\ANALYSIS\\\\機械学習(MSSQ込み)\\\\閾値FMS1\\\\FINAL_THRESHOLDS_POSTHOC.CSV に tau_high_WGF1 が見つからない'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 160\u001b[0m\n\u001b[0;32m    158\u001b[0m source_kind, pred_csv, thr_csv \u001b[38;5;241m=\u001b[39m _pick_source()\n\u001b[0;32m    159\u001b[0m pred_df, y_pool, s_pool, g_pool \u001b[38;5;241m=\u001b[39m _load_predictions(pred_csv)\n\u001b[1;32m--> 160\u001b[0m taus \u001b[38;5;241m=\u001b[39m \u001b[43m_load_thresholds\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_kind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthr_csv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# High/Low 抽出\u001b[39;00m\n\u001b[0;32m    163\u001b[0m maskH \u001b[38;5;241m=\u001b[39m (g_pool \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[42], line 94\u001b[0m, in \u001b[0;36m_load_thresholds\u001b[1;34m(source_kind, th_path)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m need_cols:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 94\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Cell9] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mth_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m に \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m が見つからない\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m tau_single \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtau_single\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m METRIC \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: '[Cell9] C:\\\\Users\\\\taiki\\\\OneDrive - Science Tokyo\\\\デスクトップ\\\\研究\\\\本実験結果\\\\ANALYSIS\\\\機械学習(MSSQ込み)\\\\閾値FMS1\\\\FINAL_THRESHOLDS_POSTHOC.CSV に tau_high_WGF1 が見つからない'"
     ]
    }
   ],
   "source": [
    "# ===== Cell 9 (NEW): Probability density plots — Single vs High vs Low（しきい値は他セルの出力を読込） =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ユーザ環境前提：\n",
    "# - outpath() が既に定義済み（他セルと同じ）\n",
    "# - THRESH_OBJECTIVE in {\"f1\",\"ba\"} が必要なら定義済み（未定義なら \"f1\"）\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# ===== パラメータ =====\n",
    "CELL9_BINS: int = int(globals().get(\"CELL9_BINS\", 30))             # ヒストのビン数\n",
    "CELL9_SOURCE: str = str(globals().get(\"CELL9_SOURCE\", \"auto\"))     # \"auto\" / \"inner\" / \"outer\"\n",
    "CELL9_TAU_AGG: str = str(globals().get(\"CELL9_TAU_AGG\", \"wmean\"))  # τ代表値の集計: \"wmean\"|\"mean\"|\"median\"\n",
    "\n",
    "METRIC = str(globals().get(\"THRESH_OBJECTIVE\", \"f1\")).lower()\n",
    "if METRIC not in {\"f1\", \"ba\"}:\n",
    "    METRIC = \"f1\"\n",
    "METRIC_NAME = \"F1\" if METRIC == \"f1\" else \"BA\"\n",
    "\n",
    "# ===== 入力候補ファイル =====\n",
    "PRED_INNER = outpath(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "FOLD_INNER = outpath(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "\n",
    "PRED_OUTER = outpath(\"PREDICTIONS_OUTERONLY.CSV\")\n",
    "THRS_OUTER = outpath(\"FINAL_THRESHOLDS_POSTHOC.CSV\")\n",
    "\n",
    "METR_INNER = outpath(\"LOSO_METRICS.CSV\")\n",
    "METR_OUTER = outpath(\"METRICS_POSTHOC.CSV\")\n",
    "\n",
    "# ===== ユーティリティ =====\n",
    "def _exists(p): return isinstance(p, str) and os.path.exists(p)\n",
    "\n",
    "def _pick_source():\n",
    "    \"\"\"描画に用いる予測CSV/しきい値CSVを自動選択（外側post-hocを優先）\"\"\"\n",
    "    if CELL9_SOURCE == \"outer\":\n",
    "        if not (_exists(PRED_OUTER) and _exists(THRS_OUTER)):\n",
    "            raise FileNotFoundError(\"[Cell9] outer 指定だが PREDICTIONS_OUTERONLY/FINAL_THRESHOLDS_POSTHOC が見つからない\")\n",
    "        return (\"outer\", PRED_OUTER, THRS_OUTER)\n",
    "    if CELL9_SOURCE == \"inner\":\n",
    "        if not (_exists(PRED_INNER) and _exists(FOLD_INNER)):\n",
    "            raise FileNotFoundError(\"[Cell9] inner 指定だが GROUP_AWARE_* CSV が見つからない\")\n",
    "        return (\"inner\", PRED_INNER, FOLD_INNER)\n",
    "\n",
    "    # auto: outer を優先、無ければ inner\n",
    "    if _exists(PRED_OUTER) and _exists(THRS_OUTER):\n",
    "        return (\"outer\", PRED_OUTER, THRS_OUTER)\n",
    "    if _exists(PRED_INNER) and _exists(FOLD_INNER):\n",
    "        return (\"inner\", PRED_INNER, FOLD_INNER)\n",
    "    raise FileNotFoundError(\"[Cell9] 予測/しきい値CSVが見つからない（Cell 8 or 8b を先に実行）\")\n",
    "\n",
    "def _load_predictions(pred_path: str):\n",
    "    df = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "    for col in (\"y_true\",\"proba\",\"group\"):\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"[Cell9] {pred_path} に {col} 列がない\")\n",
    "    y = df[\"y_true\"].astype(int).to_numpy()\n",
    "    s = df[\"proba\"].astype(float).to_numpy()\n",
    "    g = df[\"group\"].astype(str).to_numpy()\n",
    "    return df, y, s, g\n",
    "\n",
    "def _aggregate_tau(sr: pd.Series, weights: pd.Series | None, how: str) -> float:\n",
    "    sr = pd.to_numeric(sr, errors=\"coerce\").dropna()\n",
    "    if sr.empty: return np.nan\n",
    "    how = how.lower()\n",
    "    if how == \"median\":\n",
    "        return float(sr.median())\n",
    "    if how == \"wmean\" and weights is not None and len(weights)==len(sr):\n",
    "        w = pd.to_numeric(weights, errors=\"coerce\").fillna(0).to_numpy()\n",
    "        s = sr.to_numpy()\n",
    "        wsum = float(w.sum())\n",
    "        return float((s*w).sum()/wsum) if wsum>0 else float(s.mean())\n",
    "    # default: mean\n",
    "    return float(sr.mean())\n",
    "\n",
    "def _load_thresholds(source_kind: str, th_path: str):\n",
    "    \"\"\"\n",
    "    τ を“読込だけ”で取得する（再計算しない）。\n",
    "    - outer: FINAL_THRESHOLDS_POSTHOC.CSV から単一値を読む\n",
    "    - inner: GROUP_AWARE_THRESH_BY_FOLD.CSV から fold 別 τ を集計して代表値を作る\n",
    "    戻り値: dict {\"tau_single\",\"tauH_WG\",\"tauL_WG\"}  ※METRICに応じて列を選択\n",
    "    \"\"\"\n",
    "    if source_kind == \"outer\":\n",
    "        df = pd.read_csv(th_path, encoding=\"utf-8-sig\")\n",
    "        need_cols = [\"tau_single\"]\n",
    "        if METRIC == \"f1\":\n",
    "            need_cols += [\"tau_high_WGF1\",\"tau_low_WGF1\"]\n",
    "        else:\n",
    "            need_cols += [\"tau_high_WG\",\"tau_low_WG\"]\n",
    "        for c in need_cols:\n",
    "            if c not in df.columns:\n",
    "                raise KeyError(f\"[Cell9] {th_path} に {c} が見つからない\")\n",
    "\n",
    "        tau_single = float(df[\"tau_single\"].iloc[0])\n",
    "        if METRIC == \"f1\":\n",
    "            tauH, tauL = float(df[\"tau_high_WGF1\"].iloc[0]), float(df[\"tau_low_WGF1\"].iloc[0])\n",
    "        else:\n",
    "            tauH, tauL = float(df[\"tau_high_WG\"].iloc[0]),   float(df[\"tau_low_WG\"].iloc[0])\n",
    "        return {\"tau_single\": tau_single, \"tauH_WG\": tauH, \"tauL_WG\": tauL, \"note\": \"outer/post-hoc\"}\n",
    "\n",
    "    # inner: fold 別を代表値に集計\n",
    "    df = pd.read_csv(th_path, encoding=\"utf-8-sig\")\n",
    "    if \"tau_single\" not in df.columns:\n",
    "        raise KeyError(f\"[Cell9] {th_path} に tau_single が見つからない\")\n",
    "    weights = df[\"n_test\"] if \"n_test\" in df.columns else None\n",
    "\n",
    "    tau_single = _aggregate_tau(df[\"tau_single\"], weights, CELL9_TAU_AGG)\n",
    "\n",
    "    if METRIC == \"f1\":\n",
    "        needH, needL = \"tau_high_WGF1\", \"tau_low_WGF1\"\n",
    "    else:\n",
    "        needH, needL = \"tau_high_WG\", \"tau_low_WG\"\n",
    "    for c in (needH, needL):\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(f\"[Cell9] {th_path} に {c} が見つからない\")\n",
    "\n",
    "    tauH = _aggregate_tau(df[needH], weights, CELL9_TAU_AGG)\n",
    "    tauL = _aggregate_tau(df[needL], weights, CELL9_TAU_AGG)\n",
    "    return {\"tau_single\": tau_single, \"tauH_WG\": tauH, \"tauL_WG\": tauL, \"note\": f\"inner/agg={CELL9_TAU_AGG}\"}\n",
    "\n",
    "def _load_metrics_title():\n",
    "    \"\"\"AUC/n 情報があればタイトル用に取得（無ければ None）\"\"\"\n",
    "    src = METR_OUTER if _exists(METR_OUTER) else (METR_INNER if _exists(METR_INNER) else None)\n",
    "    if not src: return None\n",
    "    try:\n",
    "        m = pd.read_csv(src, encoding=\"utf-8-sig\")\n",
    "        auc = float(m.filter(regex=\"AUC\", axis=1).iloc[0].dropna().values[0])\n",
    "        n  = int(m[\"n_samples\"].iloc[0]) if \"n_samples\" in m.columns else None\n",
    "        return {\"auc\": auc, \"n\": n, \"src\": os.path.basename(src)}\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _draw_density(ax, s0, s1, tau: float | None, title: str, bins: int):\n",
    "    \"\"\"\n",
    "    s0: P(Sick) for Non-Sick (y=0)\n",
    "    s1: P(Sick) for Sick     (y=1)\n",
    "    \"\"\"\n",
    "    # 規格化ヒスト（density=True）\n",
    "    bin_edges = np.linspace(0, 1, bins+1)\n",
    "    ax.hist(s0, bins=bin_edges, density=True, alpha=0.45, label=\"Non-Sick\", edgecolor=\"none\")\n",
    "    ax.hist(s1, bins=bin_edges, density=True, alpha=0.45, label=\"Sick\",     edgecolor=\"none\")\n",
    "\n",
    "    # しきい値（読込のみ）\n",
    "    if tau is not None and np.isfinite(tau):\n",
    "        ax.axvline(float(tau), linestyle=\"--\", linewidth=1.5)\n",
    "        ax.text(float(tau), ax.get_ylim()[1]*0.95, f\"τ={float(tau):.3f}\",\n",
    "                ha=\"center\", va=\"top\", fontsize=20, bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"none\", alpha=0.7))\n",
    "\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(\"Predicted probability P(Sick)\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(title, pad=10)\n",
    "    ax.legend(loc=\"upper left\", frameon=True)\n",
    "\n",
    "# ===== メイン処理 =====\n",
    "source_kind, pred_csv, thr_csv = _pick_source()\n",
    "pred_df, y_pool, s_pool, g_pool = _load_predictions(pred_csv)\n",
    "taus = _load_thresholds(source_kind, thr_csv)\n",
    "\n",
    "# High/Low 抽出\n",
    "maskH = (g_pool == \"High\")\n",
    "maskL = (g_pool == \"Low\")\n",
    "if not (maskH.any() and maskL.any()):\n",
    "    raise RuntimeError(\"[Cell9] High/Low いずれかが存在しないため属性別パネルが作図不可\")\n",
    "\n",
    "# y=0/1 の確率配列\n",
    "s_all_0, s_all_1 = s_pool[y_pool==0], s_pool[y_pool==1]\n",
    "s_high_0, s_high_1 = s_pool[(y_pool==0) & maskH], s_pool[(y_pool==1) & maskH]\n",
    "s_low_0,  s_low_1  = s_pool[(y_pool==0) & maskL], s_pool[(y_pool==1) & maskL]\n",
    "\n",
    "# スタイル（ユーザ規約）\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"axes.titlesize\": 30,\n",
    "    \"axes.labelsize\": 24,\n",
    "    \"legend.fontsize\": 20,\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "})\n",
    "\n",
    "# タイトル補助（AUC 等）\n",
    "info = _load_metrics_title()\n",
    "title_tail = \"\"\n",
    "if info is not None:\n",
    "    if info.get(\"n\") is not None:\n",
    "        pos = int((y_pool==1).sum()); neg = int((y_pool==0).sum())\n",
    "        title_tail = f\" (metric={METRIC_NAME}, AUC={info['auc']:.3f}, n={info['n']}, pos={pos}, neg={neg})\"\n",
    "    else:\n",
    "        title_tail = f\" (metric={METRIC_NAME}, AUC={info['auc']:.3f})\"\n",
    "else:\n",
    "    title_tail = f\" (metric={METRIC_NAME})\"\n",
    "\n",
    "# 作図\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 9), constrained_layout=True)\n",
    "\n",
    "_draw_density(axes[0], s_all_0,  s_all_1,  taus[\"tau_single\"], title=\"Single (attribute-agnostic)\", bins=CELL9_BINS)\n",
    "_draw_density(axes[1], s_high_0, s_high_1, taus[\"tauH_WG\"],    title=\"High group (WG-optimized τ_H)\", bins=CELL9_BINS)\n",
    "_draw_density(axes[2], s_low_0,  s_low_1,  taus[\"tauL_WG\"],    title=\"Low group (WG-optimized τ_L)\", bins=CELL9_BINS)\n",
    "\n",
    "supt = f\"Probability distributions of P(Sick){title_tail}\\nsource={source_kind}, thresholds={taus.get('note','')}\"\n",
    "fig.suptitle(supt, fontsize=32, y=1.08)\n",
    "fig.set_constrained_layout_pads(w_pad=0.02, h_pad=0.02, wspace=0.28, hspace=0.02)\n",
    "\n",
    "out_png = outpath(\"PROB_DENS_SINGLE_vs_GROUP.png\")\n",
    "plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"[Cell9] source={source_kind}, metric={METRIC_NAME}, bins={CELL9_BINS}\")\n",
    "print(f\"[Cell9] thresholds loaded: tau_single={taus['tau_single']:.3f}, tau_H={taus['tauH_WG']:.3f}, tau_L={taus['tauL_WG']:.3f} (from {os.path.basename(thr_csv)})\")\n",
    "print(f\"[Cell9] saved -> {out_png}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] s-list=[8, 10, 12, 14, 16], k-list=[2, 4, 5, 6, 8, 10, 12, 16, 20, 24, 32], repeats=10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_tr)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_te)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfit_xgb_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# predict_proba を前提（fit_xgb_classifier がXGB想定）\u001b[39;00m\n\u001b[0;32m    112\u001b[0m proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_te)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[27], line 26\u001b[0m, in \u001b[0;36mfit_xgb_classifier\u001b[1;34m(X_train, y_train)\u001b[0m\n\u001b[0;32m     15\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32)\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBClassifier(\n\u001b[0;32m     17\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     18\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1682\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1660\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[0;32m   1661\u001b[0m     xgb_model, params, feature_weights\n\u001b[0;32m   1662\u001b[0m )\n\u001b[0;32m   1663\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1664\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1665\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1680\u001b[0m )\n\u001b[1;32m-> 1682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:192\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     evals_result\u001b[38;5;241m.\u001b[39mupdate(cb_container\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\taiki\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2076\u001b[0m, in \u001b[0;36mBooster.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBooster\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2071\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reset the booster object to release data caches used for training.\u001b[39;00m\n\u001b[0;32m   2072\u001b[0m \n\u001b[0;32m   2073\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 3.0.0\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \n\u001b[0;32m   2075\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2076\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterReset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2077\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===== Cell 10: AUC–k 曲線（被験者数 s ごとの複数本；SHAPランキング利用） =====\n",
    "\"\"\"\n",
    "機能：\n",
    "- Cell 4で保存済みの SHAP_FEATURE_RANKING*.CSV から「特徴量重要度順」を取得し，\n",
    "  Cell 6 の k→AUC（pooled, LOSO）ロジックを拡張して，\n",
    "  学習に使う被験者数 s を変えた複数本の k–AUC 曲線を描画する．\n",
    "- 各 s について：被験者 s 名を無作為抽出 → その s 名の中で LeaveOneGroupOut を回し，\n",
    "  k 本の上位特徴を使って pooled AUC を算出．これを複数リピートして平均と95%分位を描く．\n",
    "\n",
    "前提：\n",
    "- X_scaled_all, y_all, groups, fit_xgb_classifier(), outpath() が定義済みであること\n",
    "- Cell 4 が生成した SHAP_FEATURE_RANKING.CSV（または _LABELED.CSV）が OUT_DIR にあること\n",
    "\n",
    "出力：\n",
    "- CSV: AUC_K_vs_SUBJECTS.csv（列: s, k, repeat, auc）\n",
    "- 図 : AUC_K_vs_SUBJECTS.png（横軸=k，縦軸=AUC，曲線=各 s の平均，帯=2.5–97.5%）\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ---------- ランキング読込（Cell 4の成果物） ----------\n",
    "rank_candidates = [outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"),\n",
    "                   outpath(\"SHAP_FEATURE_RANKING.CSV\")]\n",
    "rank_path = None\n",
    "for p in rank_candidates:\n",
    "    if os.path.exists(p):\n",
    "        rank_path = p; break\n",
    "if rank_path is None:\n",
    "    raise FileNotFoundError(\"[ERROR] SHAP_FEATURE_RANKING(_LABELED).CSV が OUT_DIR に存在しない．Cell 4 を先に実行すること．\")\n",
    "\n",
    "rank_df = pd.read_csv(rank_path, encoding=\"utf-8-sig\", index_col=0)\n",
    "rank_col = \"mean_abs\" if \"mean_abs\" in rank_df.columns else (\"mean_abs_shap\" if \"mean_abs_shap\" in rank_df.columns else None)\n",
    "if rank_col is None:\n",
    "    raise KeyError(\"[ERROR] ランキングCSVに mean_abs / mean_abs_shap 列が無い．\")\n",
    "rank_df = rank_df.sort_values(rank_col, ascending=False)\n",
    "\n",
    "# Xの列に存在する順序リスト\n",
    "feature_order = [f for f in rank_df.index if f in X_scaled_all.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[ERROR] ランキングに載っている特徴が X_scaled_all に見つからない．\")\n",
    "\n",
    "# ---------- パラメータ ----------\n",
    "SUBJ_SIZES = None   # 例: [6, 8, 10, 12, 14]。Noneなら自動（{4,6,8,...,全体-1}）\n",
    "K_LIST     = None   # 例: [2,4,6,8,10,12,16,20,24]。Noneなら自動（BEST_Kを含めつつ列数以内）\n",
    "REPEATS    = 10     # 各(s,k)の反復回数\n",
    "SEED_BASE  = 31415  # 乱数種\n",
    "SAVE_PREFIX = \"AUC_K_vs_SUBJECTS\"\n",
    "\n",
    "# ---------- 自動セットアップ ----------\n",
    "subj_ids_all = groups.astype(str).unique()\n",
    "n_subjects_total = len(subj_ids_all)\n",
    "\n",
    "if SUBJ_SIZES is None:\n",
    "    # {8,10,12,..., 全体-1}（4,6は除外）\n",
    "    SUBJ_SIZES = [s for s in range(8, max(9, n_subjects_total), 2) if s < n_subjects_total]\n",
    "    if not SUBJ_SIZES:\n",
    "        SUBJ_SIZES = [max(8, n_subjects_total-1)]  # どうしても小さい場合のフォールバック\n",
    "\n",
    "maxK = len(feature_order)\n",
    "if K_LIST is None:\n",
    "    _bk = globals().get(\"BEST_K\", None)\n",
    "    base = [2,4,6,8,10,12,16,20,24,32,48,64]\n",
    "    if isinstance(_bk, (int, np.integer)):\n",
    "        base.append(int(_bk))\n",
    "    # 列数以内に制限し，重複を削除して昇順に\n",
    "    K_LIST = sorted({k for k in base if 1 <= k <= maxK})\n",
    "    if not K_LIST:\n",
    "        K_LIST = list(range(2, min(1+maxK, 20), 2))\n",
    "\n",
    "print(f\"[INFO] s-list={SUBJ_SIZES}, k-list={K_LIST}, repeats={REPEATS}\")\n",
    "\n",
    "# ---------- 主処理 ----------\n",
    "logo = LeaveOneGroupOut()\n",
    "rng  = np.random.default_rng(SEED_BASE)\n",
    "records = []\n",
    "\n",
    "X_all = X_scaled_all.astype(np.float32).copy()\n",
    "y_all = pd.Series(np.asarray(y_all)).reset_index(drop=True)\n",
    "g_all = pd.Series(groups.astype(str).values).reset_index(drop=True)\n",
    "\n",
    "for s in SUBJ_SIZES:\n",
    "    for r in range(REPEATS):\n",
    "        # 学習対象の被験者 s 名を無作為抽出\n",
    "        chosen = rng.choice(subj_ids_all, size=s, replace=False)\n",
    "        mask   = g_all.isin(chosen)\n",
    "        X_sub  = X_all.loc[mask, feature_order]\n",
    "        y_sub  = y_all.loc[mask].values\n",
    "        g_sub  = g_all.loc[mask].values\n",
    "\n",
    "        # kごとに pooled AUC（LOSO over chosen）を計算\n",
    "        for k in K_LIST:\n",
    "            feats = feature_order[:k]\n",
    "            Xk = X_sub[feats]\n",
    "\n",
    "            try:\n",
    "                y_true_all, proba_all = [], []\n",
    "                for tr_idx, te_idx in logo.split(Xk, y_sub, g_sub):\n",
    "                    X_tr, X_te = Xk.iloc[tr_idx], Xk.iloc[te_idx]\n",
    "                    y_tr, y_te = y_sub[tr_idx], y_sub[te_idx]\n",
    "\n",
    "                    # 片クラスfoldはスキップ（そのままではAUC不可）\n",
    "                    if len(np.unique(y_tr)) < 2 or len(np.unique(y_te)) < 2:\n",
    "                        continue\n",
    "\n",
    "                    model = fit_xgb_classifier(X_tr, pd.Series(y_tr))\n",
    "                    # predict_proba を前提（fit_xgb_classifier がXGB想定）\n",
    "                    proba = model.predict_proba(X_te)[:, 1]\n",
    "                    y_true_all.append(y_te)\n",
    "                    proba_all.append(proba)\n",
    "\n",
    "                if len(y_true_all) == 0:\n",
    "                    auc_val = np.nan\n",
    "                else:\n",
    "                    y_true_k  = np.concatenate(y_true_all)\n",
    "                    proba_k   = np.concatenate(proba_all)\n",
    "                    if len(np.unique(y_true_k)) < 2:\n",
    "                        auc_val = np.nan\n",
    "                    else:\n",
    "                        auc_val = float(roc_auc_score(y_true_k, proba_k))\n",
    "\n",
    "                records.append({\"s\": int(s), \"k\": int(k), \"repeat\": int(r), \"auc\": auc_val})\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP] s={s}, r={r}, k={k} (reason: {e})\")\n",
    "                continue\n",
    "\n",
    "# ---------- 保存 ----------\n",
    "df_rec = pd.DataFrame(records)\n",
    "csv_path = outpath(f\"{SAVE_PREFIX}.csv\")\n",
    "df_rec.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] CSV -> {csv_path}\")\n",
    "\n",
    "# ---------- 集計＆描画 ----------\n",
    "# ---------- 集計＆描画（kで着色／極端kは大きめ、x軸反転、s=4/6は非表示） ----------\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "# ---------- 集計＆描画（明るい配色／sごと色固定／まとめ図＋個別図） ----------\n",
    "agg = (df_rec.groupby([\"s\",\"k\"])[\"auc\"]\n",
    "       .agg(mean=\"mean\",\n",
    "            p2p5=lambda x: np.nanquantile(x, 0.025) if np.isfinite(x).any() else np.nan,\n",
    "            p97p5=lambda x: np.nanquantile(x, 0.975) if np.isfinite(x).any() else np.nan,\n",
    "            n=\"count\")\n",
    "       .reset_index())\n",
    "\n",
    "# s=4,6 を除外\n",
    "s_list = sorted([s for s in agg[\"s\"].unique() if s not in (4, 6)])\n",
    "\n",
    "# 明るいカラーパレット（Set2→Set3→tab10 を順に使う）\n",
    "palettes = [plt.cm.get_cmap(\"Set2\").colors,\n",
    "            plt.cm.get_cmap(\"Set3\").colors,\n",
    "            plt.cm.get_cmap(\"tab10\").colors]\n",
    "color_pool = [c for pal in palettes for c in pal]\n",
    "color_map_s = {s: color_pool[i % len(color_pool)] for i, s in enumerate(s_list)}\n",
    "\n",
    "# ===== まとめ図 =====\n",
    "plt.figure(figsize=(10, 7))\n",
    "for s in s_list:\n",
    "    d  = agg[agg[\"s\"]==s].sort_values(\"k\")\n",
    "    xs = d[\"k\"].values\n",
    "    ys = d[\"mean\"].values\n",
    "    lo = d[\"p2p5\"].values\n",
    "    hi = d[\"p97p5\"].values\n",
    "    c  = color_map_s[s]\n",
    "\n",
    "    # 95%帯（同色で明るめ）\n",
    "    if np.isfinite(lo).all() and np.isfinite(hi).all():\n",
    "        plt.fill_between(xs, lo, hi, color=c, alpha=0.22, linewidth=0, zorder=1)\n",
    "    # 線＆マーカー\n",
    "    plt.plot(xs, ys, marker=\"o\", linewidth=1.5, color=c, label=f\"s={s}\", zorder=3)\n",
    "\n",
    "plt.title(\"AUC vs Number of Features (k) by Training Subjects\", fontsize=30)\n",
    "plt.xlabel(\"Number of features (k)\", fontsize=24)\n",
    "plt.ylabel(\"ROC AUC (pooled, LOSO within chosen s)\", fontsize=24)\n",
    "\n",
    "# 目盛は降順（左=大きいk, 右=小さいk）\n",
    "xticks = sorted(K_LIST, reverse=True)\n",
    "plt.xticks(xticks, fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.grid(True, linestyle=\"--\", linewidth=1.0, alpha=0.4)\n",
    "plt.legend(title=\"Training subjects\", fontsize=16, title_fontsize=16, ncol=2)\n",
    "\n",
    "plt.gca().invert_xaxis()\n",
    "fig_path = outpath(f\"{SAVE_PREFIX}.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_path, dpi=300)\n",
    "plt.close()\n",
    "print(f\"[OK] FIG (combined) -> {fig_path}\")\n",
    "\n",
    "# ===== sごとの個別図 =====\n",
    "for s in s_list:\n",
    "    d  = agg[agg[\"s\"]==s].sort_values(\"k\")\n",
    "    xs = d[\"k\"].values\n",
    "    ys = d[\"mean\"].values\n",
    "    lo = d[\"p2p5\"].values\n",
    "    hi = d[\"p97p5\"].values\n",
    "    c  = color_map_s[s]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if np.isfinite(lo).all() and np.isfinite(hi).all():\n",
    "        plt.fill_between(xs, lo, hi, color=c, alpha=0.22, linewidth=0, zorder=1)\n",
    "    plt.plot(xs, ys, marker=\"o\", linewidth=1.5, color=c, label=f\"s={s}\", zorder=3)\n",
    "\n",
    "    plt.title(f\"AUC vs Number of Features (k) — s={s}\", fontsize=30)\n",
    "    plt.xlabel(\"Number of features (k)\", fontsize=24)\n",
    "    plt.ylabel(\"ROC AUC (pooled, LOSO within s)\", fontsize=24)\n",
    "    plt.xticks(sorted(K_LIST, reverse=True), fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.ylim(0.5, 1.0)\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=1.0, alpha=0.4)\n",
    "    plt.gca().invert_xaxis()\n",
    "\n",
    "    fig_s_path = outpath(f\"{SAVE_PREFIX}_s{s}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_s_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[OK] FIG (per-s) -> {fig_s_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1767d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] OOF saved -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\OOF_PRED_BESTK.CSV\n",
      "[INFO] OOF AUC = 0.747  (n=340)\n",
      "[BOOT] start: B=2000, mode=oof, BEST_K=5, SEED=20251101\n",
      "[BOOT] b= 200  auc=0.697\n",
      "[BOOT] b= 400  auc=0.758\n",
      "[BOOT] b= 600  auc=0.751\n",
      "[BOOT] b= 800  auc=0.677\n",
      "[BOOT] b=1000  auc=0.771\n",
      "[BOOT] b=1200  auc=0.718\n",
      "[BOOT] b=1400  auc=0.742\n",
      "[BOOT] b=1600  auc=0.731\n",
      "[BOOT] b=1800  auc=0.659\n",
      "[BOOT] b=2000  auc=0.817\n",
      "[OK] CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUC_BOOTSTRAP_SUBJECT.csv\n",
      "[OK] CSV -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUC_BOOTSTRAP_SUMMARY.csv\n",
      "[INFO] 95% CI: [0.637, 0.836]  mean=0.746  se=0.0011  (n_boot=2000, skipped=0)\n",
      "[OK] FIG -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUC_BOOTSTRAP_HIST.png\n",
      "[OK] FIG -> C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(MSSQ込み)\\閾値FMS1\\AUC_BOOTSTRAP_ECDF.png\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell X: 被験者ブートストラップCI（AUC, OOFベース既定） =====\n",
    "\"\"\"\n",
    "目的：\n",
    "- LOSO OOF予測を固定し，被験者（cluster）単位のブートストラップで AUC の95%CIを推定する．\n",
    "- 既に OOF が無ければ，上位 BEST_K 特徴（Cell4のSHAPランキング）で一度だけLOSOしてOOFを作成してから実行．\n",
    "\n",
    "出力：\n",
    "- OOF_PRED_BESTK.CSV（無ければ作成）\n",
    "- AUC_BOOTSTRAP_SUBJECT.csv（各反復のAUC）\n",
    "- AUC_BOOTSTRAP_SUMMARY.csv（平均/SE/95%CI/有効反復数 等）\n",
    "- AUC_BOOTSTRAP_HIST.png（分布＋CI） / AUC_BOOTSTRAP_ECDF.png（累積分布）\n",
    "\n",
    "主要パラメータ（下の CONFIG を調整）：\n",
    "- B = 2000（反復回数）\n",
    "- SEED = 20251101（乱数）\n",
    "- MAX_REDRAW = 20（単一クラス回避の再抽選上限）\n",
    "- MODE = \"oof\" または \"retrain\"（既定は oof）\n",
    "- STRATIFY_BY = None または \"MSSQ_group\"（被験者層別の比率維持；既定 None）\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# -------- CONFIG --------\n",
    "B = 2000\n",
    "SEED = 20251101\n",
    "MAX_REDRAW = 20\n",
    "MODE = \"oof\"            # \"oof\" or \"retrain\"\n",
    "STRATIFY_BY = None      # None or \"MSSQ_group\"\n",
    "OOF_CSV = \"OOF_PRED_BESTK.CSV\"\n",
    "BOOT_CSV = \"AUC_BOOTSTRAP_SUBJECT.csv\"\n",
    "SUMM_CSV = \"AUC_BOOTSTRAP_SUMMARY.csv\"\n",
    "HIST_PNG = \"AUC_BOOTSTRAP_HIST.png\"\n",
    "ECDF_PNG = \"AUC_BOOTSTRAP_ECDF.png\"\n",
    "\n",
    "# ------------- 前提確認 -------------\n",
    "assert 'X_scaled_all' in globals(), \"[ERROR] X_scaled_all が未定義\"\n",
    "assert 'y_all'        in globals(), \"[ERROR] y_all が未定義\"\n",
    "assert 'groups'       in globals(), \"[ERROR] groups が未定義\"\n",
    "assert 'outpath'      in globals(), \"[ERROR] outpath() が未定義\"\n",
    "\n",
    "# ------------- ユーティリティ -------------\n",
    "def _load_feature_order():\n",
    "    \"\"\"Cell4のランキングCSVから重要度降順の特徴順を取得\"\"\"\n",
    "    rank_candidates = [outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"),\n",
    "                       outpath(\"SHAP_FEATURE_RANKING.CSV\")]\n",
    "    rank_path = None\n",
    "    for p in rank_candidates:\n",
    "        if os.path.exists(p):\n",
    "            rank_path = p; break\n",
    "    if rank_path is None:\n",
    "        raise FileNotFoundError(\"[ERROR] SHAP_FEATURE_RANKING*.CSV が見つかりません（Cell 4 実行を確認）\")\n",
    "\n",
    "    df = pd.read_csv(rank_path, encoding=\"utf-8-sig\", index_col=0)\n",
    "    rcol = \"mean_abs\" if \"mean_abs\" in df.columns else (\"mean_abs_shap\" if \"mean_abs_shap\" in df.columns else None)\n",
    "    if rcol is None:\n",
    "        raise KeyError(\"[ERROR] ランキングCSVに mean_abs / mean_abs_shap が無い\")\n",
    "    order = [f for f in df.sort_values(rcol, ascending=False).index if f in X_scaled_all.columns]\n",
    "    if not order:\n",
    "        raise RuntimeError(\"[ERROR] ランキングの特徴が X_scaled_all に存在しません\")\n",
    "    return order\n",
    "\n",
    "def _predict_proba_safe(model, X):\n",
    "    \"\"\"predict_proba が無い学習器への保険\"\"\"\n",
    "    try:\n",
    "        return model.predict_proba(X.astype(np.float32))[:, 1]\n",
    "    except Exception:\n",
    "        p = model.decision_function(X)\n",
    "        p = (p - p.min()) / (p.max() - p.min() + 1e-12)\n",
    "        return p\n",
    "\n",
    "def build_oof_bestk(feature_order, best_k):\n",
    "    \"\"\"上位 BEST_K 特徴で LOSO OOF を作成（1回だけ学習）\"\"\"\n",
    "    feats = feature_order[:int(best_k)]\n",
    "    X = X_scaled_all[feats].astype(np.float32)\n",
    "    y = pd.Series(np.asarray(y_all)).reset_index(drop=True)\n",
    "    g = pd.Series(groups.astype(str).values).reset_index(drop=True)\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    rows = []\n",
    "    for tr_idx, te_idx in logo.split(X, y, g):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y.iloc[tr_idx], y.iloc[te_idx]\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(\"[ERROR] 学習foldが単一クラス（OOF作成中）\")\n",
    "        model = fit_xgb_classifier(X_tr, y_tr)\n",
    "        proba = _predict_proba_safe(model, X_te)\n",
    "        sub_ids = g.iloc[te_idx].astype(str).values  # 同一subjectが並ぶ\n",
    "        df_fold = pd.DataFrame({\"subject\": sub_ids, \"y_true\": y_te.values, \"y_score\": proba})\n",
    "        rows.append(df_fold)\n",
    "\n",
    "    oof = pd.concat(rows, ignore_index=True)\n",
    "    oof.to_csv(outpath(OOF_CSV), index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[OK] OOF saved -> {outpath(OOF_CSV)}\")\n",
    "    return oof\n",
    "\n",
    "def _attach_strata_if_needed(oof_df):\n",
    "    \"\"\"STRATIFY_BY='MSSQ_group' の場合，被験者メタから層ラベルを付与\"\"\"\n",
    "    if STRATIFY_BY is None:\n",
    "        oof_df[\"strata\"] = \"ALL\"\n",
    "        return oof_df\n",
    "    if STRATIFY_BY == \"MSSQ_group\" and 'SUBJECT_META' in globals():\n",
    "        meta = SUBJECT_META.copy()\n",
    "        # 代表名の推定\n",
    "        id_col  = next((c for c in meta.columns if c.lower() in (\"subject\",\"subject_id\",\"id\",\"sid\")), None)\n",
    "        grp_col = next((c for c in meta.columns if c.lower() in (\"mssq_group\",\"mssqgroup\",\"group\",\"mssq_highlow\")), None)\n",
    "        if (id_col is not None) and (grp_col is not None):\n",
    "            d = dict(zip(meta[id_col].astype(str), meta[grp_col].astype(str)))\n",
    "            oof_df[\"strata\"] = oof_df[\"subject\"].astype(str).map(d).fillna(\"ALL\")\n",
    "            return oof_df\n",
    "    # フォールバック\n",
    "    oof_df[\"strata\"] = \"ALL\"\n",
    "    return oof_df\n",
    "\n",
    "def bootstrap_auc_subject(oof_df, B=2000, seed=20251101, max_redraw=20):\n",
    "    \"\"\"被験者（cluster）ブートストラップでAUC分布を推定（OOF固定）\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    subjects = oof_df[\"subject\"].astype(str).unique()\n",
    "    n_subj = len(subjects)\n",
    "\n",
    "    # strataごとに被験者集合を準備\n",
    "    strata_by_subj = (oof_df[[\"subject\",\"strata\"]].drop_duplicates()\n",
    "                      .set_index(\"subject\")[\"strata\"].to_dict())\n",
    "    strata_levels = sorted(oof_df[\"strata\"].unique())\n",
    "    subj_by_strata = {s: [sub for sub in subjects if strata_by_subj.get(sub,\"ALL\")==s] for s in strata_levels}\n",
    "\n",
    "    rec, skipped = [], 0\n",
    "    print(f\"[BOOT] start: B={B}, mode=oof, BEST_K={globals().get('BEST_K','?')}, SEED={seed}\")\n",
    "\n",
    "    for b in range(B):\n",
    "        redraw = 0\n",
    "        while True:\n",
    "            # 層別（必要なら各層で元の被験者数と同数を復元）\n",
    "            chosen = []\n",
    "            for st in strata_levels:\n",
    "                pool = subj_by_strata[st]\n",
    "                if len(pool) == 0:\n",
    "                    continue\n",
    "                chosen.extend(list(rng.choice(pool, size=len(pool), replace=True)))\n",
    "            # 連結（重複subjectは複数回分を結合）\n",
    "            parts = [oof_df[oof_df[\"subject\"]==sid] for sid in chosen]\n",
    "            boot = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "            yb = boot[\"y_true\"].values\n",
    "            if np.unique(yb).size >= 2:\n",
    "                break\n",
    "            redraw += 1\n",
    "            if redraw > max_redraw:\n",
    "                skipped += 1\n",
    "                boot = None\n",
    "                break\n",
    "        if boot is None:\n",
    "            continue\n",
    "\n",
    "        auc_b = float(roc_auc_score(boot[\"y_true\"].values, boot[\"y_score\"].values))\n",
    "        rec.append(dict(\n",
    "            b_id=int(b), auc=auc_b, n_subjects=int(n_subj),\n",
    "            n_pos=int((boot[\"y_true\"].values==1).sum()),\n",
    "            n_neg=int((boot[\"y_true\"].values==0).sum()),\n",
    "            seed=int(seed)\n",
    "        ))\n",
    "        if (b+1) % 200 == 0:\n",
    "            print(f\"[BOOT] b={b+1:4d}  auc={auc_b:.3f}\")\n",
    "\n",
    "    df_boot = pd.DataFrame(rec)\n",
    "    return df_boot, skipped\n",
    "\n",
    "def summarize_bootstrap(df_boot, auc_oof):\n",
    "    \"\"\"平均・SE・95%percentile CI を計算\"\"\"\n",
    "    vals = df_boot[\"auc\"].dropna().values\n",
    "    mean = float(np.nanmean(vals)) if len(vals) else np.nan\n",
    "    se   = float(np.nanstd(vals, ddof=1)/np.sqrt(max(1,len(vals)))) if len(vals)>1 else np.nan\n",
    "    p2p5 = float(np.nanquantile(vals, 0.025)) if len(vals) else np.nan\n",
    "    p97p5= float(np.nanquantile(vals, 0.975)) if len(vals) else np.nan\n",
    "    return dict(auc_oof=float(auc_oof), mean=mean, se=se, p2p5=p2p5, p97p5=p97p5,\n",
    "                n_boot=int(len(vals)))\n",
    "\n",
    "def _set_plot_style():\n",
    "    plt.rcParams.update({\n",
    "        \"font.size\": 20, \"axes.titlesize\": 30, \"axes.labelsize\": 24,\n",
    "        \"xtick.labelsize\": 20, \"ytick.labelsize\": 20, \"legend.fontsize\": 20\n",
    "    })\n",
    "\n",
    "def plot_bootstrap_hist(df_boot, auc_oof, ci_low, ci_high, note, png_name):\n",
    "    _set_plot_style()\n",
    "    plt.figure(figsize=(9,6))\n",
    "    vals = df_boot[\"auc\"].dropna().values\n",
    "    plt.hist(vals, bins=30, alpha=0.8)\n",
    "    # 目標線\n",
    "    ax = plt.gca()\n",
    "    ax.axvline(auc_oof, color=\"red\", linewidth=1.5, label=f\"OOF AUC = {auc_oof:.3f}\")\n",
    "    ax.axvline(ci_low, color=\"black\", linewidth=1.5, linestyle=\"--\", label=f\"95% CI [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "    ax.axvline(ci_high, color=\"black\", linewidth=1.5, linestyle=\"--\")\n",
    "    plt.title(\"Subject Bootstrap of AUC (Histogram)\")\n",
    "    plt.xlabel(\"AUC\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.text(0.98, 0.02, note, ha=\"right\", va=\"bottom\", transform=ax.transAxes, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(png_name), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[OK] FIG -> {outpath(png_name)}\")\n",
    "\n",
    "def plot_bootstrap_ecdf(df_boot, auc_oof, ci_low, ci_high, note, png_name):\n",
    "    _set_plot_style()\n",
    "    plt.figure(figsize=(9,6))\n",
    "    vals = np.sort(df_boot[\"auc\"].dropna().values)\n",
    "    y = np.arange(1, len(vals)+1) / max(1, len(vals))\n",
    "    plt.plot(vals, y, linewidth=1.5)\n",
    "    ax = plt.gca()\n",
    "    ax.axvline(auc_oof, color=\"red\", linewidth=1.5, label=f\"OOF AUC = {auc_oof:.3f}\")\n",
    "    ax.axvline(ci_low, color=\"black\", linewidth=1.5, linestyle=\"--\", label=f\"95% CI [{ci_low:.3f}, {ci_high:.3f}]\")\n",
    "    ax.axvline(ci_high, color=\"black\", linewidth=1.5, linestyle=\"--\")\n",
    "    plt.title(\"Subject Bootstrap of AUC (ECDF)\")\n",
    "    plt.xlabel(\"AUC\")\n",
    "    plt.ylabel(\"Cumulative probability\")\n",
    "    plt.legend()\n",
    "    plt.text(0.98, 0.02, note, ha=\"right\", va=\"bottom\", transform=ax.transAxes, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(png_name), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[OK] FIG -> {outpath(png_name)}\")\n",
    "\n",
    "# ------------- 実行 -------------\n",
    "# 1) OOFの用意（無ければ作成）\n",
    "oof_path = outpath(OOF_CSV)\n",
    "if os.path.exists(oof_path):\n",
    "    oof = pd.read_csv(oof_path, encoding=\"utf-8-sig\")\n",
    "    print(f\"[INFO] load OOF -> {oof_path}\")\n",
    "else:\n",
    "    assert 'BEST_K' in globals(), \"[ERROR] BEST_K が未定義（Cell 6 実行で決定してください）\"\n",
    "    feat_order = _load_feature_order()\n",
    "    oof = build_oof_bestk(feat_order, BEST_K)\n",
    "\n",
    "# OOF基準のAUC\n",
    "auc_oof = float(roc_auc_score(oof[\"y_true\"].values, oof[\"y_score\"].values))\n",
    "print(f\"[INFO] OOF AUC = {auc_oof:.3f}  (n={len(oof)})\")\n",
    "\n",
    "# 層ラベル付与（必要時）\n",
    "oof = _attach_strata_if_needed(oof)\n",
    "\n",
    "# 2) ブートストラップ（MODE=\"oof\" の場合）\n",
    "if MODE == \"oof\":\n",
    "    df_boot, skipped = bootstrap_auc_subject(oof, B=B, seed=SEED, max_redraw=MAX_REDRAW)\n",
    "\n",
    "# 3) まとめ・保存\n",
    "df_boot.to_csv(outpath(BOOT_CSV), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] CSV -> {outpath(BOOT_CSV)}\")\n",
    "\n",
    "summ = summarize_bootstrap(df_boot, auc_oof)\n",
    "summ_df = pd.DataFrame([{\n",
    "    **summ,\n",
    "    \"skipped\": int(skipped),\n",
    "    \"B\": int(B),\n",
    "    \"BEST_K\": int(globals().get(\"BEST_K\", -1)),\n",
    "    \"MODE\": MODE,\n",
    "    \"SEED\": int(SEED)\n",
    "}])\n",
    "summ_df.to_csv(outpath(SUMM_CSV), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] CSV -> {outpath(SUMM_CSV)}\")\n",
    "print(f\"[INFO] 95% CI: [{summ['p2p5']:.3f}, {summ['p97p5']:.3f}]  mean={summ['mean']:.3f}  se={summ['se']:.4f}  (n_boot={summ['n_boot']}, skipped={skipped})\")\n",
    "\n",
    "# 4) 図\n",
    "note = f\"B={B}, BEST_K={globals().get('BEST_K','?')}, MODE={MODE}, SEED={SEED}\"\n",
    "plot_bootstrap_hist(df_boot, auc_oof, summ[\"p2p5\"], summ[\"p97p5\"], note, HIST_PNG)\n",
    "plot_bootstrap_ecdf(df_boot, auc_oof, summ[\"p2p5\"], summ[\"p97p5\"], note, ECDF_PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8244f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== NEW CELL (REPLACE): Probability distributions — pooled only + 3 kinds of τ =====\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ============ 調整パラメータ（ここだけ触ればOK） ============\n",
    "FIGSIZE_FULL = (18, 4.4)      # 全域 [0,1]\n",
    "FIGSIZE_ZOOM = (18, 4.4)      # 拡大 [0, XMAX_ZOOM]\n",
    "XMAX_ZOOM    = 0.20\n",
    "DPI_FULL     = 300\n",
    "DPI_ZOOM     = 300\n",
    "\n",
    "FONTS = dict(                   # 文字サイズと線幅\n",
    "    suptitle=18, title=14, label=12, tick=10, legend=11, box=11,\n",
    "    kde_lw=1.6, hist_edge=0.2, hist_alpha=0.55, vline_lw=1.8\n",
    ")\n",
    "\n",
    "# KDE サンプル数（ピクセルではなく曲線の“滑らかさ”）\n",
    "XGRID_N_FULL = 800\n",
    "XGRID_N_ZOOM = 4000\n",
    "# ===========================================================\n",
    "\n",
    "# ---- 予測明細（Cell 8 の出力） ----\n",
    "pred = pd.read_csv(outpath(\"GROUP_AWARE_PREDICTIONS.CSV\"), encoding=\"utf-8-sig\")\n",
    "pred[\"group\"] = pred[\"group\"].astype(str)\n",
    "y = pred[\"y_true\"].astype(int).to_numpy()\n",
    "s = pred[\"proba\"].to_numpy()\n",
    "\n",
    "# グループ（しきい値最適化用にのみ使う／分布は使わない）\n",
    "H = pred.query(\"group=='High'\")\n",
    "L = pred.query(\"group=='Low'\")\n",
    "high_s = H[\"proba\"].to_numpy(); high_y = H[\"y_true\"].astype(int).to_numpy()\n",
    "low_s  = L[\"proba\"].to_numpy(); low_y  = L[\"y_true\"].astype(int).to_numpy()\n",
    "\n",
    "# 分布（プールのみ）\n",
    "pooled_neg, pooled_pos = s[y==0], s[y==1]\n",
    "\n",
    "# ---- 共通ビン（Freedman–Diaconis） ----\n",
    "def _fd_bins(a, min_bins=40, max_bins=160):\n",
    "    a = np.asarray(a); a = a[np.isfinite(a)]\n",
    "    if a.size < 5: return min_bins\n",
    "    q75, q25 = np.percentile(a, [75, 25]); iqr = max(q75-q25, 1e-6)\n",
    "    h = 2*iqr*(a.size**(-1/3)); \n",
    "    if h <= 0: return min_bins\n",
    "    n = int(np.clip((a.max()-a.min())/h, min_bins, max_bins))\n",
    "    return max(n, min_bins)\n",
    "\n",
    "NBINS = _fd_bins(np.r_[pooled_neg, pooled_pos])\n",
    "BINS  = np.linspace(0.0, 1.0, NBINS+1)\n",
    "\n",
    "# ---- BA と τ 探索（図用） ----\n",
    "def _ba_from_preds(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    tpr = TP/(TP+FN) if (TP+FN)>0 else 0.0\n",
    "    tnr = TN/(TN+FP) if (TN+FP)>0 else 0.0\n",
    "    return 0.5*(tpr+tnr), tpr, tnr\n",
    "\n",
    "def _best_tau_single(scores, labels, step=0.001):\n",
    "    taus = np.arange(0.0, 1.0+1e-12, step)\n",
    "    best = (-1.0, 0.5, (0.0,0.0))\n",
    "    for t in taus:\n",
    "        ba, tpr, tnr = _ba_from_preds(labels, (scores>=t).astype(int))\n",
    "        if ba > best[0]: best = (ba, float(t), (tpr, tnr))\n",
    "    return best  # (BA, tau, (TPR,TNR))\n",
    "\n",
    "def _best_tau_group(hs, hy, ls, ly, coarse=0.01, fine=0.001, margin=0.03, mode=\"BA\"):\n",
    "    def eval_pair(tH, tL):\n",
    "        baH,_,_ = _ba_from_preds(hy, (hs>=tH).astype(int))\n",
    "        baL,_,_ = _ba_from_preds(ly, (ls>=tL).astype(int))\n",
    "        BA = (baH+baL)/2.0\n",
    "        WG = min(baH, baL)\n",
    "        return BA, WG, baH, baL\n",
    "    taus = np.arange(0.0, 1.0+1e-12, coarse)\n",
    "    best = (-1.0, 0.5, 0.5)\n",
    "    for tH in taus:\n",
    "        for tL in taus:\n",
    "            BA, WG, *_ = eval_pair(tH, tL)\n",
    "            score = BA if mode==\"BA\" else WG\n",
    "            if score > best[0]: best = (score, tH, tL)\n",
    "    def span(t):\n",
    "        lo, hi = max(0.0, t-margin), min(1.0, t+margin)\n",
    "        return np.arange(lo, hi+1e-12, fine)\n",
    "    score0, tH0, tL0 = best\n",
    "    for tH in span(tH0):\n",
    "        for tL in span(tL0):\n",
    "            BA, WG, *_ = eval_pair(tH, tL)\n",
    "            score = BA if mode==\"BA\" else WG\n",
    "            if score > score0: score0, tH0, tL0 = score, tH, tL\n",
    "    BA, WG, baH, baL = eval_pair(tH0, tL0)\n",
    "    return {\"tau_H\":float(tH0), \"tau_L\":float(tL0), \"BA\":float(BA), \"WG\":float(WG),\n",
    "            \"BA_H\":float(baH), \"BA_L\":float(baL)}\n",
    "\n",
    "# Single τ\n",
    "pooled_s = np.r_[pooled_neg, pooled_pos]\n",
    "pooled_y = np.r_[np.zeros_like(pooled_neg, int), np.ones_like(pooled_pos, int)]\n",
    "BA_single, tau_single, (TPR_single, TNR_single) = _best_tau_single(pooled_s, pooled_y)\n",
    "\n",
    "# Group τ（2種）\n",
    "gBA = _best_tau_group(high_s, high_y, low_s, low_y, mode=\"BA\")\n",
    "gWG = _best_tau_group(high_s, high_y, low_s, low_y, mode=\"WG\")\n",
    "\n",
    "# ---- 描画ヘルパ（プール分布のみ） ----\n",
    "def _kde(ax, data, n):\n",
    "    if len(data) > 1:\n",
    "        xs = np.linspace(0, 1, n)\n",
    "        ax.plot(xs, gaussian_kde(data)(xs), lw=FONTS[\"kde_lw\"], color=\"#2ca02c\")\n",
    "\n",
    "def _panel(ax, xlim, xgrid_n, show_legend=True):\n",
    "    ax.set_xlim(*xlim); ax.set_ylim(bottom=0)\n",
    "    ax.set_xlabel(\"Predicted probability\", fontsize=FONTS[\"label\"])\n",
    "    ax.set_ylabel(\"Density\", fontsize=FONTS[\"label\"])\n",
    "    ax.tick_params(labelsize=FONTS[\"tick\"])\n",
    "    ax.grid(True, ls=\"--\", alpha=0.25)\n",
    "    # ヒスト（pooledのみ）\n",
    "    ax.hist(pooled_neg, bins=BINS, density=True, alpha=FONTS[\"hist_alpha\"],\n",
    "            color=\"#4e79a7\", edgecolor=\"k\", linewidth=FONTS[\"hist_edge\"], label=\"y=0\")\n",
    "    ax.hist(pooled_pos, bins=BINS, density=True, alpha=FONTS[\"hist_alpha\"],\n",
    "            color=\"#f28e2b\", edgecolor=\"k\", linewidth=FONTS[\"hist_edge\"], label=\"y=1\")\n",
    "    _kde(ax, pooled_neg, xgrid_n); _kde(ax, pooled_pos, xgrid_n)\n",
    "    if show_legend:\n",
    "        ax.legend(loc=\"upper left\", framealpha=0.9, fontsize=FONTS[\"legend\"])\n",
    "\n",
    "def _draw_thresholds(ax):\n",
    "    # Single（黒点線）\n",
    "    ax.axvline(tau_single, color=\"black\", lw=FONTS[\"vline_lw\"], ls=\":\")\n",
    "    # Group BA-opt（青：High=実線 / Low=破線）\n",
    "    ax.axvline(gBA[\"tau_H\"], color=\"#1f77b4\", lw=FONTS[\"vline_lw\"], ls=\"-\")\n",
    "    ax.axvline(gBA[\"tau_L\"], color=\"#1f77b4\", lw=FONTS[\"vline_lw\"], ls=\"--\")\n",
    "    # Group WG-opt（橙：High=実線 / Low=破線）\n",
    "    ax.axvline(gWG[\"tau_H\"], color=\"#ff7f0e\", lw=FONTS[\"vline_lw\"], ls=\"-\")\n",
    "    ax.axvline(gWG[\"tau_L\"], color=\"#ff7f0e\", lw=FONTS[\"vline_lw\"], ls=\"--\")\n",
    "\n",
    "def _annot(ax, where=\"right\"):\n",
    "    txt = (f\"Single: τ={tau_single:.3f}, BA={BA_single:.3f}\\n\"\n",
    "           f\"Group BA-opt: τ_H={gBA['tau_H']:.3f}, τ_L={gBA['tau_L']:.3f}, BA={gBA['BA']:.3f}\\n\"\n",
    "           f\"Group WG-opt: τ_H={gWG['tau_H']:.3f}, τ_L={gWG['tau_L']:.3f}, WG-BA={gWG['WG']:.3f}\")\n",
    "    ax.text(0.99 if where==\"right\" else 0.01, 0.98, txt,\n",
    "            ha=\"right\" if where==\"right\" else \"left\", va=\"top\",\n",
    "            transform=ax.transAxes, fontsize=FONTS[\"box\"],\n",
    "            bbox=dict(facecolor=\"white\", alpha=0.85, boxstyle=\"round,pad=0.25\"))\n",
    "\n",
    "# ===================== FULL =====================\n",
    "figF, axF = plt.subplots(1, 1, figsize=FIGSIZE_FULL)\n",
    "figF.subplots_adjust(left=0.07, right=0.98, bottom=0.18, top=0.86)\n",
    "_panel(axF, (0.0, 1.0), XGRID_N_FULL, show_legend=True)\n",
    "_draw_thresholds(axF)\n",
    "_annot(axF, \"right\")\n",
    "axF.set_title(\"Probability distributions (pooled) — full range\", fontsize=FONTS[\"title\"])\n",
    "figF.suptitle(\"Pooled score distributions with Single τ / Group BA-opt / Group WG-opt\", fontsize=FONTS[\"suptitle\"], y=0.96)\n",
    "plt.savefig(outpath(\"PROB_DENSITY_CLEAN_FULL.png\"), dpi=DPI_FULL)\n",
    "plt.close(figF)\n",
    "\n",
    "# ===================== ZOOM =====================\n",
    "figZ, axZ = plt.subplots(1, 1, figsize=FIGSIZE_ZOOM)\n",
    "figZ.subplots_adjust(left=0.07, right=0.98, bottom=0.18, top=0.86)\n",
    "_panel(axZ, (0.0, XMAX_ZOOM), XGRID_N_ZOOM, show_legend=True)\n",
    "_draw_thresholds(axZ)\n",
    "_annot(axZ, \"right\")\n",
    "axZ.set_title(f\"Probability distributions (pooled) — zoom to {XMAX_ZOOM:.2f}\", fontsize=FONTS[\"title\"])\n",
    "figZ.suptitle(\"Pooled score distributions (zoomed) with three thresholds\", fontsize=FONTS[\"suptitle\"], y=0.96)\n",
    "plt.savefig(outpath(\"PROB_DENSITY_CLEAN_ZOOM.png\"), dpi=DPI_ZOOM)\n",
    "plt.close(figZ)\n",
    "\n",
    "print(\"[OK] Saved:\", outpath(\"PROB_DENSITY_CLEAN_FULL.png\"), \" / \", outpath(\"PROB_DENSITY_CLEAN_ZOOM.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa772e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ===== Cell X (final, rev3): FMS histogram with Top-percentile in-bar lines =====\n",
    "\"\"\"\n",
    "変更点：\n",
    "- パーセンタイルは「上の方（FMSが高い側）から」カウント：Top 20% / 50% / 10%\n",
    "- 赤い点線は該当ビンの内部に水平表示、ラベルは点線より少し上に表示\n",
    "- 件数ラベルもやや上に（ymaxの6%）\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "\n",
    "# ===== 設定 =====\n",
    "USE_ML_WINDOW = True\n",
    "LINEWIDTH = 1.5\n",
    "COUNT_LABEL_OFFSET_FRAC = 0.06   # 棒上の件数ラベルの上げ幅（ymax比）\n",
    "PCT_LABEL_OFFSET_FRAC = 0.02     # 点線ラベルの上げ幅（ymax比）\n",
    "\n",
    "FMS_DIR = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\FMS\")\n",
    "FMS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===== データ取得 =====\n",
    "CANDIDATE_DF_NAMES_ML = [\"df_ml\", \"df_ml_epoch\"] if USE_ML_WINDOW else []\n",
    "CANDIDATE_DF_NAMES_ALL = [\"df\"]\n",
    "CANDIDATE_DF_NAMES = CANDIDATE_DF_NAMES_ML + CANDIDATE_DF_NAMES_ALL\n",
    "CANDIDATE_FMS_COLS = [\"FMS\", \"fms\", \"Fms\", \"FMS_score\", \"FMS_Score\", \"FMS_label\", \"FMSLabel\"]\n",
    "\n",
    "src_name = None\n",
    "series = None\n",
    "picked_df_name = None\n",
    "picked_col = None\n",
    "\n",
    "for df_name in CANDIDATE_DF_NAMES:\n",
    "    if df_name in globals() and isinstance(globals()[df_name], pd.DataFrame):\n",
    "        df_candidate = globals()[df_name]\n",
    "        for col in CANDIDATE_FMS_COLS:\n",
    "            if col in df_candidate.columns:\n",
    "                series = df_candidate[col].copy()\n",
    "                picked_df_name = df_name\n",
    "                picked_col = col\n",
    "                src_name = \"ML\" if (\"ml\" in df_name.lower()) else \"All\"\n",
    "                break\n",
    "        if series is not None:\n",
    "            break\n",
    "\n",
    "if series is None:\n",
    "    raise RuntimeError(\n",
    "        \"[ERROR] Could not find FMS series.\\n\"\n",
    "        f\"  Tried dataframes: {CANDIDATE_DF_NAMES}\\n\"\n",
    "        f\"  Tried columns:    {CANDIDATE_FMS_COLS}\\n\"\n",
    "        \"  → 変数名／列名を確認してください。\"\n",
    "    )\n",
    "\n",
    "print(f\"[OK] Using {picked_df_name}['{picked_col}'] as FMS source (src={src_name}).\")\n",
    "\n",
    "# ===== 前処理 =====\n",
    "series = pd.to_numeric(series, errors=\"coerce\").dropna().astype(int).clip(0, 4)\n",
    "N = int(series.size)\n",
    "if N == 0:\n",
    "    raise RuntimeError(\"[SKIP] Input FMS series is empty after cleaning.\")\n",
    "\n",
    "# ===== スタイル =====\n",
    "mpl.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"font.size\": 20,\n",
    "    \"axes.titlesize\": 30,\n",
    "    \"axes.labelsize\": 24,\n",
    "    \"xtick.labelsize\": 20,\n",
    "    \"ytick.labelsize\": 20,\n",
    "    \"legend.fontsize\": 20,\n",
    "})\n",
    "\n",
    "# ===== 集計 =====\n",
    "bin_values = np.arange(0, 5)  # 0..4\n",
    "bin_counts = np.array([(series == k).sum() for k in bin_values], dtype=int)\n",
    "\n",
    "# ===== 図作成 =====\n",
    "bins = np.arange(-0.5, 5.5, 1)\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "counts, _, _ = ax.hist(series.values, bins=bins, edgecolor='black', alpha=0.85, linewidth=LINEWIDTH)\n",
    "\n",
    "ax.set_xlim(-0.5, 4.5)\n",
    "ymax = max(10, int(max(counts) * 1.25)) if counts.size else 10\n",
    "ax.set_ylim(0, ymax)\n",
    "ax.set_xticks(bin_values)\n",
    "ax.set_xlabel(\"FMS\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "title = f\"FMS Histogram ({'ML' if src_name=='ML' else 'All'})\"\n",
    "ax.set_title(title)\n",
    "\n",
    "# 棒の上に件数(n)を表示：さらに上へ\n",
    "offset = max(1, int(COUNT_LABEL_OFFSET_FRAC * ymax))\n",
    "for x, c in zip(bin_values, counts):\n",
    "    if c > 0:\n",
    "        y_text = min(c + offset, ymax * 0.98)\n",
    "        ax.text(x, y_text, f\"{int(c)}\", ha='center', va='bottom', fontsize=18, fontweight='bold')\n",
    "\n",
    "# ===== 「上の方から」パーセンタイル（Top 20/50/10%） =====\n",
    "targets_top = [(0.20, \"20%\"), (0.50, \"50%\"), (0.10, \"10%\")]\n",
    "\n",
    "def draw_top_percentile_in_bar(p_top: float, label: str):\n",
    "    \"\"\"\n",
    "    上位 p_top の位置に赤点線を引く。\n",
    "    手順：FMS=4→0 の順に累積し、need_top = p_top*N を満たす最初のビンを見つけ、\n",
    "    そのビン内で「上から within_top 本目」に相当する高さに線を引く。\n",
    "    \"\"\"\n",
    "    need_top = p_top * N\n",
    "    cum = 0.0\n",
    "    bin_idx = None\n",
    "    within_top = 0.0\n",
    "\n",
    "    # 4,3,2,1,0 の順で走査\n",
    "    for idx in range(len(bin_counts) - 1, -1, -1):\n",
    "        c = float(bin_counts[idx])\n",
    "        if cum + c >= need_top:\n",
    "            bin_idx = idx\n",
    "            within_top = need_top - cum  # そのビンの「上から」within_top 本目\n",
    "            break\n",
    "        cum += c\n",
    "\n",
    "    if bin_idx is None or bin_counts[bin_idx] == 0:\n",
    "        return\n",
    "\n",
    "    bar_h = float(bin_counts[bin_idx])\n",
    "\n",
    "    # y座標（下からの高さ）= bar_h - within_top\n",
    "    y_line = bar_h - within_top\n",
    "    # 線が棒の外に出ないようクランプ（最小0.5、最大bar_h-0.5）\n",
    "    y_line = float(np.clip(y_line, 0.5, max(0.5, bar_h - 0.5)))\n",
    "\n",
    "    # 点線\n",
    "    x_left, x_right = bin_idx - 0.5, bin_idx + 0.5\n",
    "    ax.hlines(y_line, xmin=x_left, xmax=x_right, colors='red', linestyles='--', linewidth=LINEWIDTH + 1.0)\n",
    "\n",
    "    # ラベル（点線より少し上、棒の外に出ないように）\n",
    "    y_label = min(y_line + PCT_LABEL_OFFSET_FRAC * ymax, max(1.0, bar_h - 0.2))\n",
    "    ax.text(x_right - 0.03, y_label, f\"{label} (n={int(round(need_top))})\",\n",
    "            color='red', ha='right', va='bottom', fontsize=16, fontweight='bold')\n",
    "\n",
    "for p_top, lab in targets_top:\n",
    "    draw_top_percentile_in_bar(p_top, lab)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ===== 保存 =====\n",
    "out_name = \"FMS_HISTOGRAM_ML_percentile_inbar.png\" if (src_name == \"ML\") else \"FMS_HISTOGRAM_ALL_percentile_inbar.png\"\n",
    "out_path = FMS_DIR / out_name\n",
    "plt.savefig(out_path)\n",
    "plt.close()\n",
    "print(f\"[OK] Plot -> {out_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
