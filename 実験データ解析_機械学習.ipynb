{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 0: 環境設定（全セル共通で利用）=====\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib           # 追加\n",
    "import matplotlib.backends  # 追加（←これがポイント）\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ------------------------\n",
    "# 実験スイッチ（Notebook全体で共有）\n",
    "# ------------------------\n",
    "FMS_THRESHOLD: int = 1              # FMS >= 1 を陽性ラベルとみなす\n",
    "EPOCH_LEN: int = 30                 # 30 / 60 / 120 のいずれか（本実験では 30 固定運用）\n",
    "MODEL_BACKEND: str = \"xgb\"          # \"xgb\" / \"rf\" のいずれか\n",
    "\n",
    "USE_MSSQ_FEATURE: bool = False      # True: MSSQ を特徴量に含める\n",
    "USE_VIMSSQ_FEATURE: bool = False    # True: VIMSSQ を特徴量に含める\n",
    "\n",
    "SEED_BASE: int = 20251101\n",
    "TOP_SUBSET_K: int = 15              # subset探索で使う上位特徴数\n",
    "\n",
    "# ---- XGB 正則化レベル（パターン選択）----\n",
    "# 0: 正則化0（初期パラ）\n",
    "# 1: 正則化1（改変パラ）\n",
    "# ★ ドライバノートから上書きできるように globals() を参照\n",
    "XGB_REG_LEVEL: int = int(globals().get(\"XGB_REG_LEVEL\", 0))\n",
    "\n",
    "# ---- 時系列特徴（履歴連結）の設定 ----\n",
    "# HISTORY_N_EPOCHS = 1: 従来どおり履歴なし（f_k）\n",
    "# HISTORY_N_EPOCHS = H>1: 直近 H エポック分を連結 [f_{k-H+1}, ..., f_k]\n",
    "HISTORY_N_EPOCHS: int = 1\n",
    "if HISTORY_N_EPOCHS < 1:\n",
    "    raise ValueError(\"HISTORY_N_EPOCHS は 1 以上で指定してください。\")\n",
    "\n",
    "# ---- MSSQ / VIMSSQ の High / Low 閾値 ----\n",
    "MSSQ_THRESHOLD_FIXED: float   = 11.0  # 例：MSSQ >= 11 を High\n",
    "VIMSSQ_THRESHOLD_FIXED: float = 3     # 例：VIMSSQ >= 3 を High\n",
    "\n",
    "# ---- グループ分けの基準（FMS推移プロットなどで使用）----\n",
    "# \"MSSQ\" または \"VIMSSQ\" を指定\n",
    "GROUPING_BASIS_FOR_PLOTS: str = \"MSSQ\"\n",
    "\n",
    "if EPOCH_LEN not in (30, 60, 120):\n",
    "    raise ValueError(\"EPOCH_LEN は 30/60/120 から選択してください。\")\n",
    "\n",
    "# ---------------- 設定（等間隔グリッド＋近傍再探索） ----------------\n",
    "COARSE_STEPS = 101      # 0.0〜1.0 を等間隔\n",
    "FINE_STEPS   = 101      # 近傍再探索の細かさ\n",
    "FINE_MARGIN  = 0.01     # 近傍幅（±0.01）\n",
    "\n",
    "# ★ ドライバノートから上書きできるように globals() を参照\n",
    "CORR_THRESHOLD = float(globals().get(\"CORR_THRESHOLD\", 0.7))  # 相関除去の閾値（Cell3A-pre でも使用）\n",
    "\n",
    "VERBOSE      = True\n",
    "\n",
    "# ---- 実行フラグ（Cell4/Cell5/Cell6 を切替）----\n",
    "RUN_CELL4: bool = bool(globals().get(\"RUN_CELL4\", True))\n",
    "RUN_CELL5: bool = bool(globals().get(\"RUN_CELL5\", True))\n",
    "RUN_CELL6_NEUTRAL: bool = RUN_CELL4\n",
    "RUN_CELL6_STRAT: bool = RUN_CELL5\n",
    "\n",
    "# ------------------------\n",
    "# ファイル入出力ルート\n",
    "# ------------------------\n",
    "BASE_INPUT_DIR = r\"C:\\\\Users\\\\taiki\\\\OneDrive - Science Tokyo\\\\デスクトップ\\\\研究\\\\本実験結果\"\n",
    "BASE_ANALYSIS_DIR = os.path.join(BASE_INPUT_DIR, \"ANALYSIS\")\n",
    "BASE_LEVEL0_DIR = os.path.join(BASE_ANALYSIS_DIR, \"機械学習\")  # 階層0\n",
    "\n",
    "# --- 階層1パラメータタグ（可変要素）---\n",
    "# ★ FSモードは文字列 FS_MODE から決める\n",
    "FS_MODE = globals().get(\"FS_MODE\", \"mean\")  # \"mean\" / \"rank\" / \"rfe\"\n",
    "\n",
    "USE_FS_SHAP_MEAN: bool = (FS_MODE == \"mean\")  # SHAP値平均（TreeSHAPのmean(|SHAP|)）\n",
    "USE_FS_SHAP_RANK: bool = (FS_MODE == \"rank\")  # SHAP順位平均（fold rankの平均）\n",
    "USE_FS_RFE: bool        = (FS_MODE == \"rfe\")  # RFE（fold rank）\n",
    "\n",
    "_fs_flags = [USE_FS_SHAP_MEAN, USE_FS_SHAP_RANK, USE_FS_RFE]\n",
    "if sum(_fs_flags) != 1:\n",
    "    raise ValueError(\"FS_MODE は 'mean' / 'rank' / 'rfe' のいずれかになるようにしてください\")\n",
    "\n",
    "# 正則化セットのラベル（XGB_REG_LEVEL から自動生成）\n",
    "REGULARIZATION_TAG = f\"正則化{XGB_REG_LEVEL}\"\n",
    "\n",
    "if USE_FS_SHAP_MEAN:\n",
    "    FEATURE_SELECTION_METHOD = \"SHAP-mean\"\n",
    "    FEATURE_RANKING_FILE = \"SHAP_MEAN_FEATURE_RANKING.CSV\"\n",
    "    GROUP_RANKING_FILE = \"SHAP_MEAN_GROUP_RANKING.CSV\"\n",
    "elif USE_FS_SHAP_RANK:\n",
    "    FEATURE_SELECTION_METHOD = \"SHAP-rank\"\n",
    "    FEATURE_RANKING_FILE = \"SHAP_RANK_FEATURE_RANKING.CSV\"\n",
    "    GROUP_RANKING_FILE = \"SHAP_RANK_GROUP_RANKING.CSV\"\n",
    "else:\n",
    "    FEATURE_SELECTION_METHOD = \"RFE\"\n",
    "    FEATURE_RANKING_FILE = \"RFE_FEATURE_RANKING.CSV\"\n",
    "    GROUP_RANKING_FILE = \"RFE_GROUP_RANKING.CSV\"\n",
    "\n",
    "\n",
    "def build_level1_dir(\n",
    "    feature_selection: str,\n",
    "    history_epochs: int,\n",
    "    coarse_steps: int,\n",
    "    fine_steps: int,\n",
    "    corr_threshold: float,\n",
    ") -> str:\n",
    "    parts = [\n",
    "        feature_selection,\n",
    "        f\"H{history_epochs:02d}\",\n",
    "        f\"Grid{coarse_steps}\",\n",
    "        f\"Corr{corr_threshold:.2f}\",\n",
    "        REGULARIZATION_TAG,\n",
    "    ]\n",
    "    return os.path.join(BASE_LEVEL0_DIR, \"__\".join(parts))\n",
    "\n",
    "\n",
    "LEVEL1_DIR = build_level1_dir(\n",
    "    FEATURE_SELECTION_METHOD,\n",
    "    HISTORY_N_EPOCHS,\n",
    "    COARSE_STEPS,\n",
    "    FINE_STEPS,\n",
    "    CORR_THRESHOLD,\n",
    ")\n",
    "os.makedirs(LEVEL1_DIR, exist_ok=True)\n",
    "\n",
    "CURRENT_CELL_ID = None\n",
    "OUT_DIR = None\n",
    "\n",
    "\n",
    "def set_cell_output(cell_id: int) -> None:\n",
    "    \"\"\"階層2: Cellごとの出力先をセット（Cell0, Cell1, ...）\"\"\"\n",
    "    global CURRENT_CELL_ID, OUT_DIR\n",
    "    CURRENT_CELL_ID = cell_id\n",
    "    OUT_DIR = os.path.join(LEVEL1_DIR, f\"Cell{cell_id}\")\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(f\"[Cell{cell_id}] OUT_DIR -> {OUT_DIR}\")\n",
    "\n",
    "\n",
    "def outpath(filename: str) -> str:\n",
    "    if OUT_DIR is None:\n",
    "        raise RuntimeError(\"先に set_cell_output(cell_id) を呼んでください。\")\n",
    "    return os.path.join(OUT_DIR, filename)\n",
    "\n",
    "\n",
    "def cell_output_path(cell_id: int, filename: str) -> str:\n",
    "    \"\"\"前セルのCSVや図を参照するときに使う\"\"\"\n",
    "    return os.path.join(LEVEL1_DIR, f\"Cell{cell_id}\", filename)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"[LEVEL1_DIR] {LEVEL1_DIR}  |  \"\n",
    "    f\"EPOCH_LEN={EPOCH_LEN}s | HISTORY_N_EPOCHS={HISTORY_N_EPOCHS} | \"\n",
    "    f\"FS_MODE={FS_MODE} | CORR_THRESHOLD={CORR_THRESHOLD} | XGB_REG_LEVEL={XGB_REG_LEVEL}\"\n",
    ")\n",
    "set_cell_output(0)  # Cell0 の出力先をセット\n",
    "\n",
    "# ------------------------\n",
    "# 対象被験者・時間窓\n",
    "# ------------------------\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "BASELINE_EPOCH = 1770               # ベースライン行（必須）\n",
    "ML_START, ML_END = 1800, 2400       # 学習に使う epoch_start 範囲 [start, end)\n",
    "\n",
    "# ------------------------\n",
    "# 描画スタイル\n",
    "# ------------------------\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"font.size\": 20, \"axes.titlesize\": 26, \"axes.labelsize\": 22,\n",
    "    \"xtick.labelsize\": 20, \"ytick.labelsize\": 20, \"legend.fontsize\": 20,\n",
    "})\n",
    "\n",
    "# ------------------------\n",
    "# FMS二値化ヘルパ\n",
    "# ------------------------\n",
    "def binarize_fms(series: pd.Series, threshold: Optional[int] = None) -> pd.Series:\n",
    "    th = FMS_THRESHOLD if threshold is None else int(threshold)\n",
    "    return (series >= th).astype(int)\n",
    "\n",
    "# ------------------------\n",
    "# モデルレジストリ\n",
    "# ------------------------\n",
    "ModelBuilder = Callable[..., Any]\n",
    "MODEL_REGISTRY: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "\n",
    "def register_backend(name: str, params: Dict[str, Any], builder: ModelBuilder) -> None:\n",
    "    MODEL_REGISTRY[name] = {\"params\": params, \"builder\": builder}\n",
    "\n",
    "\n",
    "def _build_xgb(params: Dict[str, Any], *, scale_pos_weight: Optional[float] = None):\n",
    "    cfg = params.copy()\n",
    "    if scale_pos_weight is not None:\n",
    "        cfg[\"scale_pos_weight\"] = float(scale_pos_weight)\n",
    "    return xgb.XGBClassifier(**cfg)\n",
    "\n",
    "\n",
    "def _build_rf(params: Dict[str, Any], **_):\n",
    "    return RandomForestClassifier(**params)\n",
    "\n",
    "\n",
    "def set_model_backend(name: str) -> None:\n",
    "    name = name.lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録: {list(MODEL_REGISTRY.keys())}\")\n",
    "    global MODEL_BACKEND\n",
    "    MODEL_BACKEND = name\n",
    "\n",
    "\n",
    "def build_estimator(\n",
    "    backend: Optional[str] = None,\n",
    "    *,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    name = (backend or MODEL_BACKEND).lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録。\")\n",
    "    base = MODEL_REGISTRY[name][\"params\"].copy()\n",
    "    if overrides:\n",
    "        base.update(overrides)\n",
    "    builder = MODEL_REGISTRY[name][\"builder\"]\n",
    "    return builder(base, scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "\n",
    "def fit_estimator(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    model = build_estimator(\n",
    "        backend=backend, scale_pos_weight=scale_pos_weight, overrides=overrides\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_positive_score(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return np.asarray(model.decision_function(X), dtype=float)\n",
    "    return model.predict(X).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28264db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 0A: XGBoost 正則化プリセット定義＆登録 =====\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "# ------------------------\n",
    "# XGB パラメータプリセット\n",
    "# ------------------------\n",
    "# level:\n",
    "#   0 -> 正則化0（初期パラ）\n",
    "#   1 -> 正則化1（改変①: 低データ数・高特徴量向けに正則化を強めた設定）\n",
    "XGB_PARAM_PRESETS: Dict[int, Dict[str, Any]] = {\n",
    "    0: dict(\n",
    "        # 正則化0: 初期パラ\n",
    "        n_estimators=100,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cpu\",\n",
    "        seed=0,\n",
    "        random_state=0,\n",
    "    ),\n",
    "    1: dict(\n",
    "        # 正則化1: 改変①（浅い木＋強めの正則化）\n",
    "        n_estimators=100,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cpu\",\n",
    "        seed=0,\n",
    "        random_state=0,\n",
    "\n",
    "        # ---- 学習率（少しだけ低く） ----\n",
    "        learning_rate=0.1,      # デフォルト0.3 → 0.1 にして一歩ずつ学習\n",
    "\n",
    "        # ---- 木の複雑さ（軽く制限）----\n",
    "        max_depth=3,            # デフォルト6 → 3（浅めの木に）\n",
    "        min_child_weight=5,     # デフォルト1 → 5（少数サンプルでの分割を禁止気味に）\n",
    "        gamma=0.5,              # デフォルト0 → 0.5（ショボい分割は切り捨て）\n",
    "\n",
    "        # ---- サブサンプリング（軽く正則化）----\n",
    "        subsample=0.8,          # 1.0 → 0.8（各木が見るデータを8割に）\n",
    "        colsample_bytree=0.6,   # 1.0 → 0.6（各木が見る特徴量を6割に）\n",
    "\n",
    "        # ---- L2 / L1 正則化（本命）----\n",
    "        reg_lambda=20.0,        # L2 正則化\n",
    "        reg_alpha=2.0,          # L1 正則化で“自動特徴選択”を少し効かせる\n",
    "    ),\n",
    "}\n",
    "\n",
    "# ------------------------\n",
    "# レベル説明（ログ・ドキュメント用）\n",
    "# ------------------------\n",
    "XGB_REG_DESCRIPTIONS: Dict[int, str] = {\n",
    "    0: \"正則化0: 初期パラ（ほぼデフォルト設定に近い XGB）\",\n",
    "    1: \"正則化1: 改変①（浅い木 + 強めの L1/L2 正則化 + サブサンプリング）\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_xgb_params_by_level(level: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    正則化レベルに応じた XGB ハイパーパラメータ dict を返す.\n",
    "\n",
    "    level:\n",
    "        0 -> 正則化0（初期パラ）\n",
    "        1 -> 正則化1（改変①）\n",
    "    \"\"\"\n",
    "    if level not in XGB_PARAM_PRESETS:\n",
    "        raise ValueError(\n",
    "            f\"[get_xgb_params_by_level] 未定義の level={level} です。\"\n",
    "            f\" 定義済み: {sorted(XGB_PARAM_PRESETS.keys())}\"\n",
    "        )\n",
    "    return XGB_PARAM_PRESETS[level].copy()\n",
    "\n",
    "\n",
    "def describe_xgb_reg_level(level: int) -> str:\n",
    "    \"\"\"\n",
    "    正則化レベルの概要説明を返す（ログ・ドキュメント用）.\n",
    "    \"\"\"\n",
    "    if level not in XGB_REG_DESCRIPTIONS:\n",
    "        raise ValueError(\n",
    "            f\"[describe_xgb_reg_level] 未定義の level={level} です。\"\n",
    "            f\" 定義済み: {sorted(XGB_REG_DESCRIPTIONS.keys())}\"\n",
    "        )\n",
    "    return XGB_REG_DESCRIPTIONS[level]\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# Cell0 で決めたレベルに応じて XGB を登録\n",
    "# ------------------------\n",
    "XGB_PARAMS: Dict[str, Any] = get_xgb_params_by_level(XGB_REG_LEVEL)\n",
    "\n",
    "# Cell0 側で定義済みの register_backend, _build_xgb を使う\n",
    "register_backend(\"xgb\", XGB_PARAMS, _build_xgb)\n",
    "\n",
    "# 簡単なログ\n",
    "print(f\"[Cell0A] XGB_REG_LEVEL={XGB_REG_LEVEL} ({REGULARIZATION_TAG})\")\n",
    "try:\n",
    "    print(f\"[Cell0A] {describe_xgb_reg_level(XGB_REG_LEVEL)}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Cell0A][WARN] 正則化レベル説明の取得に失敗: {e}\")\n",
    "\n",
    "main_keys = [\n",
    "    \"learning_rate\", \"max_depth\", \"min_child_weight\", \"gamma\",\n",
    "    \"subsample\", \"colsample_bytree\", \"reg_lambda\", \"reg_alpha\",\n",
    "]\n",
    "print(\"[Cell0A] XGB_PARAMS (main):\",\n",
    "      {k: XGB_PARAMS[k] for k in main_keys if k in XGB_PARAMS})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3365e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 0B: RandomForest パラメータ定義＆登録 =====\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Dict\n",
    "\n",
    "# ------------------------\n",
    "# RF ハイパーパラメータ\n",
    "# ------------------------\n",
    "RF_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=100,      # 論文：決定木100本\n",
    "    max_features=1,        # 論文：max feature of one\n",
    "\n",
    "    # 以下は論文に記載がないので，ほぼデフォルト＋再現性用\n",
    "    bootstrap=True,        # scikit-learn のデフォルト\n",
    "    random_state=SEED_BASE,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "# Cell0 側で定義済みの register_backend, _build_rf を使う\n",
    "register_backend(\"rf\", RF_PARAMS, _build_rf)\n",
    "\n",
    "# ------------------------\n",
    "# 最終ログ（利用可能バックエンドの一覧など）\n",
    "# ------------------------\n",
    "MODEL_ID = MODEL_BACKEND.upper()\n",
    "print(f\"[Cell0B] MODEL_BACKEND={MODEL_ID} / SEED={SEED_BASE} / backends={list(MODEL_REGISTRY.keys())}\")\n",
    "print(f\"[Cell0B] XGB_REG_LEVEL={XGB_REG_LEVEL} ({REGULARIZATION_TAG})\")\n",
    "\n",
    "try:\n",
    "    desc = describe_xgb_reg_level(XGB_REG_LEVEL)\n",
    "    print(f\"[Cell0B] {desc}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Cell0B][WARN] 正則化レベル説明の取得に失敗: {e}\")\n",
    "\n",
    "# XGB / RF の主なパラメータをざっくり表示\n",
    "xgb_main_keys = [\n",
    "    \"learning_rate\", \"max_depth\", \"min_child_weight\", \"gamma\",\n",
    "    \"subsample\", \"colsample_bytree\", \"reg_lambda\", \"reg_alpha\",\n",
    "]\n",
    "if \"xgb\" in MODEL_REGISTRY:\n",
    "    xgb_cfg = MODEL_REGISTRY[\"xgb\"][\"params\"]\n",
    "    print(\"[Cell0B] XGB_PARAMS (main):\",\n",
    "          {k: xgb_cfg[k] for k in xgb_main_keys if k in xgb_cfg})\n",
    "\n",
    "print(\"[Cell0B] RF_PARAMS:\", RF_PARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: データ準備（CSV読込 → EPOCH合成 → SUBJECT_META → 行列出力）=====\n",
    "set_cell_output(1)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --------------------------------------------\n",
    "# ① 30秒EPOCH CSVの読み込み・検証\n",
    "# --------------------------------------------\n",
    "def subject_csv_path(sid: str) -> str:\n",
    "    path = os.path.join(BASE_INPUT_DIR, sid, \"EPOCH\", f\"{sid}_epoch.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"[Cell1] CSV missing for subject {sid}: {path}\")\n",
    "    return path\n",
    "\n",
    "dfs = []\n",
    "for sid in SUBJECT_IDS:\n",
    "    df = pd.read_csv(subject_csv_path(sid))\n",
    "    if df.shape[1] < 4:\n",
    "        raise ValueError(f\"[Cell1] {sid}: 列数が不足（>=4 必須）\")\n",
    "    df = df.copy()\n",
    "\n",
    "    # 4列目以降の列名を文字列化（数値列名対策）\n",
    "    df.columns = list(df.columns[:3]) + [str(c) for c in df.columns[3:]]\n",
    "    c1, c2, c3 = df.columns[:3]\n",
    "    df = df.rename(columns={c1: \"epoch_start\", c2: \"epoch_end\", c3: \"FMS\"})\n",
    "\n",
    "    df[\"epoch_start\"] = pd.to_numeric(df[\"epoch_start\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"epoch_end\"]   = pd.to_numeric(df[\"epoch_end\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"FMS\"]         = pd.to_numeric(df[\"FMS\"],         errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    if df[[\"epoch_start\", \"epoch_end\", \"FMS\"]].isna().any().any():\n",
    "        raise ValueError(f\"[Cell1] {sid}: epoch_start/epoch_end/FMS に NaN があります。\")\n",
    "\n",
    "    df.insert(0, \"subject_id\", sid)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 除外する特徴量（周波数領域など）\n",
    "exclude_feats = {\"HF_power\", \"LF_power\", \"LF_HF_ratio\"}\n",
    "feature_cols_all = [\n",
    "    c for c in combined_raw.columns\n",
    "    if c not in {\"subject_id\", \"epoch_start\", \"epoch_end\", \"FMS\"} and c not in exclude_feats\n",
    "]\n",
    "if not feature_cols_all:\n",
    "    raise RuntimeError(\"[Cell1] 特徴量列が0です。列名や除外設定を確認してください。\")\n",
    "\n",
    "print(f\"[Cell1] Loaded subjects={len(SUBJECT_IDS)}, rows={len(combined_raw)}, \"\n",
    "      f\"features(after drop)={len(feature_cols_all)}\")\n",
    "\n",
    "print(\"[Cell1] Physiological feature columns (feature_cols_all):\")\n",
    "for col in feature_cols_all:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ② EPOCH_LEN 秒への合成 + baseline差分 + ラベル生成\n",
    "# --------------------------------------------\n",
    "if (ML_END - ML_START) % EPOCH_LEN != 0:\n",
    "    raise ValueError(f\"[Cell1] ML window {ML_END-ML_START} が EPOCH_LEN={EPOCH_LEN} で割り切れません。\")\n",
    "\n",
    "rows_per_bin = EPOCH_LEN // 30  # 30秒エポックを何個まとめるか\n",
    "df_out_list = []\n",
    "\n",
    "# デバッグ用ディレクトリ\n",
    "DEBUG_DIR = os.path.join(OUT_DIR, \"Cell1_デバッグ\")\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "for sid, sdf in combined_raw.groupby(\"subject_id\", sort=False):\n",
    "    # baseline 行（BASELINE_EPOCH）の取得\n",
    "    base_row = sdf.loc[sdf[\"epoch_start\"] == BASELINE_EPOCH]\n",
    "    if len(base_row) != 1:\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline epoch_start=={BASELINE_EPOCH} が見つからない（{len(base_row)}件）\")\n",
    "\n",
    "    base_vals = base_row[feature_cols_all].astype(float).iloc[0]\n",
    "    if base_vals.isna().any():\n",
    "        bad_cols = base_vals.index[base_vals.isna()].tolist()\n",
    "        raise ValueError(f\"[Cell1] {sid}: baselineにNaN -> {bad_cols}\")\n",
    "\n",
    "    # 学習に使う時間窓だけ抽出\n",
    "    sdf_ml = sdf[(sdf[\"epoch_start\"] >= ML_START) & (sdf[\"epoch_start\"] < ML_END)].copy()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: ML window [{ML_START},{ML_END}) が空です。\")\n",
    "\n",
    "    # 30秒epochを EPOCH_LEN 秒にまとめるためのbin\n",
    "    sdf_ml[\"bin_start\"] = ML_START + ((sdf_ml[\"epoch_start\"] - ML_START) // EPOCH_LEN) * EPOCH_LEN\n",
    "    sdf_ml[\"bin_end\"]   = sdf_ml[\"bin_start\"] + EPOCH_LEN\n",
    "\n",
    "    # 行数が揃っている bin のみ採用\n",
    "    bin_counts = sdf_ml.groupby([\"bin_start\", \"bin_end\"]).size()\n",
    "    complete_bins = bin_counts[bin_counts == rows_per_bin].index\n",
    "    sdf_ml = sdf_ml.set_index([\"bin_start\", \"bin_end\"]).loc[complete_bins].reset_index()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: EPOCH_LEN={EPOCH_LEN} で完全なbinが無い\")\n",
    "\n",
    "    # 各 bin で平均を取る（FMS も平均）\n",
    "    agg_dict = {c: \"mean\" for c in feature_cols_all}\n",
    "    agg_dict[\"FMS\"] = \"mean\"\n",
    "    g = sdf_ml.groupby([\"subject_id\", \"bin_start\", \"bin_end\"], as_index=False).agg(agg_dict)\n",
    "\n",
    "    # baseline 差分（生理特徴量のみ）\n",
    "    g_features = g[feature_cols_all].astype(float) - base_vals.values\n",
    "    if g_features.isna().any().any():\n",
    "        bad = g_features.columns[g_features.isna().any()].tolist()\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline差分後にNaN -> {bad}\")\n",
    "\n",
    "    # 出力用に整形\n",
    "    g_out = pd.concat(\n",
    "        [g[[\"subject_id\", \"bin_start\", \"bin_end\", \"FMS\"]], g_features],\n",
    "        axis=1\n",
    "    )\n",
    "    g_out = g_out.rename(columns={\"bin_start\": \"epoch_start\", \"bin_end\": \"epoch_end\"})\n",
    "\n",
    "    # FMS を二値化\n",
    "    g_out[\"label\"] = binarize_fms(g_out[\"FMS\"])\n",
    "\n",
    "    # 列順を整える\n",
    "    g_out = g_out[[\"subject_id\", \"epoch_start\", \"epoch_end\", \"FMS\", \"label\"] + feature_cols_all]\n",
    "\n",
    "    # デバッグ: この被験者のベースライン差分後データをCSVに保存\n",
    "    debug_path = os.path.join(DEBUG_DIR, f\"Cell1_debug_{sid}_E{EPOCH_LEN}s.csv\")\n",
    "    g_out.to_csv(debug_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell1-DEBUG] Saved baseline-diff data for subject {sid} -> {debug_path}\")\n",
    "\n",
    "    df_out_list.append(g_out)\n",
    "\n",
    "df_ml_epoch = pd.concat(df_out_list, ignore_index=True)\n",
    "\n",
    "print(f\"[Cell1] df_ml_epoch shape={df_ml_epoch.shape}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ③ SUBJECT_META & MSSQ / VIMSSQ group（被験者属性読み込み）\n",
    "# --------------------------------------------\n",
    "CANDIDATE_SCORE_PATHS = [\n",
    "    \"/mnt/data/summary_scores.xlsx\",\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"機械学習\", \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_INPUT_DIR, \"summary_scores.xlsx\"),\n",
    "]\n",
    "score_path = next((p for p in CANDIDATE_SCORE_PATHS if os.path.exists(p)), None)\n",
    "if score_path is None:\n",
    "    raise FileNotFoundError(\"[Cell1] summary_scores.xlsx が見つかりません。\")\n",
    "\n",
    "meta_raw = pd.read_excel(score_path, sheet_name=\"Summary\")\n",
    "\n",
    "required = [\"ID\", \"MSSQ\", \"VIMSSQ\"]\n",
    "missing = [c for c in required if c not in meta_raw.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"[Cell1] summary_scores.xlsx に必須列がありません -> {missing}\")\n",
    "\n",
    "meta = meta_raw[required].copy()\n",
    "meta[\"ID\"] = (\n",
    "    meta[\"ID\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "for c in [\"MSSQ\", \"VIMSSQ\"]:\n",
    "    meta[c] = pd.to_numeric(meta[c], errors=\"raise\")\n",
    "\n",
    "sid_set = set(map(str, SUBJECT_IDS))\n",
    "meta = meta[meta[\"ID\"].isin(sid_set)].copy()\n",
    "if meta[\"ID\"].duplicated().any():\n",
    "    dup_ids = meta.loc[meta[\"ID\"].duplicated(), \"ID\"].tolist()\n",
    "    raise ValueError(f\"[Cell1] ID 重複 -> {dup_ids}\")\n",
    "\n",
    "# MSSQ_group / VIMSSQ_group を作成\n",
    "meta[\"MSSQ_group\"]   = np.where(meta[\"MSSQ\"]   >= MSSQ_THRESHOLD_FIXED,   \"High\", \"Low\")\n",
    "meta[\"VIMSSQ_group\"] = np.where(meta[\"VIMSSQ\"] >= VIMSSQ_THRESHOLD_FIXED, \"High\", \"Low\")\n",
    "\n",
    "SUBJECT_META = (\n",
    "    meta.rename(columns={\"ID\": \"subject_id\"})\n",
    "        .set_index(\"subject_id\")[[\"MSSQ\", \"VIMSSQ\", \"MSSQ_group\", \"VIMSSQ_group\"]]\n",
    "        .copy()\n",
    ")\n",
    "\n",
    "SUBJECT_META.to_csv(outpath(\"subject_meta.csv\"), encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell1] SUBJECT_META saved -> {outpath('subject_meta.csv')} (source='{score_path}')\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ④ MSSQ / VIMSSQ をフラグに応じて特徴量に追加（1回だけ）\n",
    "# --------------------------------------------\n",
    "trait_cols_to_use: list[str] = []\n",
    "if USE_MSSQ_FEATURE:\n",
    "    trait_cols_to_use.append(\"MSSQ\")\n",
    "if USE_VIMSSQ_FEATURE:\n",
    "    trait_cols_to_use.append(\"VIMSSQ\")\n",
    "\n",
    "if trait_cols_to_use:\n",
    "    merge_cols = [\"subject_id\"] + trait_cols_to_use\n",
    "    df_ml_epoch = df_ml_epoch.merge(\n",
    "        SUBJECT_META.reset_index()[merge_cols],\n",
    "        on=\"subject_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    if df_ml_epoch[trait_cols_to_use].isna().any().any():\n",
    "        bad_sids = df_ml_epoch.loc[\n",
    "            df_ml_epoch[trait_cols_to_use].isna().any(axis=1), \"subject_id\"\n",
    "        ].unique().tolist()\n",
    "        raise ValueError(f\"[Cell1] MSSQ/VIMSSQ が欠損の subject_id があります -> {bad_sids}\")\n",
    "\n",
    "    print(\"[Cell1] df_ml_epoch with trait features (MSSQ/VIMSSQ):\")\n",
    "    print(df_ml_epoch[[\"subject_id\"] + trait_cols_to_use].drop_duplicates().head())\n",
    "else:\n",
    "    print(\"[Cell1] Trait features (MSSQ/VIMSSQ) are disabled by flags.\")\n",
    "\n",
    "# 生理特徴量 + オプションtraitsをまとめた最終的な特徴量リスト\n",
    "feature_cols_full = feature_cols_all + trait_cols_to_use\n",
    "\n",
    "print(f\"[Cell1] Final feature columns (n={len(feature_cols_full)}):\")\n",
    "for col in feature_cols_full:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ⑤ 履歴導入「前」のラベル分布チェック（診断用）\n",
    "# --------------------------------------------\n",
    "label_counts = (\n",
    "    df_ml_epoch\n",
    "    .groupby(\"subject_id\")[\"label\"]\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# 列 0/1 が必ず存在するようにしてから rename\n",
    "for val in (0, 1):\n",
    "    if val not in label_counts.columns:\n",
    "        label_counts[val] = 0\n",
    "label_counts = label_counts[[0, 1]].rename(columns={0: \"neg_before\", 1: \"pos_before\"})\n",
    "\n",
    "LABEL_BEFORE_PATH = outpath(\"LABEL_DIST_BEFORE_HISTORY.csv\")\n",
    "label_counts.to_csv(LABEL_BEFORE_PATH, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell1] Saved label distribution BEFORE history -> {LABEL_BEFORE_PATH}\")\n",
    "\n",
    "print(\"[Cell1] Label distribution BEFORE history (per subject):\")\n",
    "print(label_counts)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ⑥ 学習行列（従来仕様）を一旦作成\n",
    "#    ※ Cell2 で時系列変換後に X_all / y_all / groups は上書きされる\n",
    "# --------------------------------------------\n",
    "fname_raw = f\"ML_DATA_DELTA_{EPOCH_LEN}S_RAW.CSV\"\n",
    "df_ml_epoch.to_csv(outpath(fname_raw), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "X_all = df_ml_epoch[feature_cols_full].copy().astype(float)\n",
    "y_all = df_ml_epoch[\"label\"].copy().astype(int)\n",
    "groups = df_ml_epoch[\"subject_id\"].copy()\n",
    "\n",
    "X_all.to_csv(outpath(f\"X_RAW_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "X_all.to_csv(outpath(f\"X_SCALED_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")  # 木系でスケーリング不要\n",
    "pd.DataFrame({\n",
    "    \"subject_id\": groups,\n",
    "    \"label\": y_all,\n",
    "    \"FMS_mean\": df_ml_epoch[\"FMS\"],\n",
    "}).to_csv(outpath(f\"Y_AND_GROUPS_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[Cell1] Saved -> {outpath(fname_raw)} / X_RAW_ALL / X_SCALED_ALL / Y_AND_GROUPS\")\n",
    "print(f\"[Cell1] Matrices ready (pre-history): X_all={X_all.shape}, y_all={y_all.shape}, \"\n",
    "      f\"SUBJECT_META={SUBJECT_META.shape}\")\n",
    "print(f\"[Cell1] n_features(physio)={len(feature_cols_all)}, \"\n",
    "      f\"+ traits({len(trait_cols_to_use)}) -> {len(feature_cols_full)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1A: MSSQ / VIMSSQ 群別の FMS 推移プロット =====\n",
    "set_cell_output(1)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 出力ディレクトリ\n",
    "FMS_PLOT_DIR = os.path.join(OUT_DIR, \"Cell1A_FMS_trajectory\")\n",
    "os.makedirs(FMS_PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# 描画スタイル（ユーザー規約）\n",
    "LW = 1.5\n",
    "FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "\n",
    "COLOR_HIGH = \"red\"\n",
    "COLOR_LOW  = \"blue\"\n",
    "\n",
    "\n",
    "def _prepare_fms_long_use_epoch_end():\n",
    "    \"\"\"\n",
    "    df_ml_epoch から FMS 時系列を取り出し、\n",
    "    ML_START からの経過時間（分）を epoch_end 基準で付与した長データを返す。\n",
    "    \"\"\"\n",
    "    required_cols = {\"subject_id\", \"epoch_end\", \"FMS\"}\n",
    "    missing = required_cols - set(df_ml_epoch.columns)\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell1A] df_ml_epoch に必須列がありません -> {missing}\")\n",
    "\n",
    "    df = df_ml_epoch[[\"subject_id\", \"epoch_end\", \"FMS\"]].copy()\n",
    "\n",
    "    # ML_START からの経過時間（秒 → 分）を epoch_end 基準で計算\n",
    "    df[\"t_min\"] = (df[\"epoch_end\"] - ML_START).astype(float) / 60.0\n",
    "    return df\n",
    "\n",
    "\n",
    "def _plot_fms_by_group_min_axis(group_col: str, title_prefix: str, save_name: str):\n",
    "    \"\"\"\n",
    "    group_col: \"MSSQ_group\" または \"VIMSSQ_group\"\n",
    "    title_prefix: 図タイトルのプレフィックス（\"MSSQ group\" など）\n",
    "    save_name: 保存ファイル名\n",
    "    \"\"\"\n",
    "    if group_col not in SUBJECT_META.columns:\n",
    "        raise KeyError(\n",
    "            f\"[Cell1A] SUBJECT_META に {group_col} 列がありません。\"\n",
    "            \"Cell1 の SUBJECT_META 作成部を確認してください。\"\n",
    "        )\n",
    "\n",
    "    # 群に属する被験者 ID をプリント\n",
    "    for level in [\"High\", \"Low\"]:\n",
    "        sids = SUBJECT_META.index[SUBJECT_META[group_col] == level].tolist()\n",
    "        print(f\"[Cell1A] {group_col} = {level}: subjects = {sorted(sids)}\")\n",
    "\n",
    "    # FMS 長データに group_col をマージ（epoch_end 基準）\n",
    "    df_long = _prepare_fms_long_use_epoch_end()\n",
    "    df_long = df_long.merge(\n",
    "        SUBJECT_META.reset_index()[[\"subject_id\", group_col]],\n",
    "        on=\"subject_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    if df_long[group_col].isna().any():\n",
    "        bad = df_long.loc[df_long[group_col].isna(), \"subject_id\"].unique().tolist()\n",
    "        raise ValueError(f\"[Cell1A] {group_col} が欠損の subject_id があります -> {bad}\")\n",
    "\n",
    "    # group × t_min ごとに FMS の平均・標準偏差\n",
    "    agg = (\n",
    "        df_long\n",
    "        .groupby([group_col, \"t_min\"])[\"FMS\"]\n",
    "        .agg([\"mean\", \"std\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "    agg[\"std\"] = agg[\"std\"].fillna(0.0)\n",
    "\n",
    "    # プロット\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for level, color in [(\"High\", COLOR_HIGH), (\"Low\", COLOR_LOW)]:\n",
    "        sub = agg[agg[group_col] == level].sort_values(\"t_min\")\n",
    "        if sub.empty:\n",
    "            print(f\"[Cell1A] 注意: {group_col}={level} のデータがありません。\")\n",
    "            continue\n",
    "\n",
    "        t = sub[\"t_min\"].values  # [分] 単位\n",
    "        m = sub[\"mean\"].values\n",
    "        s = sub[\"std\"].values\n",
    "\n",
    "        # 平均（太線）\n",
    "        ax.plot(t, m, label=f\"{level} (mean)\", linewidth=LW * 2.0, color=color)\n",
    "        # ±1 SD バンド\n",
    "        ax.fill_between(t, m - s, m + s, alpha=0.2, color=color, linewidth=0)\n",
    "\n",
    "    # ----- 軸設定 -----\n",
    "\n",
    "    # 横軸：0〜10分（または ML 窓長に応じて自動）\n",
    "    duration_min = (ML_END - ML_START) / 60.0\n",
    "    # 安全側で 0〜duration_min、整数目盛（0,1,2,...）\n",
    "    max_tick = int(np.floor(duration_min))\n",
    "    xticks = np.arange(0, max_tick + 1, 1)\n",
    "    ax.set_xlim(0.0, duration_min)\n",
    "    ax.set_xticks(xticks)\n",
    "    # ラベルは整数表示（0,1,2,...）\n",
    "    ax.set_xlabel(\"Time [min]\", fontsize=FS_LABEL)\n",
    "\n",
    "    # 縦軸：FMS 0〜4、整数目盛\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_yticks([0, 1, 2, 3, 4])\n",
    "    ax.set_ylabel(\"FMS\", fontsize=FS_LABEL)\n",
    "\n",
    "    # タイトル\n",
    "    ax.set_title(f\"{title_prefix}別 FMS 推移\", fontsize=FS_TITLE)\n",
    "\n",
    "    # グリッド\n",
    "    ax.grid(True)\n",
    "\n",
    "    # 3分 と 6分30秒 に縦の点線\n",
    "    ax.axvline(3.0,  linestyle=\"--\", linewidth=LW, color=\"gray\")\n",
    "    ax.axvline(6.5,  linestyle=\"--\", linewidth=LW, color=\"gray\")\n",
    "\n",
    "    # 目盛フォント\n",
    "    ax.tick_params(axis=\"both\", labelsize=FS_TICK)\n",
    "\n",
    "    # 凡例\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_path = os.path.join(FMS_PLOT_DIR, save_name)\n",
    "    fig.savefig(save_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[Cell1A] Saved FMS trajectory plot -> {save_path}\")\n",
    "\n",
    "\n",
    "# ---- 基準切り替え：MSSQ / VIMSSQ ----\n",
    "basis = GROUPING_BASIS_FOR_PLOTS.upper()\n",
    "\n",
    "if basis == \"MSSQ\":\n",
    "    group_col    = \"MSSQ_group\"\n",
    "    title_prefix = \"MSSQ group\"\n",
    "    save_name    = f\"FMS_MSSQ_group_E{EPOCH_LEN}s.png\"\n",
    "\n",
    "elif basis == \"VIMSSQ\":\n",
    "    group_col    = \"VIMSSQ_group\"\n",
    "    title_prefix = \"VIMSSQ group\"\n",
    "    save_name    = f\"FMS_VIMSSQ_group_E{EPOCH_LEN}s.png\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"[Cell1A] GROUPING_BASIS_FOR_PLOTS は 'MSSQ' か 'VIMSSQ' を指定してください \"\n",
    "        f\"（現在: {GROUPING_BASIS_FOR_PLOTS}）\"\n",
    "    )\n",
    "\n",
    "print(f\"[Cell1A] GROUPING_BASIS_FOR_PLOTS = {GROUPING_BASIS_FOR_PLOTS} で FMS 推移を描画します。\")\n",
    "\n",
    "_plot_fms_by_group_min_axis(\n",
    "    group_col=group_col,\n",
    "    title_prefix=title_prefix,\n",
    "    save_name=save_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45007fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2: モデリング共通ヘルパ（fit / SHAP / 評価）=====\n",
    "set_cell_output(2)\n",
    "\n",
    "\n",
    "\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 学習ラッパー（Cell0のレジストリAPIを利用）\n",
    "# --------------------------------------------\n",
    "def fit_classifier(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cell0 の fit_estimator を直接包む薄いラッパ。\n",
    "    - SHAP/評価セルから backend を差し替えたい場合のみ backend / overrides を指定する。\n",
    "    \"\"\"\n",
    "    if \"fit_estimator\" not in globals():\n",
    "        raise RuntimeError(\"[Cell2] fit_estimator が未定義です。Cell0 を先に実行してください。\")\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    return fit_estimator(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        backend=backend,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# TreeSHAP ベースの特徴重要度算出\n",
    "# --------------------------------------------\n",
    "def compute_train_shap_abs_mean(model, X_ref: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    学習データ X_ref 上での平均絶対SHAP値（降順）。\n",
    "    - XGB/RF 等の木モデルを想定（TreeSHAP）。\n",
    "    - SVM など非対応モデルでは ValueError を送出する。\n",
    "    \"\"\"\n",
    "    X_ref = X_ref.astype(np.float32, copy=False)\n",
    "\n",
    "    # 背景データ（最大128行）\n",
    "    bg_n = min(128, len(X_ref))\n",
    "    X_bg = X_ref.sample(n=bg_n, random_state=SEED_BASE) if bg_n >= 2 else X_ref\n",
    "\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            data=X_bg,\n",
    "            model_output=\"probability\",\n",
    "            feature_perturbation=\"interventional\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "    except Exception:\n",
    "        # probability指定が非対応な場合に raw へフォールバック\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            model_output=\"raw\",\n",
    "            feature_perturbation=\"tree_path_dependent\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "\n",
    "    # shap_values の戻り値形状を統一（2D: n_samples × n_features）\n",
    "    classes = getattr(model, \"classes_\", None)\n",
    "    pos_idx = int(np.where(classes == 1)[0][0]) if classes is not None and 1 in list(classes) else -1\n",
    "\n",
    "    if isinstance(sv_any, list):\n",
    "        sv = sv_any[pos_idx]\n",
    "    else:\n",
    "        sv = getattr(sv_any, \"values\", sv_any)\n",
    "        sv = np.asarray(sv)\n",
    "        if sv.ndim == 3:\n",
    "            sv = sv[..., pos_idx]\n",
    "        elif sv.ndim == 1:\n",
    "            sv = sv.reshape(-1, 1)\n",
    "\n",
    "    if sv.shape[1] != X_ref.shape[1]:\n",
    "        raise RuntimeError(\n",
    "            f\"[Cell2] SHAP shape mismatch: sv.shape={sv.shape}, X_ref.shape={X_ref.shape}\"\n",
    "        )\n",
    "\n",
    "    abs_mean = np.mean(np.abs(sv), axis=0)\n",
    "    return pd.Series(abs_mean, index=X_ref.columns, name=\"mean_abs\").sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 評価ユーティリティ\n",
    "# --------------------------------------------\n",
    "def _is_probability_like(scores: np.ndarray) -> bool:\n",
    "    return np.isfinite(scores).all() and 0.0 <= scores.min() and scores.max() <= 1.0\n",
    "\n",
    "\n",
    "def evaluate_fold(model, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - ROC AUC: 2クラス時のみ。\n",
    "    - Accuracy: 確率なら 0.5、スコアなら 0.0 を閾値とする（詳細な最適化は別セル）。\n",
    "    \"\"\"\n",
    "    X_test = X_test.astype(np.float32, copy=False)\n",
    "    scores = predict_positive_score(model, X_test)\n",
    "\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, scores)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "\n",
    "    thr = 0.5 if _is_probability_like(scores) else 0.0\n",
    "    pred = (scores >= thr).astype(int)\n",
    "    acc = accuracy_score(y_test.astype(int), pred)\n",
    "\n",
    "    return {\"roc_auc\": float(roc_auc), \"accuracy\": float(acc)}\n",
    "\n",
    "\n",
    "print(\"[Cell2] Modeling helpers ready (fit_classifier / compute_train_shap_abs_mean / evaluate_fold)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab12b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3: 時系列特徴量変換（固定幅 H エポック）＋統計ログ =====\n",
    "set_cell_output(3)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 必須オブジェクトの存在チェック\n",
    "required = [\n",
    "    \"df_ml_epoch\",\n",
    "    \"feature_cols_all\",\n",
    "    \"trait_cols_to_use\",\n",
    "    \"HISTORY_N_EPOCHS\",\n",
    "    \"EPOCH_LEN\",\n",
    "    \"outpath\",\n",
    "]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3] 未定義の変数があります: {missing}\")\n",
    "\n",
    "H = int(HISTORY_N_EPOCHS)\n",
    "if H < 1:\n",
    "    raise ValueError(\"[Cell3] HISTORY_N_EPOCHS は 1 以上である必要があります。\")\n",
    "\n",
    "print(f\"[Cell3] 時系列特徴量変換を開始: HISTORY_N_EPOCHS={H}, EPOCH_LEN={EPOCH_LEN}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ① 履歴導入「前」のラベル分布を再計算（保険）\n",
    "#    ※ Cell1でも保存しているが、ここでも再作成しておく\n",
    "# --------------------------------------------\n",
    "label_before = (\n",
    "    df_ml_epoch\n",
    "    .groupby(\"subject_id\")[\"label\"]\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "for val in (0, 1):\n",
    "    if val not in label_before.columns:\n",
    "        label_before[val] = 0\n",
    "label_before = label_before[[0, 1]].rename(columns={0: \"neg_before\", 1: \"pos_before\"})\n",
    "\n",
    "print(\"[Cell3] Label distribution BEFORE history (recomputed):\")\n",
    "print(label_before)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ② HISTORY_N_EPOCHS == 1 の場合は変換せず、そのまま利用\n",
    "# --------------------------------------------\n",
    "if H == 1:\n",
    "    print(\"[Cell3] HISTORY_N_EPOCHS == 1 のため、履歴連結は行わず df_ml_epoch をそのまま使用します。\")\n",
    "\n",
    "    df_ml_ts = df_ml_epoch.copy()\n",
    "\n",
    "    # サンプル統計（履歴導入後＝beforeと同じ）\n",
    "    per_subject_stats = []\n",
    "    for sid, g in df_ml_epoch.groupby(\"subject_id\", sort=False):\n",
    "        n_raw = len(g)\n",
    "        pos_after = int((g[\"label\"] == 1).sum())\n",
    "        neg_after = int((g[\"label\"] == 0).sum())\n",
    "        per_subject_stats.append({\n",
    "            \"subject_id\": sid,\n",
    "            \"n_raw\": n_raw,\n",
    "            \"n_ts\": n_raw,\n",
    "            \"pos_after\": pos_after,\n",
    "            \"neg_after\": neg_after,\n",
    "        })\n",
    "\n",
    "    subject_stats_df = pd.DataFrame(per_subject_stats).set_index(\"subject_id\")\n",
    "    subject_stats_df = subject_stats_df.join(label_before, how=\"left\")\n",
    "\n",
    "    # 比率なども一応追加\n",
    "    subject_stats_df[\"ratio_n_ts\"] = subject_stats_df[\"n_ts\"] / subject_stats_df[\"n_raw\"].replace(0, np.nan)\n",
    "\n",
    "    STATS_PATH = outpath(\"SUBJECT_TS_STATS.csv\")\n",
    "    subject_stats_df.to_csv(STATS_PATH, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3] SUBJECT_TS_STATS saved -> {STATS_PATH}\")\n",
    "    print(subject_stats_df)\n",
    "\n",
    "    # 特徴量リストは Cell1 での feature_cols_full をそのまま使用\n",
    "    if \"feature_cols_full\" not in globals():\n",
    "        raise RuntimeError(\"[Cell3] feature_cols_full が未定義です。Cell1 を先に実行してください。\")\n",
    "\n",
    "    ts_feature_cols = feature_cols_full\n",
    "\n",
    "    # グローバル行列を設定（従来仕様）\n",
    "    X_all = df_ml_ts[ts_feature_cols].astype(float)\n",
    "    y_all = df_ml_ts[\"label\"].astype(int)\n",
    "    groups = df_ml_ts[\"subject_id\"].copy()\n",
    "\n",
    "    df_ml_ts.to_csv(outpath(f\"ML_DATA_TS_{EPOCH_LEN}S_H{H}.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3] df_ml_ts saved -> {outpath(f'ML_DATA_TS_{EPOCH_LEN}S_H{H}.CSV')}\")\n",
    "    print(f\"[Cell3] Matrices ready (H=1): X_all={X_all.shape}, y_all={y_all.shape}\")\n",
    "else:\n",
    "    # --------------------------------------------\n",
    "    # ③ HISTORY_N_EPOCHS >= 2: 固定幅 H エポック履歴に変換\n",
    "    # --------------------------------------------\n",
    "    # 物理特徴量部分の lag付き列名\n",
    "    physio_lag_cols = [\n",
    "        f\"{col}_lag{lag}\"\n",
    "        for lag in range(H - 1, -1, -1)  # 例: H=3 -> lag2, lag1, lag0\n",
    "        for col in feature_cols_all\n",
    "    ]\n",
    "    ts_feature_cols = physio_lag_cols + trait_cols_to_use\n",
    "\n",
    "    print(f\"[Cell3] physio_lag_cols: {len(physio_lag_cols)} 列, traits: {trait_cols_to_use}\")\n",
    "\n",
    "    df_ts_list = []\n",
    "    per_subject_stats = []\n",
    "\n",
    "    for sid, g in df_ml_epoch.groupby(\"subject_id\", sort=False):\n",
    "        g = g.sort_values(\"epoch_start\").reset_index(drop=True)\n",
    "        n_raw = len(g)\n",
    "\n",
    "        rows_ts = []\n",
    "\n",
    "        # i: 履歴ウィンドウの末尾インデックス\n",
    "        for i in range(H - 1, n_raw):\n",
    "            block = g.iloc[i - H + 1: i + 1]\n",
    "\n",
    "            # epoch_start が EPOCH_LEN 刻みで連続しているか確認\n",
    "            starts = block[\"epoch_start\"].to_numpy()\n",
    "            diffs = np.diff(starts)\n",
    "            if not np.all(diffs == EPOCH_LEN):\n",
    "                # 欠損をまたぐウィンドウは使わない\n",
    "                continue\n",
    "\n",
    "            # 直近Hエポック分の生理特徴量を連結\n",
    "            features_seq = []\n",
    "            for row_idx in range(i - H + 1, i + 1):\n",
    "                features_seq.append(\n",
    "                    g.loc[row_idx, feature_cols_all].to_numpy(dtype=float)\n",
    "                )\n",
    "            features_concat = np.concatenate(features_seq, axis=0)\n",
    "\n",
    "            if features_concat.shape[0] != len(physio_lag_cols):\n",
    "                raise RuntimeError(\n",
    "                    f\"[Cell3] features_concat length mismatch: \"\n",
    "                    f\"{features_concat.shape[0]} vs {len(physio_lag_cols)}\"\n",
    "                )\n",
    "\n",
    "            row = {\n",
    "                \"subject_id\": g.loc[i, \"subject_id\"],\n",
    "                \"epoch_start\": int(g.loc[i, \"epoch_start\"]),\n",
    "                \"epoch_end\": int(g.loc[i, \"epoch_end\"]),\n",
    "                \"FMS\": float(g.loc[i, \"FMS\"]),\n",
    "                \"label\": int(g.loc[i, \"label\"]),\n",
    "            }\n",
    "\n",
    "            # lag付き物理特徴\n",
    "            for c_idx, col_name in enumerate(physio_lag_cols):\n",
    "                row[col_name] = float(features_concat[c_idx])\n",
    "\n",
    "            # trait（MSSQ/VIMSSQ）はlagなしでそのまま\n",
    "            for tcol in trait_cols_to_use:\n",
    "                row[tcol] = float(g.loc[i, tcol])\n",
    "\n",
    "            rows_ts.append(row)\n",
    "\n",
    "        df_sub_ts = pd.DataFrame(rows_ts)\n",
    "        n_ts = len(df_sub_ts)\n",
    "\n",
    "        if n_ts > 0:\n",
    "            pos_after = int((df_sub_ts[\"label\"] == 1).sum())\n",
    "            neg_after = int((df_sub_ts[\"label\"] == 0).sum())\n",
    "        else:\n",
    "            pos_after = 0\n",
    "            neg_after = 0\n",
    "\n",
    "        per_subject_stats.append({\n",
    "            \"subject_id\": sid,\n",
    "            \"n_raw\": n_raw,\n",
    "            \"n_ts\": n_ts,\n",
    "            \"pos_after\": pos_after,\n",
    "            \"neg_after\": neg_after,\n",
    "        })\n",
    "\n",
    "        df_ts_list.append(df_sub_ts)\n",
    "\n",
    "        print(f\"[Cell3] subject {sid}: n_raw={n_raw}, n_ts={n_ts}, \"\n",
    "              f\"pos_after={pos_after}, neg_after={neg_after}\")\n",
    "\n",
    "    # 被験者ごと時系列ウィンドウを縦結合\n",
    "    df_ml_ts = pd.concat(df_ts_list, ignore_index=True) if df_ts_list else pd.DataFrame()\n",
    "\n",
    "    print(f\"[Cell3] df_ml_ts shape={df_ml_ts.shape}\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # ④ サンプル数・ラベル分布統計（before/after）を集計\n",
    "    # --------------------------------------------\n",
    "    subject_stats_df = pd.DataFrame(per_subject_stats).set_index(\"subject_id\")\n",
    "    subject_stats_df = subject_stats_df.join(label_before, how=\"left\")\n",
    "\n",
    "    # 比率などを追加\n",
    "    subject_stats_df[\"ratio_n_ts\"] = subject_stats_df[\"n_ts\"] / subject_stats_df[\"n_raw\"].replace(0, np.nan)\n",
    "\n",
    "    STATS_PATH = outpath(\"SUBJECT_TS_STATS.csv\")\n",
    "    subject_stats_df.to_csv(STATS_PATH, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3] SUBJECT_TS_STATS saved -> {STATS_PATH}\")\n",
    "    print(subject_stats_df)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # ⑤ グローバル行列 X_all / y_all / groups を履歴版に差し替え\n",
    "    # --------------------------------------------\n",
    "    if df_ml_ts.empty:\n",
    "        raise RuntimeError(\"[Cell3] df_ml_ts が空です。履歴ウィンドウ条件が厳しすぎる可能性があります。\")\n",
    "\n",
    "    X_all = df_ml_ts[ts_feature_cols].astype(float)\n",
    "    y_all = df_ml_ts[\"label\"].astype(int)\n",
    "    groups = df_ml_ts[\"subject_id\"].copy()\n",
    "\n",
    "    df_ml_ts.to_csv(outpath(f\"ML_DATA_TS_{EPOCH_LEN}S_H{H}.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3] df_ml_ts saved -> {outpath(f'ML_DATA_TS_{EPOCH_LEN}S_H{H}.CSV')}\")\n",
    "    print(f\"[Cell3] Matrices ready (H={H}): X_all={X_all.shape}, y_all={y_all.shape}\")\n",
    "    print(f\"[Cell3] ts_feature_cols n={len(ts_feature_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6751dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A: 高相関特徴の事前除去（グループ単位） =====\n",
    "set_cell_output(3)\n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "required = [\"X_all\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-pre] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "MIN_VARIANCE = 1e-8\n",
    "FEATURE_LIST_PATH = outpath(\"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "# ---------- グループ名取得ヘルパ ----------\n",
    "def get_feature_group(col: str) -> str:\n",
    "    \"\"\"\n",
    "    ベース特徴名を返す:\n",
    "      - 'xxx_lag0', 'xxx_lag1', ... → 'xxx'\n",
    "      - それ以外（MSSQ, VIMSSQ 等）は列名そのまま\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "    return m.group(1) if m else col\n",
    "\n",
    "# ---------- 数値列だけ抽出 ----------\n",
    "X_num = X_all.select_dtypes(include=[np.number]).copy()\n",
    "if X_num.empty:\n",
    "    raise RuntimeError(\"[Cell3A-pre] 数値列がありません。\")\n",
    "\n",
    "# ---------- 列 → グループ / グループ → 列 ----------\n",
    "col_to_group: dict[str, str] = {}\n",
    "group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "\n",
    "for col in X_num.columns:\n",
    "    g = get_feature_group(col)\n",
    "    col_to_group[col] = g\n",
    "    group_to_cols[g].append(col)\n",
    "\n",
    "group_names_in_order: list[str] = []\n",
    "seen = set()\n",
    "for col in X_num.columns:\n",
    "    g = col_to_group[col]\n",
    "    if g not in seen:\n",
    "        group_names_in_order.append(g)\n",
    "        seen.add(g)\n",
    "\n",
    "# ---------- グループ代表系列（lagの平均） ----------\n",
    "X_group = pd.DataFrame(index=X_num.index)\n",
    "for g, cols in group_to_cols.items():\n",
    "    X_group[g] = X_num[cols].mean(axis=1)\n",
    "\n",
    "# ---------- 分散がほぼゼロのグループを除外 ----------\n",
    "var = X_group.var(axis=0, ddof=1).fillna(0.0)\n",
    "valid_groups = var[var > MIN_VARIANCE].index.tolist()\n",
    "if not valid_groups:\n",
    "    raise RuntimeError(\"[Cell3A-pre] 分散がほぼゼロのため使用可能なグループがありません。\")\n",
    "\n",
    "# 優先順は X_all の列順に従ってグループ順序を決定\n",
    "priority_groups = [g for g in group_names_in_order if g in valid_groups]\n",
    "X_use = X_group[priority_groups]\n",
    "\n",
    "# ---------- グループ代表同士の相関行列 ----------\n",
    "corr = X_use.corr(method=\"pearson\").abs()\n",
    "\n",
    "keep_groups: list[str] = []\n",
    "dropped_groups_detail: list[dict] = []\n",
    "\n",
    "for g in priority_groups:\n",
    "    conflict = None\n",
    "    for kept in keep_groups:\n",
    "        if corr.loc[g, kept] >= CORR_THRESHOLD:\n",
    "            conflict = kept\n",
    "            break\n",
    "    if conflict is None:\n",
    "        # まだどの kept とも高相関でない → 代表として残す\n",
    "        keep_groups.append(g)\n",
    "    else:\n",
    "        # すでに keep に入っている代表 (conflict) を残し、後から出てきた g を除去\n",
    "        dropped_groups_detail.append({\n",
    "            \"group\": g,\n",
    "            \"representative_group\": conflict,\n",
    "            \"abs_corr\": float(corr.loc[g, conflict]),\n",
    "            \"dropped_columns\": group_to_cols[g],\n",
    "            \"representative_columns\": group_to_cols[conflict],\n",
    "        })\n",
    "\n",
    "# ---------- グループ→列 への展開 ----------\n",
    "keep_columns = [c for c in X_num.columns if col_to_group[c] in keep_groups]\n",
    "dropped_columns = [c for c in X_num.columns if col_to_group[c] not in keep_groups]\n",
    "\n",
    "payload = {\n",
    "    # 既存セル互換：ここは「残す列名」のリスト\n",
    "    \"keep\": keep_columns,\n",
    "\n",
    "    # 追加情報：グループ単位の情報\n",
    "    \"keep_groups\": keep_groups,\n",
    "    \"dropped_groups\": dropped_groups_detail,\n",
    "    \"threshold\": CORR_THRESHOLD,\n",
    "    \"total_groups\": len(priority_groups),\n",
    "    \"total_columns\": len(X_num.columns),\n",
    "}\n",
    "with open(FEATURE_LIST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[Cell3A-pre] group_keep={len(keep_groups)} / total_groups={len(priority_groups)}, \"\n",
    "      f\"drop_groups={len(dropped_groups_detail)}\")\n",
    "print(f\"[Cell3A-pre] keep_columns={len(keep_columns)} / total_columns={len(X_num.columns)}\")\n",
    "print(f\"[Cell3A-pre] JSON -> {FEATURE_LIST_PATH}\")\n",
    "\n",
    "# ---- ログ出力 ----\n",
    "if dropped_groups_detail:\n",
    "    print(f\"[Cell3A-pre] |r| >= {CORR_THRESHOLD:.2f} のグループペア（後から出てきた方を DROP）：\")\n",
    "    for d in dropped_groups_detail:\n",
    "        print(\n",
    "            f\"  [KEEP-G] {d['representative_group']}  \"\n",
    "            f\"[DROP-G] {d['group']}  \"\n",
    "            f\"(abs_corr={d['abs_corr']:.3f})  \"\n",
    "            f\"cols_keep={len(d['representative_columns'])}, \"\n",
    "            f\"cols_drop={len(d['dropped_columns'])}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"[Cell3A-pre] 高相関によるグループ除去はありませんでした。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d60e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3B-mean: SHAP値平均（LOSO, TreeSHAP・グループ単位） =====\n",
    "set_cell_output(3)\n",
    "\n",
    "\n",
    "RUN_CELL_3A_SHAP = USE_FS_SHAP_MEAN  # デフォルトはTrue\n",
    "\n",
    "if RUN_CELL_3A_SHAP:\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    import json\n",
    "    import os\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import matplotlib.backends\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    required = [\n",
    "        \"X_all\", \"y_all\", \"groups\",\n",
    "        \"fit_classifier\", \"evaluate_fold\",\n",
    "        \"outpath\", \"compute_train_shap_abs_mean\"\n",
    "    ]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-SHAP-RANK] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    # ---------- グループ名取得ヘルパ ----------\n",
    "    def get_feature_group(col: str) -> str:\n",
    "        m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "        return m.group(1) if m else col\n",
    "\n",
    "    # TreeSHAP のバックエンド（木モデル）\n",
    "    SHAP_BACKEND = \"xgb\"\n",
    "\n",
    "    FEATURE_LIST_PATH = cell_output_path(3, \"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "    # --- 特徴量プールの決定（相関事前除去の結果があれば利用） ---\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        # 事前除去で KEEP された列だけ使う\n",
    "        keep_cols = keep_payload.get(\"keep\", [])\n",
    "        feature_pool = [c for c in keep_cols if c in X_all.columns]\n",
    "        print(f\"[Cell3A-SHAP-RANK] correlation-pruned columns loaded ({len(feature_pool)} cols)\")\n",
    "    else:\n",
    "        feature_pool = list(X_all.columns)\n",
    "        print(\"[Cell3A-SHAP-RANK] correlation-pruned list not found. Using all columns.\")\n",
    "\n",
    "    if not feature_pool:\n",
    "        raise RuntimeError(\"[Cell3A-SHAP-RANK] feature_pool が空です。Cell3A-pre の結果を確認してください。\")\n",
    "\n",
    "    X_source = X_all[feature_pool].copy()\n",
    "\n",
    "    # 列→グループ / グループ→列\n",
    "    col_to_group: dict[str, str] = {}\n",
    "    group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "    for col in X_source.columns:\n",
    "        g = get_feature_group(col)\n",
    "        col_to_group[col] = g\n",
    "        group_to_cols[g].append(col)\n",
    "\n",
    "    group_names = list(group_to_cols.keys())\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    # ★列単位: foldごとの mean(|SHAP|) を保持\n",
    "    col_shap_frames = []\n",
    "    # ★グループ単位: foldごとの mean(|SHAP|) を保持\n",
    "    group_shap_frames = []\n",
    "    # foldごとの性能\n",
    "    metrics_rows = []\n",
    "\n",
    "    # --- LOSO ループ ---\n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_source, y_all, groups), start=1):\n",
    "        X_tr = X_source.iloc[tr_idx].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_source.iloc[te_idx].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(f\"[Cell3A-SHAP-RANK] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "        # --- 木モデルで学習（TreeSHAP 対応） ---\n",
    "        model = fit_classifier(X_tr, y_tr, backend=SHAP_BACKEND)\n",
    "\n",
    "        # --- 列単位の mean(|SHAP|) を計算 ---\n",
    "        shap_mean_col = compute_train_shap_abs_mean(model, X_tr)\n",
    "        # 全列順に並べ直し（fold間でインデックス整合のため）\n",
    "        shap_mean_col = shap_mean_col.reindex(X_tr.columns)\n",
    "\n",
    "        # ★列単位: mean(|SHAP|) をそのまま保存（順位は後でまとめて計算）\n",
    "        shap_mean_col.name = f\"fold{fold_id}\"\n",
    "        col_shap_frames.append(shap_mean_col)\n",
    "\n",
    "        # --- グループ単位への集約（mean_abs を「合計」） ---\n",
    "        group_importance: dict[str, float] = defaultdict(float)\n",
    "        for col, val in shap_mean_col.items():\n",
    "            g = col_to_group[col]\n",
    "            group_importance[g] += float(val)\n",
    "\n",
    "        shap_mean_group = pd.Series(group_importance)\n",
    "\n",
    "        # ★グループ単位: mean(|SHAP|) を保存\n",
    "        shap_mean_group.name = f\"fold{fold_id}\"\n",
    "        group_shap_frames.append(shap_mean_group)\n",
    "\n",
    "        # --- Fold ごとの性能評価 ---\n",
    "        metrics = evaluate_fold(model, X_te, y_te)\n",
    "        metrics.update({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "        })\n",
    "        metrics_rows.append(metrics)\n",
    "\n",
    "        preview_groups = shap_mean_group.sort_values(ascending=False).head(5).index.tolist()\n",
    "        print(f\"[Cell3A-SHAP-RANK] fold{fold_id}: ranked groups={len(shap_mean_group)} (top5 groups={preview_groups})\")\n",
    "\n",
    "    # ---------- 列単位ランキング（純粋な |SHAP| 平均ベース） ----------\n",
    "    # 各列×各fold の mean(|SHAP|) テーブル\n",
    "    col_shap_df = pd.concat(col_shap_frames, axis=1)\n",
    "\n",
    "    # ここで fold 平均の mean(|SHAP|) を計算\n",
    "    col_rank_df = col_shap_df.copy()\n",
    "    col_rank_df[\"shap_mean\"] = col_shap_df.mean(axis=1)\n",
    "    col_rank_df[\"shap_median\"] = col_shap_df.median(axis=1)\n",
    "\n",
    "    # ★重要度は shap_mean が大きいほど高いので，降順にランク付け\n",
    "    #   rank_mean は「fold平均 |SHAP| に基づく総合順位（1が最重要）」とする\n",
    "    col_rank_df[\"rank_mean\"] = col_rank_df[\"shap_mean\"].rank(\n",
    "        ascending=False, method=\"min\"\n",
    "    ).astype(int)\n",
    "\n",
    "    # shap_mean の順位でソート\n",
    "    col_rank_df = col_rank_df.sort_values(\"rank_mean\")\n",
    "\n",
    "    rank_path_cols = outpath(\"SHAP_MEAN_FEATURE_RANKING.CSV\")\n",
    "    col_rank_df.to_csv(rank_path_cols, encoding=\"utf-8-sig\")\n",
    "    col_rank_df.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] 列単位ランキング saved -> {rank_path_cols}\")\n",
    "\n",
    "    # ---------- グループ単位ランキング（純粋な |SHAP| 平均ベース・メイン） ----------\n",
    "    group_shap_df = pd.concat(group_shap_frames, axis=1)\n",
    "    group_rank_df = group_shap_df.copy()\n",
    "    group_rank_df[\"shap_mean\"] = group_shap_df.mean(axis=1)\n",
    "    group_rank_df[\"shap_median\"] = group_shap_df.median(axis=1)\n",
    "\n",
    "    # ★グループも shap_mean の大きさで順位付け\n",
    "    group_rank_df[\"rank_mean\"] = group_rank_df[\"shap_mean\"].rank(\n",
    "        ascending=False, method=\"min\"\n",
    "    ).astype(int)\n",
    "\n",
    "    group_rank_df = group_rank_df.sort_values(\"rank_mean\")\n",
    "\n",
    "    rank_path_groups = outpath(\"SHAP_MEAN_GROUP_RANKING.CSV\")\n",
    "    group_rank_df.to_csv(rank_path_groups, encoding=\"utf-8-sig\")\n",
    "    group_rank_df.to_csv(outpath(\"SHAP_GROUP_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] グループ単位ランキング saved -> {rank_path_groups}\")\n",
    "\n",
    "    # ---------- Foldごとの性能 ----------\n",
    "    metrics_path = outpath(\"LOSO_METRICS.CSV\")\n",
    "    pd.DataFrame(metrics_rows).to_csv(metrics_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-SHAP-RANK] LOSO metrics saved -> {metrics_path}\")\n",
    "\n",
    "    # ---------- グループランキングの可視化 ----------\n",
    "    TOP_K = 8\n",
    "\n",
    "    # 全グループ（★rank ではなく shap_mean を描画）\n",
    "    plt.figure(figsize=(10, max(5, len(group_rank_df)//3)))\n",
    "    plt.barh(group_rank_df.index[::-1], group_rank_df[\"shap_mean\"][::-1])\n",
    "    plt.xlabel(\"Mean |SHAP| over folds\")\n",
    "    plt.ylabel(\"Feature group\")\n",
    "    plt.title(\"SHAP-based Feature Group Importance (All)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_MEAN_GROUP_RANKING_ALL.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 上位 TOP_K グループ\n",
    "    topk = group_rank_df.head(TOP_K).iloc[::-1]\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = plt.gca()\n",
    "    ax.barh(topk.index, topk[\"shap_mean\"])\n",
    "    ax.set_xlabel(\"Mean |SHAP| over folds\")\n",
    "    ax.set_ylabel(\"Feature group\")\n",
    "    ax.set_title(f\"Top-{TOP_K} SHAP-based Feature Group Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_MEAN_GROUP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] 図を保存 -> \"\n",
    "          f\"{outpath('SHAP_MEAN_GROUP_RANKING_ALL.PNG')} / {outpath('SHAP_MEAN_GROUP_TOP8_RANKING.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a061c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3B-rank: SHAP順位平均（LOSO, TreeSHAP・グループ単位） =====\n",
    "set_cell_output(3)\n",
    "\n",
    "\n",
    "RUN_CELL_3A_SHAP = USE_FS_SHAP_RANK  # デフォルトはTrue\n",
    "\n",
    "if RUN_CELL_3A_SHAP:\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    import json\n",
    "    import os\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import matplotlib.backends\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    required = [\n",
    "        \"X_all\", \"y_all\", \"groups\",\n",
    "        \"fit_classifier\", \"evaluate_fold\",\n",
    "        \"outpath\", \"compute_train_shap_abs_mean\"\n",
    "    ]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-SHAP-RANK] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    # ---------- グループ名取得ヘルパ ----------\n",
    "    def get_feature_group(col: str) -> str:\n",
    "        m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "        return m.group(1) if m else col\n",
    "\n",
    "    # TreeSHAP のバックエンド（木モデル）\n",
    "    SHAP_BACKEND = \"xgb\"\n",
    "\n",
    "    FEATURE_LIST_PATH = cell_output_path(3, \"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "    # --- 特徴量プールの決定（相関事前除去の結果があれば利用） ---\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        # 事前除去で KEEP された列だけ使う\n",
    "        keep_cols = keep_payload.get(\"keep\", [])\n",
    "        feature_pool = [c for c in keep_cols if c in X_all.columns]\n",
    "        print(f\"[Cell3A-SHAP-RANK] correlation-pruned columns loaded ({len(feature_pool)} cols)\")\n",
    "    else:\n",
    "        feature_pool = list(X_all.columns)\n",
    "        print(\"[Cell3A-SHAP-RANK] correlation-pruned list not found. Using all columns.\")\n",
    "\n",
    "    if not feature_pool:\n",
    "        raise RuntimeError(\"[Cell3A-SHAP-RANK] feature_pool が空です。Cell3A-pre の結果を確認してください。\")\n",
    "\n",
    "    X_source = X_all[feature_pool].copy()\n",
    "\n",
    "    # 列→グループ / グループ→列\n",
    "    col_to_group: dict[str, str] = {}\n",
    "    group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "    for col in X_source.columns:\n",
    "        g = get_feature_group(col)\n",
    "        col_to_group[col] = g\n",
    "        group_to_cols[g].append(col)\n",
    "\n",
    "    group_names = list(group_to_cols.keys())\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    # 列単位の fold別ランキング（従来互換）\n",
    "    col_ranking_frames = []\n",
    "    # グループ単位の fold別ランキング（新仕様）\n",
    "    group_ranking_frames = []\n",
    "    # foldごとの性能\n",
    "    metrics_rows = []\n",
    "\n",
    "    # --- LOSO ループ ---\n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_source, y_all, groups), start=1):\n",
    "        X_tr = X_source.iloc[tr_idx].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_source.iloc[te_idx].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(f\"[Cell3A-SHAP-RANK] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "        # --- 木モデルで学習（TreeSHAP 対応） ---\n",
    "        model = fit_classifier(X_tr, y_tr, backend=SHAP_BACKEND)\n",
    "\n",
    "        # --- 列単位の mean(|SHAP|) を計算 ---\n",
    "        shap_mean_col = compute_train_shap_abs_mean(model, X_tr)\n",
    "        # 全列順に並べ直し（fold間でインデックス整合のため）\n",
    "        shap_mean_col = shap_mean_col.reindex(X_tr.columns)\n",
    "\n",
    "        # --- 列単位の順位（互換用） ---\n",
    "        col_ranks = shap_mean_col.rank(ascending=False, method=\"min\").astype(int)\n",
    "        col_ranks.name = f\"fold{fold_id}\"\n",
    "        col_ranking_frames.append(col_ranks)\n",
    "\n",
    "        # --- グループ単位への集約（mean_abs を「合計」） ---\n",
    "        group_importance: dict[str, float] = defaultdict(float)\n",
    "        for col, val in shap_mean_col.items():\n",
    "            g = col_to_group[col]\n",
    "            group_importance[g] += float(val)\n",
    "\n",
    "        shap_mean_group = pd.Series(group_importance)\n",
    "        # グループ単位の順位\n",
    "        group_ranks = shap_mean_group.rank(ascending=False, method=\"min\").astype(int)\n",
    "        group_ranks.name = f\"fold{fold_id}\"\n",
    "        group_ranking_frames.append(group_ranks)\n",
    "\n",
    "        # --- Fold ごとの性能評価 ---\n",
    "        metrics = evaluate_fold(model, X_te, y_te)\n",
    "        metrics.update({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "        })\n",
    "        metrics_rows.append(metrics)\n",
    "\n",
    "        preview_groups = shap_mean_group.sort_values(ascending=False).head(5).index.tolist()\n",
    "        print(f\"[Cell3A-SHAP-RANK] fold{fold_id}: ranked groups={len(shap_mean_group)} (top5 groups={preview_groups})\")\n",
    "\n",
    "    # ---------- 列単位ランキング（互換用） ----------\n",
    "    col_rank_df = pd.concat(col_ranking_frames, axis=1)\n",
    "    col_rank_df[\"rank_mean\"] = col_rank_df.mean(axis=1)\n",
    "    col_rank_df[\"rank_median\"] = col_rank_df.median(axis=1)\n",
    "    col_rank_df = col_rank_df.sort_values(\"rank_mean\")\n",
    "\n",
    "    rank_path_cols = outpath(\"SHAP_RANK_FEATURE_RANKING.CSV\")\n",
    "    col_rank_df.to_csv(rank_path_cols, encoding=\"utf-8-sig\")\n",
    "    col_rank_df.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] 列単位ランキング saved -> {rank_path_cols}\")\n",
    "\n",
    "    # ---------- グループ単位ランキング（新仕様・メイン） ----------\n",
    "    group_rank_df = pd.concat(group_ranking_frames, axis=1)\n",
    "    group_rank_df[\"rank_mean\"] = group_rank_df.mean(axis=1)\n",
    "    group_rank_df[\"rank_median\"] = group_rank_df.median(axis=1)\n",
    "    group_rank_df = group_rank_df.sort_values(\"rank_mean\")\n",
    "\n",
    "    rank_path_groups = outpath(\"SHAP_RANK_GROUP_RANKING.CSV\")\n",
    "    group_rank_df.to_csv(rank_path_groups, encoding=\"utf-8-sig\")\n",
    "    group_rank_df.to_csv(outpath(\"SHAP_GROUP_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] グループ単位ランキング saved -> {rank_path_groups}\")\n",
    "\n",
    "    # ---------- Foldごとの性能 ----------\n",
    "    metrics_path = outpath(\"LOSO_METRICS.CSV\")\n",
    "    pd.DataFrame(metrics_rows).to_csv(metrics_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-SHAP-RANK] LOSO metrics saved -> {metrics_path}\")\n",
    "\n",
    "    # ---------- グループランキングの可視化 ----------\n",
    "    TOP_K = 8\n",
    "\n",
    "    # 全グループ\n",
    "    plt.figure(figsize=(10, max(5, len(group_rank_df)//3)))\n",
    "    plt.barh(group_rank_df.index[::-1], group_rank_df[\"rank_mean\"][::-1])\n",
    "    plt.xlabel(\"Average rank (lower=better)\")\n",
    "    plt.ylabel(\"Feature group\")\n",
    "    plt.title(\"SHAP-based Feature Group Ranking (All)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_RANK_GROUP_RANKING_ALL.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 上位 TOP_K グループ\n",
    "    topk = group_rank_df.head(TOP_K).iloc[::-1]\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = plt.gca()\n",
    "    ax.barh(topk.index, topk[\"rank_mean\"])\n",
    "    ax.set_xlabel(\"Average rank (lower=better)\")\n",
    "    ax.set_ylabel(\"Feature group\")\n",
    "    ax.set_title(f\"Top-{TOP_K} SHAP-based Feature Group Ranking\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_RANK_GROUP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] 図を保存 -> \"\n",
    "          f\"{outpath('SHAP_RANK_GROUP_RANKING_ALL.PNG')} / {outpath('SHAP_RANK_GROUP_TOP8_RANKING.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3B-RFE: RFE特徴量ランキング（LOSO, グループ単位） =====\n",
    "set_cell_output(3)\n",
    "\n",
    "\n",
    "\n",
    "RUN_CELL_3A_RFE = USE_FS_RFE  # デフォルトでは使わない\n",
    "\n",
    "if RUN_CELL_3A_RFE:\n",
    "\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    import json\n",
    "    import os\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import matplotlib.backends\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    required = [\"X_all\", \"y_all\", \"groups\", \"build_estimator\", \"fit_classifier\", \"evaluate_fold\", \"outpath\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-RFE] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    # ---------- グループ名取得ヘルパ ----------\n",
    "    def get_feature_group(col: str) -> str:\n",
    "        \"\"\"\n",
    "        ベース特徴名:\n",
    "          - 'xxx_lag0', 'xxx_lag1', ... → 'xxx'\n",
    "          - それ以外 → 列名そのまま\n",
    "        \"\"\"\n",
    "        m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "        return m.group(1) if m else col\n",
    "\n",
    "    RFE_BACKEND = \"xgb\"   # RFE では XGB 固定\n",
    "    RFE_STEP = 1          # 1本ずつ削除\n",
    "    RFE_MIN_FEATURES = 1  # 最低1列まで落としてフルランキングを得る\n",
    "\n",
    "    FEATURE_LIST_PATH = cell_output_path(3, \"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "    # --- 特徴量プールの決定（相関事前除去の結果があれば利用） ---\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        feature_pool = [c for c in keep_payload.get(\"keep\", []) if c in X_all.columns]\n",
    "        print(f\"[Cell3A-RFE] correlation-pruned columns loaded ({len(feature_pool)} cols)\")\n",
    "    else:\n",
    "        feature_pool = list(X_all.columns)\n",
    "        print(\"[Cell3A-RFE] correlation-pruned list not found. Using all columns.\")\n",
    "\n",
    "    if not feature_pool:\n",
    "        raise RuntimeError(\"[Cell3A-RFE] feature_pool が空です。Cell3A-pre の結果を確認してください。\")\n",
    "\n",
    "    X_source = X_all[feature_pool].copy()\n",
    "\n",
    "    # 列→グループ / グループ→列\n",
    "    col_to_group: dict[str, str] = {}\n",
    "    group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "    for col in X_source.columns:\n",
    "        g = get_feature_group(col)\n",
    "        col_to_group[col] = g\n",
    "        group_to_cols[g].append(col)\n",
    "\n",
    "    group_names = list(group_to_cols.keys())\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    # 列単位RFEランキング（参考用）\n",
    "    col_ranking_frames = []\n",
    "    # グループ単位RFEランキング（メイン）\n",
    "    group_ranking_frames = []\n",
    "    # Foldごとの性能指標\n",
    "    metrics_rows = []\n",
    "\n",
    "    # --- LOSO ループ ---\n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_source, y_all, groups), start=1):\n",
    "        X_tr = X_source.iloc[tr_idx].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_source.iloc[te_idx].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(f\"[Cell3A-RFE] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "        # 元の実装と同じく「雛形モデル」を作る\n",
    "        base_estimator = build_estimator(backend=RFE_BACKEND)\n",
    "\n",
    "        selector = RFE(\n",
    "            estimator=base_estimator,\n",
    "            step=max(1, int(RFE_STEP)),\n",
    "            n_features_to_select=max(1, int(RFE_MIN_FEATURES)),\n",
    "        )\n",
    "        selector.fit(X_tr, y_tr)\n",
    "\n",
    "        # --- 列単位のRFE順位（1=最重要） ---\n",
    "        ranks_col = pd.Series(selector.ranking_, index=X_tr.columns, name=f\"fold{fold_id}\")\n",
    "        col_ranking_frames.append(ranks_col)\n",
    "\n",
    "        # --- 列順位 → グループ順位へ集約 ---\n",
    "        #   各グループに属する列の「最小rank」をそのグループのrankとする\n",
    "        group_rank_dict: dict[str, int] = {}\n",
    "        for col, r in ranks_col.items():\n",
    "            g = col_to_group[col]\n",
    "            if g not in group_rank_dict:\n",
    "                group_rank_dict[g] = int(r)\n",
    "            else:\n",
    "                group_rank_dict[g] = min(group_rank_dict[g], int(r))\n",
    "\n",
    "        group_ranks = pd.Series(group_rank_dict, name=f\"fold{fold_id}\")\n",
    "        group_ranking_frames.append(group_ranks)\n",
    "\n",
    "        # --- Foldごとの性能評価 ---\n",
    "        # ここがバグっていたので修正：\n",
    "        #   selector で「どの列を残すか」だけ決めておいて、\n",
    "        #   選ばれた列だけで改めてモデルを学習 → それを評価に使う\n",
    "        selected_cols = list(X_tr.columns[selector.support_])\n",
    "\n",
    "        # ★FIX: ここでちゃんと学習し直す\n",
    "        model = fit_classifier(\n",
    "            X_tr[selected_cols],\n",
    "            y_tr,\n",
    "            backend=RFE_BACKEND,\n",
    "        )\n",
    "\n",
    "        X_te_sel = X_te[selected_cols]\n",
    "        metrics = evaluate_fold(model, X_te_sel, y_te)\n",
    "        metrics.update({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "            \"n_selected_features\": len(selected_cols),\n",
    "        })\n",
    "        metrics_rows.append(metrics)\n",
    "\n",
    "        # ログ表示\n",
    "        preview_groups = group_ranks.sort_values().head(5).index.tolist()\n",
    "        print(f\"[Cell3A-RFE] fold{fold_id}: groups={len(group_ranks)}, \"\n",
    "              f\"top5_groups(by rank)={preview_groups}\")\n",
    "\n",
    "    # ---------- 列単位RFEランキング（参考・互換用） ----------\n",
    "    rfe_col_rank = pd.concat(col_ranking_frames, axis=1)\n",
    "    rfe_col_rank[\"rank_mean\"] = rfe_col_rank.mean(axis=1)\n",
    "    rfe_col_rank[\"rank_median\"] = rfe_col_rank.median(axis=1)\n",
    "    rfe_col_rank = rfe_col_rank.sort_values(\"rank_mean\")\n",
    "\n",
    "    col_rank_path = outpath(\"RFE_FEATURE_RANKING.CSV\")\n",
    "    rfe_col_rank.to_csv(col_rank_path, encoding=\"utf-8-sig\")\n",
    "    rfe_col_rank.to_csv(outpath(\"RFE_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-RFE] 列単位RFEランキング saved -> {col_rank_path}\")\n",
    "\n",
    "    # ---------- グループ単位RFEランキング（メイン） ----------\n",
    "    rfe_group_rank = pd.concat(group_ranking_frames, axis=1)\n",
    "    rfe_group_rank[\"rank_mean\"] = rfe_group_rank.mean(axis=1)\n",
    "    rfe_group_rank[\"rank_median\"] = rfe_group_rank.median(axis=1)\n",
    "    rfe_group_rank = rfe_group_rank.sort_values(\"rank_mean\")\n",
    "\n",
    "    group_rank_path = outpath(\"RFE_GROUP_RANKING.CSV\")\n",
    "    rfe_group_rank.to_csv(group_rank_path, encoding=\"utf-8-sig\")\n",
    "    rfe_group_rank.to_csv(outpath(\"RFE_GROUP_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-RFE] グループ単位RFEランキング saved -> {group_rank_path}\")\n",
    "\n",
    "    # ---------- LOSO性能ログ ----------\n",
    "    metrics_path = outpath(\"RFE_LOSO_METRICS.CSV\")\n",
    "    pd.DataFrame(metrics_rows).to_csv(metrics_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-RFE] LOSO metrics saved -> {metrics_path}\")\n",
    "\n",
    "    # ---------- グループRFEランキングの可視化 ----------\n",
    "    TOP_K = 8\n",
    "\n",
    "    plt.figure(figsize=(10, max(5, len(rfe_group_rank)//3)))\n",
    "    plt.barh(rfe_group_rank.index[::-1], rfe_group_rank[\"rank_mean\"][::-1])\n",
    "    plt.xlabel(\"Average RFE rank (lower=better)\")\n",
    "    plt.ylabel(\"Feature group\")\n",
    "    plt.title(\"RFE-based Feature Group Ranking (All)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"RFE_GROUP_RANKING_ALL.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    topk = rfe_group_rank.head(TOP_K).iloc[::-1]\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = plt.gca()\n",
    "    ax.barh(topk.index, topk[\"rank_mean\"])\n",
    "    ax.set_xlabel(\"Average RFE rank (lower=better)\")\n",
    "    ax.set_ylabel(\"Feature group\")\n",
    "    ax.set_title(f\"Top-{TOP_K} RFE-based Feature Group Ranking\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"RFE_GROUP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[Cell3A-RFE] 図を保存 -> \"\n",
    "          f\"{outpath('RFE_GROUP_RANKING_ALL.PNG')} / {outpath('RFE_GROUP_TOP8_RANKING.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4: Top-k ROC-AUC（ニュートラル） =====\n",
    "set_cell_output(4)\n",
    "\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "required = [\"X_all\", \"y_all\", \"groups\",\n",
    "            \"fit_classifier\", \"predict_positive_score\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell4-Subset] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "# ---------- グループ名取得ヘルパ（lag付き列を1つのグループにまとめる） ----------\n",
    "def get_feature_group(col: str) -> str:\n",
    "    \"\"\"\n",
    "    ベース特徴名:\n",
    "      - 'xxx_lag0', 'xxx_lag1', ... → 'xxx'\n",
    "      - それ以外 → 列名そのまま\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "    return m.group(1) if m else col\n",
    "\n",
    "# 列→グループ / グループ→列 の対応を作成\n",
    "col_to_group: dict[str, str] = {}\n",
    "group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "for col in X_all.columns:\n",
    "    g = get_feature_group(col)\n",
    "    col_to_group[col] = g\n",
    "    group_to_cols[g].append(col)\n",
    "\n",
    "print(f\"[Cell4-Subset] feature groups = {len(group_to_cols)} \"\n",
    "      f\"(columns={len(X_all.columns)})\")\n",
    "\n",
    "# ---------- SHAPランキングの読み込み（行＝グループまたは列） ----------\n",
    "rank_csv = cell_output_path(3, FEATURE_RANKING_FILE)\n",
    "if not os.path.exists(rank_csv):\n",
    "    raise FileNotFoundError(\"[Cell4-Subset] ランキングCSVがありません。選択した特徴量選定セルを実行してください。\")\n",
    "\n",
    "rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "\n",
    "# 「小さいほど重要」なスコアを作る（rank_mean 優先, なければ mean_abs に基づく）\n",
    "if \"rank_mean\" in rank_df.columns:\n",
    "    base_score = rank_df[\"rank_mean\"].astype(float)\n",
    "elif \"mean_abs\" in rank_df.columns:\n",
    "    # mean_abs が大きいほど重要 → 符号を反転して「小さいほど重要」にする\n",
    "    base_score = (-rank_df[\"mean_abs\"].astype(float))\n",
    "else:\n",
    "    # 何もなければ行順そのものをスコアにする（最初がもっとも重要）\n",
    "    base_score = pd.Series(\n",
    "        np.arange(len(rank_df), dtype=float),\n",
    "        index=rank_df.index,\n",
    "    )\n",
    "\n",
    "# ---------- 行（特徴orグループ）→ グループスコアへ集約 ----------\n",
    "group_score: dict[str, float] = {}\n",
    "for name, score in base_score.items():\n",
    "    g = get_feature_group(str(name))  # すでにグループ名ならそのまま\n",
    "    if g not in group_score:\n",
    "        group_score[g] = float(score)\n",
    "    else:\n",
    "        # より重要な列が1つでもあれば、そのグループはそのスコアを採用（min）\n",
    "        group_score[g] = min(group_score[g], float(score))\n",
    "\n",
    "group_rank = pd.Series(group_score, name=\"score\").sort_values(ascending=True)\n",
    "\n",
    "# 実際に X_all に存在するグループだけに絞る\n",
    "group_order = [g for g in group_rank.index if g in group_to_cols]\n",
    "if not group_order:\n",
    "    raise RuntimeError(\"[Cell4-Subset] ランキングに該当するグループが X_all に存在しません。\")\n",
    "\n",
    "total_groups = len(group_order)\n",
    "print(f\"[Cell4-Subset] Using SHAP group ranking ({total_groups} groups):\")\n",
    "print(group_order)\n",
    "\n",
    "# 旧コードとの互換用：TOP_SUBSET_K があればそれを使い，なければ total_groups で上書き\n",
    "TOP_SUBSET_K = int(globals().get(\"TOP_SUBSET_K\", 0))\n",
    "if TOP_SUBSET_K <= 0 or TOP_SUBSET_K > total_groups:\n",
    "    TOP_SUBSET_K = total_groups\n",
    "globals()[\"TOP_SUBSET_K\"] = TOP_SUBSET_K\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "results = []\n",
    "\n",
    "# ---- 上位kグループの累積セットで評価（k=1..total_groups）----\n",
    "for k in range(1, total_groups + 1):\n",
    "    use_groups = group_order[:k]\n",
    "\n",
    "    # この k グループに属する全ての列を集約\n",
    "    feats: list[str] = []\n",
    "    for g in use_groups:\n",
    "        feats.extend(group_to_cols[g])\n",
    "    # 念のため重複を除去（順序はグループ順→列順を維持）\n",
    "    seen = set()\n",
    "    feats_unique = []\n",
    "    for f in feats:\n",
    "        if f not in seen:\n",
    "            feats_unique.append(f)\n",
    "            seen.add(f)\n",
    "    feats = feats_unique\n",
    "\n",
    "    y_true_all = []\n",
    "    y_score_all = []\n",
    "\n",
    "    for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "        X_tr = X_all.iloc[tr_idx][feats].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_all.iloc[te_idx][feats].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        # 学習側が単一クラスならこのfoldはスキップ\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            continue\n",
    "\n",
    "        model = fit_classifier(X_tr, y_tr)\n",
    "        proba = predict_positive_score(model, X_te)\n",
    "\n",
    "        y_true_all.append(y_te)\n",
    "        y_score_all.append(proba)\n",
    "\n",
    "    if not y_true_all:\n",
    "        auc = float(\"nan\")\n",
    "    else:\n",
    "        y_true = np.concatenate(y_true_all)\n",
    "        y_score = np.concatenate(y_score_all)\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            auc = float(\"nan\")\n",
    "        else:\n",
    "            auc = float(roc_auc_score(y_true, y_score))\n",
    "\n",
    "    results.append({\n",
    "        \"size\": k,                 # 使用した「グループ数」k\n",
    "        \"n_features\": len(feats),  # 実際に使った列数\n",
    "        \"groups\": use_groups,      # 使用したグループ名\n",
    "        \"features\": feats,         # 使用した列名\n",
    "        \"auc\": auc,\n",
    "    })\n",
    "    print(f\"[Cell4-Subset] k(groups)={k}/{total_groups}, \"\n",
    "          f\"n_features={len(feats)}, AUC={auc:.4f}, groups={use_groups}\")\n",
    "\n",
    "# ---- 結果の整形・保存 ----\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(\"size\")  # k昇順\n",
    "\n",
    "results_df[\"groups_str\"] = results_df[\"groups\"].apply(lambda lst: \",\".join(lst))\n",
    "results_df[\"features_str\"] = results_df[\"features\"].apply(lambda lst: \",\".join(lst))\n",
    "\n",
    "# 1) このセル独自のファイル（ALLK_*）\n",
    "subset_csv_name_allk = \"ALLK_TOPORDER_AUC.csv\"\n",
    "subset_path_allk = outpath(subset_csv_name_allk)\n",
    "results_df[[\"size\", \"n_features\", \"groups_str\", \"features_str\", \"auc\"]].to_csv(\n",
    "    subset_path_allk, index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[Cell4-Subset] 保存 (ALLK, group-based) -> {subset_path_allk}\")\n",
    "\n",
    "# 最良のk（AUC最大、同点なら小さいk）を取得\n",
    "best_row = results_df.sort_values([\"auc\", \"size\"], ascending=[False, True]).iloc[0]\n",
    "print(f\"[Cell4-Subset] best k(groups)={int(best_row['size'])}, \"\n",
    "      f\"n_features={int(best_row['n_features'])}, \"\n",
    "      f\"auc={best_row['auc']:.4f}, \"\n",
    "      f\"groups={best_row['groups']}\")\n",
    "\n",
    "# 2) このセル独自の JSON（ALLK_SUBSET_BEST.json）\n",
    "best_json_name_allk = \"ALLK_SUBSET_BEST.json\"\n",
    "with open(outpath(best_json_name_allk), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        # 互換用：従来の \"size\" は「グループ数」として扱う\n",
    "        \"size\": int(best_row[\"size\"]),\n",
    "        \"n_features\": int(best_row[\"n_features\"]),\n",
    "        \"auc\": float(best_row[\"auc\"]),\n",
    "        \"groups\": best_row[\"groups\"],\n",
    "        \"features\": best_row[\"features\"],\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[Cell4-Subset] BEST (ALLK, group-based) -> {outpath(best_json_name_allk)}\")\n",
    "\n",
    "# 3) 旧「組合せ探索版」と同じ名前・形式でも保存（互換用）\n",
    "subset_csv_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_AUC.csv\"\n",
    "subset_path_compat = outpath(subset_csv_name_compat)\n",
    "results_df[[\"size\", \"n_features\", \"groups_str\", \"features_str\", \"auc\"]].to_csv(\n",
    "    subset_path_compat, index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[Cell4-Subset] 互換CSV (group-based) -> {subset_path_compat}\")\n",
    "\n",
    "best_json_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST.json\"\n",
    "with open(outpath(best_json_name_compat), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"size\": int(best_row[\"size\"]),             # グループ数\n",
    "        \"n_features\": int(best_row[\"n_features\"]), # 列数\n",
    "        \"auc\": float(best_row[\"auc\"]),\n",
    "        \"groups\": best_row[\"groups\"],\n",
    "        \"features\": best_row[\"features\"],\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[Cell4-Subset] 互換BEST JSON (group-based) -> {outpath(best_json_name_compat)}\")\n",
    "\n",
    "# グローバル変数も旧仕様に合わせて更新\n",
    "globals()[\"BEST_SUBSET_FEATURES\"] = best_row[\"features\"]   # 実際に使う列\n",
    "globals()[\"BEST_SUBSET_GROUPS\"] = best_row[\"groups\"]       # 追加：使ったグループ名\n",
    "globals()[\"BEST_SUBSET_K\"] = int(best_row[\"size\"])         # グループ数として解釈\n",
    "\n",
    "# ---- グラフ描画：横軸=グループ数k（右ほど少ない）, 縦軸=ROC-AUC ----\n",
    "FS_TITLE, FS_LABEL, FS_TICK = 30, 24, 20\n",
    "LW = 1.5\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(results_df[\"size\"], results_df[\"auc\"], marker=\"o\", linewidth=LW)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(\"Number of feature groups (k)\", fontsize=FS_LABEL)\n",
    "ax.set_ylabel(\"ROC-AUC (pooled LOSO)\", fontsize=FS_LABEL)\n",
    "ax.set_title(\"ROC-AUC vs. number of feature groups (all SHAP-ranked groups)\", fontsize=FS_TITLE)\n",
    "\n",
    "# 横軸：左が k = total_groups（全部）、右が k = 1（1グループだけ）\n",
    "ax.set_xlim(total_groups, 1)\n",
    "ax.set_xticks(range(1, total_groups + 1))\n",
    "\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "ax.tick_params(axis=\"x\", labelsize=FS_TICK)\n",
    "ax.tick_params(axis=\"y\", labelsize=FS_TICK)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"ALLK_TOPORDER_AUC.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"[Cell4-Subset] 図を保存 -> {outpath('ALLK_TOPORDER_AUC.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3C-Subset: Top-k 組合せ探索 =====\n",
    "set_cell_output(3)\n",
    "\n",
    "\n",
    "RUN_CELL_3A_Topk_FEATURE=False\n",
    "\n",
    "if RUN_CELL_3A_Topk_FEATURE:\n",
    "    import os\n",
    "    from itertools import combinations\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "    required = [\"X_all\", \"y_all\", \"groups\", \"fit_classifier\", \"predict_positive_score\", \"outpath\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-Subset] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    TOP_SUBSET_K = int(globals().get(\"TOP_SUBSET_K\", 15))\n",
    "    if TOP_SUBSET_K <= 0:\n",
    "        raise ValueError(\"TOP_SUBSET_K must be positive\")\n",
    "\n",
    "    rank_csv = cell_output_path(3, FEATURE_RANKING_FILE)\n",
    "    if not os.path.exists(rank_csv):\n",
    "        raise FileNotFoundError(\"[Cell3A-Subset] ランキングCSVがありません。選択した特徴量選定セルを実行してください。\")\n",
    "\n",
    "    rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "    if \"rank_mean\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "    elif \"mean_abs\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"mean_abs\", ascending=False).index.tolist()\n",
    "    else:\n",
    "        feature_order = list(rank_df.index)\n",
    "\n",
    "    feature_order = [f for f in feature_order if f in X_all.columns]\n",
    "    if not feature_order:\n",
    "        raise RuntimeError(\"[Cell3A-Subset] ランキングに該当する特徴が X_all に存在しません。\")\n",
    "\n",
    "    limit = min(TOP_SUBSET_K, len(feature_order))\n",
    "    top_features = feature_order[:limit]\n",
    "    print(f\"[Cell3A-Subset] Top features ({len(top_features)}): {top_features}\")\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    results = []\n",
    "\n",
    "    for r in range(1, len(top_features) + 1):\n",
    "        for comb in combinations(top_features, r):\n",
    "            feats = list(comb)\n",
    "            y_true_all = []\n",
    "            y_score_all = []\n",
    "            for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "                X_tr = X_all.iloc[tr_idx][feats].astype(np.float32)\n",
    "                y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "                X_te = X_all.iloc[te_idx][feats].astype(np.float32)\n",
    "                y_te = y_all.iloc[te_idx].astype(int)\n",
    "                if len(np.unique(y_tr)) < 2:\n",
    "                    continue\n",
    "                model = fit_classifier(X_tr, y_tr)\n",
    "                proba = predict_positive_score(model, X_te)\n",
    "                y_true_all.append(y_te)\n",
    "                y_score_all.append(proba)\n",
    "            if not y_true_all:\n",
    "                auc = float(\"nan\")\n",
    "            else:\n",
    "                y_true = np.concatenate(y_true_all)\n",
    "                y_score = np.concatenate(y_score_all)\n",
    "                if len(np.unique(y_true)) < 2:\n",
    "                    auc = float(\"nan\")\n",
    "                else:\n",
    "                    auc = float(roc_auc_score(y_true, y_score))\n",
    "            results.append({\n",
    "                \"size\": r,\n",
    "                \"features\": feats,\n",
    "                \"auc\": auc,\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values([\"auc\", \"size\"], ascending=[False, True])\n",
    "    results_df[\"features_str\"] = results_df[\"features\"].apply(lambda lst: \",\".join(lst))\n",
    "    subset_csv_name = f\"TOP{TOP_SUBSET_K}_SUBSET_AUC.csv\"\n",
    "    subset_path = outpath(subset_csv_name)\n",
    "    results_df[[\"size\", \"features_str\", \"auc\"]].to_csv(subset_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    print(f\"[Cell3A-Subset] best size={int(best_row['size'])}, auc={best_row['auc']:.4f}, features={best_row['features']}\")\n",
    "\n",
    "    best_json_name = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST.json\"\n",
    "    with open(outpath(best_json_name), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"size\": int(best_row[\"size\"]),\n",
    "            \"auc\": float(best_row[\"auc\"]),\n",
    "            \"features\": best_row[\"features\"],\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    globals()[\"BEST_SUBSET_FEATURES\"] = best_row[\"features\"]\n",
    "    globals()[\"BEST_SUBSET_K\"] = len(best_row[\"features\"])\n",
    "\n",
    "    print(f\"[Cell3A-Subset] 保存 -> {subset_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4A: MAXk ROC-AUC（CI計算＋ROC曲線, ニュートラル） =====\n",
    "RUN_CELL4 = bool(globals().get('RUN_CELL4', True))\n",
    "if not RUN_CELL4:\n",
    "    print('[Cell4-14] RUN_CELL4=False -> skip')\n",
    "else:\n",
    "    set_cell_output(4)\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    \n",
    "    \n",
    "    \n",
    "    subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "    subset_candidates = [\n",
    "        cell_output_path(4, subset_primary),\n",
    "        cell_output_path(4, \"TOP10_SUBSET_BEST.json\"),\n",
    "        cell_output_path(3, subset_primary),\n",
    "        cell_output_path(3, \"TOP10_SUBSET_BEST.json\"),\n",
    "    ]\n",
    "    subset_json = next((p for p in subset_candidates if os.path.exists(p)), None)\n",
    "    if subset_json is None:\n",
    "        raise FileNotFoundError(\"[Cell4A] TOP*_SUBSET_BEST.json がありません。Cell4 を実行してください。\")\n",
    "    \n",
    "    with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        subset_info = json.load(f)\n",
    "    \n",
    "    best_features = subset_info.get(\"features\", [])\n",
    "    if not best_features:\n",
    "        raise RuntimeError(\"[Cell4A] JSON 内に features がありません。\")\n",
    "    \n",
    "    # X_all に存在するものだけに絞る\n",
    "    best_features = [f for f in best_features if f in X_all.columns]\n",
    "    if not best_features:\n",
    "        raise RuntimeError(\"[Cell4A] X_all に存在する特徴がありません。\")\n",
    "    \n",
    "    # ★ オプションで MSSQ / VIMSSQ を追加 ★\n",
    "    extra_traits = []\n",
    "    if globals().get(\"USE_MSSQ_FEATURE\", False) and \"MSSQ\" in X_all.columns:\n",
    "        extra_traits.append(\"MSSQ\")\n",
    "    if globals().get(\"USE_VIMSSQ_FEATURE\", False) and \"VIMSSQ\" in X_all.columns:\n",
    "        extra_traits.append(\"VIMSSQ\")\n",
    "    \n",
    "    extra_traits = [f for f in extra_traits if f not in best_features]\n",
    "    if extra_traits:\n",
    "        print(f\"[Cell4A] 追加で使用する属性特徴: {extra_traits}\")\n",
    "        best_features = best_features + extra_traits\n",
    "    # ★ ここまで ★\n",
    "    \n",
    "    best_k = len(best_features)\n",
    "    \n",
    "    # もともとの表示\n",
    "    print(f\"[Cell4A] 使用特徴 ({best_k}) from {os.path.basename(subset_json)}: {best_features}\")\n",
    "    \n",
    "    # ★ デバッグ用: 実際に使う最終特徴一覧を明示的にプリント ★\n",
    "    print(\"[Cell4A][DEBUG] 実際にモデルに渡した特徴量リスト:\")\n",
    "    for i, f_name in enumerate(best_features, start=1):\n",
    "        print(f\"  {i:2d}: {f_name}\")\n",
    "    # ★ ここまで ★\n",
    "    \n",
    "    logo = LeaveOneGroupOut()\n",
    "    y_true_all, proba_all, subj_all = [], [], []\n",
    "    for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "        X_tr = X_all.iloc[tr_idx][best_features].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_all.iloc[te_idx][best_features].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            continue\n",
    "        model = fit_classifier(X_tr, y_tr)\n",
    "        proba = predict_positive_score(model, X_te)\n",
    "        y_true_all.append(y_te)\n",
    "        proba_all.append(proba)\n",
    "        subj_all.append(groups.iloc[te_idx].values)\n",
    "    \n",
    "    if not y_true_all:\n",
    "        raise RuntimeError(\"[Cell4A] 評価に必要な fold が得られませんでした。\")\n",
    "    \n",
    "    y_pool = np.concatenate(y_true_all)\n",
    "    s_pool = np.concatenate(proba_all)\n",
    "    subj_pool = np.concatenate(subj_all)\n",
    "    if len(np.unique(y_pool)) < 2:\n",
    "        raise RuntimeError(\"[Cell4A] 真値が単一クラスのため ROC-AUC を計算できません。\")\n",
    "    \n",
    "    auc_obs = float(roc_auc_score(y_pool, s_pool))\n",
    "    \n",
    "    rng = np.random.default_rng(20251101)\n",
    "    df_pool = pd.DataFrame({\"subject\": subj_pool, \"y_true\": y_pool, \"y_score\": s_pool})\n",
    "    subj_ids = df_pool[\"subject\"].unique()\n",
    "    auc_boot = []\n",
    "    for _ in range(2000):\n",
    "        sampled = rng.choice(subj_ids, size=len(subj_ids), replace=True)\n",
    "        df_boot = pd.concat([df_pool[df_pool[\"subject\"] == sid] for sid in sampled], ignore_index=True)\n",
    "        if df_boot[\"y_true\"].nunique() < 2:\n",
    "            continue\n",
    "        auc_boot.append(float(roc_auc_score(df_boot[\"y_true\"], df_boot[\"y_score\"])))\n",
    "    if auc_boot:\n",
    "        ci_low = float(np.quantile(auc_boot, 0.025))\n",
    "        ci_high = float(np.quantile(auc_boot, 0.975))\n",
    "    else:\n",
    "        ci_low = ci_high = float(\"nan\")\n",
    "    \n",
    "    pd.DataFrame([{\"k\": best_k, \"auc\": auc_obs, \"ci_low\": ci_low, \"ci_high\": ci_high}]).to_csv(\n",
    "        outpath(\"AUC_K_CI.csv\"), index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "    print(f\"[Cell4A] AUC={auc_obs:.4f} (95% CI [{ci_low:.4f}, {ci_high:.4f}])\")\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_pool, s_pool)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc_obs:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Chance\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve (Best Subset)\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"AUC_K_CI.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell4A] ROC 図を保存 -> {outpath('AUC_K_CI.png')}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae819ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4B: SHAP 可視化（ニュートラル・MAXkハイライト） =====\n",
    "RUN_CELL4 = bool(globals().get('RUN_CELL4', True))\n",
    "if not RUN_CELL4:\n",
    "    print('[Cell4-13] RUN_CELL4=False -> skip')\n",
    "else:\n",
    "    set_cell_output(4)\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    import re\n",
    "    import json\n",
    "    import shap\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    required = [\"X_all\", \"y_all\", \"fit_classifier\", \"outpath\", \"SEED_BASE\", \"OUT_DIR\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell4B-SHAP] 未定義の変数/関数があります: {missing}\")\n",
    "    \n",
    "    # ---------- グループ名取得ヘルパ ----------\n",
    "    def get_feature_group(col: str) -> str:\n",
    "        \"\"\"\n",
    "        ベース特徴名:\n",
    "          - 'xxx_lag0', 'xxx_lag1', ... → 'xxx'\n",
    "          - それ以外 → 列名そのまま\n",
    "        \"\"\"\n",
    "        m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "        return m.group(1) if m else col\n",
    "    \n",
    "    # ---------- 相関事前除去リスト（列） ----------\n",
    "    FEATURE_LIST_PATH = cell_output_path(3, \"FEATURES_AFTER_CORR.json\")\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        keep_cols = [c for c in keep_payload.get(\"keep\", []) if c in X_all.columns]\n",
    "        if not keep_cols:\n",
    "            raise RuntimeError(\"[Cell4B-SHAP] FEATURES_AFTER_CORR.json の keep に有効な列がありません。\")\n",
    "        cols_for_model = keep_cols\n",
    "        print(f\"[Cell4B-SHAP] Using correlation-pruned columns ({len(cols_for_model)})\")\n",
    "    else:\n",
    "        cols_for_model = list(X_all.columns)\n",
    "        print(\"[Cell4B-SHAP] FEATURES_AFTER_CORR.json が無いため、全列を使用します。\")\n",
    "    \n",
    "    # ---------- 列→グループ / グループ→列 ----------\n",
    "    col_to_group: dict[str, str] = {}\n",
    "    group_to_cols: dict[str, list[str]] = {}\n",
    "    for col in cols_for_model:\n",
    "        g = get_feature_group(col)\n",
    "        col_to_group[col] = g\n",
    "        group_to_cols.setdefault(g, []).append(col)\n",
    "    \n",
    "    group_names = list(group_to_cols.keys())\n",
    "    \n",
    "    # ---------- SHAPグループランキング（順序を決める） ----------\n",
    "    group_rank_csv = cell_output_path(3, GROUP_RANKING_FILE)\n",
    "    if os.path.exists(group_rank_csv):\n",
    "        group_rank_df = pd.read_csv(group_rank_csv, index_col=0)\n",
    "        if \"rank_mean\" in group_rank_df.columns:\n",
    "            group_order = group_rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "        else:\n",
    "            group_order = list(group_rank_df.index)\n",
    "        print(f\"[Cell4B-SHAP] グループランキング順に従って描画します。groups={len(group_order)}\")\n",
    "    else:\n",
    "        group_order = group_names\n",
    "        print(\"[Cell4B-SHAP] GROUP_RANKING_FILE が無いため、グループ名の順で描画します。\")\n",
    "    \n",
    "    # 念のため、現在の group_to_cols に存在するグループだけに制限\n",
    "    group_order = [g for g in group_order if g in group_to_cols]\n",
    "    \n",
    "    # ---------- SHAP計算用データ（列レベル） ----------\n",
    "    X_shap = X_all[cols_for_model].astype(np.float32)\n",
    "    y_shap = y_all.astype(int)\n",
    "    \n",
    "    print(f\"[Cell4B-SHAP] samples={X_shap.shape[0]}, columns={X_shap.shape[1]}\")\n",
    "    \n",
    "    # ---------- モデル学習 ----------\n",
    "    model = fit_classifier(X_shap, y_shap)\n",
    "    \n",
    "    # ---------- TreeExplainer で列レベル SHAP 計算 ----------\n",
    "    background = shap.sample(X_shap, min(256, len(X_shap)), random_state=SEED_BASE)\n",
    "    explainer = shap.TreeExplainer(\n",
    "        model,\n",
    "        data=background,\n",
    "        model_output=\"probability\",\n",
    "        feature_perturbation=\"interventional\",\n",
    "    )\n",
    "    \n",
    "    shap_values_any = explainer.shap_values(X_shap)\n",
    "    if isinstance(shap_values_any, list):\n",
    "        # 2値分類などで [neg, pos] みたいなリストになる場合は「陽性クラス(1)」を採用\n",
    "        if hasattr(model, \"classes_\") and 1 in list(model.classes_):\n",
    "            class_idx = list(model.classes_).index(1)\n",
    "        else:\n",
    "            class_idx = -1  # 最後のクラス\n",
    "        shap_values_col = shap_values_any[class_idx]\n",
    "    else:\n",
    "        shap_values_col = shap_values_any\n",
    "    \n",
    "    shap_values_col = np.asarray(shap_values_col)\n",
    "    if shap_values_col.ndim == 3:\n",
    "        # (n_samples, n_features, n_classes) → 陽性クラスを取り出す\n",
    "        if hasattr(model, \"classes_\") and 1 in list(model.classes_):\n",
    "            pos_idx = list(model.classes_).index(1)\n",
    "        else:\n",
    "            pos_idx = -1\n",
    "        shap_values_col = shap_values_col[:, :, pos_idx]\n",
    "    elif shap_values_col.ndim == 1:\n",
    "        shap_values_col = shap_values_col.reshape(-1, 1)\n",
    "    \n",
    "    if shap_values_col.shape[1] != X_shap.shape[1]:\n",
    "        raise RuntimeError(f\"[Cell4B-SHAP] shap_values 形状が一致しません: {shap_values_col.shape} vs {X_shap.shape}\")\n",
    "    \n",
    "    print(\"[Cell4B-SHAP] 列レベルの SHAP 値を計算しました。\")\n",
    "    \n",
    "    # ---------- 列レベル SHAP → グループ SHAP への集約 ----------\n",
    "    n_samples = shap_values_col.shape[0]\n",
    "    n_groups = len(group_order)\n",
    "    \n",
    "    # 列名→インデックス\n",
    "    col_index_map = {col: idx for idx, col in enumerate(X_shap.columns)}\n",
    "    \n",
    "    # グループ SHAP行列とグループ値行列を作成\n",
    "    shap_values_group = np.zeros((n_samples, n_groups), dtype=float)\n",
    "    X_group_df = pd.DataFrame(index=X_shap.index, columns=group_order, dtype=float)\n",
    "    \n",
    "    for g_idx, g in enumerate(group_order):\n",
    "        cols = group_to_cols[g]\n",
    "        # 現在の X_shap に存在する列だけ\n",
    "        cols = [c for c in cols if c in X_shap.columns]\n",
    "        if not cols:\n",
    "            continue\n",
    "        col_indices = [col_index_map[c] for c in cols]\n",
    "    \n",
    "        # SHAP: 各サンプルにおいて、当該グループに属する列の SHAP を合計\n",
    "        shap_values_group[:, g_idx] = shap_values_col[:, col_indices].sum(axis=1)\n",
    "    \n",
    "        # グループの代表値（色付け用）は、当該列の平均値とする\n",
    "        X_group_df[g] = X_shap[cols].mean(axis=1)\n",
    "    \n",
    "    print(f\"[Cell4B-SHAP] グループ数={n_groups}, shap_values_group.shape={shap_values_group.shape}\")\n",
    "    \n",
    "    # ---------- Top-K の設定 ----------\n",
    "    TOP_K = int(globals().get(\"TOP_SUBSET_K\", 15))\n",
    "    TOP_K = max(1, min(TOP_K, n_groups))  # 1〜グループ数にクリップ\n",
    "    \n",
    "    # ---------- Top-k subset をグループにマップ（ハイライト用） ----------\n",
    "    highlight_groups: list[str] = []\n",
    "    try:\n",
    "        subset_json_candidates = [\n",
    "            cell_output_path(4, f\"TOP{TOP_K}_SUBSET_BEST.json\"),\n",
    "            cell_output_path(4, \"TOP10_SUBSET_BEST.json\"),\n",
    "            cell_output_path(3, f\"TOP{TOP_K}_SUBSET_BEST.json\"),\n",
    "            cell_output_path(3, \"TOP10_SUBSET_BEST.json\"),\n",
    "        ]\n",
    "        subset_json_path = next((p for p in subset_json_candidates if os.path.exists(p)), None)\n",
    "    \n",
    "        if subset_json_path is not None:\n",
    "            with open(subset_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                info = json.load(f)\n",
    "            feature_cols_subset = info.get(\"features\", [])\n",
    "            # 列名 -> グループ名 に変換して一意集合に\n",
    "            highlight_groups = sorted({get_feature_group(c) for c in feature_cols_subset if c in col_to_group})\n",
    "            print(f\"[Cell4B-SHAP] subset highlight groups (from {os.path.basename(subset_json_path)}):\")\n",
    "            print(f\"  {highlight_groups}\")\n",
    "        else:\n",
    "            print(\"[Cell4B-SHAP] subset JSON が見つからなかったため、ハイライトなし。\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Cell4B-SHAP][WARN] subset読み込み失敗: {e}\")\n",
    "        highlight_groups = []\n",
    "    \n",
    "    # ---------- 出力ディレクトリ ----------\n",
    "    shap_dir = os.path.join(OUT_DIR, \"SHAP_GROUP\")\n",
    "    os.makedirs(shap_dir, exist_ok=True)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 1. Summary plot（Top-K グループ）\n",
    "    # =============================================================================\n",
    "    # Top-K グループを group_order から切り出し\n",
    "    top_groups = group_order[:TOP_K]\n",
    "    top_idx = [group_order.index(g) for g in top_groups]\n",
    "    \n",
    "    X_top = X_group_df[top_groups]\n",
    "    shap_top = shap_values_group[:, top_idx]\n",
    "    \n",
    "    plt.figure()\n",
    "    shap.summary_plot(\n",
    "        shap_top,\n",
    "        X_top,\n",
    "        show=False,\n",
    "        plot_type=\"dot\",\n",
    "        max_display=TOP_K,\n",
    "        sort=False,  # こちらで列順を制御する\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    for label in ax.get_yticklabels():\n",
    "        if label.get_text() in highlight_groups:\n",
    "            label.set_color(\"red\")\n",
    "    plt.tight_layout()\n",
    "    summary_top_path = os.path.join(shap_dir, f\"SHAP_GROUP_SUMMARY_TOP{TOP_K}.png\")\n",
    "    plt.savefig(summary_top_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[Cell4B-SHAP] Summary plot (Top-{TOP_K} groups) -> {summary_top_path}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 2. Summary plot（全グループ）\n",
    "    # =============================================================================\n",
    "    plt.figure()\n",
    "    shap.summary_plot(\n",
    "        shap_values_group,\n",
    "        X_group_df[group_order],\n",
    "        show=False,\n",
    "        plot_type=\"dot\",\n",
    "        max_display=len(group_order),\n",
    "        sort=False,\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    for label in ax.get_yticklabels():\n",
    "        if label.get_text() in highlight_groups:\n",
    "            label.set_color(\"red\")\n",
    "    plt.tight_layout()\n",
    "    summary_all_path = os.path.join(shap_dir, f\"SHAP_GROUP_SUMMARY_ALL.png\")\n",
    "    plt.savefig(summary_all_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[Cell4B-SHAP] Summary plot (ALL groups) -> {summary_all_path}\")\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 3. 各グループごとの dependence plot\n",
    "    # =============================================================================\n",
    "    for g in group_order:\n",
    "        plt.figure()\n",
    "        shap.dependence_plot(\n",
    "            g,\n",
    "            shap_values_group,\n",
    "            X_group_df[group_order],  # 全グループを渡す\n",
    "            show=False,\n",
    "            interaction_index=None,   # 一変数の関係だけを描画\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        safe_name = g.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "        dep_path = os.path.join(shap_dir, f\"SHAP_GROUP_DEP_{safe_name}.png\")\n",
    "        plt.savefig(dep_path, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"[Cell4B-SHAP] Dependence plot (group={g}) -> {dep_path}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f12f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5: Top-k ROC-AUC（MSSQ層別） =====\n",
    "RUN_CELL5 = bool(globals().get('RUN_CELL5', True))\n",
    "if not RUN_CELL5:\n",
    "    print('[Cell5-15] RUN_CELL5=False -> skip')\n",
    "else:\n",
    "    set_cell_output(5)\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    from collections import defaultdict\n",
    "    import re\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import json\n",
    "    \n",
    "    required = [\n",
    "        \"X_all\", \"y_all\", \"groups\",\n",
    "        \"SUBJECT_META\",\n",
    "        \"fit_classifier\", \"predict_positive_score\",\n",
    "        \"outpath\", \"cell_output_path\",\n",
    "        \"FEATURE_RANKING_FILE\",\n",
    "    ]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell5-MSSQ] 未定義の変数/関数があります: {missing}\")\n",
    "    \n",
    "    # ---------- MSSQグループ情報の準備 ----------\n",
    "    if \"MSSQ_group\" not in SUBJECT_META.columns:\n",
    "        raise RuntimeError(\"[Cell5-MSSQ] SUBJECT_META に 'MSSQ_group' 列がありません。\")\n",
    "    \n",
    "    # subject_id → MSSQ_group の辞書\n",
    "    _subj_mssq_group = SUBJECT_META[\"MSSQ_group\"].astype(str).to_dict()\n",
    "    \n",
    "    # 各サンプル（行）ごとの subject_id / MSSQ_group\n",
    "    subj_series = groups.astype(str)\n",
    "    mssq_group_series = subj_series.map(_subj_mssq_group)\n",
    "    \n",
    "    if mssq_group_series.isna().any():\n",
    "        missing_ids = sorted(subj_series[mssq_group_series.isna()].unique())\n",
    "        raise RuntimeError(\n",
    "            \"[Cell5-MSSQ] MSSQ_group が未設定の被験者があります: \"\n",
    "            + \", \".join(map(str, missing_ids))\n",
    "        )\n",
    "    \n",
    "    # ラベル名（既定: \"Low\" / \"High\"）\n",
    "    MSSQ_LOW_LABEL = str(globals().get(\"MSSQ_LOW_LABEL\", \"Low\"))\n",
    "    MSSQ_HIGH_LABEL = str(globals().get(\"MSSQ_HIGH_LABEL\", \"High\"))\n",
    "    \n",
    "    print(f\"[Cell5-MSSQ] MSSQ_group ラベル: Low='{MSSQ_LOW_LABEL}', High='{MSSQ_HIGH_LABEL}'\")\n",
    "    print(f\"[Cell5-MSSQ] MSSQ_group 割合:\\n{mssq_group_series.value_counts()}\")\n",
    "    \n",
    "    # ---------- グループ名取得ヘルパ（lag付き列を1つのグループにまとめる） ----------\n",
    "    def get_feature_group(col: str) -> str:\n",
    "        \"\"\"\n",
    "        ベース特徴名:\n",
    "          - 'xxx_lag0', 'xxx_lag1', ... → 'xxx'\n",
    "          - それ以外 → 列名そのまま\n",
    "        \"\"\"\n",
    "        m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "        return m.group(1) if m else col\n",
    "    \n",
    "    # 列→グループ / グループ→列 の対応を作成\n",
    "    col_to_group: dict[str, str] = {}\n",
    "    group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "    for col in X_all.columns:\n",
    "        g = get_feature_group(col)\n",
    "        col_to_group[col] = g\n",
    "        group_to_cols[g].append(col)\n",
    "    \n",
    "    print(f\"[Cell5-MSSQ] feature groups = {len(group_to_cols)} \"\n",
    "          f\"(columns={len(X_all.columns)})\")\n",
    "    \n",
    "    # ---------- SHAPランキングの読み込み（行＝グループまたは列） ----------\n",
    "    rank_csv = cell_output_path(3, FEATURE_RANKING_FILE)\n",
    "    if not os.path.exists(rank_csv):\n",
    "        raise FileNotFoundError(\"[Cell5-MSSQ] ランキングCSVがありません。Cell3A-SHAP を実行してください。\")\n",
    "    \n",
    "    rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "    \n",
    "    # 「小さいほど重要」なスコアを作る（rank_mean 優先, なければ mean_abs に基づく）\n",
    "    if \"rank_mean\" in rank_df.columns:\n",
    "        base_score = rank_df[\"rank_mean\"].astype(float)\n",
    "    elif \"mean_abs\" in rank_df.columns:\n",
    "        # mean_abs が大きいほど重要 → 符号を反転して「小さいほど重要」にする\n",
    "        base_score = (-rank_df[\"mean_abs\"].astype(float))\n",
    "    else:\n",
    "        # 何もなければ行順そのものをスコアにする（最初がもっとも重要）\n",
    "        base_score = pd.Series(\n",
    "            np.arange(len(rank_df), dtype=float),\n",
    "            index=rank_df.index,\n",
    "        )\n",
    "    \n",
    "    # ---------- 行（特徴orグループ）→ グループスコアへ集約 ----------\n",
    "    group_score: dict[str, float] = {}\n",
    "    for name, score in base_score.items():\n",
    "        g = get_feature_group(str(name))  # すでにグループ名ならそのまま\n",
    "        if g not in group_score:\n",
    "            group_score[g] = float(score)\n",
    "        else:\n",
    "            # より重要な列が1つでもあれば、そのグループはそのスコアを採用（min）\n",
    "            group_score[g] = min(group_score[g], float(score))\n",
    "    \n",
    "    group_rank = pd.Series(group_score, name=\"score\").sort_values(ascending=True)\n",
    "    \n",
    "    # 実際に X_all に存在するグループだけに絞る\n",
    "    group_order = [g for g in group_rank.index if g in group_to_cols]\n",
    "    if not group_order:\n",
    "        raise RuntimeError(\"[Cell5-MSSQ] ランキングに該当するグループが X_all に存在しません。\")\n",
    "    \n",
    "    total_groups = len(group_order)\n",
    "    print(f\"[Cell5-MSSQ] Using SHAP group ranking ({total_groups} groups):\")\n",
    "    print(group_order)\n",
    "    \n",
    "    # 旧コードとの互換用：TOP_SUBSET_K があればそれを使い，なければ total_groups で上書き\n",
    "    TOP_SUBSET_K = int(globals().get(\"TOP_SUBSET_K\", 0))\n",
    "    if TOP_SUBSET_K <= 0 or TOP_SUBSET_K > total_groups:\n",
    "        TOP_SUBSET_K = total_groups\n",
    "    globals()[\"TOP_SUBSET_K\"] = TOP_SUBSET_K\n",
    "    \n",
    "    logo = LeaveOneGroupOut()\n",
    "    results = []\n",
    "    \n",
    "    # ---- 上位kグループの累積セットで評価（k=1..total_groups）----\n",
    "    for k in range(1, total_groups + 1):\n",
    "        use_groups = group_order[:k]\n",
    "    \n",
    "        # この k グループに属する全ての列を集約\n",
    "        feats: list[str] = []\n",
    "        for g in use_groups:\n",
    "            feats.extend(group_to_cols[g])\n",
    "        # 念のため重複を除去（順序はグループ順→列順を維持）\n",
    "        seen = set()\n",
    "        feats_unique = []\n",
    "        for f in feats:\n",
    "            if f not in seen:\n",
    "                feats_unique.append(f)\n",
    "                seen.add(f)\n",
    "        feats = feats_unique\n",
    "    \n",
    "        y_true_all = []\n",
    "        y_score_all = []\n",
    "        mssq_group_all = []\n",
    "    \n",
    "        skip_folds = 0\n",
    "    \n",
    "        for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "            tr_idx = np.asarray(tr_idx)\n",
    "            te_idx = np.asarray(te_idx)\n",
    "    \n",
    "            # テスト側の被験者ID（LOSOなので1名のみを想定）\n",
    "            te_subj_ids = np.unique(subj_series.iloc[te_idx].astype(str))\n",
    "            if len(te_subj_ids) != 1:\n",
    "                raise RuntimeError(\n",
    "                    f\"[Cell5-MSSQ] te_idx に複数被験者が含まれています: {te_subj_ids}\"\n",
    "                )\n",
    "            test_sid = te_subj_ids[0]\n",
    "            test_group = _subj_mssq_group.get(test_sid, None)\n",
    "            if test_group is None:\n",
    "                skip_folds += 1\n",
    "                continue\n",
    "    \n",
    "            # 訓練側を「テスト被験者と同じ MSSQ_group」に絞る\n",
    "            tr_groups = subj_series.iloc[tr_idx].map(_subj_mssq_group)\n",
    "            tr_mask_same_group = (tr_groups == test_group)\n",
    "            tr_idx_group = tr_idx[tr_mask_same_group.to_numpy()]\n",
    "    \n",
    "            if tr_idx_group.size == 0:\n",
    "                skip_folds += 1\n",
    "                continue\n",
    "    \n",
    "            X_tr = X_all.iloc[tr_idx_group][feats].astype(np.float32)\n",
    "            y_tr = y_all.iloc[tr_idx_group].astype(int)\n",
    "            X_te = X_all.iloc[te_idx][feats].astype(np.float32)\n",
    "            y_te = y_all.iloc[te_idx].astype(int)\n",
    "    \n",
    "            # 学習側 or テスト側が単一クラスならこの fold はスキップ\n",
    "            if len(np.unique(y_tr)) < 2 or len(np.unique(y_te)) < 2:\n",
    "                skip_folds += 1\n",
    "                continue\n",
    "    \n",
    "            model = fit_classifier(X_tr, y_tr)\n",
    "            proba = predict_positive_score(model, X_te)\n",
    "    \n",
    "            y_true_all.append(y_te.to_numpy())\n",
    "            y_score_all.append(proba)\n",
    "            # この fold のテストサンプルは全て同じ MSSQ_group\n",
    "            mssq_group_all.append(\n",
    "                np.full_like(y_te.to_numpy(), fill_value=test_group, dtype=object)\n",
    "            )\n",
    "    \n",
    "        if not y_true_all:\n",
    "            auc_total = float(\"nan\")\n",
    "            auc_low = float(\"nan\")\n",
    "            auc_high = float(\"nan\")\n",
    "        else:\n",
    "            y_true = np.concatenate(y_true_all)\n",
    "            y_score = np.concatenate(y_score_all)\n",
    "            g_all = np.concatenate(mssq_group_all)\n",
    "    \n",
    "            if len(np.unique(y_true)) < 2:\n",
    "                auc_total = float(\"nan\")\n",
    "                auc_low = float(\"nan\")\n",
    "                auc_high = float(\"nan\")\n",
    "            else:\n",
    "                auc_total = float(roc_auc_score(y_true, y_score))\n",
    "    \n",
    "                # MSSQ-Low / High の部分 AUC\n",
    "                def _safe_auc(mask: np.ndarray) -> float:\n",
    "                    if mask.sum() == 0:\n",
    "                        return float(\"nan\")\n",
    "                    y_sub = y_true[mask]\n",
    "                    s_sub = y_score[mask]\n",
    "                    if len(np.unique(y_sub)) < 2:\n",
    "                        return float(\"nan\")\n",
    "                    return float(roc_auc_score(y_sub, s_sub))\n",
    "    \n",
    "                auc_low = _safe_auc(g_all == MSSQ_LOW_LABEL)\n",
    "                auc_high = _safe_auc(g_all == MSSQ_HIGH_LABEL)\n",
    "    \n",
    "        results.append({\n",
    "            \"size\": k,                 # 使用した「グループ数」k\n",
    "            \"n_features\": len(feats),  # 実際に使った列数\n",
    "            \"groups\": use_groups,      # 使用したグループ名\n",
    "            \"features\": feats,         # 使用した列名\n",
    "            \"auc_total\": auc_total,\n",
    "            \"auc_low\": auc_low,\n",
    "            \"auc_high\": auc_high,\n",
    "            \"skip_folds\": skip_folds,\n",
    "        })\n",
    "        print(\n",
    "            f\"[Cell5-MSSQ] k(groups)={k}/{total_groups}, \"\n",
    "            f\"n_features={len(feats)}, \"\n",
    "            f\"AUC_total={auc_total:.4f}, \"\n",
    "            f\"AUC_Low={auc_low:.4f}, AUC_High={auc_high:.4f}, \"\n",
    "            f\"skip_folds={skip_folds}\"\n",
    "        )\n",
    "    \n",
    "    # ---- 結果の整形・保存 ----\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values(\"size\")  # k昇順\n",
    "    \n",
    "    results_df[\"groups_str\"] = results_df[\"groups\"].apply(lambda lst: \",\".join(lst))\n",
    "    results_df[\"features_str\"] = results_df[\"features\"].apply(lambda lst: \",\".join(lst))\n",
    "    \n",
    "    # 1) このセル独自のファイル（ALLK_*）\n",
    "    subset_csv_name_allk = \"ALLK_TOPORDER_AUC_MSSQ_SPLIT.csv\"\n",
    "    subset_path_allk = outpath(subset_csv_name_allk)\n",
    "    results_df[\n",
    "        [\"size\", \"n_features\", \"groups_str\", \"features_str\",\n",
    "         \"auc_total\", \"auc_low\", \"auc_high\", \"skip_folds\"]\n",
    "    ].to_csv(subset_path_allk, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell5-MSSQ] 保存 (ALLK, MSSQ-split) -> {subset_path_allk}\")\n",
    "    \n",
    "    # 最良のk（AUC_total 最大、同点なら小さいk）を取得\n",
    "    valid_df = results_df.dropna(subset=[\"auc_total\"])\n",
    "    if valid_df.empty:\n",
    "        raise RuntimeError(\"[Cell5-MSSQ] 有効な AUC_total が得られませんでした。\")\n",
    "    \n",
    "    best_row = valid_df.sort_values(\n",
    "        [\"auc_total\", \"size\"], ascending=[False, True]\n",
    "    ).iloc[0]\n",
    "    print(\n",
    "        f\"[Cell5-MSSQ] best k(groups)={int(best_row['size'])}, \"\n",
    "        f\"n_features={int(best_row['n_features'])}, \"\n",
    "        f\"AUC_total={best_row['auc_total']:.4f}, \"\n",
    "        f\"AUC_Low={best_row['auc_low']:.4f}, \"\n",
    "        f\"AUC_High={best_row['auc_high']:.4f}, \"\n",
    "        f\"groups={best_row['groups']}\"\n",
    "    )\n",
    "    \n",
    "    # 2) このセル独自の JSON（ALLK_SUBSET_BEST_MSSQ_SPLIT.json）\n",
    "    best_json_name_allk = \"ALLK_SUBSET_BEST_MSSQ_SPLIT.json\"\n",
    "    with open(outpath(best_json_name_allk), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"size\": int(best_row[\"size\"]),              # グループ数\n",
    "            \"n_features\": int(best_row[\"n_features\"]),  # 列数\n",
    "            \"auc_total\": float(best_row[\"auc_total\"]),\n",
    "            \"auc_low\": float(best_row[\"auc_low\"]),\n",
    "            \"auc_high\": float(best_row[\"auc_high\"]),\n",
    "            \"groups\": best_row[\"groups\"],\n",
    "            \"features\": best_row[\"features\"],\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[Cell5-MSSQ] BEST (ALLK, MSSQ-split) -> {outpath(best_json_name_allk)}\")\n",
    "    \n",
    "    # 3) 旧「組合せ探索版」と同じ名前・形式でも保存（互換用）\n",
    "    subset_csv_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_AUC_MSSQ_SPLIT.csv\"\n",
    "    subset_path_compat = outpath(subset_csv_name_compat)\n",
    "    results_df[\n",
    "        [\"size\", \"n_features\", \"groups_str\", \"features_str\", \"auc_total\"]\n",
    "    ].rename(columns={\"auc_total\": \"auc\"}).to_csv(\n",
    "        subset_path_compat, index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "    print(f\"[Cell5-MSSQ] 互換CSV (MSSQ-split) -> {subset_path_compat}\")\n",
    "    \n",
    "    best_json_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST_MSSQ_SPLIT.json\"\n",
    "    with open(outpath(best_json_name_compat), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"size\": int(best_row[\"size\"]),              # グループ数\n",
    "            \"n_features\": int(best_row[\"n_features\"]),  # 列数\n",
    "            \"auc\": float(best_row[\"auc_total\"]),        # 互換用：auc_total を auc として保存\n",
    "            \"groups\": best_row[\"groups\"],\n",
    "            \"features\": best_row[\"features\"],\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[Cell5-MSSQ] 互換BEST JSON (MSSQ-split) -> {outpath(best_json_name_compat)}\")\n",
    "    \n",
    "    # グローバル変数も旧仕様に合わせて更新（別名変数として保持）\n",
    "    globals()[\"BEST_SUBSET_FEATURES_MSSQ_SPLIT\"] = best_row[\"features\"]   # 実際に使う列\n",
    "    globals()[\"BEST_SUBSET_GROUPS_MSSQ_SPLIT\"] = best_row[\"groups\"]       # 使ったグループ名\n",
    "    globals()[\"BEST_SUBSET_K_MSSQ_SPLIT\"] = int(best_row[\"size\"])         # グループ数として解釈\n",
    "    \n",
    "    # ---- グラフ描画：横軸=グループ数k（右ほど少ない）, 縦軸=ROC-AUC ----\n",
    "    FS_TITLE, FS_LABEL, FS_TICK = 30, 24, 20\n",
    "    LW = 1.5\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(results_df[\"size\"], results_df[\"auc_total\"], marker=\"o\",\n",
    "             linewidth=LW, label=\"Overall\")\n",
    "    plt.plot(results_df[\"size\"], results_df[\"auc_low\"], marker=\"s\",\n",
    "             linewidth=LW, linestyle=\"--\", label=f\"MSSQ={MSSQ_LOW_LABEL}\")\n",
    "    plt.plot(results_df[\"size\"], results_df[\"auc_high\"], marker=\"^\",\n",
    "             linewidth=LW, linestyle=\":\", label=f\"MSSQ={MSSQ_HIGH_LABEL}\")\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.set_xlabel(\"Number of feature groups (k)\", fontsize=FS_LABEL)\n",
    "    ax.set_ylabel(\"ROC-AUC (pooled LOSO, MSSQ-split models)\", fontsize=FS_LABEL)\n",
    "    ax.set_title(\n",
    "        \"ROC-AUC vs. number of feature groups\\n(MSSQ-split models, all SHAP-ranked groups)\",\n",
    "        fontsize=FS_TITLE\n",
    "    )\n",
    "    \n",
    "    # 横軸：左が k = total_groups（全部）、右が k = 1（1グループだけ）\n",
    "    ax.set_xlim(total_groups, 1)\n",
    "    ax.set_xticks(range(1, total_groups + 1))\n",
    "    \n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    \n",
    "    ax.tick_params(axis=\"x\", labelsize=FS_TICK)\n",
    "    ax.tick_params(axis=\"y\", labelsize=FS_TICK)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=FS_TICK)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"ALLK_TOPORDER_AUC_MSSQ_SPLIT.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"[Cell5-MSSQ] 図を保存 -> {outpath('ALLK_TOPORDER_AUC_MSSQ_SPLIT.png')}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf78f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5A: MAXk ROC-AUC（CI計算＋ROC曲線, MSSQ層別） =====\n",
    "RUN_CELL5 = bool(globals().get('RUN_CELL5', True))\n",
    "if not RUN_CELL5:\n",
    "    print('[Cell5-16] RUN_CELL5=False -> skip')\n",
    "else:\n",
    "    set_cell_output(5)\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    \n",
    "    \n",
    "    required = [\n",
    "        \"X_all\", \"y_all\", \"groups\",\n",
    "        \"SUBJECT_META\",\n",
    "        \"fit_classifier\", \"predict_positive_score\",\n",
    "        \"outpath\", \"cell_output_path\",\n",
    "    ]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell5A-MSSQ] 未定義の変数/関数があります: {missing}\")\n",
    "    \n",
    "    # ---------- MSSQグループ情報 ----------\n",
    "    if \"MSSQ_group\" not in SUBJECT_META.columns:\n",
    "        raise RuntimeError(\"[Cell5A-MSSQ] SUBJECT_META に 'MSSQ_group' 列がありません。\")\n",
    "    \n",
    "    _subj_mssq_group = SUBJECT_META[\"MSSQ_group\"].astype(str).to_dict()\n",
    "    subj_series = groups.astype(str)\n",
    "    mssq_group_series = subj_series.map(_subj_mssq_group)\n",
    "    \n",
    "    if mssq_group_series.isna().any():\n",
    "        missing_ids = sorted(subj_series[mssq_group_series.isna()].unique())\n",
    "        raise RuntimeError(\n",
    "            \"[Cell5A-MSSQ] MSSQ_group が未設定の被験者があります: \"\n",
    "            + \", \".join(map(str, missing_ids))\n",
    "        )\n",
    "    \n",
    "    MSSQ_LOW_LABEL = str(globals().get(\"MSSQ_LOW_LABEL\", \"Low\"))\n",
    "    MSSQ_HIGH_LABEL = str(globals().get(\"MSSQ_HIGH_LABEL\", \"High\"))\n",
    "    \n",
    "    print(f\"[Cell5A-MSSQ] MSSQ_group ラベル: Low='{MSSQ_LOW_LABEL}', High='{MSSQ_HIGH_LABEL}'\")\n",
    "    print(f\"[Cell5A-MSSQ] MSSQ_group 割合:\\n{mssq_group_series.value_counts()}\")\n",
    "    \n",
    "    # ---------- 使用する特徴サブセット（JSON）を決定 ----------\n",
    "    # まず MSSQ-split 専用の BEST を優先\n",
    "    subset_primary_mssq = f\"TOP{int(globals().get('BEST_SUBSET_K_MSSQ_SPLIT', globals().get('TOP_SUBSET_K', 15)))}_SUBSET_BEST_MSSQ_SPLIT.json\"\n",
    "    \n",
    "    subset_candidates = [\n",
    "        cell_output_path(5, \"ALLK_SUBSET_BEST_MSSQ_SPLIT.json\"),\n",
    "        cell_output_path(5, subset_primary_mssq),\n",
    "    ]\n",
    "    \n",
    "    # それでも無ければ、従来の TOP*_SUBSET_BEST.json をフォールバックとして探す\n",
    "    subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "    subset_candidates.extend([\n",
    "        cell_output_path(3, subset_primary),\n",
    "        cell_output_path(3, \"TOP10_SUBSET_BEST.json\"),\n",
    "    ])\n",
    "    \n",
    "    subset_json = next((p for p in subset_candidates if os.path.exists(p)), None)\n",
    "    if subset_json is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"[Cell5A-MSSQ] TOP*_SUBSET_BEST(_MSSQ_SPLIT).json が見つかりません。\"\n",
    "            \" Cell3A-Subset や Cell5-MSSQ を実行してください。\"\n",
    "        )\n",
    "    \n",
    "    with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        subset_info = json.load(f)\n",
    "    \n",
    "    best_features = subset_info.get(\"features\", [])\n",
    "    if not best_features:\n",
    "        raise RuntimeError(\"[Cell5A-MSSQ] JSON 内に features がありません。\")\n",
    "    \n",
    "    # X_all に存在するものだけに絞る\n",
    "    best_features = [f for f in best_features if f in X_all.columns]\n",
    "    if not best_features:\n",
    "        raise RuntimeError(\"[Cell5A-MSSQ] X_all に存在する特徴がありません。\")\n",
    "    \n",
    "    best_k = len(best_features)\n",
    "    \n",
    "    print(f\"[Cell5A-MSSQ] 使用特徴 ({best_k}) from {os.path.basename(subset_json)}: {best_features}\")\n",
    "    print(\"[Cell5A-MSSQ][DEBUG] 実際にモデルに渡した特徴量リスト:\")\n",
    "    for i, f_name in enumerate(best_features, start=1):\n",
    "        print(f\"  {i:2d}: {f_name}\")\n",
    "    \n",
    "    # ---------- LOSO（MSSQ 2群別モデル）でスコアを作成 ----------\n",
    "    logo = LeaveOneGroupOut()\n",
    "    y_true_all, proba_all, subj_all, group_all = [], [], [], []\n",
    "    \n",
    "    skip_folds = 0\n",
    "    \n",
    "    for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "        tr_idx = np.asarray(tr_idx)\n",
    "        te_idx = np.asarray(te_idx)\n",
    "    \n",
    "        te_subj_ids = np.unique(subj_series.iloc[te_idx].astype(str))\n",
    "        if len(te_subj_ids) != 1:\n",
    "            raise RuntimeError(\n",
    "                f\"[Cell5A-MSSQ] te_idx に複数被験者が含まれています: {te_subj_ids}\"\n",
    "            )\n",
    "        test_sid = te_subj_ids[0]\n",
    "        test_group = _subj_mssq_group.get(test_sid, None)\n",
    "        if test_group is None:\n",
    "            skip_folds += 1\n",
    "            continue\n",
    "    \n",
    "        # 訓練側を「テスト被験者と同じ MSSQ_group」に絞る\n",
    "        tr_groups = subj_series.iloc[tr_idx].map(_subj_mssq_group)\n",
    "        tr_mask_same_group = (tr_groups == test_group)\n",
    "        tr_idx_group = tr_idx[tr_mask_same_group.to_numpy()]\n",
    "    \n",
    "        if tr_idx_group.size == 0:\n",
    "            skip_folds += 1\n",
    "            continue\n",
    "    \n",
    "        X_tr = X_all.iloc[tr_idx_group][best_features].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx_group].astype(int)\n",
    "        X_te = X_all.iloc[te_idx][best_features].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "    \n",
    "        # 学習側 or テスト側が単一クラスならこの fold はスキップ\n",
    "        if len(np.unique(y_tr)) < 2 or len(np.unique(y_te)) < 2:\n",
    "            skip_folds += 1\n",
    "            continue\n",
    "    \n",
    "        model = fit_classifier(X_tr, y_tr)\n",
    "        proba = predict_positive_score(model, X_te)\n",
    "    \n",
    "        y_true_all.append(y_te.to_numpy())\n",
    "        proba_all.append(proba)\n",
    "        subj_all.append(subj_series.iloc[te_idx].to_numpy())\n",
    "        group_all.append(\n",
    "            np.full_like(y_te.to_numpy(), fill_value=test_group, dtype=object)\n",
    "        )\n",
    "    \n",
    "    print(f\"[Cell5A-MSSQ] スキップされた fold 数: {skip_folds}\")\n",
    "    \n",
    "    if not y_true_all:\n",
    "        raise RuntimeError(\"[Cell5A-MSSQ] 評価に必要な fold が得られませんでした。\")\n",
    "    \n",
    "    y_pool = np.concatenate(y_true_all)\n",
    "    s_pool = np.concatenate(proba_all)\n",
    "    subj_pool = np.concatenate(subj_all)\n",
    "    g_pool = np.concatenate(group_all)\n",
    "    \n",
    "    if len(np.unique(y_pool)) < 2:\n",
    "        raise RuntimeError(\"[Cell5A-MSSQ] 真値が単一クラスのため ROC-AUC を計算できません。\")\n",
    "    \n",
    "    # ---------- AUC + 95% CI を被験者単位ブートストラップで計算 ----------\n",
    "    rng = np.random.default_rng(20251101)\n",
    "    \n",
    "    df_pool = pd.DataFrame({\n",
    "        \"subject\": subj_pool,\n",
    "        \"mssq_group\": g_pool,\n",
    "        \"y_true\": y_pool,\n",
    "        \"y_score\": s_pool,\n",
    "    })\n",
    "    pred_csv_path = cell_output_path(5, \"MSSQ_SPLIT_PREDICTIONS.csv\")\n",
    "    df_pool.to_csv(pred_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell5A-MSSQ] cross-validated predictions -> {pred_csv_path}\")\n",
    "    \n",
    "    def _bootstrap_auc_by_subject(df: pd.DataFrame, n_boot: int = 2000) -> tuple[float, float, float]:\n",
    "        \"\"\"被験者単位で再標本化して AUC の95%CIを推定する。\"\"\"\n",
    "        # 観測値\n",
    "        if df[\"y_true\"].nunique() < 2:\n",
    "            return (float(\"nan\"), float(\"nan\"), float(\"nan\"))\n",
    "        auc_obs = float(roc_auc_score(df[\"y_true\"], df[\"y_score\"]))\n",
    "    \n",
    "        subj_ids = df[\"subject\"].unique()\n",
    "        if len(subj_ids) == 0:\n",
    "            return (auc_obs, float(\"nan\"), float(\"nan\"))\n",
    "    \n",
    "        auc_boot = []\n",
    "        for _ in range(n_boot):\n",
    "            sampled = rng.choice(subj_ids, size=len(subj_ids), replace=True)\n",
    "            df_boot = pd.concat(\n",
    "                [df[df[\"subject\"] == sid] for sid in sampled],\n",
    "                ignore_index=True\n",
    "            )\n",
    "            if df_boot[\"y_true\"].nunique() < 2:\n",
    "                continue\n",
    "            auc_boot.append(float(roc_auc_score(df_boot[\"y_true\"], df_boot[\"y_score\"])))\n",
    "        if auc_boot:\n",
    "            ci_low = float(np.quantile(auc_boot, 0.025))\n",
    "            ci_high = float(np.quantile(auc_boot, 0.975))\n",
    "        else:\n",
    "            ci_low = ci_high = float(\"nan\")\n",
    "        return auc_obs, ci_low, ci_high\n",
    "    \n",
    "    # overall\n",
    "    auc_overall, ci_low_overall, ci_high_overall = _bootstrap_auc_by_subject(df_pool)\n",
    "    \n",
    "    # MSSQ-Low\n",
    "    df_low = df_pool[df_pool[\"mssq_group\"] == MSSQ_LOW_LABEL]\n",
    "    auc_low, ci_low_low, ci_high_low = _bootstrap_auc_by_subject(df_low)\n",
    "    \n",
    "    # MSSQ-High\n",
    "    df_high = df_pool[df_pool[\"mssq_group\"] == MSSQ_HIGH_LABEL]\n",
    "    auc_high, ci_low_high, ci_high_high = _bootstrap_auc_by_subject(df_high)\n",
    "    \n",
    "    records = [\n",
    "        {\n",
    "            \"group\": \"overall\",\n",
    "            \"k\": best_k,\n",
    "            \"auc\": auc_overall,\n",
    "            \"ci_low\": ci_low_overall,\n",
    "            \"ci_high\": ci_high_overall,\n",
    "            \"n_subjects\": int(df_pool[\"subject\"].nunique()),\n",
    "            \"n_samples\": int(len(df_pool)),\n",
    "        },\n",
    "        {\n",
    "            \"group\": f\"MSSQ={MSSQ_LOW_LABEL}\",\n",
    "            \"k\": best_k,\n",
    "            \"auc\": auc_low,\n",
    "            \"ci_low\": ci_low_low,\n",
    "            \"ci_high\": ci_high_low,\n",
    "            \"n_subjects\": int(df_low[\"subject\"].nunique()),\n",
    "            \"n_samples\": int(len(df_low)),\n",
    "        },\n",
    "        {\n",
    "            \"group\": f\"MSSQ={MSSQ_HIGH_LABEL}\",\n",
    "            \"k\": best_k,\n",
    "            \"auc\": auc_high,\n",
    "            \"ci_low\": ci_low_high,\n",
    "            \"ci_high\": ci_high_high,\n",
    "            \"n_subjects\": int(df_high[\"subject\"].nunique()),\n",
    "            \"n_samples\": int(len(df_high)),\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    auc_summary_path = outpath(\"AUC_K_CI_MSSQ_SPLIT.csv\")\n",
    "    pd.DataFrame(records).to_csv(auc_summary_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell5A-MSSQ] AUC summary を保存 -> {auc_summary_path}\")\n",
    "    \n",
    "    print(\n",
    "        f\"[Cell5A-MSSQ] overall AUC={auc_overall:.4f} \"\n",
    "        f\"(95% CI [{ci_low_overall:.4f}, {ci_high_overall:.4f}])\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[Cell5A-MSSQ] MSSQ={MSSQ_LOW_LABEL} AUC={auc_low:.4f} \"\n",
    "        f\"(95% CI [{ci_low_low:.4f}, {ci_high_low:.4f}])\"\n",
    "    )\n",
    "    print(\n",
    "        f\"[Cell5A-MSSQ] MSSQ={MSSQ_HIGH_LABEL} AUC={auc_high:.4f} \"\n",
    "        f\"(95% CI [{ci_low_high:.4f}, {ci_high_high:.4f}])\"\n",
    "    )\n",
    "    \n",
    "    # ---------- ROC 曲線（overall） ----------\n",
    "    fpr, tpr, _ = roc_curve(y_pool, s_pool)\n",
    "    \n",
    "    FS_TITLE, FS_LABEL, FS_TICK = 30, 24, 20\n",
    "    LW = 1.5\n",
    "    \n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.plot(fpr, tpr, linewidth=LW,\n",
    "             label=f\"MSSQ-split models (AUC = {auc_overall:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Chance\")\n",
    "    \n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=FS_LABEL)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=FS_LABEL)\n",
    "    plt.title(\"ROC Curve (Best Subset, MSSQ-split models)\", fontsize=FS_TITLE)\n",
    "    plt.legend(loc=\"lower right\", fontsize=FS_TICK)\n",
    "    plt.grid(True, alpha=0.4)\n",
    "    \n",
    "    plt.tick_params(axis=\"x\", labelsize=FS_TICK)\n",
    "    plt.tick_params(axis=\"y\", labelsize=FS_TICK)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"AUC_K_CI_MSSQ_SPLIT.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell5A-MSSQ] ROC 図を保存 -> {outpath('AUC_K_CI_MSSQ_SPLIT.png')}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5B: SHAP 可視化（MSSQ層別・MAXkハイライト） =====\n",
    "RUN_CELL5 = bool(globals().get('RUN_CELL5', True))\n",
    "if not RUN_CELL5:\n",
    "    print('[Cell5B] RUN_CELL5=False -> skip')\n",
    "else:\n",
    "    set_cell_output(5)\n",
    "\n",
    "    import os\n",
    "    import re\n",
    "    import json\n",
    "    import shap\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    required = [\"X_all\", \"y_all\", \"fit_classifier\", \"outpath\", \"SEED_BASE\", \"OUT_DIR\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell5B-SHAP] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    def get_feature_group(col: str) -> str:\n",
    "        m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "        return m.group(1) if m else col\n",
    "\n",
    "    FEATURE_LIST_PATH = cell_output_path(3, \"FEATURES_AFTER_CORR.json\")\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        keep_cols = [c for c in keep_payload.get(\"keep\", []) if c in X_all.columns]\n",
    "        if not keep_cols:\n",
    "            raise RuntimeError(\"[Cell5B-SHAP] FEATURES_AFTER_CORR.json の keep に有効な列がありません。\")\n",
    "        cols_for_model = keep_cols\n",
    "        print(f\"[Cell5B-SHAP] Using correlation-pruned columns ({len(cols_for_model)})\")\n",
    "    else:\n",
    "        cols_for_model = list(X_all.columns)\n",
    "        print(\"[Cell5B-SHAP] FEATURES_AFTER_CORR.json が無いため、全列を使用します。\")\n",
    "\n",
    "    col_to_group: dict[str, str] = {}\n",
    "    group_to_cols: dict[str, list[str]] = {}\n",
    "    for col in cols_for_model:\n",
    "        g = get_feature_group(col)\n",
    "        col_to_group[col] = g\n",
    "        group_to_cols.setdefault(g, []).append(col)\n",
    "\n",
    "    group_names = list(group_to_cols.keys())\n",
    "\n",
    "    group_rank_csv = cell_output_path(3, GROUP_RANKING_FILE)\n",
    "    if os.path.exists(group_rank_csv):\n",
    "        group_rank_df = pd.read_csv(group_rank_csv, index_col=0)\n",
    "        if \"rank_mean\" in group_rank_df.columns:\n",
    "            group_order = group_rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "        else:\n",
    "            group_order = list(group_rank_df.index)\n",
    "        print(f\"[Cell5B-SHAP] グループランキング順に従って描画します。groups={len(group_order)}\")\n",
    "    else:\n",
    "        group_order = group_names\n",
    "        print(\"[Cell5B-SHAP] GROUP_RANKING_FILE が無いため、グループ名の順で描画します。\")\n",
    "    group_order = [g for g in group_order if g in group_to_cols]\n",
    "\n",
    "    X_shap = X_all[cols_for_model].astype(np.float32)\n",
    "    y_shap = y_all.astype(int)\n",
    "    print(f\"[Cell5B-SHAP] samples={X_shap.shape[0]}, columns={X_shap.shape[1]}\")\n",
    "\n",
    "    model = fit_classifier(X_shap, y_shap)\n",
    "\n",
    "    background = shap.sample(X_shap, min(256, len(X_shap)), random_state=SEED_BASE)\n",
    "    explainer = shap.TreeExplainer(\n",
    "        model,\n",
    "        data=background,\n",
    "        model_output=\"probability\",\n",
    "        feature_perturbation=\"interventional\",\n",
    "    )\n",
    "\n",
    "    shap_values_any = explainer.shap_values(X_shap)\n",
    "    if isinstance(shap_values_any, list):\n",
    "        if hasattr(model, \"classes_\") and 1 in list(model.classes_):\n",
    "            class_idx = list(model.classes_).index(1)\n",
    "        else:\n",
    "            class_idx = -1\n",
    "        shap_values_col = shap_values_any[class_idx]\n",
    "    else:\n",
    "        shap_values_col = shap_values_any\n",
    "\n",
    "    shap_values_col = np.asarray(shap_values_col)\n",
    "    if shap_values_col.ndim == 3:\n",
    "        if hasattr(model, \"classes_\") and 1 in list(model.classes_):\n",
    "            pos_idx = list(model.classes_).index(1)\n",
    "        else:\n",
    "            pos_idx = -1\n",
    "        shap_values_col = shap_values_col[:, :, pos_idx]\n",
    "    elif shap_values_col.ndim == 1:\n",
    "        shap_values_col = shap_values_col.reshape(-1, 1)\n",
    "\n",
    "    if shap_values_col.shape[1] != X_shap.shape[1]:\n",
    "        raise RuntimeError(f\"[Cell5B-SHAP] shap_values 形状が一致しません: {shap_values_col.shape} vs {X_shap.shape}\")\n",
    "\n",
    "    n_samples = shap_values_col.shape[0]\n",
    "    n_groups = len(group_order)\n",
    "\n",
    "    shap_values_group = np.zeros((n_samples, n_groups), dtype=float)\n",
    "    for j, g in enumerate(group_order):\n",
    "        cols = group_to_cols[g]\n",
    "        idx_cols = [X_shap.columns.get_loc(c) for c in cols]\n",
    "        shap_values_group[:, j] = np.sum(shap_values_col[:, idx_cols], axis=1)\n",
    "\n",
    "    shap_abs_mean_group = np.mean(np.abs(shap_values_group), axis=0)\n",
    "    shap_abs_mean_group_series = pd.Series(shap_abs_mean_group, index=group_order, name=\"mean_abs\")\n",
    "\n",
    "    shap_mean_df = pd.DataFrame({\n",
    "        \"group\": group_order,\n",
    "        \"mean_abs\": shap_abs_mean_group,\n",
    "    }).sort_values(\"mean_abs\", ascending=False)\n",
    "\n",
    "    shap_csv_path = cell_output_path(5, \"SHAP_GROUP_MEAN_ABS.csv\")\n",
    "    shap_mean_df.to_csv(shap_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell5B-SHAP] グループ SHAP 平均絶対値を保存 -> {shap_csv_path}\")\n",
    "\n",
    "    TOP_K = int(globals().get(\"TOP_SUBSET_K\", 15))\n",
    "    highlight_groups: list[str] = []\n",
    "    subset_json_candidates = [\n",
    "        cell_output_path(5, f\"TOP{TOP_K}_SUBSET_BEST_MSSQ_SPLIT.json\"),\n",
    "        cell_output_path(5, \"TOP10_SUBSET_BEST_MSSQ_SPLIT.json\"),\n",
    "        cell_output_path(5, \"ALLK_SUBSET_BEST_MSSQ_SPLIT.json\"),\n",
    "        cell_output_path(4, f\"TOP{TOP_K}_SUBSET_BEST.json\"),\n",
    "        cell_output_path(3, f\"TOP{TOP_K}_SUBSET_BEST.json\"),\n",
    "    ]\n",
    "    subset_json_path = next((p for p in subset_json_candidates if os.path.exists(p)), None)\n",
    "    if subset_json_path is not None:\n",
    "        try:\n",
    "            with open(subset_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                info = json.load(f)\n",
    "            feature_cols_subset = info.get(\"features\", [])\n",
    "            highlight_groups = sorted({get_feature_group(c) for c in feature_cols_subset if c in col_to_group})\n",
    "            print(f\"[Cell5B-SHAP] subset highlight groups (from {os.path.basename(subset_json_path)}):\")\n",
    "            print(f\"  {highlight_groups}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Cell5B-SHAP][WARN] subset読み込み失敗: {e}\")\n",
    "            highlight_groups = []\n",
    "    else:\n",
    "        print(\"[Cell5B-SHAP] subset JSON が見つからなかったため、ハイライトなし。\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values_group,\n",
    "        features=pd.DataFrame(shap_values_group, columns=group_order),\n",
    "        feature_names=group_order,\n",
    "        max_display=TOP_K,\n",
    "        show=False,\n",
    "        plot_type=\"bar\",\n",
    "    )\n",
    "    if highlight_groups:\n",
    "        ax = plt.gca()\n",
    "        for label in ax.get_yticklabels():\n",
    "            if label.get_text() in highlight_groups:\n",
    "                label.set_color(\"red\")\n",
    "                label.set_fontweight(\"bold\")\n",
    "    plt.tight_layout()\n",
    "    summary_top_path = cell_output_path(5, f\"SHAP_GROUP_SUMMARY_TOP{TOP_K}.png\")\n",
    "    plt.savefig(summary_top_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell5B-SHAP] Summary plot (Top-{TOP_K} groups) -> {summary_top_path}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(\n",
    "        shap_values_group,\n",
    "        features=pd.DataFrame(shap_values_group, columns=group_order),\n",
    "        feature_names=group_order,\n",
    "        max_display=len(group_order),\n",
    "        show=False,\n",
    "        plot_type=\"bar\",\n",
    "    )\n",
    "    if highlight_groups:\n",
    "        ax = plt.gca()\n",
    "        for label in ax.get_yticklabels():\n",
    "            if label.get_text() in highlight_groups:\n",
    "                label.set_color(\"red\")\n",
    "                label.set_fontweight(\"bold\")\n",
    "    plt.tight_layout()\n",
    "    summary_all_path = cell_output_path(5, \"SHAP_GROUP_SUMMARY_ALL.png\")\n",
    "    plt.savefig(summary_all_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell5B-SHAP] Summary plot (ALL groups) -> {summary_all_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ced576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6 helper: Inner LOSO folds builder =====\n",
    "set_cell_output(6)\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def choose_inner_folds_loso(train_subject_ids: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    外側LOSOで得た “学習側の被験者ID” リストを受け取り、\n",
    "    1名ずつ検証に回す inner-LOSO のfoldリスト（[[sid1], [sid2], ...]）を返す。\n",
    "    \"\"\"\n",
    "    if not isinstance(train_subject_ids, (list, tuple)):\n",
    "        raise RuntimeError(\"[inner folds] train_subject_ids は list/tuple を想定しています。\")\n",
    "    uniq = list(pd.unique(pd.Series([str(sid) for sid in train_subject_ids])))\n",
    "    if len(uniq) == 0:\n",
    "        raise RuntimeError(\"[inner folds] train_subject_ids が空です。\")\n",
    "    uniq_sorted = sorted(uniq, key=lambda x: (len(x), x))\n",
    "    folds = [[sid] for sid in uniq_sorted]\n",
    "    print(f\"[inner folds] {len(folds)} splits -> val subjects = {', '.join(uniq_sorted)}\")\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6-Neutral: inner-LOSO τ最適化（グリッド探索→CSV出力のみ） =====\n",
    "RUN_CELL6_NEUTRAL = bool(globals().get('RUN_CELL6_NEUTRAL', True))\n",
    "if not RUN_CELL6_NEUTRAL:\n",
    "    print('[Cell6N-18] RUN_CELL6_NEUTRAL=False -> skip')\n",
    "else:\n",
    "    set_cell_output(6)\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn.metrics as skm\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    \n",
    "    # ---------------- モード切替（F1 / BA） ----------------\n",
    "    CELL6_MODE = str(globals().get(\"CELL6_MODE\", \"F1\")).upper()\n",
    "    if CELL6_MODE not in {\"F1\", \"BA\"}:\n",
    "        raise ValueError(f\"[Cell6] CELL6_MODE は 'F1' または 'BA' を指定してください（今: {CELL6_MODE}）\")\n",
    "    MODE_TAG = CELL6_MODE\n",
    "    METRIC_LABEL = MODE_TAG\n",
    "    \n",
    "    # グリッド探索の分解能（未定義ならデフォルト値）\n",
    "    COARSE_STEPS = int(globals().get(\"COARSE_STEPS\", 51))\n",
    "    FINE_STEPS   = int(globals().get(\"FINE_STEPS\", 51))\n",
    "    FINE_MARGIN  = float(globals().get(\"FINE_MARGIN\", 0.05))\n",
    "    \n",
    "    # ---------------- 出力ディレクトリ（Cell6 内に F1/BA サブディレクトリ） ----------------\n",
    "    CELL6_ROOT = OUT_DIR  # 例: .../Cell6\n",
    "    MODE_DIR = os.path.join(CELL6_ROOT, MODE_TAG)\n",
    "    os.makedirs(MODE_DIR, exist_ok=True)\n",
    "    \n",
    "    def cell6_out(filename: str) -> str:\n",
    "        path = os.path.join(MODE_DIR, filename)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        return path\n",
    "    \n",
    "    GROUP_AWARE_DIR = os.path.join(MODE_DIR, \"GROUP_AWARE\")\n",
    "    os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "    \n",
    "    def groupaware_out(filename: str) -> str:\n",
    "        path = os.path.join(GROUP_AWARE_DIR, filename)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        return path\n",
    "    \n",
    "    # ---------------- 群分け基準（MSSQ / VIMSSQ 切り替え） ----------------\n",
    "    GROUPING_BASIS_FOR_FAIRNESS = globals().get(\n",
    "        \"GROUPING_BASIS_FOR_FAIRNESS\",\n",
    "        globals().get(\"GROUPING_BASIS_FOR_PLOTS\", \"MSSQ\"),\n",
    "    )\n",
    "    basis = str(GROUPING_BASIS_FOR_FAIRNESS).upper()\n",
    "    if basis == \"MSSQ\":\n",
    "        GROUP_COL_NAME = \"MSSQ_group\"\n",
    "    elif basis == \"VIMSSQ\":\n",
    "        GROUP_COL_NAME = \"VIMSSQ_group\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"[Cell6] GROUPING_BASIS_FOR_FAIRNESS は 'MSSQ' か 'VIMSSQ' を指定してください（今: {GROUPING_BASIS_FOR_FAIRNESS}）\"\n",
    "        )\n",
    "    print(f\"[Cell6] MODE={MODE_TAG} / group_col={GROUP_COL_NAME}\")\n",
    "    \n",
    "    # ---------------- 基本チェック ----------------\n",
    "    req = [\n",
    "        \"X_all\", \"y_all\", \"groups\", \"SUBJECT_META\",\n",
    "        \"choose_inner_folds_loso\", \"fit_classifier\",\n",
    "        \"predict_positive_score\", \"outpath\"\n",
    "    ]\n",
    "    missing = [v for v in req if v not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell6] 未定義の変数/関数があります: {missing}\")\n",
    "    \n",
    "    # ---------------- 入力整形 ----------------\n",
    "    X_base = X_all.astype(np.float32)\n",
    "    y_base = y_all.astype(int)\n",
    "    g_base = groups.astype(str)\n",
    "    \n",
    "    # 群ラベルを High/Low に正規化\n",
    "    if \"subject_id\" in SUBJECT_META.columns:\n",
    "        mapper = SUBJECT_META.set_index(\"subject_id\")[GROUP_COL_NAME].astype(str).to_dict()\n",
    "    else:\n",
    "        mapper = SUBJECT_META[GROUP_COL_NAME].astype(str).to_dict()\n",
    "    \n",
    "    fair_groups_raw = g_base.map(mapper)\n",
    "    if fair_groups_raw.isna().any():\n",
    "        raise RuntimeError(f\"[Cell6] {GROUP_COL_NAME} 未割当ID: {sorted(set(g_base[fair_groups_raw.isna()]))}\")\n",
    "    \n",
    "    fair_groups = (\n",
    "        fair_groups_raw.astype(str).str.strip().str.lower().map({\"high\": \"High\", \"low\": \"Low\"})\n",
    "    )\n",
    "    if fair_groups.isna().any():\n",
    "        bad_labels = sorted(set(fair_groups_raw[~fair_groups_raw.isin([\"High\",\"Low\",\"high\",\"low\"]) ]))\n",
    "        raise RuntimeError(f\"[Cell6] {GROUP_COL_NAME} に 'High'/'Low' 以外のラベル: {bad_labels}\")\n",
    "    \n",
    "    print(f\"\\n[Cell6-DEBUG] SUBJECT_META {GROUP_COL_NAME} 分類一覧\")\n",
    "    if \"subject_id\" in SUBJECT_META.columns:\n",
    "        dbg_meta = SUBJECT_META[[\"subject_id\", GROUP_COL_NAME]].copy()\n",
    "    else:\n",
    "        dbg_meta = SUBJECT_META.reset_index()[[\"subject_id\", GROUP_COL_NAME]].copy()\n",
    "    print(dbg_meta.sort_values([GROUP_COL_NAME, \"subject_id\"]).to_string(index=False))\n",
    "    \n",
    "    # ---------------- 特徴選抜（Cell3A-Subset の JSON を流用） ----------------\n",
    "    subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "    subset_candidates = [\n",
    "        cell_output_path(4, subset_primary),\n",
    "        cell_output_path(4, \"TOP10_SUBSET_BEST.json\"),\n",
    "        cell_output_path(3, subset_primary),\n",
    "        cell_output_path(3, \"TOP10_SUBSET_BEST.json\"),\n",
    "    ]\n",
    "    subset_json = next((p for p in subset_candidates if os.path.exists(p)), None)\n",
    "    if subset_json is None:\n",
    "        raise FileNotFoundError(\"[Cell6] TOP*_SUBSET_BEST.json が見つからない．Cell4 を実行すること．\")\n",
    "    \n",
    "    with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        subset_info = json.load(f)\n",
    "    raw_features = subset_info.get(\"features\", [])\n",
    "    if not raw_features:\n",
    "        raise RuntimeError(f\"[Cell6] JSON 内に 'features' が空です -> {os.path.basename(subset_json)}\")\n",
    "    \n",
    "    feature_order = [f for f in raw_features if f in X_base.columns]\n",
    "    if not feature_order:\n",
    "        raise RuntimeError(f\"[Cell6] JSON の features が X_all に1つも存在しない: {raw_features}\")\n",
    "    \n",
    "    extra_traits = []\n",
    "    if globals().get(\"USE_MSSQ_FEATURE\", False) and \"MSSQ\" in X_base.columns:\n",
    "        extra_traits.append(\"MSSQ\")\n",
    "    if globals().get(\"USE_VIMSSQ_FEATURE\", False) and \"VIMSSQ\" in X_base.columns:\n",
    "        extra_traits.append(\"VIMSSQ\")\n",
    "    extra_traits = [f for f in extra_traits if f not in feature_order]\n",
    "    \n",
    "    feats_k = feature_order + extra_traits\n",
    "    print(f\"[Cell6] Using subset features from {os.path.basename(subset_json)}\")\n",
    "    print(f\"[Cell6] JSON features (base) k={len(feature_order)}: {feature_order}\")\n",
    "    if extra_traits:\n",
    "        print(f\"[Cell6] 追加で使用する属性特徴: {extra_traits}\")\n",
    "    print(f\"[Cell6] 最終的に使用する特徴数 = {len(feats_k)}\")\n",
    "    \n",
    "    X_k = X_base[feats_k]\n",
    "    \n",
    "    # ---------------- 指標ユーティリティ（F1/BA をモードで切替） ----------------\n",
    "    def _conf_from_preds(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        TN, FP, FN, TP = skm.confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "        return TP, FP, FN, TN\n",
    "    \n",
    "    def _metric_from_conf(TP, FP, FN, TN) -> float:\n",
    "        TP = float(TP); FP = float(FP); FN = float(FN); TN = float(TN)\n",
    "        if MODE_TAG == \"F1\":\n",
    "            denom = (2*TP + FP + FN)\n",
    "            return (2*TP / denom) if denom > 0 else 0.0\n",
    "        # BA\n",
    "        tpr = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        tnr = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "        return 0.5 * (tpr + tnr)\n",
    "    \n",
    "    def _metric_binary(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        TP, FP, FN, TN = _conf_from_preds(y_true, y_pred)\n",
    "        return _metric_from_conf(TP, FP, FN, TN)\n",
    "    \n",
    "    def _grid(l, r, steps):\n",
    "        l = float(max(0.0, l)); r = float(min(1.0, r))\n",
    "        if l > r: l, r = r, l\n",
    "        return np.linspace(l, r, int(steps), dtype=float)\n",
    "    \n",
    "    # ---------------- τ最適化（Single / WG / Group / Attr） ----------------\n",
    "    def _single_tau_opt(scores: np.ndarray, y: np.ndarray):\n",
    "        cands = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "        vals = np.array([_metric_binary(y, (scores >= t).astype(int)) for t in cands])\n",
    "        idx = int(np.nanargmax(vals)); tau0 = float(cands[idx]); best0 = float(vals[idx])\n",
    "    \n",
    "        left, right = max(0.0, tau0 - FINE_MARGIN), min(1.0, tau0 + FINE_MARGIN)\n",
    "        cands2 = _grid(left, right, FINE_STEPS)\n",
    "        vals2 = np.array([_metric_binary(y, (scores >= t).astype(int)) for t in cands2])\n",
    "        idx2 = int(np.nanargmax(vals2)); tau = float(cands2[idx2]); best = float(vals2[idx2])\n",
    "    \n",
    "        return {\n",
    "            \"tau\": tau,\n",
    "            f\"{METRIC_LABEL}_val\": best,\n",
    "            \"tau_coarse\": tau0,\n",
    "            f\"{METRIC_LABEL}_coarse\": best0,\n",
    "        }\n",
    "    \n",
    "    def _wg_opt_joint(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "        maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "        if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "            raise RuntimeError(\"[WG] 連結valに High/Low の両群が必要\")\n",
    "    \n",
    "        sH, yH = scores[maskH], y[maskH]\n",
    "        sL, yL = scores[maskL], y[maskL]\n",
    "    \n",
    "        candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "        candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    \n",
    "        def _metric_vec(s, yy, cands):\n",
    "            return np.array([_metric_binary(yy, (s >= t).astype(int)) for t in cands])\n",
    "    \n",
    "        mH = _metric_vec(sH, yH, candH)\n",
    "        mL = _metric_vec(sL, yL, candL)\n",
    "    \n",
    "        best = {\"WG\": -np.inf, \"pooled\": -np.inf, \"tH\": 0.5, \"tL\": 0.5, \"mH\": 0.0, \"mL\": 0.0}\n",
    "    \n",
    "        for i, tH in enumerate(candH):\n",
    "            wg_row = np.minimum(mH[i], mL)\n",
    "            j = int(np.nanargmax(wg_row))\n",
    "            WG = float(wg_row[j])\n",
    "    \n",
    "            yhatH = (sH >= tH).astype(int)\n",
    "            yhatL = (sL >= candL[j]).astype(int)\n",
    "            pooled = _metric_binary(\n",
    "                np.concatenate([yH, yL]),\n",
    "                np.concatenate([yhatH, yhatL]),\n",
    "            )\n",
    "    \n",
    "            cand = {\n",
    "                \"WG\": WG,\n",
    "                \"pooled\": float(pooled),\n",
    "                \"tH\": float(tH),\n",
    "                \"tL\": float(candL[j]),\n",
    "                \"mH\": float(mH[i]),\n",
    "                \"mL\": float(mL[j]),\n",
    "            }\n",
    "    \n",
    "            def _is_better(cur, new):\n",
    "                if new[\"WG\"] > cur[\"WG\"]: return True\n",
    "                if new[\"WG\"] < cur[\"WG\"]: return False\n",
    "                if new[\"pooled\"] > cur[\"pooled\"]: return True\n",
    "                if new[\"pooled\"] < cur[\"pooled\"]: return False\n",
    "                if abs(new[\"tH\"]-new[\"tL\"]) < abs(cur[\"tH\"]-cur[\"tL\"]): return True\n",
    "                if abs(new[\"tH\"]-new[\"tL\"]) > abs(cur[\"tH\"]-cur[\"tL\"]): return False\n",
    "                if (new[\"tH\"], new[\"tL\"]) < (cur[\"tH\"], cur[\"tL\"]): return True\n",
    "                return False\n",
    "    \n",
    "            if _is_better(best, cand):\n",
    "                best = cand\n",
    "    \n",
    "        # fine search\n",
    "        lH, rH = max(0.0, best[\"tH\"] - FINE_MARGIN), min(1.0, best[\"tH\"] + FINE_MARGIN)\n",
    "        lL, rL = max(0.0, best[\"tL\"] - FINE_MARGIN), min(1.0, best[\"tL\"] + FINE_MARGIN)\n",
    "        candH2 = _grid(lH, rH, FINE_STEPS)\n",
    "        candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "    \n",
    "        best2 = dict(best)\n",
    "    \n",
    "        def _metric_vec2(s, yy, cands):\n",
    "            return np.array([_metric_binary(yy, (s >= t).astype(int)) for t in cands])\n",
    "    \n",
    "        mH2 = _metric_vec2(sH, yH, candH2)\n",
    "        mL2 = _metric_vec2(sL, yL, candL2)\n",
    "    \n",
    "        for i, tH in enumerate(candH2):\n",
    "            wg_row = np.minimum(mH2[i], mL2)\n",
    "            j = int(np.nanargmax(wg_row))\n",
    "            WG = float(wg_row[j])\n",
    "    \n",
    "            yhatH = (sH >= tH).astype(int)\n",
    "            yhatL = (sL >= candL2[j]).astype(int)\n",
    "            pooled = _metric_binary(\n",
    "                np.concatenate([yH, yL]),\n",
    "                np.concatenate([yhatH, yhatL]),\n",
    "            )\n",
    "    \n",
    "            cand = {\n",
    "                \"WG\": WG,\n",
    "                \"pooled\": float(pooled),\n",
    "                \"tH\": float(tH),\n",
    "                \"tL\": float(candL2[j]),\n",
    "                \"mH\": float(mH2[i]),\n",
    "                \"mL\": float(mL2[j]),\n",
    "            }\n",
    "    \n",
    "            if (cand[\"WG\"] > best2[\"WG\"] or\n",
    "                (cand[\"WG\"] == best2[\"WG\"] and\n",
    "                 (cand[\"pooled\"] > best2[\"pooled\"] or\n",
    "                  (cand[\"pooled\"] == best2[\"pooled\"] and\n",
    "                   abs(cand[\"tH\"]-cand[\"tL\"]) <= abs(best2[\"tH\"]-best2[\"tL\"])) ))):\n",
    "                best2 = cand\n",
    "    \n",
    "        return {\n",
    "            \"tauH\": best2[\"tH\"], \"tauL\": best2[\"tL\"],\n",
    "            f\"{METRIC_LABEL}_H_val\": best2[\"mH\"],\n",
    "            f\"{METRIC_LABEL}_L_val\": best2[\"mL\"],\n",
    "            f\"WG_{METRIC_LABEL}_val\": best2[\"WG\"],\n",
    "            f\"{METRIC_LABEL}_pooled_val\": best2[\"pooled\"],\n",
    "        }\n",
    "    \n",
    "    def _group_opt(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "        \"\"\"全体（High+Low）の METRIC_LABEL を最大化する group-wise τ\"\"\"\n",
    "        maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "        if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "            raise RuntimeError(\"[Group] 連結valに High/Low の両群が必要\")\n",
    "    \n",
    "        sH, yH = scores[maskH], y[maskH]\n",
    "        sL, yL = scores[maskL], y[maskL]\n",
    "    \n",
    "        candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "        candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    \n",
    "        def _pooled_for(tH, tL):\n",
    "            yhatH = (sH >= tH).astype(int)\n",
    "            yhatL = (sL >= tL).astype(int)\n",
    "            return _metric_binary(\n",
    "                np.concatenate([yH, yL]),\n",
    "                np.concatenate([yhatH, yhatL]),\n",
    "            )\n",
    "    \n",
    "        best = {\"val\": -np.inf, \"tH\": None, \"tL\": None}\n",
    "        for tH in candH:\n",
    "            vals = np.array([_pooled_for(tH, tL) for tL in candL])\n",
    "            jmax = int(np.nanargmax(vals))\n",
    "            if float(vals[jmax]) > best[\"val\"]:\n",
    "                best = {\"val\": float(vals[jmax]), \"tH\": float(tH), \"tL\": float(candL[jmax])}\n",
    "    \n",
    "        lH, rH = max(0.0, best[\"tH\"] - FINE_MARGIN), min(1.0, best[\"tH\"] + FINE_MARGIN)\n",
    "        lL, rL = max(0.0, best[\"tL\"] - FINE_MARGIN), min(1.0, best[\"tL\"] + FINE_MARGIN)\n",
    "        candH2 = _grid(lH, rH, FINE_STEPS)\n",
    "        candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "    \n",
    "        best2 = dict(best)\n",
    "        for tH in candH2:\n",
    "            vals2 = np.array([_pooled_for(tH, tL) for tL in candL2])\n",
    "            jmax = int(np.nanargmax(vals2))\n",
    "            if float(vals2[jmax]) > best2[\"val\"]:\n",
    "                best2 = {\"val\": float(vals2[jmax]), \"tH\": float(tH), \"tL\": float(candL2[jmax])}\n",
    "    \n",
    "        return {\"tauH\": best2[\"tH\"], \"tauL\": best2[\"tL\"], f\"{METRIC_LABEL}_val\": best2[\"val\"]}\n",
    "    \n",
    "    def _attr_independent_opt(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "        \"\"\"\n",
    "        属性ごとに分割し，High / Low それぞれの METRIC_LABEL（F1 or BA）を\n",
    "        独立に最大化する τ を求めるアルゴリズム。\n",
    "        \"\"\"\n",
    "        maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "        if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "            raise RuntimeError(\"[Attr] 連結valに High/Low の両群が必要\")\n",
    "    \n",
    "        def _best_for_mask(mask):\n",
    "            s_g, y_g = scores[mask], y[mask]\n",
    "            if s_g.size == 0:\n",
    "                raise RuntimeError(\"[Attr] ある属性群の val が空です。\")\n",
    "            cands = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "            vals = np.array([_metric_binary(y_g, (s_g >= t).astype(int)) for t in cands])\n",
    "            idx = int(np.nanargmax(vals)); tau0 = float(cands[idx])\n",
    "    \n",
    "            left, right = max(0.0, tau0 - FINE_MARGIN), min(1.0, tau0 + FINE_MARGIN)\n",
    "            cands2 = _grid(left, right, FINE_STEPS)\n",
    "            vals2 = np.array([_metric_binary(y_g, (s_g >= t).astype(int)) for t in cands2])\n",
    "            idx2 = int(np.nanargmax(vals2)); tau = float(cands2[idx2]); best = float(vals2[idx2])\n",
    "            return tau, best\n",
    "    \n",
    "        tauH, bestH = _best_for_mask(maskH)\n",
    "        tauL, bestL = _best_for_mask(maskL)\n",
    "    \n",
    "        return {\n",
    "            \"tauH\": tauH,\n",
    "            \"tauL\": tauL,\n",
    "            f\"{METRIC_LABEL}_H_val\": bestH,\n",
    "            f\"{METRIC_LABEL}_L_val\": bestL,\n",
    "        }\n",
    "    \n",
    "    # ---------------- outer LOSO with inner concatenation ----------------\n",
    "    logo_outer = LeaveOneGroupOut()\n",
    "    rows, pred_rows = [], []\n",
    "    \n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(logo_outer.split(X_k, y_base.values, g_base.values), start=1):\n",
    "        train_mask = pd.Series(False, index=g_base.index); train_mask.iloc[tr_idx] = True\n",
    "        test_mask  = pd.Series(False, index=g_base.index);  test_mask.iloc[te_idx]  = True\n",
    "        test_sid = g_base.iloc[te_idx].iloc[0]\n",
    "    \n",
    "        inner_ids   = sorted(g_base[train_mask].unique())\n",
    "        inner_folds = choose_inner_folds_loso(inner_ids)\n",
    "    \n",
    "        val_scores_all, val_y_all, val_grp_all = [], [], []\n",
    "    \n",
    "        inner_train_groups = fair_groups[train_mask]\n",
    "        if not ((\"High\" in set(inner_train_groups)) and (\"Low\" in set(inner_train_groups))):\n",
    "            raise RuntimeError(f\"[Cell6] fold{fold_id}: inner-train に両群(High/Low)が必要\")\n",
    "    \n",
    "        for inner_val in inner_folds:\n",
    "            val_mask  = g_base.isin(inner_val) & train_mask\n",
    "            trn_mask  = train_mask & (~val_mask)\n",
    "            if not trn_mask.any() or not val_mask.any():\n",
    "                continue\n",
    "    \n",
    "            X_tr, y_tr = X_k[trn_mask], y_base[trn_mask]\n",
    "            X_vl, y_vl = X_k[val_mask], y_base[val_mask]\n",
    "            grp_vl     = fair_groups[val_mask].to_numpy()\n",
    "    \n",
    "            model_inner = fit_classifier(X_tr, y_tr)\n",
    "            sc_vl = predict_positive_score(model_inner, X_vl).astype(float)\n",
    "    \n",
    "            val_scores_all.append(sc_vl)\n",
    "            val_y_all.append(y_vl.to_numpy())\n",
    "            val_grp_all.append(grp_vl)\n",
    "    \n",
    "        if not val_scores_all:\n",
    "            raise RuntimeError(f\"[Cell6] fold{fold_id}: inner val が空です。\")\n",
    "    \n",
    "        val_scores = np.concatenate(val_scores_all)\n",
    "        val_y = np.concatenate(val_y_all)\n",
    "        val_grp = np.concatenate(val_grp_all)\n",
    "    \n",
    "        res_single = _single_tau_opt(val_scores, val_y)\n",
    "        res_wg     = _wg_opt_joint(val_scores, val_y, val_grp)\n",
    "        res_group  = _group_opt(val_scores, val_y, val_grp)\n",
    "        res_attr   = _attr_independent_opt(val_scores, val_y, val_grp)\n",
    "    \n",
    "        # outer train/test\n",
    "        X_tr_out, y_tr_out = X_k[train_mask], y_base[train_mask]\n",
    "        X_te_out, y_te_out = X_k[test_mask], y_base[test_mask]\n",
    "        grp_te_out = fair_groups[test_mask].to_numpy()\n",
    "    \n",
    "        model_outer = fit_classifier(X_tr_out, y_tr_out)\n",
    "        sc_te = predict_positive_score(model_outer, X_te_out).astype(float)\n",
    "    \n",
    "        tau_single      = float(res_single[\"tau\"])\n",
    "        tau_high_group  = float(res_group[\"tauH\"])\n",
    "        tau_low_group   = float(res_group[\"tauL\"])\n",
    "        tau_high_wg     = float(res_wg[\"tauH\"])\n",
    "        tau_low_wg      = float(res_wg[\"tauL\"])\n",
    "        tau_high_attr   = float(res_attr[\"tauH\"])\n",
    "        tau_low_attr    = float(res_attr[\"tauL\"])\n",
    "    \n",
    "        yhat_single = (sc_te >= tau_single).astype(int)\n",
    "        yhat_group  = np.where(\n",
    "            grp_te_out == \"High\",\n",
    "            (sc_te >= tau_high_group).astype(int),\n",
    "            (sc_te >= tau_low_group).astype(int),\n",
    "        )\n",
    "        yhat_wg = np.where(\n",
    "            grp_te_out == \"High\",\n",
    "            (sc_te >= tau_high_wg).astype(int),\n",
    "            (sc_te >= tau_low_wg).astype(int),\n",
    "        )\n",
    "        yhat_attr = np.where(\n",
    "            grp_te_out == \"High\",\n",
    "            (sc_te >= tau_high_attr).astype(int),\n",
    "            (sc_te >= tau_low_attr).astype(int),\n",
    "        )\n",
    "    \n",
    "        metric_single = _metric_binary(y_te_out, yhat_single)\n",
    "        metric_group  = _metric_binary(y_te_out, yhat_group)\n",
    "        metric_wg     = _metric_binary(y_te_out, yhat_wg)\n",
    "        metric_attr   = _metric_binary(y_te_out, yhat_attr)\n",
    "    \n",
    "        pred_rows.append(pd.DataFrame({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_id\": test_sid,\n",
    "            \"group\": grp_te_out,\n",
    "            \"y_true\": y_te_out,\n",
    "            \"proba\": sc_te,\n",
    "            \"tau_single\": tau_single,\n",
    "            \"tau_high_group\": tau_high_group,\n",
    "            \"tau_low_group\": tau_low_group,\n",
    "            \"tau_high_wg\": tau_high_wg,\n",
    "            \"tau_low_wg\": tau_low_wg,\n",
    "            \"tau_high_attr\": tau_high_attr,\n",
    "            \"tau_low_attr\": tau_low_attr,\n",
    "            \"y_pred_single\": yhat_single,\n",
    "            \"y_pred_group\": yhat_group,\n",
    "            \"y_pred_wg\": yhat_wg,\n",
    "            \"y_pred_attr\": yhat_attr,\n",
    "            f\"metric_single_{MODE_TAG}\": metric_single,\n",
    "            f\"metric_group_{MODE_TAG}\": metric_group,\n",
    "            f\"metric_wg_{MODE_TAG}\": metric_wg,\n",
    "            f\"metric_attr_{MODE_TAG}\": metric_attr,\n",
    "        }))\n",
    "    \n",
    "        col_high_group = f\"tau_high_Group{MODE_TAG}\"\n",
    "        col_low_group  = f\"tau_low_Group{MODE_TAG}\"\n",
    "        col_high_wg    = f\"tau_high_WG{MODE_TAG}\"\n",
    "        col_low_wg     = f\"tau_low_WG{MODE_TAG}\"\n",
    "        col_high_attr  = f\"tau_high_Attr{MODE_TAG}\"\n",
    "        col_low_attr   = f\"tau_low_Attr{MODE_TAG}\"\n",
    "    \n",
    "        rows.append({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_id\": test_sid,\n",
    "            \"tau_single\": tau_single,\n",
    "            col_high_group: tau_high_group,\n",
    "            col_low_group:  tau_low_group,\n",
    "            col_high_wg:    tau_high_wg,\n",
    "            col_low_wg:     tau_low_wg,\n",
    "            col_high_attr:  tau_high_attr,\n",
    "            col_low_attr:   tau_low_attr,\n",
    "            f\"val_single_{MODE_TAG}\": res_single[f\"{METRIC_LABEL}_val\"],\n",
    "            f\"val_group_{MODE_TAG}\":  res_group[f\"{METRIC_LABEL}_val\"],\n",
    "            f\"val_wg_{MODE_TAG}\":     res_wg[f\"WG_{METRIC_LABEL}_val\"],\n",
    "            f\"val_attr_H_{MODE_TAG}\": res_attr[f\"{METRIC_LABEL}_H_val\"],\n",
    "            f\"val_attr_L_{MODE_TAG}\": res_attr[f\"{METRIC_LABEL}_L_val\"],\n",
    "        })\n",
    "    \n",
    "    # ---------------- 結果 CSV 出力＋固定0.5閾値の参照値 ----------------\n",
    "    df_pred = pd.concat(pred_rows, ignore_index=True)\n",
    "    df_rows = pd.DataFrame(rows)\n",
    "\n",
    "    # F1 at fixed threshold 0.5 (reference)\n",
    "    y_all = df_pred[\"y_true\"].to_numpy(int)\n",
    "    y_pred_fixed05 = (df_pred[\"proba\"] >= 0.5).astype(int)\n",
    "    try:\n",
    "        f1_fixed05 = skm.f1_score(y_all, y_pred_fixed05) if np.unique(y_all).size == 2 else float(\"nan\")\n",
    "    except Exception:\n",
    "        f1_fixed05 = float(\"nan\")\n",
    "    df_pred[\"y_pred_fixed05\"] = y_pred_fixed05\n",
    "    print(f\"[Cell6] F1@0.5 = {f1_fixed05:.4f} (overall)\")\n",
    "    \n",
    "    pred_path = groupaware_out(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "    df_pred.to_csv(pred_path, index=False, encoding=\"utf-8-sig\")\n",
    "    fold_path = groupaware_out(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "    df_rows.to_csv(fold_path, index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(f\"[Cell6] predictions -> {pred_path}\")\n",
    "    print(f\"[Cell6] thresholds  -> {fold_path}\")\n",
    "    print(f\"[Cell6] DONE. MODE={MODE_TAG}, k={len(feats_k)} features.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441513e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6-Neutral: F1/BA メトリクス計算＋混同行列図＋ROC 図 =====\n",
    "RUN_CELL6_NEUTRAL = bool(globals().get('RUN_CELL6_NEUTRAL', True))\n",
    "if not RUN_CELL6_NEUTRAL:\n",
    "    print('[Cell6N-19] RUN_CELL6_NEUTRAL=False -> skip')\n",
    "else:\n",
    "    set_cell_output(6)\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn.metrics as skm\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # ---------------- モード切替（F1 / BA） ----------------\n",
    "    CELL6_MODE = str(globals().get(\"CELL6_MODE\", \"F1\")).upper()\n",
    "    if CELL6_MODE not in {\"F1\", \"BA\"}:\n",
    "        raise ValueError(f\"[Cell6N.1] CELL6_MODE は 'F1' または 'BA' を指定してください（今: {CELL6_MODE}）\")\n",
    "    MODE_TAG = CELL6_MODE\n",
    "    METRIC_LABEL = MODE_TAG\n",
    "    \n",
    "    # 描画体裁\n",
    "    LW = 1.5\n",
    "    FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "    \n",
    "    # ---------------- パス設定 ----------------\n",
    "    CELL6_ROOT = OUT_DIR              # 例: .../Cell6\n",
    "    MODE_DIR   = os.path.join(CELL6_ROOT, MODE_TAG)\n",
    "    GROUP_AWARE_DIR = os.path.join(MODE_DIR, \"GROUP_AWARE\")\n",
    "    os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "    \n",
    "    def cell6_out(filename: str) -> str:\n",
    "        path = os.path.join(MODE_DIR, filename)\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        return path\n",
    "    \n",
    "    MAT_DIR = os.path.join(MODE_DIR, f\"MATRIX_{MODE_TAG}\")\n",
    "    os.makedirs(MAT_DIR, exist_ok=True)\n",
    "    \n",
    "    pred_path = os.path.join(GROUP_AWARE_DIR, \"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "    fold_path = os.path.join(GROUP_AWARE_DIR, \"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "    if not (os.path.exists(pred_path) and os.path.exists(fold_path)):\n",
    "        raise FileNotFoundError(f\"[Cell6N.1] 必要CSVが見つからない（先に Cell6 {MODE_TAG} を実行しておくこと）\")\n",
    "    \n",
    "    df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "    df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # ---------------- メトリクス用ユーティリティ ----------------\n",
    "    def _conf_from_preds(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "        TN, FP, FN, TP = skm.confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "        return TP, FP, FN, TN\n",
    "    \n",
    "    def _metric_from_conf(TP, FP, FN, TN) -> float:\n",
    "        TP = float(TP); FP = float(FP); FN = float(FN); TN = float(TN)\n",
    "        if MODE_TAG == \"F1\":\n",
    "            denom = (2 * TP + FP + FN)\n",
    "            return (2 * TP / denom) if denom > 0 else 0.0\n",
    "        # BA\n",
    "        tpr = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "        tnr = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "        return 0.5 * (tpr + tnr)\n",
    "    \n",
    "    def _metric_binary(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        TP, FP, FN, TN = _conf_from_preds(y_true, y_pred)\n",
    "        return _metric_from_conf(TP, FP, FN, TN)\n",
    "    \n",
    "    # ---------------- AUC + ROC 図 ----------------\n",
    "    y_pool = df_pred[\"y_true\"].to_numpy()\n",
    "    s_pool = df_pred[\"proba\"].to_numpy()\n",
    "    subj_pool = df_pred[\"test_id\"].to_numpy()\n",
    "    \n",
    "    if len(np.unique(y_pool)) < 2:\n",
    "        raise RuntimeError(\"[Cell6N.1] 真値が単一クラスのため ROC-AUC を計算できません。\")\n",
    "    \n",
    "    auc_obs = float(skm.roc_auc_score(y_pool, s_pool))\n",
    "    \n",
    "    rng = np.random.default_rng(20251101)\n",
    "    df_pool = pd.DataFrame({\"subject\": subj_pool, \"y_true\": y_pool, \"y_score\": s_pool})\n",
    "    subj_ids = df_pool[\"subject\"].unique()\n",
    "    auc_boot = []\n",
    "    for _ in range(2000):\n",
    "        sampled = rng.choice(subj_ids, size=len(subj_ids), replace=True)\n",
    "        df_boot = pd.concat(\n",
    "            [df_pool[df_pool[\"subject\"] == sid] for sid in sampled],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        if df_boot[\"y_true\"].nunique() < 2:\n",
    "            continue\n",
    "        auc_boot.append(float(skm.roc_auc_score(df_boot[\"y_true\"], df_boot[\"y_score\"])))\n",
    "    \n",
    "    if auc_boot:\n",
    "        ci_low = float(np.quantile(auc_boot, 0.025))\n",
    "        ci_high = float(np.quantile(auc_boot, 0.975))\n",
    "    else:\n",
    "        ci_low = ci_high = float(\"nan\")\n",
    "    \n",
    "    pd.DataFrame([{\n",
    "        \"mode\": MODE_TAG,\n",
    "        \"auc\": auc_obs,\n",
    "        \"ci_low\": ci_low,\n",
    "        \"ci_high\": ci_high,\n",
    "    }]).to_csv(cell6_out(f\"AUC_K_CI_{MODE_TAG}.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(f\"[Cell6N.1] AUC={auc_obs:.4f} (95% CI [{ci_low:.4f}, {ci_high:.4f}])\")\n",
    "    \n",
    "    fpr, tpr, _ = skm.roc_curve(y_pool, s_pool)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc_obs:.3f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=LW, color=\"gray\", label=\"Chance\")\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=FS_LABEL)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=FS_LABEL)\n",
    "    plt.title(f\"ROC Curve (Best Subset, {MODE_TAG})\", fontsize=FS_TITLE)\n",
    "    plt.legend(loc=\"lower right\", fontsize=FS_LEGEND)\n",
    "    plt.grid(True, alpha=0.4)\n",
    "    plt.xticks(fontsize=FS_TICK)\n",
    "    plt.yticks(fontsize=FS_TICK)\n",
    "    plt.tight_layout()\n",
    "    roc_path = cell6_out(f\"AUC_K_CI_{MODE_TAG}.png\")\n",
    "    plt.savefig(roc_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell6N.1] ROC 図を保存 -> {roc_path}\")\n",
    "    \n",
    "    # ---------------- F1/BA メトリクス & 混同行列図 ----------------\n",
    "    summary_rows = []\n",
    "    y_true_all = df_pred[\"y_true\"].to_numpy(int)\n",
    "    \n",
    "    # クラスラベル（必要なら「Not CS」「CS」などに変更）\n",
    "    CLASS_LABELS = [\"Non-Sick\", \"Sick\"]\n",
    "    \n",
    "    def _plot_confusion(cm_counts: np.ndarray, key: str, metric_val: float):\n",
    "        \"\"\"\n",
    "        cm_counts : [[TN, FP],\n",
    "                     [FN, TP]]\n",
    "        を入力として，\n",
    "        - Trueラベルごとに正規化（行和=1）\n",
    "        - x軸 = Predicted, y軸 = True\n",
    "        の2×2ヒートマップを描画する。\n",
    "        セルには TN/FP/FN/TP と 実数＋割合 を表示する。\n",
    "        \"\"\"\n",
    "        # ---- Trueラベルごとに正規化（各行の合計=1） ----\n",
    "        cm = cm_counts.astype(float)\n",
    "        row_sum = cm.sum(axis=1, keepdims=True)\n",
    "        cm_norm = np.divide(cm, row_sum, out=np.zeros_like(cm), where=row_sum != 0)\n",
    "        cm_plot = cm_norm  # 行=True, 列=Pred\n",
    "    \n",
    "        # セル名（行=True, 列=Pred）\n",
    "        CELL_NAMES = [[\"TN\", \"FP\"],\n",
    "                      [\"FN\", \"TP\"]]\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "        # 0〜1固定スケール & カラーバーを少し短く細く\n",
    "        im = ax.imshow(cm_plot, vmin=0.0, vmax=1.0, cmap=\"Blues\")\n",
    "        cbar = plt.colorbar(im, ax=ax, shrink=0.8, fraction=0.046, pad=0.04)\n",
    "        cbar.ax.tick_params(labelsize=FS_TICK)\n",
    "    \n",
    "        # 軸目盛（x = Pred, y = True）\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_xticklabels(CLASS_LABELS, fontsize=FS_TICK)\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_yticklabels(CLASS_LABELS, fontsize=FS_TICK, rotation=90, va=\"center\")\n",
    "    \n",
    "        # 各セルに「TN\\n40 (50%)」のように表示\n",
    "        fs_cell = FS_LEGEND - 2  # ちょっと小さめ\n",
    "        for i in range(2):      # 行 = True\n",
    "            for j in range(2):  # 列 = Pred\n",
    "                val = cm_plot[i, j]          # 割合 (0〜1)\n",
    "                cnt = int(cm_counts[i, j])   # 実数カウント\n",
    "                name = CELL_NAMES[i][j]\n",
    "                pct = val * 100.0\n",
    "                txt_color = \"white\" if val >= 0.5 else \"black\"\n",
    "                ax.text(\n",
    "                    j, i,\n",
    "                    f\"{name}\\n{cnt} ({pct:.0f}%)\",\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    fontsize=fs_cell,\n",
    "                    color=txt_color,\n",
    "                )\n",
    "    \n",
    "        # 軸ラベル\n",
    "        ax.set_xlabel(\"Predicted Label\", fontsize=FS_LABEL)\n",
    "        # True Label を少し右（プロット側）に寄せる → labelpad を小さく\n",
    "        ax.set_ylabel(\"True Label\", fontsize=FS_LABEL,\n",
    "                      rotation=90, labelpad=10)\n",
    "    \n",
    "        # タイトルは少し上に\n",
    "        FS_TITLE_CM = 26\n",
    "        ax.set_title(\n",
    "            f\"{MODE_TAG} — {key.upper()}  ({METRIC_LABEL}={metric_val:.3f})\",\n",
    "            fontsize=FS_TITLE_CM,\n",
    "            pad=20\n",
    "        )\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(MAT_DIR, f\"CONF_MATRIX_{MODE_TAG}_{key.upper()}.png\")\n",
    "        plt.savefig(out_path, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"[Cell6N.1] Confusion matrix ({key}) -> {out_path}\")\n",
    "    \n",
    "    \n",
    "    for key, pred_col in [\n",
    "        (\"single\", \"y_pred_single\"),\n",
    "        (\"group\",  \"y_pred_group\"),\n",
    "        (\"wg\",     \"y_pred_wg\"),\n",
    "        (\"attr\",   \"y_pred_attr\"),\n",
    "    ]:\n",
    "        if pred_col not in df_pred.columns:\n",
    "            print(f\"[Cell6N.1] 列 {pred_col} が存在しないため {key} はスキップ\")\n",
    "            continue\n",
    "    \n",
    "        y_pred_all = df_pred[pred_col].to_numpy(int)\n",
    "    \n",
    "        # メトリクス用（カウント）\n",
    "        TP, FP, FN, TN = _conf_from_preds(y_true_all, y_pred_all)\n",
    "        metric_val = _metric_from_conf(TP, FP, FN, TN)\n",
    "    \n",
    "        # 混同行列のカウント（行=True, 列=Pred）\n",
    "        cm_counts = skm.confusion_matrix(y_true_all, y_pred_all, labels=[0, 1])\n",
    "    \n",
    "        summary_rows.append({\n",
    "            \"mode\": MODE_TAG,\n",
    "            \"decision\": key,\n",
    "            \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN,\n",
    "            METRIC_LABEL: metric_val,\n",
    "        })\n",
    "    \n",
    "        print(\n",
    "            f\"[Cell6N.1] {MODE_TAG}-{key.upper()} => \"\n",
    "            f\"{METRIC_LABEL}={metric_val:.4f}  TP={TP}, FP={FP}, FN={FN}, TN={TN}\"\n",
    "        )\n",
    "        _plot_confusion(cm_counts, key, metric_val)\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_df.to_csv(\n",
    "        cell6_out(f\"METRICS_SUMMARY_{MODE_TAG}.csv\"),\n",
    "        index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "    print(f\"[Cell6N.1] METRICS_SUMMARY_{MODE_TAG}.csv を保存しました。\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708c725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6-Neutral: 確率スコア分布図（F1/BA切替） =====\n",
    "RUN_CELL6_NEUTRAL = bool(globals().get('RUN_CELL6_NEUTRAL', True))\n",
    "if not RUN_CELL6_NEUTRAL:\n",
    "    print('[Cell6N-20] RUN_CELL6_NEUTRAL=False -> skip')\n",
    "else:\n",
    "    set_cell_output(6)\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # ---------------- モード切替（F1 / BA） ----------------\n",
    "    CELL6_MODE = str(globals().get(\"CELL6_MODE\", \"F1\")).upper()\n",
    "    if CELL6_MODE not in {\"F1\", \"BA\"}:\n",
    "        raise ValueError(f\"[Cell6N.2] CELL6_MODE は 'F1' または 'BA' を指定してください（今: {CELL6_MODE}）\")\n",
    "    \n",
    "    # ---------- 設定 ----------  \n",
    "    BINS = 40\n",
    "    LW = 1.5\n",
    "    FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "    \n",
    "    COLOR_SICK = \"red\"     # True:Sick\n",
    "    COLOR_NON  = \"blue\"    # True:Non-Sick\n",
    "    \n",
    "    COLOR_SINGLE = \"black\" # single τ\n",
    "    COLOR_GROUP  = \"green\" # group τ\n",
    "    COLOR_WG     = \"purple\"# WG τ\n",
    "    \n",
    "    # ---------- ディレクトリ ----------  \n",
    "    RUN_ROOT = OUT_DIR  # Level1/Cell6\n",
    "    MODE_DIR = os.path.join(RUN_ROOT, CELL6_MODE)\n",
    "    IMG_DIR  = os.path.join(MODE_DIR, f\"PROBA_DIST_{CELL6_MODE}\")\n",
    "    os.makedirs(IMG_DIR, exist_ok=True)\n",
    "    \n",
    "    SAVE_OVERALL_SvG   = os.path.join(IMG_DIR, \"OVERALL_SvGroup.png\")\n",
    "    SAVE_OVERALL_SvWG  = os.path.join(IMG_DIR, \"OVERALL_SvWG.png\")\n",
    "    SAVE_BYGROUP_SvG   = os.path.join(IMG_DIR, \"BYGROUP_SvGroup.png\")\n",
    "    SAVE_BYGROUP_SvWG  = os.path.join(IMG_DIR, \"BYGROUP_SvWG.png\")\n",
    "    \n",
    "    GROUP_AWARE_DIR = os.path.join(MODE_DIR, \"GROUP_AWARE\")\n",
    "    os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "    \n",
    "    def groupaware_path(filename: str) -> str:\n",
    "        return os.path.join(GROUP_AWARE_DIR, filename)\n",
    "    \n",
    "    pred_path = groupaware_path(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "    fold_path = groupaware_path(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "    if not (os.path.exists(pred_path) and os.path.exists(fold_path)):\n",
    "        raise FileNotFoundError(f\"[Cell6N.2] 必要CSVが見つからない（先に Cell6 {CELL6_MODE} を実行）\")\n",
    "    \n",
    "    df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "    df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "    \n",
    "    # ---------- モード自動判定（F1 or BA、列名で確認） ----------  \n",
    "    cols_f1 = {\"high\":\"tau_high_GroupF1\", \"low\":\"tau_low_GroupF1\",\n",
    "               \"wgh\":\"tau_high_WGF1\",     \"wgl\":\"tau_low_WGF1\"}\n",
    "    cols_ba = {\"high\":\"tau_high_GroupBA\", \"low\":\"tau_low_GroupBA\",\n",
    "               \"wgh\":\"tau_high_WGBA\",     \"wgl\":\"tau_low_WGBA\"}\n",
    "    \n",
    "    if all(c in df_fold.columns for c in [cols_f1[\"high\"], cols_f1[\"low\"]]):\n",
    "        mode = \"F1\"\n",
    "        c_high, c_low, c_wgh, c_wgl = (\n",
    "            cols_f1[\"high\"], cols_f1[\"low\"], cols_f1[\"wgh\"], cols_f1[\"wgl\"]\n",
    "        )\n",
    "    elif all(c in df_fold.columns for c in [cols_ba[\"high\"], cols_ba[\"low\"]]):\n",
    "        mode = \"BA\"\n",
    "        c_high, c_low, c_wgh, c_wgl = (\n",
    "            cols_ba[\"high\"], cols_ba[\"low\"], cols_ba[\"wgh\"], cols_ba[\"wgl\"]\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\"[Cell6N.2] しきい値列が見つからない（F1/BAどちらかのCell6出力が必要）\")\n",
    "    \n",
    "    # ---------- 集約: 中央値/IQR（Q1〜Q3） ----------  \n",
    "    def _qstats(s):\n",
    "        s = pd.to_numeric(s, errors=\"coerce\")\n",
    "        s = s[np.isfinite(s)]\n",
    "        if s.size == 0:\n",
    "            return np.nan, np.nan, np.nan, np.nan\n",
    "        med = float(np.nanmedian(s))\n",
    "        q1, q3 = np.nanpercentile(s, [25, 75])\n",
    "        half = float((q3 - q1)/2.0)\n",
    "        return float(med), float(q1), float(q3), half\n",
    "    \n",
    "    tau_single_med, tau_single_q1, tau_single_q3, _ = _qstats(df_fold[\"tau_single\"])\n",
    "    tau_high_med,   tau_high_q1,   tau_high_q3,   _ = _qstats(df_fold[c_high])\n",
    "    tau_low_med,    tau_low_q1,    tau_low_q3,    _ = _qstats(df_fold[c_low])\n",
    "    tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, _ = (\n",
    "        _qstats(df_fold[c_wgh]) if c_wgh in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "    )\n",
    "    tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  _ = (\n",
    "        _qstats(df_fold[c_wgl]) if c_wgl in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "    )\n",
    "    \n",
    "    # ---------- データ分解 ----------  \n",
    "    proba = pd.to_numeric(df_pred[\"proba\"], errors=\"coerce\").values\n",
    "    ytrue = pd.to_numeric(df_pred[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "    grp   = df_pred[\"group\"].astype(str).str.strip()\n",
    "    \n",
    "    p_sick = proba[ytrue == 1]\n",
    "    p_non  = proba[ytrue == 0]\n",
    "    n_sick, n_non = len(p_sick), len(p_non)\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    \n",
    "    # ---------- ユーティリティ ----------  \n",
    "    def _style_axes(ax, title=None):\n",
    "        if title:\n",
    "            ax.set_title(title, fontsize=FS_TITLE)\n",
    "        ax.set_xlabel(\"Predicted probability\", fontsize=FS_LABEL)\n",
    "        ax.set_ylabel(\"Density\", fontsize=FS_LABEL)\n",
    "        ax.tick_params(axis=\"both\", labelsize=FS_TICK)\n",
    "        ax.set_xlim(0, 1)\n",
    "    \n",
    "    def _hist_overall(ax):\n",
    "        ax.hist(p_sick, bins=BINS, density=True, alpha=0.5,\n",
    "                label=f\"True:Sick (n={n_sick})\", color=COLOR_SICK)\n",
    "        ax.hist(p_non,  bins=BINS, density=True, alpha=0.5,\n",
    "                label=f\"True:Non-Sick (n={n_non})\", color=COLOR_NON)\n",
    "    \n",
    "    def _hist_bygroup(axes):\n",
    "        p_sick_H = proba[(ytrue==1) & maskH]\n",
    "        p_non_H  = proba[(ytrue==0) & maskH]\n",
    "        axes[0].hist(p_sick_H, bins=BINS, density=True, alpha=0.5,\n",
    "                     label=f\"True:Sick (n={len(p_sick_H)})\", color=COLOR_SICK)\n",
    "        axes[0].hist(p_non_H,  bins=BINS, density=True, alpha=0.5,\n",
    "                     label=f\"True:Non-Sick (n={len(p_non_H)})\", color=COLOR_NON)\n",
    "        _style_axes(axes[0], \"High group\")\n",
    "    \n",
    "        p_sick_L = proba[(ytrue==1) & maskL]\n",
    "        p_non_L  = proba[(ytrue==0) & maskL]\n",
    "        axes[1].hist(p_sick_L, bins=BINS, density=True, alpha=0.5,\n",
    "                     label=f\"True:Sick (n={len(p_sick_L)})\", color=COLOR_SICK)\n",
    "        axes[1].hist(p_non_L,  bins=BINS, density=True, alpha=0.5,\n",
    "                     label=f\"True:Non-Sick (n={len(p_non_L)})\", color=COLOR_NON)\n",
    "        _style_axes(axes[1], \"Low group\")\n",
    "    \n",
    "    def _vline_with_iqr(ax, x_med, q1, q3, color, ls, label_core):\n",
    "        if np.isfinite(x_med):\n",
    "            if np.isfinite(q1) and np.isfinite(q3):\n",
    "                ax.axvline(x_med, color=color, linestyle=ls, linewidth=LW,\n",
    "                           label=f\"{label_core} = {x_med:.3f} ± {(q3-q1)/2:.3f}\")\n",
    "            else:\n",
    "                ax.axvline(x_med, color=color, linestyle=ls, linewidth=LW,\n",
    "                           label=f\"{label_core} = {x_med:.3f}\")\n",
    "        if np.isfinite(q1) and np.isfinite(q3):\n",
    "            ax.axvspan(q1, q3, color=color, alpha=0.12)\n",
    "    \n",
    "    # ---------- 1) OVERALL: Single vs Group ----------  \n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    _hist_overall(ax)\n",
    "    _vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3,\n",
    "                    COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "    _vline_with_iqr(ax, tau_high_med,   tau_high_q1,   tau_high_q3,\n",
    "                    COLOR_GROUP,  \"--\", f\"τ_high_{mode}\")\n",
    "    _vline_with_iqr(ax, tau_low_med,    tau_low_q1,    tau_low_q3,\n",
    "                    COLOR_GROUP,  \"--\", f\"τ_low_{mode}\")\n",
    "    _style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs Group [{mode}]\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SAVE_OVERALL_SvG, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell6N.2] Saved -> {SAVE_OVERALL_SvG}\")\n",
    "    \n",
    "    # ---------- 2) OVERALL: Single vs WG ----------  \n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    _hist_overall(ax)\n",
    "    _vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3,\n",
    "                    COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "    _vline_with_iqr(ax, tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3,\n",
    "                    COLOR_WG, \":\", \"τ_high_WG\")\n",
    "    _vline_with_iqr(ax, tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,\n",
    "                    COLOR_WG, \":\", \"τ_low_WG\")\n",
    "    _style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs WG [{mode}]\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SAVE_OVERALL_SvWG, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell6N.2] Saved -> {SAVE_OVERALL_SvWG}\")\n",
    "    \n",
    "    # ---------- 3) BY_GROUP: Single vs Group ----------  \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "    _hist_bygroup(axes)\n",
    "    _vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3,\n",
    "                    COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "    _vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3,\n",
    "                    COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "    _vline_with_iqr(axes[0], tau_high_med, tau_high_q1, tau_high_q3,\n",
    "                    COLOR_GROUP, \"--\", f\"τ_high_{mode}\")\n",
    "    _vline_with_iqr(axes[1], tau_low_med,  tau_low_q1,  tau_low_q3,\n",
    "                    COLOR_GROUP, \"--\", f\"τ_low_{mode}\")\n",
    "    axes[0].legend(fontsize=FS_LEGEND)\n",
    "    axes[1].legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SAVE_BYGROUP_SvG, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell6N.2] Saved -> {SAVE_BYGROUP_SvG}\")\n",
    "    \n",
    "    # ---------- 4) BY_GROUP: Single vs WG ----------  \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "    _hist_bygroup(axes)\n",
    "    _vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3,\n",
    "                    COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "    _vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3,\n",
    "                    COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "    _vline_with_iqr(axes[0], tau_high_wg, tau_high_wg_q1, tau_high_wg_q3,\n",
    "                    COLOR_WG, \":\", \"τ_high_WG\")\n",
    "    _vline_with_iqr(axes[1], tau_low_wg,  tau_low_wg_q1,  tau_low_wg_q3,\n",
    "                    COLOR_WG, \":\", \"τ_low_WG\")\n",
    "    axes[0].legend(fontsize=FS_LEGEND)\n",
    "    axes[1].legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(SAVE_BYGROUP_SvWG, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Cell6N.2] Saved -> {SAVE_BYGROUP_SvWG}\")\n",
    "    \n",
    "    # ---------- 5) Fold単位：OVERALL の確率分布と各Foldの τ ----------  \n",
    "    for _, row in df_fold.iterrows():\n",
    "        fid = int(row.get(\"fold_id\", -1)) if \"fold_id\" in row else None\n",
    "        test_id = str(row.get(\"test_id\", f\"fold{fid}\"))\n",
    "        sub = df_pred[df_pred[\"fold_id\"] == fid] if \"fold_id\" in df_pred.columns and fid is not None else df_pred.copy()\n",
    "    \n",
    "        p = pd.to_numeric(sub[\"proba\"], errors=\"coerce\").values\n",
    "        yt = pd.to_numeric(sub[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "        p_s, p_n = p[yt==1], p[yt==0]\n",
    "    \n",
    "        t_single = float(row[\"tau_single\"])\n",
    "        t_high   = float(row[c_high]) if c_high in row else np.nan\n",
    "        t_low    = float(row[c_low])  if c_low  in row else np.nan\n",
    "        t_high_w = float(row[c_wgh])  if c_wgh  in row else np.nan\n",
    "        t_low_w  = float(row[c_wgl])  if c_wgl  in row else np.nan\n",
    "    \n",
    "        p_sg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvGroup.png\")\n",
    "        p_wg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvWG.png\")\n",
    "    \n",
    "        # Single vs Group\n",
    "        fig, ax = plt.subplots(figsize=(9,6))\n",
    "        ax.hist(p_s, bins=BINS, density=True, alpha=0.5,\n",
    "                label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "        ax.hist(p_n, bins=BINS, density=True, alpha=0.5,\n",
    "                label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "        if np.isfinite(t_single):\n",
    "            ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW,\n",
    "                       label=f\"τ_single = {t_single:.3f}\")\n",
    "        if np.isfinite(t_high):\n",
    "            ax.axvline(t_high,   color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW,\n",
    "                       label=f\"τ_high_{mode} = {t_high:.3f}\")\n",
    "        if np.isfinite(t_low):\n",
    "            ax.axvline(t_low,    color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW,\n",
    "                       label=f\"τ_low_{mode}  = {t_low:.3f}\")\n",
    "        _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs Group [{mode}]  (test={test_id})\")\n",
    "        ax.legend(fontsize=FS_LEGEND)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(p_sg, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"[Cell6N.2] Saved -> {p_sg}\")\n",
    "    \n",
    "        # Single vs WG\n",
    "        fig, ax = plt.subplots(figsize=(9,6))\n",
    "        ax.hist(p_s, bins=BINS, density=True, alpha=0.5,\n",
    "                label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "        ax.hist(p_n, bins=BINS, density=True, alpha=0.5,\n",
    "                label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "        if np.isfinite(t_single):\n",
    "            ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW,\n",
    "                       label=f\"τ_single = {t_single:.3f}\")\n",
    "        if np.isfinite(t_high_w):\n",
    "            ax.axvline(t_high_w, color=COLOR_WG,     linestyle=\":\", linewidth=LW,\n",
    "                       label=f\"τ_high_WG = {t_high_w:.3f}\")\n",
    "        if np.isfinite(t_low_w):\n",
    "            ax.axvline(t_low_w,  color=COLOR_WG,     linestyle=\":\", linewidth=LW,\n",
    "                       label=f\"τ_low_WG  = {t_low_w:.3f}\")\n",
    "        _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs WG [{mode}]  (test={test_id})\")\n",
    "        ax.legend(fontsize=FS_LEGEND)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(p_wg, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"[Cell6N.2] Saved -> {p_wg}\")\n",
    "    \n",
    "    print(f\"[Cell6N.2] All images saved in: {IMG_DIR}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6-Stratified: MSSQ層別 τ最適化（Singleのみ）＋F1 =====\n",
    "RUN_CELL6_STRAT = bool(globals().get('RUN_CELL6_STRAT', True))\n",
    "if not RUN_CELL6_STRAT:\n",
    "    print('[Cell6S] RUN_CELL6_STRAT=False -> skip')\n",
    "else:\n",
    "    set_cell_output(6)\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn.metrics as skm\n",
    "\n",
    "    pred_path = cell_output_path(5, \"MSSQ_SPLIT_PREDICTIONS.csv\")\n",
    "    if not os.path.exists(pred_path):\n",
    "        raise FileNotFoundError(\"[Cell6S] MSSQ_SPLIT_PREDICTIONS.CSV が見つかりません。Cell5B を先に実行してください。\")\n",
    "\n",
    "    df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "    required = {\"mssq_group\", \"y_true\", \"y_score\"}\n",
    "    missing = required - set(df_pred.columns)\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell6S] 予測CSVに必要列がありません -> {missing}\")\n",
    "\n",
    "    MSSQ_LOW_LABEL = str(globals().get(\"MSSQ_LOW_LABEL\", \"Low\"))\n",
    "    MSSQ_HIGH_LABEL = str(globals().get(\"MSSQ_HIGH_LABEL\", \"High\"))\n",
    "\n",
    "    df_pred = df_pred.copy()\n",
    "    df_pred[\"mssq_group_norm\"] = (\n",
    "        df_pred[\"mssq_group\"].astype(str).str.strip().map({\n",
    "            MSSQ_LOW_LABEL: \"Low\",\n",
    "            MSSQ_LOW_LABEL.lower(): \"Low\",\n",
    "            MSSQ_HIGH_LABEL: \"High\",\n",
    "            MSSQ_HIGH_LABEL.lower(): \"High\",\n",
    "        })\n",
    "    )\n",
    "    if df_pred[\"mssq_group_norm\"].isna().any():\n",
    "        bad = df_pred.loc[df_pred[\"mssq_group_norm\"].isna(), \"mssq_group\"].unique()\n",
    "        raise RuntimeError(f\"[Cell6S] MSSQ_group に未対応ラベルがあります: {bad}\")\n",
    "\n",
    "    scores = df_pred[\"y_score\"].to_numpy(dtype=float)\n",
    "    y_true = df_pred[\"y_true\"].to_numpy(dtype=int)\n",
    "    grp = df_pred[\"mssq_group_norm\"].to_numpy()\n",
    "\n",
    "    # overall ROC-AUC (reference)\n",
    "    try:\n",
    "        auc_overall = skm.roc_auc_score(y_true, scores) if np.unique(y_true).size == 2 else float(\"nan\")\n",
    "    except Exception:\n",
    "        auc_overall = float(\"nan\")\n",
    "    print(f\"[Cell6S] overall ROC-AUC = {auc_overall:.4f}\")\n",
    "\n",
    "    # F1 at fixed threshold 0.5 (reference)\n",
    "    y_pred_fixed05 = (scores >= 0.5).astype(int)\n",
    "    try:\n",
    "        f1_fixed05 = skm.f1_score(y_true, y_pred_fixed05) if np.unique(y_true).size == 2 else float(\"nan\")\n",
    "    except Exception:\n",
    "        f1_fixed05 = float(\"nan\")\n",
    "    print(f\"[Cell6S] F1@0.5 = {f1_fixed05:.4f}\")\n",
    "\n",
    "    maskH = grp == \"High\"\n",
    "    maskL = grp == \"Low\"\n",
    "    if maskH.sum() == 0 or maskL.sum() == 0:\n",
    "        raise RuntimeError(\"[Cell6S] High/Low 両群のデータが必要です。Cell5B の出力を確認してください。\")\n",
    "\n",
    "    COARSE_STEPS = int(globals().get(\"COARSE_STEPS\", 51))\n",
    "    FINE_STEPS   = int(globals().get(\"FINE_STEPS\", 51))\n",
    "    FINE_MARGIN  = float(globals().get(\"FINE_MARGIN\", 0.05))\n",
    "\n",
    "    def _conf_from_preds(y_t, y_p):\n",
    "        TN, FP, FN, TP = skm.confusion_matrix(y_t, y_p, labels=[0, 1]).ravel()\n",
    "        return TP, FP, FN, TN\n",
    "\n",
    "    def _f1_score(y_t, y_p):\n",
    "        TP, FP, FN, TN = _conf_from_preds(y_t, y_p)\n",
    "        denom = 2 * TP + FP + FN\n",
    "        return (2 * TP / denom) if denom > 0 else 0.0\n",
    "\n",
    "    def _grid(l, r, steps):\n",
    "        l = float(max(0.0, l)); r = float(min(1.0, r))\n",
    "        if l > r:\n",
    "            l, r = r, l\n",
    "        return np.linspace(l, r, int(steps), dtype=float)\n",
    "\n",
    "    def _best_tau(scores_arr, labels_arr):\n",
    "        coarse = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "        vals = np.array([_f1_score(labels_arr, (scores_arr >= t).astype(int)) for t in coarse])\n",
    "        idx = int(np.nanargmax(vals)); tau0 = float(coarse[idx]); best0 = float(vals[idx])\n",
    "\n",
    "        left, right = max(0.0, tau0 - FINE_MARGIN), min(1.0, tau0 + FINE_MARGIN)\n",
    "        fine = _grid(left, right, FINE_STEPS)\n",
    "        vals_f = np.array([_f1_score(labels_arr, (scores_arr >= t).astype(int)) for t in fine])\n",
    "        idx_f = int(np.nanargmax(vals_f)); tau = float(fine[idx_f]); best = float(vals_f[idx_f])\n",
    "        return tau, best, tau0, best0\n",
    "\n",
    "    tauH, f1H, tauH0, f1H0 = _best_tau(scores[maskH], y_true[maskH])\n",
    "    tauL, f1L, tauL0, f1L0 = _best_tau(scores[maskL], y_true[maskL])\n",
    "\n",
    "    y_pred = np.where(\n",
    "        grp == \"High\",\n",
    "        (scores >= tauH).astype(int),\n",
    "        (scores >= tauL).astype(int),\n",
    "    )\n",
    "    f1_all = _f1_score(y_true, y_pred)\n",
    "\n",
    "    summary = pd.DataFrame([\n",
    "        {\"group\": \"overall\", \"tau\": None, \"F1\": f1_all},\n",
    "        {\"group\": \"High\", \"tau\": tauH, \"F1\": f1H, \"tau_coarse\": tauH0, \"F1_coarse\": f1H0},\n",
    "        {\"group\": \"Low\",  \"tau\": tauL, \"F1\": f1L, \"tau_coarse\": tauL0, \"F1_coarse\": f1L0},\n",
    "        {\"group\": \"overall_tau0.5\", \"tau\": 0.5, \"F1\": f1_fixed05},\n",
    "    ])\n",
    "\n",
    "    # save predictions with applied thresholds\n",
    "    df_pred_out = df_pred.copy()\n",
    "    df_pred_out[\"y_pred_best\"] = y_pred\n",
    "    df_pred_out[\"tau_used\"] = np.where(grp == \"High\", tauH, tauL)\n",
    "    pred_with_tau_path = cell_output_path(6, \"MSSQ_SPLIT_PREDICTIONS_WITH_TAU.csv\")\n",
    "    df_pred_out.to_csv(pred_with_tau_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell6S] predictions with tau -> {pred_with_tau_path}\")\n",
    "\n",
    "    out_csv = cell_output_path(6, \"MSSQ_SPLIT_F1_SINGLE.csv\")\n",
    "    summary.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell6S] MSSQ-split thresholds -> High={tauH:.3f}, Low={tauL:.3f}, F1_overall={f1_all:.3f}\")\n",
    "    print(f\"[Cell6S] 保存 -> {out_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6-Stratified: F1メトリクス＋混同行列図＋ROC =====\n",
    "RUN_CELL6_STRAT = bool(globals().get('RUN_CELL6_STRAT', True))\n",
    "if not RUN_CELL6_STRAT:\n",
    "    print('[Cell6S-metrics] RUN_CELL6_STRAT=False -> skip')\n",
    "else:\n",
    "    set_cell_output(6)\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn.metrics as skm\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    pred_path = cell_output_path(6, \"MSSQ_SPLIT_PREDICTIONS_WITH_TAU.csv\")\n",
    "    summary_path = cell_output_path(6, \"MSSQ_SPLIT_F1_SINGLE.csv\")\n",
    "    if not (os.path.exists(pred_path) and os.path.exists(summary_path)):\n",
    "        raise FileNotFoundError(\"[Cell6S-metrics] 必要ファイルがありません。Cell6-Stratified を先に実行してください。\")\n",
    "\n",
    "    df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "    df_sum = pd.read_csv(summary_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "    y_true = df_pred[\"y_true\"].to_numpy(int)\n",
    "    y_pred = df_pred[\"y_pred_best\"].to_numpy(int)\n",
    "    scores = df_pred[\"y_score\"].to_numpy(float)\n",
    "    grp = df_pred[\"mssq_group_norm\"].astype(str).to_numpy()\n",
    "\n",
    "    if np.unique(y_true).size < 2:\n",
    "        raise RuntimeError(\"[Cell6S-metrics] 真値が単一クラスのためメトリクス計算不可\")\n",
    "\n",
    "    def _conf(y_t, y_p):\n",
    "        TN, FP, FN, TP = skm.confusion_matrix(y_t, y_p, labels=[0,1]).ravel()\n",
    "        return TP, FP, FN, TN\n",
    "\n",
    "    def _f1(y_t, y_p):\n",
    "        TP, FP, FN, TN = _conf(y_t, y_p)\n",
    "        denom = 2*TP + FP + FN\n",
    "        return (2*TP/denom) if denom>0 else 0.0\n",
    "\n",
    "    f1_overall = _f1(y_true, y_pred)\n",
    "    maskH = grp == 'High'\n",
    "    maskL = grp == 'Low'\n",
    "    f1_H = _f1(y_true[maskH], y_pred[maskH]) if maskH.sum()>0 else float('nan')\n",
    "    f1_L = _f1(y_true[maskL], y_pred[maskL]) if maskL.sum()>0 else float('nan')\n",
    "\n",
    "    # ROC\n",
    "    auc_overall = skm.roc_auc_score(y_true, scores)\n",
    "    fpr, tpr, _ = skm.roc_curve(y_true, scores)\n",
    "\n",
    "    # 混同行列図（overall）\n",
    "    cm = skm.confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    for (i,j), val in np.ndenumerate(cm):\n",
    "        ax.text(j, i, int(val), ha='center', va='center', color='black', fontsize=14)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_xticks([0,1]); ax.set_yticks([0,1]);\n",
    "    ax.set_title('Confusion matrix (overall)')\n",
    "    plt.tight_layout()\n",
    "    cm_path = cell_output_path(6, 'MSSQ_SPLIT_CM_OVERALL.png')\n",
    "    plt.savefig(cm_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # ROC 図\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc_overall:.3f}')\n",
    "    plt.plot([0,1],[0,1],'--',color='gray')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC (MSSQ-split models)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    roc_path = cell_output_path(6, 'MSSQ_SPLIT_ROC.png')\n",
    "    plt.savefig(roc_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 集計保存\n",
    "    metrics_path = cell_output_path(6, 'MSSQ_SPLIT_METRICS.csv')\n",
    "    pd.DataFrame([\n",
    "        {'group':'overall','F1':f1_overall,'AUC':auc_overall},\n",
    "        {'group':'High','F1':f1_H},\n",
    "        {'group':'Low','F1':f1_L},\n",
    "    ]).to_csv(metrics_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f\"[Cell6S-metrics] F1_overall={f1_overall:.3f}, F1_H={f1_H:.3f}, F1_L={f1_L:.3f}\")\n",
    "    print(f\"[Cell6S-metrics] AUC_overall={auc_overall:.3f}\")\n",
    "    print(f\"[Cell6S-metrics] saved cm->{cm_path}, roc->{roc_path}, metrics->{metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6-Stratified: 確率スコア分布図 =====\n",
    "RUN_CELL6_STRAT = bool(globals().get('RUN_CELL6_STRAT', True))\n",
    "if not RUN_CELL6_STRAT:\n",
    "    print('[Cell6S-proba] RUN_CELL6_STRAT=False -> skip')\n",
    "else:\n",
    "    set_cell_output(6)\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    pred_path = cell_output_path(6, \"MSSQ_SPLIT_PREDICTIONS_WITH_TAU.csv\")\n",
    "    if not os.path.exists(pred_path):\n",
    "        raise FileNotFoundError(\"[Cell6S-proba] MSSQ_SPLIT_PREDICTIONS_WITH_TAU.csv がありません。Cell6-Stratified を実行してください。\")\n",
    "\n",
    "    df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "    proba = df_pred[\"y_score\"].to_numpy(float)\n",
    "    ytrue = df_pred[\"y_true\"].to_numpy(int)\n",
    "    grp   = df_pred[\"mssq_group_norm\"].astype(str).to_numpy()\n",
    "    tau_used = df_pred[\"tau_used\"].to_numpy(float)\n",
    "\n",
    "    BINS = 40\n",
    "    LW = 1.5\n",
    "    FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 24, 18, 16, 14\n",
    "\n",
    "    def _style(ax, title=None):\n",
    "        if title:\n",
    "            ax.set_title(title, fontsize=FS_TITLE)\n",
    "        ax.set_xlabel('Predicted probability', fontsize=FS_LABEL)\n",
    "        ax.set_ylabel('Density', fontsize=FS_LABEL)\n",
    "        ax.tick_params(axis='both', labelsize=FS_TICK)\n",
    "        ax.set_xlim(0,1)\n",
    "\n",
    "    # overall hist\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.hist(proba[ytrue==1], bins=BINS, density=True, alpha=0.5, label=f'True=1 (n={(ytrue==1).sum()})', color='red')\n",
    "    plt.hist(proba[ytrue==0], bins=BINS, density=True, alpha=0.5, label=f'True=0 (n={(ytrue==0).sum()})', color='blue')\n",
    "    # draw representative tau (median of tau_used)\n",
    "    tau_med = float(np.nanmedian(tau_used)) if tau_used.size else 0.5\n",
    "    plt.axvline(tau_med, color='black', linestyle='--', linewidth=LW, label=f'tau_med={tau_med:.2f}')\n",
    "    _style(plt.gca(), 'Probability distribution (overall)')\n",
    "    plt.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout()\n",
    "    path_overall = cell_output_path(6, 'MSSQ_SPLIT_PROBA_OVERALL.png')\n",
    "    plt.savefig(path_overall, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # by group\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,5), sharex=True, sharey=True)\n",
    "    for ax, label, mask, color in [\n",
    "        (axes[0], 'High', grp=='High', 'purple'),\n",
    "        (axes[1], 'Low', grp=='Low', 'green'),\n",
    "    ]:\n",
    "        ax.hist(proba[(ytrue==1)&mask], bins=BINS, density=True, alpha=0.5, label=f'True=1 (n={( (ytrue==1)&mask ).sum()})', color='red')\n",
    "        ax.hist(proba[(ytrue==0)&mask], bins=BINS, density=True, alpha=0.5, label=f'True=0 (n={( (ytrue==0)&mask ).sum()})', color='blue')\n",
    "        tau_g = float(np.nanmedian(tau_used[mask])) if mask.any() else 0.5\n",
    "        ax.axvline(tau_g, color=color, linestyle='--', linewidth=LW, label=f'tau_med={tau_g:.2f}')\n",
    "        _style(ax, f'Group={label}')\n",
    "        ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout()\n",
    "    path_bygrp = cell_output_path(6, 'MSSQ_SPLIT_PROBA_BYGROUP.png')\n",
    "    plt.savefig(path_bygrp, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[Cell6S-proba] saved -> {path_overall}, {path_bygrp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f532b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Summary: ANALYSIS/機械学習 配下の実験を集約 =====\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\\ANALYSIS\\機械学習(層別学習)\")\n",
    "\n",
    "\n",
    "def load_subset_features(base_dir: Path, *, include=None, exclude=None):\n",
    "    \"\"\"base_dir 配下で *SUBSET_BEST*.json を探し、最初に読めた features を返す。\"\"\"\n",
    "    def _ok(name: str) -> bool:\n",
    "        lname = name.lower()\n",
    "        if include and not any(tok.lower() in lname for tok in include):\n",
    "            return False\n",
    "        if exclude and any(tok.lower() in lname for tok in exclude):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    for cand in sorted(base_dir.glob('**/*SUBSET_BEST*.json')):\n",
    "        if not _ok(cand.name):\n",
    "            continue\n",
    "        try:\n",
    "            with open(cand, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            feats = data.get('features', []) or []\n",
    "            return cand.name, feats\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None, []\n",
    "\n",
    "\n",
    "def read_metrics_neutral(level1: Path, mode: str):\n",
    "    \"\"\"Cell6-Neutral の METRICS_SUMMARY_{mode}.csv を読む。\"\"\"\n",
    "    res = {\"single\": None, \"group\": None, \"wg\": None, \"attr\": None}\n",
    "    path = level1 / 'Cell6' / mode / f'METRICS_SUMMARY_{mode}.csv'\n",
    "    if not path.exists():\n",
    "        return res\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        if mode in df.columns and 'decision' in df.columns:\n",
    "            for dec in ['single', 'group', 'wg', 'attr']:\n",
    "                row = df[df['decision'] == dec]\n",
    "                if not row.empty and not pd.isna(row[mode].iloc[0]):\n",
    "                    res[dec] = float(row[mode].iloc[0])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return res\n",
    "\n",
    "\n",
    "def read_f1_fixed05_neutral(level1: Path):\n",
    "    pred_path = level1 / 'Cell6' / 'F1' / 'GROUP_AWARE' / 'GROUP_AWARE_PREDICTIONS.CSV'\n",
    "    if not pred_path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(pred_path)\n",
    "        if 'y_pred_fixed05' in df.columns:\n",
    "            y_pred = df['y_pred_fixed05']\n",
    "        elif {'proba', 'y_true'}.issubset(df.columns):\n",
    "            y_pred = (pd.to_numeric(df['proba'], errors='coerce') >= 0.5).astype(int)\n",
    "        else:\n",
    "            return None\n",
    "        y_true = pd.to_numeric(df['y_true'], errors='coerce').astype(int)\n",
    "        if y_true.nunique() < 2:\n",
    "            return None\n",
    "        tn, fp, fn, tp = pd.crosstab(y_true, y_pred, dropna=False).reindex(index=[0,1], columns=[0,1], fill_value=0).values.ravel()\n",
    "        denom = 2*tp + fp + fn\n",
    "        return float((2*tp/denom) if denom>0 else 0.0)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_auc_neutral(level1: Path):\n",
    "    path = level1 / 'Cell4' / 'AUC_K_CI.csv'\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        return float(df.get('auc', pd.Series([None])).iloc[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def read_auc_strat(level1: Path):\n",
    "    \"\"\"Cell5A出力 AUC_K_CI_MSSQ_SPLIT.csv から overall/Low/High を取得。\"\"\"\n",
    "    path = level1 / 'Cell5' / 'AUC_K_CI_MSSQ_SPLIT.csv'\n",
    "    res = {\"overall\": None, \"low\": None, \"high\": None}\n",
    "    if not path.exists():\n",
    "        return res\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        if 'group' in df.columns and 'auc' in df.columns:\n",
    "            for _, row in df.iterrows():\n",
    "                g = str(row['group']).lower()\n",
    "                auc = row.get('auc', None)\n",
    "                if pd.isna(auc):\n",
    "                    continue\n",
    "                if g == 'overall':\n",
    "                    res['overall'] = float(auc)\n",
    "                elif 'low' in g:\n",
    "                    res['low'] = float(auc)\n",
    "                elif 'high' in g:\n",
    "                    res['high'] = float(auc)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return res\n",
    "\n",
    "\n",
    "def read_metrics_strat(level1: Path):\n",
    "    \"\"\"Cell6-Stratified の集計 (F1/AUC + 0.5固定F1)。\"\"\"\n",
    "    res = {\"F1_overall\": None, \"F1_H\": None, \"F1_L\": None, \"F1_tau05\": None, \"AUC_overall\": None}\n",
    "\n",
    "    metrics_path = level1 / 'Cell6' / 'MSSQ_SPLIT_METRICS.csv'\n",
    "    if metrics_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(metrics_path)\n",
    "            for _, row in df.iterrows():\n",
    "                g = str(row.get('group', '')).lower()\n",
    "                if 'overall' in g:\n",
    "                    res['F1_overall'] = float(row.get('F1', None)) if not pd.isna(row.get('F1', None)) else None\n",
    "                    res['AUC_overall'] = float(row.get('AUC', None)) if not pd.isna(row.get('AUC', None)) else res['AUC_overall']\n",
    "                elif g == 'high':\n",
    "                    res['F1_H'] = float(row.get('F1', None)) if not pd.isna(row.get('F1', None)) else None\n",
    "                elif g == 'low':\n",
    "                    res['F1_L'] = float(row.get('F1', None)) if not pd.isna(row.get('F1', None)) else None\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    f1single_path = level1 / 'Cell6' / 'MSSQ_SPLIT_F1_SINGLE.csv'\n",
    "    if f1single_path.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(f1single_path)\n",
    "            row = df[df['group'] == 'overall_tau0.5']\n",
    "            if not row.empty and 'F1' in row.columns and not pd.isna(row['F1'].iloc[0]):\n",
    "                res['F1_tau05'] = float(row['F1'].iloc[0])\n",
    "        except Exception:\n",
    "            pass\n",
    "    return res\n",
    "\n",
    "\n",
    "rows = []\n",
    "for level1 in ROOT.glob('*'):\n",
    "    if not level1.is_dir():\n",
    "        continue\n",
    "    parts = level1.name.split('__')\n",
    "    tags = {\"FS_TAG\": \"\", \"H_TAG\": \"\", \"GRID_TAG\": \"\", \"CORR_TAG\": \"\", \"REG_TAG\": \"\"}\n",
    "    for p in parts:\n",
    "        if p.startswith('FS'):\n",
    "            tags['FS_TAG'] = p\n",
    "        elif p.startswith('H'):\n",
    "            tags['H_TAG'] = p\n",
    "        elif p.startswith('GridC'):\n",
    "            tags['GRID_TAG'] = p\n",
    "        elif p.startswith('Corr'):\n",
    "            if '正則化' in p:\n",
    "                corr_part, reg_rest = p.split('正則化', 1)\n",
    "                tags['CORR_TAG'] = corr_part\n",
    "                tags['REG_TAG'] = '正則化' + reg_rest\n",
    "            else:\n",
    "                tags['CORR_TAG'] = p\n",
    "        elif p.startswith('正則化'):\n",
    "            tags['REG_TAG'] = p\n",
    "\n",
    "    # ----- Subset（ニュートラル/層別で分けて取得） -----\n",
    "    neutral_subset_file, neutral_feats = load_subset_features(level1 / 'Cell4', exclude=['mssq'])\n",
    "    strat_subset_file, strat_feats = load_subset_features(level1 / 'Cell5', include=['mssq'])\n",
    "    n_feat_neutral = len(neutral_feats)\n",
    "    n_feat_strat = len(strat_feats)\n",
    "\n",
    "    # ----- ROC-AUC -----\n",
    "    auc_neutral = read_auc_neutral(level1)\n",
    "    auc_strat = read_auc_strat(level1)\n",
    "\n",
    "    # ----- F1/BA（ニュートラル） -----\n",
    "    f1 = read_metrics_neutral(level1, 'F1')\n",
    "    ba = read_metrics_neutral(level1, 'BA')\n",
    "    f1_tau05_neutral = read_f1_fixed05_neutral(level1)\n",
    "\n",
    "    # ----- 層別メトリクス -----\n",
    "    strat = read_metrics_strat(level1)\n",
    "\n",
    "    rows.append({\n",
    "        'Path': str(level1),\n",
    "        'FS_TAG': tags['FS_TAG'], 'H_TAG': tags['H_TAG'], 'GRID_TAG': tags['GRID_TAG'], 'CORR_TAG': tags['CORR_TAG'], 'REG_TAG': tags['REG_TAG'],\n",
    "        'NFEAT_N': n_feat_neutral, 'FEAT_N': ','.join(neutral_feats),\n",
    "        'NFEAT_S': n_feat_strat,   'FEAT_S': ','.join(strat_feats),\n",
    "        'AUC_N': auc_neutral,\n",
    "        'AUC_S': auc_strat.get('overall'), 'AUC_S_L': auc_strat.get('low'), 'AUC_S_H': auc_strat.get('high'),\n",
    "        'F1N_single': f1['single'], 'F1N_group': f1['group'], 'F1N_wg': f1['wg'], 'F1N_attr': f1['attr'], 'F1N_tau05': f1_tau05_neutral,\n",
    "        'F1S': strat['F1_overall'], 'F1S_H': strat['F1_H'], 'F1S_L': strat['F1_L'], 'F1S_tau05': strat['F1_tau05'],\n",
    "        'BAN_single': ba['single'], 'BAN_group': ba['group'], 'BAN_wg': ba['wg'], 'BAN_attr': ba['attr'],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "out_path = ROOT / 'summary_all_runs.csv'\n",
    "summary_df.to_csv(out_path, index=False, encoding='utf-8-sig')\n",
    "print(f\"[SUMMARY] rows={len(summary_df)} -> {out_path}\")\n",
    "summary_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
