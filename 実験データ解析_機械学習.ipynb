{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 0: 環境設定（全セル共通で利用）=====\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ------------------------\n",
    "# 実験スイッチ（Notebook全体で共有）\n",
    "# ------------------------\n",
    "FMS_THRESHOLD: int = 2            # FMS >= 1 を陽性ラベルとみなす\n",
    "EPOCH_LEN: int = 30               # 30 / 60 / 120 のいずれか\n",
    "MODEL_BACKEND: str = \"xgb\"        # \"xgb\" / \"rf\" / \"svm\"\n",
    "USE_AP_FOR_K: bool = False         # APベースの best_k で上書きするか\n",
    "METRIC: str = \"ba\"\n",
    "METRIC_NAME: str = \"BA\"           # 表示用\n",
    "SEED_BASE: int = 20251101\n",
    "\n",
    "if EPOCH_LEN not in (30, 60, 120):\n",
    "    raise ValueError(\"EPOCH_LEN は 30/60/120 から選択してください。\")\n",
    "\n",
    "# ------------------------\n",
    "# ファイル入出力ルート\n",
    "# ------------------------\n",
    "BASE_INPUT_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "BASE_ANALYSIS_DIR = os.path.join(BASE_INPUT_DIR, \"ANALYSIS\")\n",
    "OUT_DIR = os.path.join(BASE_ANALYSIS_DIR, \"機械学習(MSSQ込み)\", f\"閾値FMS{FMS_THRESHOLD}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def outpath(filename: str) -> str:\n",
    "    return os.path.join(OUT_DIR, filename)\n",
    "\n",
    "print(f\"[OUT_DIR] {OUT_DIR}  |  EPOCH_LEN={EPOCH_LEN}s\")\n",
    "\n",
    "# ------------------------\n",
    "# 対象被験者・時間窓\n",
    "# ------------------------\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "BASELINE_EPOCH = 1770               # ベースライン行（必須）\n",
    "ML_START, ML_END = 1800, 2400       # 学習に使う epoch_start 範囲 [start, end)\n",
    "\n",
    "# ------------------------\n",
    "# 描画スタイル\n",
    "# ------------------------\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"font.size\": 20, \"axes.titlesize\": 26, \"axes.labelsize\": 22,\n",
    "    \"xtick.labelsize\": 20, \"ytick.labelsize\": 20, \"legend.fontsize\": 20,\n",
    "})\n",
    "\n",
    "# ------------------------\n",
    "# FMS二値化ヘルパ\n",
    "# ------------------------\n",
    "def binarize_fms(series: pd.Series, threshold: Optional[int] = None) -> pd.Series:\n",
    "    th = FMS_THRESHOLD if threshold is None else int(threshold)\n",
    "    return (series >= th).astype(int)\n",
    "\n",
    "# ------------------------\n",
    "# モデルレジストリ\n",
    "# ------------------------\n",
    "ModelBuilder = Callable[..., Any]\n",
    "MODEL_REGISTRY: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "def register_backend(name: str, params: Dict[str, Any], builder: ModelBuilder) -> None:\n",
    "    MODEL_REGISTRY[name] = {\"params\": params, \"builder\": builder}\n",
    "\n",
    "def _build_xgb(params: Dict[str, Any], *, scale_pos_weight: Optional[float] = None):\n",
    "    cfg = params.copy()\n",
    "    if scale_pos_weight is not None:\n",
    "        cfg[\"scale_pos_weight\"] = float(scale_pos_weight)\n",
    "    return xgb.XGBClassifier(**cfg)\n",
    "\n",
    "def _build_rf(params: Dict[str, Any], **_):\n",
    "    return RandomForestClassifier(**params)\n",
    "\n",
    "def _build_svm(params: Dict[str, Any], **_):\n",
    "    return SVC(**params)\n",
    "\n",
    "XGB_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=100,\n",
    "    eval_metric=\"logloss\",\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    n_jobs=1,\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cpu\",\n",
    "    seed=0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "RF_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=439,\n",
    "    max_depth=14,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=False,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED_BASE,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "SVM_PARAMS: Dict[str, Any] = dict(\n",
    "    C=1.0,\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"scale\",\n",
    "    probability=True,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED_BASE,\n",
    ")\n",
    "\n",
    "register_backend(\"xgb\", XGB_PARAMS, _build_xgb)\n",
    "register_backend(\"rf\",  RF_PARAMS,  _build_rf)\n",
    "register_backend(\"svm\", SVM_PARAMS, _build_svm)\n",
    "\n",
    "def set_model_backend(name: str) -> None:\n",
    "    name = name.lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録: {list(MODEL_REGISTRY.keys())}\")\n",
    "    global MODEL_BACKEND\n",
    "    MODEL_BACKEND = name\n",
    "\n",
    "def build_estimator(\n",
    "    backend: Optional[str] = None,\n",
    "    *,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    name = (backend or MODEL_BACKEND).lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録。\")\n",
    "    base = MODEL_REGISTRY[name][\"params\"].copy()\n",
    "    if overrides:\n",
    "        base.update(overrides)\n",
    "    builder = MODEL_REGISTRY[name][\"builder\"]\n",
    "    return builder(base, scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "def fit_estimator(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    model = build_estimator(\n",
    "        backend=backend, scale_pos_weight=scale_pos_weight, overrides=overrides\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def predict_positive_score(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return np.asarray(model.decision_function(X), dtype=float)\n",
    "    return model.predict(X).astype(float)\n",
    "\n",
    "MODEL_ID = MODEL_BACKEND.upper()\n",
    "print(f\"[INFO] MODEL_BACKEND={MODEL_ID} / SEED={SEED_BASE} / backends={list(MODEL_REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: データ準備（CSV読込 → EPOCH合成 → SUBJECT_META → 行列出力）=====\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------\n",
    "# ① 30秒EPOCH CSVの読み込み・検証\n",
    "# --------------------------------------------\n",
    "def subject_csv_path(sid: str) -> str:\n",
    "    path = os.path.join(BASE_INPUT_DIR, sid, \"EPOCH\", f\"{sid}_epoch.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"[Cell1] CSV missing for subject {sid}: {path}\")\n",
    "    return path\n",
    "\n",
    "dfs = []\n",
    "for sid in SUBJECT_IDS:\n",
    "    df = pd.read_csv(subject_csv_path(sid))\n",
    "    if df.shape[1] < 4:\n",
    "        raise ValueError(f\"[Cell1] {sid}: 列数が不足（>=4 必須）\")\n",
    "    df = df.copy()\n",
    "    df.columns = list(df.columns[:3]) + [str(c) for c in df.columns[3:]]\n",
    "    c1, c2, c3 = df.columns[:3]\n",
    "    df = df.rename(columns={c1: \"epoch_start\", c2: \"epoch_end\", c3: \"FMS\"})\n",
    "    df[\"epoch_start\"] = pd.to_numeric(df[\"epoch_start\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"epoch_end\"]   = pd.to_numeric(df[\"epoch_end\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"FMS\"]         = pd.to_numeric(df[\"FMS\"],         errors=\"coerce\").astype(\"Int64\")\n",
    "    if df[[\"epoch_start\",\"epoch_end\",\"FMS\"]].isna().any().any():\n",
    "        raise ValueError(f\"[Cell1] {sid}: epoch_start/epoch_end/FMS に NaN\")\n",
    "    df.insert(0, \"subject_id\", sid)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_raw = pd.concat(dfs, ignore_index=True)\n",
    "exclude_feats = {\"HF_power\", \"LF_power\", \"LF_HF_ratio\"}\n",
    "feature_cols_all = [\n",
    "    c for c in combined_raw.columns\n",
    "    if c not in {\"subject_id\",\"epoch_start\",\"epoch_end\",\"FMS\"} and c not in exclude_feats\n",
    "]\n",
    "if not feature_cols_all:\n",
    "    raise RuntimeError(\"[Cell1] 特徴量列が0です。列名や除外設定を確認してください。\")\n",
    "\n",
    "print(f\"[Cell1] Loaded subjects={len(SUBJECT_IDS)}, rows={len(combined_raw)}, features(after drop)={len(feature_cols_all)}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ② EPOCH_LEN 秒への合成 + baseline差分 + ラベル生成\n",
    "# --------------------------------------------\n",
    "if (ML_END - ML_START) % EPOCH_LEN != 0:\n",
    "    raise ValueError(f\"[Cell1] ML window {ML_END-ML_START} が EPOCH_LEN={EPOCH_LEN} で割り切れません。\")\n",
    "\n",
    "rows_per_bin = EPOCH_LEN // 30\n",
    "df_out_list = []\n",
    "\n",
    "for sid, sdf in combined_raw.groupby(\"subject_id\", sort=False):\n",
    "    base_row = sdf.loc[sdf[\"epoch_start\"] == BASELINE_EPOCH]\n",
    "    if len(base_row) != 1:\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline epoch_start=={BASELINE_EPOCH} が見つからない\")\n",
    "    base_vals = base_row[feature_cols_all].astype(float).iloc[0]\n",
    "    if base_vals.isna().any():\n",
    "        raise ValueError(f\"[Cell1] {sid}: baselineにNaN -> {base_vals.index[base_vals.isna()].tolist()}\")\n",
    "\n",
    "    sdf_ml = sdf[(sdf[\"epoch_start\"] >= ML_START) & (sdf[\"epoch_start\"] < ML_END)].copy()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: ML window [{ML_START},{ML_END}) が空です。\")\n",
    "\n",
    "    sdf_ml[\"bin_start\"] = ML_START + ((sdf_ml[\"epoch_start\"] - ML_START) // EPOCH_LEN) * EPOCH_LEN\n",
    "    sdf_ml[\"bin_end\"]   = sdf_ml[\"bin_start\"] + EPOCH_LEN\n",
    "\n",
    "    bin_counts = sdf_ml.groupby([\"bin_start\",\"bin_end\"]).size()\n",
    "    complete_bins = bin_counts[bin_counts == rows_per_bin].index\n",
    "    sdf_ml = sdf_ml.set_index([\"bin_start\",\"bin_end\"]).loc[complete_bins].reset_index()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: EPOCH_LEN={EPOCH_LEN} で完全なbinが無い\")\n",
    "\n",
    "    agg_dict = {c: \"mean\" for c in feature_cols_all}\n",
    "    agg_dict[\"FMS\"] = \"mean\"\n",
    "    g = sdf_ml.groupby([\"subject_id\",\"bin_start\",\"bin_end\"], as_index=False).agg(agg_dict)\n",
    "\n",
    "    g_features = g[feature_cols_all].astype(float) - base_vals.values\n",
    "    if g_features.isna().any().any():\n",
    "        bad = g_features.columns[g_features.isna().any()].tolist()\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline差分後にNaN -> {bad}\")\n",
    "\n",
    "    g_out = pd.concat([g[[\"subject_id\",\"bin_start\",\"bin_end\",\"FMS\"]], g_features], axis=1)\n",
    "    g_out = g_out.rename(columns={\"bin_start\":\"epoch_start\",\"bin_end\":\"epoch_end\"})\n",
    "    g_out[\"label\"] = binarize_fms(g_out[\"FMS\"])\n",
    "    g_out = g_out[[\"subject_id\",\"epoch_start\",\"epoch_end\",\"FMS\",\"label\"] + feature_cols_all]\n",
    "    df_out_list.append(g_out)\n",
    "\n",
    "df_ml_epoch = pd.concat(df_out_list, ignore_index=True)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ③ SUBJECT_META & MSSQ group\n",
    "# --------------------------------------------\n",
    "CANDIDATE_SCORE_PATHS = [\n",
    "    \"/mnt/data/summary_scores.xlsx\",\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"機械学習\", \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_INPUT_DIR, \"summary_scores.xlsx\"),\n",
    "]\n",
    "score_path = next((p for p in CANDIDATE_SCORE_PATHS if os.path.exists(p)), None)\n",
    "if score_path is None:\n",
    "    raise FileNotFoundError(\"[Cell1] summary_scores.xlsx が見つかりません。\")\n",
    "meta_raw = pd.read_excel(score_path, sheet_name=\"Summary\")\n",
    "\n",
    "required = [\"ID\", \"MSSQ\", \"VIMSSQ\"]\n",
    "missing = [c for c in required if c not in meta_raw.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"[Cell1] summary_scores.xlsx に必須列がありません -> {missing}\")\n",
    "\n",
    "meta = meta_raw[required].copy()\n",
    "meta[\"ID\"] = (\n",
    "    meta[\"ID\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "for c in [\"MSSQ\", \"VIMSSQ\"]:\n",
    "    meta[c] = pd.to_numeric(meta[c], errors=\"raise\")\n",
    "\n",
    "sid_set = set(map(str, SUBJECT_IDS))\n",
    "meta = meta[meta[\"ID\"].isin(sid_set)].copy()\n",
    "if meta[\"ID\"].duplicated().any():\n",
    "    raise ValueError(f\"[Cell1] ID 重複 -> {meta.loc[meta['ID'].duplicated(), 'ID'].tolist()}\")\n",
    "\n",
    "MSSQ_THRESHOLD_FIXED = 10.0\n",
    "meta[\"MSSQ_group\"] = np.where(meta[\"MSSQ\"] >= MSSQ_THRESHOLD_FIXED, \"High\", \"Low\")\n",
    "SUBJECT_META = (\n",
    "    meta.rename(columns={\"ID\": \"subject_id\"})\n",
    "        .set_index(\"subject_id\")[[\"MSSQ\", \"VIMSSQ\", \"MSSQ_group\"]]\n",
    "        .copy()\n",
    ")\n",
    "SUBJECT_META.to_csv(outpath(\"subject_meta.csv\"), encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell1] SUBJECT_META saved -> {outpath('subject_meta.csv')} (source='{score_path}')\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ④ 学習行列＆行列保存\n",
    "# --------------------------------------------\n",
    "fname_raw = f\"ML_DATA_DELTA_{EPOCH_LEN}S_RAW.CSV\"\n",
    "df_ml_epoch.to_csv(outpath(fname_raw), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "X_all = df_ml_epoch[feature_cols_all].copy().astype(float)\n",
    "y_all = df_ml_epoch[\"label\"].copy().astype(int)\n",
    "groups = df_ml_epoch[\"subject_id\"].copy()\n",
    "\n",
    "X_all.to_csv(outpath(f\"X_RAW_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "X_all.to_csv(outpath(f\"X_SCALED_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")  # 木系でスケーリング不要\n",
    "pd.DataFrame({\"subject_id\": groups, \"label\": y_all, \"FMS_mean\": df_ml_epoch[\"FMS\"]}).to_csv(\n",
    "    outpath(f\"Y_AND_GROUPS_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(f\"[Cell1] Saved -> {outpath(fname_raw)} / X_RAW_ALL / X_SCALED_ALL / Y_AND_GROUPS\")\n",
    "print(f\"[Cell1] Matrices ready: X_all={X_all.shape}, y_all={y_all.shape}, SUBJECT_META={SUBJECT_META.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45007fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2: モデリング共通ヘルパ（fit / SHAP / 評価）=====\n",
    "\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 学習ラッパー（Cell0のレジストリAPIを利用）\n",
    "# --------------------------------------------\n",
    "def fit_classifier(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cell0 の fit_estimator を直接包む薄いラッパ。\n",
    "    - SHAP/評価セルから backend を差し替えたい場合のみ backend / overrides を指定する。\n",
    "    \"\"\"\n",
    "    if \"fit_estimator\" not in globals():\n",
    "        raise RuntimeError(\"[Cell2] fit_estimator が未定義です。Cell0 を先に実行してください。\")\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    return fit_estimator(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        backend=backend,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# TreeSHAP ベースの特徴重要度算出\n",
    "# --------------------------------------------\n",
    "def compute_train_shap_abs_mean(model, X_ref: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    学習データ X_ref 上での平均絶対SHAP値（降順）。\n",
    "    - XGB/RF 等の木モデルを想定（TreeSHAP）。\n",
    "    - SVM など非対応モデルでは ValueError を送出する。\n",
    "    \"\"\"\n",
    "    X_ref = X_ref.astype(np.float32, copy=False)\n",
    "\n",
    "    # 背景データ（最大128行）\n",
    "    bg_n = min(128, len(X_ref))\n",
    "    X_bg = X_ref.sample(n=bg_n, random_state=SEED_BASE) if bg_n >= 2 else X_ref\n",
    "\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            data=X_bg,\n",
    "            model_output=\"probability\",\n",
    "            feature_perturbation=\"interventional\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "    except Exception:\n",
    "        # probability指定が非対応な場合に raw へフォールバック\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            model_output=\"raw\",\n",
    "            feature_perturbation=\"tree_path_dependent\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "\n",
    "    # shap_values の戻り値形状を統一（2D: n_samples × n_features）\n",
    "    classes = getattr(model, \"classes_\", None)\n",
    "    pos_idx = int(np.where(classes == 1)[0][0]) if classes is not None and 1 in list(classes) else -1\n",
    "\n",
    "    if isinstance(sv_any, list):\n",
    "        sv = sv_any[pos_idx]\n",
    "    else:\n",
    "        sv = getattr(sv_any, \"values\", sv_any)\n",
    "        sv = np.asarray(sv)\n",
    "        if sv.ndim == 3:\n",
    "            sv = sv[..., pos_idx]\n",
    "        elif sv.ndim == 1:\n",
    "            sv = sv.reshape(-1, 1)\n",
    "\n",
    "    if sv.shape[1] != X_ref.shape[1]:\n",
    "        raise RuntimeError(\n",
    "            f\"[Cell2] SHAP shape mismatch: sv.shape={sv.shape}, X_ref.shape={X_ref.shape}\"\n",
    "        )\n",
    "\n",
    "    abs_mean = np.mean(np.abs(sv), axis=0)\n",
    "    return pd.Series(abs_mean, index=X_ref.columns, name=\"mean_abs\").sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 評価ユーティリティ\n",
    "# --------------------------------------------\n",
    "def _is_probability_like(scores: np.ndarray) -> bool:\n",
    "    return np.isfinite(scores).all() and 0.0 <= scores.min() and scores.max() <= 1.0\n",
    "\n",
    "\n",
    "def evaluate_fold(model, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - ROC AUC: 2クラス時のみ。\n",
    "    - Accuracy: 確率なら 0.5、スコアなら 0.0 を閾値とする（詳細な最適化は別セル）。\n",
    "    \"\"\"\n",
    "    X_test = X_test.astype(np.float32, copy=False)\n",
    "    scores = predict_positive_score(model, X_test)\n",
    "\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, scores)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "\n",
    "    thr = 0.5 if _is_probability_like(scores) else 0.0\n",
    "    pred = (scores >= thr).astype(int)\n",
    "    acc = accuracy_score(y_test.astype(int), pred)\n",
    "\n",
    "    return {\"roc_auc\": float(roc_auc), \"accuracy\": float(acc)}\n",
    "\n",
    "\n",
    "print(\"[Cell2] Modeling helpers ready (fit_classifier / compute_train_shap_abs_mean / evaluate_fold)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58071f6d",
   "metadata": {},
   "source": [
    "# ===== Section: 特徴重要度と best_k 探索 =====\n",
    "\n",
    "| Cell | 目的 | 主な出力 (OUT_DIR 配下) |\n",
    "| ---- | ---- | ----------------------- |\n",
    "| 3A | LOSO学習でSHAP重要度を算出しランキング化 | `SHAP_FEATURE_RANKING.CSV`, `SHAP_FEATURE_RANKING_LABELED.CSV`, `SHAP_RANKING_ALL.PNG`, `SHAP_TOP8_RANKING.PNG`, `LOSO_METRICS.CSV` |\n",
    "| 3B | SHAP順の特徴を使って k 本ごとの pooled ROC-AUC を計測し best_k を決定 | `AUC_PER_K.CSV`, `AUC_VS_NUM_FEATURES.PNG`, `best_k` (グローバル変数) |\n",
    "| 3C | 同様に AUPRC/AP で k を走査し、必要に応じ best_k を APベースで上書きし PR 曲線を出力 | `AUPRC_PER_K.CSV`, `AP_VS_NUM_FEATURES.PNG`, `PR_CURVE_AT_BEST_K.CSV`, `PR_CURVE_AT_BEST_K.PNG` |\n",
    "| 3D | MSSQ High/Low 各群で in-group LOSO を行い、群別の AUC vs k と best_k を算出 | `AUC_VS_K_BY_GROUP.PNG`, `BEST_K_BY_GROUP.JSON` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0127a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A: SHAPランキング（LOSO学習側のみ）=====\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "shap_frames = []\n",
    "metrics_rows = []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_all, y_all, groups), start=1):\n",
    "    X_tr, X_te = X_all.iloc[tr_idx], X_all.iloc[te_idx]\n",
    "    y_tr, y_te = y_all.iloc[tr_idx], y_all.iloc[te_idx]\n",
    "    if len(np.unique(y_tr)) < 2:\n",
    "        raise RuntimeError(f\"[Cell3A] fold{fold_id}: 学習側が単一クラス\")\n",
    "\n",
    "    model = fit_classifier(X_tr, y_tr)\n",
    "    abs_mean = compute_train_shap_abs_mean(model, X_tr).rename(f\"fold{fold_id}\")\n",
    "    shap_frames.append(abs_mean)\n",
    "\n",
    "    m = evaluate_fold(model, X_te, y_te)\n",
    "    metrics_rows.append({\n",
    "        \"fold_id\": fold_id,\n",
    "        \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "        \"roc_auc\": m[\"roc_auc\"],\n",
    "        \"accuracy\": m[\"accuracy\"],\n",
    "    })\n",
    "\n",
    "shap_rank = pd.concat(shap_frames, axis=1)\n",
    "shap_rank[\"mean_abs\"] = shap_rank.mean(axis=1)\n",
    "shap_rank = shap_rank.sort_values(\"mean_abs\", ascending=False)\n",
    "\n",
    "shap_rank.to_csv(outpath(\"SHAP_FEATURE_RANKING1.CSV\"), encoding=\"utf-8-sig\")\n",
    "shap_rank.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(metrics_rows).to_csv(outpath(\"LOSO_METRICS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[Cell3A] Saved SHAP ranking & LOSO metrics\")\n",
    "\n",
    "plt.figure(figsize=(10, max(5, len(shap_rank)//3)))\n",
    "plt.barh(shap_rank.index[::-1], shap_rank[\"mean_abs\"][::-1])\n",
    "plt.xlabel(\"Mean |SHAP|\"); plt.ylabel(\"Feature\"); plt.title(\"SHAP Ranking (All)\")\n",
    "plt.tight_layout(); plt.savefig(outpath(\"SHAP_RANKING_ALL.PNG\"), dpi=300); plt.close()\n",
    "\n",
    "topk = shap_rank.head(8).iloc[::-1]\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = plt.gca()\n",
    "ax.barh(topk.index, topk[\"mean_abs\"])\n",
    "mx = float(topk[\"mean_abs\"].max()) if len(topk) else 1.0\n",
    "ax.set_xlim(0, mx * 1.08)\n",
    "ax.set_xlabel(\"Mean |SHAP value|\", fontsize=26)\n",
    "ax.set_ylabel(\"Feature\", fontsize=26)\n",
    "ax.tick_params(axis=\"both\", labelsize=22)\n",
    "ax.set_title(\"Top-8 SHAP Feature Ranking\", fontsize=34, pad=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"SHAP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A (Legacy): SHAPランキング（XGB pred_contribs版） =====\n",
    "# - XGBoost専用（pred_contribs=True）でSHAP値を算出\n",
    "# - 生成物は現行セルと同名ファイルに上書き保存\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "shap_frames = []\n",
    "metrics_rows = []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_all, y_all, groups), start=1):\n",
    "    X_tr, X_te = X_all.iloc[tr_idx], X_all.iloc[te_idx]\n",
    "    y_tr, y_te = y_all.iloc[tr_idx], y_all.iloc[te_idx]\n",
    "    if len(np.unique(y_tr)) < 2:\n",
    "        raise RuntimeError(f\"[Cell3A-legacy] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        eval_metric=\"logloss\",\n",
    "        subsample=1.0,\n",
    "        colsample_bytree=1.0,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\",\n",
    "        device=\"cpu\",\n",
    "        seed=0,\n",
    "        random_state=0,\n",
    "    )\n",
    "    model.fit(X_tr.astype(np.float32), y_tr.astype(np.int32))\n",
    "\n",
    "    dm = xgb.DMatrix(X_tr.astype(np.float32), feature_names=list(X_tr.columns))\n",
    "    contribs = model.get_booster().predict(dm, pred_contribs=True)  # (n_samples, n_features+1)\n",
    "    shap_vals = contribs[:, :-1]                                   # 最後の列はバイアス項\n",
    "    abs_mean = np.abs(shap_vals).mean(axis=0)\n",
    "    shap_frames.append(pd.Series(abs_mean, index=X_tr.columns, name=f\"fold{fold_id}\"))\n",
    "\n",
    "    m = evaluate_fold(model, X_te, y_te)\n",
    "    metrics_rows.append({\n",
    "        \"fold_id\": fold_id,\n",
    "        \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "        \"roc_auc\": m[\"roc_auc\"],\n",
    "        \"accuracy\": m[\"accuracy\"],\n",
    "    })\n",
    "\n",
    "shap_rank = pd.concat(shap_frames, axis=1)\n",
    "shap_rank[\"mean_abs\"] = shap_rank.mean(axis=1)\n",
    "shap_rank = shap_rank.sort_values(\"mean_abs\", ascending=False)\n",
    "\n",
    "# ★ 既存ファイル名へ上書き保存（現行版と同じパス）\n",
    "shap_rank.to_csv(outpath(\"SHAP_FEATURE_RANKING.CSV\"), encoding=\"utf-8-sig\")\n",
    "shap_rank.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(metrics_rows).to_csv(outpath(\"LOSO_METRICS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[Cell3A-legacy] SHAP_FEATURE_RANKING*.CSV を旧ロジックで上書きしました。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3B: 全kで pooled ROC-AUC → best_k 決定 =====\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "rank_df = pd.read_csv(outpath(\"SHAP_FEATURE_RANKING.CSV\"), index_col=0, encoding=\"utf-8-sig\")\n",
    "feature_order = [f for f in rank_df.index if f in X_all.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[Cell3B] ランキング上位特徴が X_all に存在しません。\")\n",
    "\n",
    "ks = list(range(len(feature_order), 0, -1))\n",
    "logo = LeaveOneGroupOut()\n",
    "auc_list = []\n",
    "\n",
    "for k in ks:\n",
    "    feats = feature_order[:k]\n",
    "    X = X_all[feats].astype(np.float32)\n",
    "    y = y_all.values\n",
    "    g = groups.values\n",
    "\n",
    "    y_true_all, proba_all = [], []\n",
    "    for tr_idx, te_idx in logo.split(X, y, g):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(\"[Cell3B] 学習foldが単一クラス。閾値/期間の見直しが必要です。\")\n",
    "        model = fit_classifier(X_tr, pd.Series(y_tr))\n",
    "        proba = predict_positive_score(model, X_te)\n",
    "        y_true_all.append(y_te); proba_all.append(proba)\n",
    "\n",
    "    y_true_k = np.concatenate(y_true_all)\n",
    "    proba_k = np.concatenate(proba_all)\n",
    "    if len(np.unique(y_true_k)) < 2:\n",
    "        raise RuntimeError(\"[Cell3B] pooled 真値が単一クラスで AUC 計算不可。\")\n",
    "    auc_list.append(float(roc_auc_score(y_true_k, proba_k)))\n",
    "\n",
    "pd.DataFrame({\"k\": ks, \"auc_pooled\": auc_list}).to_csv(outpath(\"AUC_PER_K.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "auc_array = np.asarray(auc_list, dtype=float)\n",
    "best_idx = int(np.nanargmax(auc_array))\n",
    "best_k = ks[best_idx]\n",
    "best_auc = auc_list[best_idx]\n",
    "print(f\"[Cell3B] Best k (AUC) = {best_k}, AUC={best_auc:.3f}\")\n",
    "globals()[\"best_k\"] = best_k  # Cell7 以降で使用\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = plt.gca()\n",
    "ax.plot(ks, auc_list, marker='o', linewidth=1.5)\n",
    "ax.scatter([best_k], [best_auc], s=180, color=\"red\", zorder=5)\n",
    "ax.annotate(f\"Max AUC = {best_auc:.3f} (k={best_k})\", xy=(best_k, best_auc),\n",
    "            xytext=(best_k, best_auc + 0.02), ha=\"center\", va=\"bottom\",\n",
    "            fontsize=20, color=\"red\")\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlabel(\"Number of Features (k)\", fontsize=26)\n",
    "ax.set_ylabel(\"ROC AUC (pooled)\", fontsize=26)\n",
    "ax.tick_params(axis=\"both\", labelsize=22)\n",
    "ax.set_title(\"AUC vs Number of Features\", fontsize=34, pad=10)\n",
    "ax.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"AUC_VS_NUM_FEATURES.PNG\"), dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e2fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3C: 全kで pooled AUPRC → best_k (AP) 決定 ＋ PR曲線 =====\n",
    "\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, auc\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "rank_df = pd.read_csv(outpath(\"SHAP_FEATURE_RANKING.CSV\"), index_col=0, encoding=\"utf-8-sig\")\n",
    "feature_order = [f for f in rank_df.index if f in X_all.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[Cell3C] ランキング上位特徴が X_all に存在しません。\")\n",
    "\n",
    "ks = list(range(len(feature_order), 0, -1))\n",
    "logo = LeaveOneGroupOut()\n",
    "ap_list, prauc_list, pi_list = [], [], []\n",
    "\n",
    "for k in ks:\n",
    "    feats = feature_order[:k]\n",
    "    X = X_all[feats].astype(np.float32)\n",
    "    y = y_all.values\n",
    "    g = groups.values\n",
    "\n",
    "    y_true_all, proba_all = [], []\n",
    "    for tr_idx, te_idx in logo.split(X, y, g):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(\"[Cell3C] 学習foldが単一クラス。\")\n",
    "        model = fit_classifier(X_tr, pd.Series(y_tr))\n",
    "        proba = predict_positive_score(model, X_te)\n",
    "        y_true_all.append(y_te); proba_all.append(proba)\n",
    "\n",
    "    y_true_k = np.concatenate(y_true_all)\n",
    "    proba_k = np.concatenate(proba_all)\n",
    "    if len(np.unique(y_true_k)) < 2:\n",
    "        raise RuntimeError(\"[Cell3C] pooled 真値が単一クラスで AUPRC 計算不可。\")\n",
    "\n",
    "    ap = float(average_precision_score(y_true_k, proba_k))\n",
    "    prec, rec, _ = precision_recall_curve(y_true_k, proba_k)\n",
    "    prauc = float(auc(rec, prec))\n",
    "    ap_list.append(ap)\n",
    "    prauc_list.append(prauc)\n",
    "    pi_list.append(float((y_true_k == 1).mean()))\n",
    "\n",
    "pd.DataFrame({\"k\": ks, \"ap\": ap_list, \"prauc\": prauc_list, \"pi\": pi_list}).to_csv(\n",
    "    outpath(\"AUPRC_PER_K.CSV\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "ap_array = np.asarray(ap_list, dtype=float)\n",
    "best_idx_ap = int(np.nanargmax(ap_array))\n",
    "best_k_ap = ks[best_idx_ap]\n",
    "print(f\"[Cell3C] Best k (AP) = {best_k_ap}, AP={ap_list[best_idx_ap]:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = plt.gca()\n",
    "ax.plot(ks, ap_list, marker='o', linewidth=1.5, label=\"AP\")\n",
    "ax.scatter([best_k_ap], [ap_list[best_idx_ap]], s=150, color=\"red\", zorder=5)\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlabel(\"Number of Features (k)\", fontsize=26)\n",
    "ax.set_ylabel(\"Average Precision\", fontsize=26)\n",
    "ax.tick_params(axis=\"both\", labelsize=22)\n",
    "ax.set_title(\"AP vs Number of Features\", fontsize=34, pad=10)\n",
    "ax.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"AP_VS_NUM_FEATURES.PNG\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "if USE_AP_FOR_K:\n",
    "    globals()[\"best_k\"] = best_k_ap\n",
    "    print(f\"[Cell3C] USE_AP_FOR_K=True → best_k を {best_k_ap} に上書き\")\n",
    "else:\n",
    "    print(\"[Cell3C] USE_AP_FOR_K=False → Cell3Bの best_k を維持\")\n",
    "\n",
    "feats_best = feature_order[:globals()[\"best_k\"]]\n",
    "logo = LeaveOneGroupOut()\n",
    "y_true_best, proba_best = [], []\n",
    "for tr_idx, te_idx in logo.split(X_all[feats_best], y_all.values, groups.values):\n",
    "    X_tr, X_te = X_all[feats_best].iloc[tr_idx], X_all[feats_best].iloc[te_idx]\n",
    "    y_tr, y_te = y_all.values[tr_idx], y_all.values[te_idx]\n",
    "    if len(np.unique(y_tr)) < 2:\n",
    "        raise RuntimeError(\"[Cell3C] best_k fold が単一クラス。\")\n",
    "    model = fit_classifier(X_tr, pd.Series(y_tr))\n",
    "    proba = predict_positive_score(model, X_te)\n",
    "    y_true_best.append(y_te); proba_best.append(proba)\n",
    "\n",
    "y_true_best = np.concatenate(y_true_best)\n",
    "proba_best = np.concatenate(proba_best)\n",
    "prec, rec, thr = precision_recall_curve(y_true_best, proba_best)\n",
    "ap_best = float(average_precision_score(y_true_best, proba_best))\n",
    "prauc_best = float(auc(rec, prec))\n",
    "pi_best = float((y_true_best == 1).mean())\n",
    "\n",
    "pd.DataFrame({\"recall\": rec, \"precision\": prec, \"threshold\": np.r_[np.nan, thr]}).to_csv(\n",
    "    outpath(\"PR_CURVE_AT_BEST_K.CSV\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "ax = plt.gca()\n",
    "ax.step(rec, prec, where=\"post\", linewidth=1.5,\n",
    "        label=f\"PR (AP={ap_best:.3f}, PR-AUC={prauc_best:.3f})\")\n",
    "ax.axhline(pi_best, linestyle=\"--\", linewidth=1.5, label=f\"Baseline π={pi_best:.3f}\", alpha=0.8)\n",
    "ax.set_xlabel(\"Recall\", fontsize=24)\n",
    "ax.set_ylabel(\"Precision\", fontsize=24)\n",
    "ax.tick_params(axis=\"both\", labelsize=20)\n",
    "ax.set_title(f\"Precision–Recall at best k = {globals()['best_k']}\", fontsize=30, pad=10)\n",
    "ax.set_xlim([0.0, 1.0]); ax.set_ylim([0.0, 1.05])\n",
    "ax.grid(True, alpha=0.4)\n",
    "ax.legend(fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"PR_CURVE_AT_BEST_K.PNG\"), dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734f63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3D: MSSQ群別 AUC vs k（in-group LOSO）=====\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "\n",
    "req_vars = [\"X_all\", \"y_all\", \"groups\", \"SUBJECT_META\"]\n",
    "missing = [v for v in req_vars if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3D] 必要変数が未定義: {missing}\")\n",
    "if \"MSSQ_group\" not in SUBJECT_META.columns:\n",
    "    raise RuntimeError(\"[Cell3D] SUBJECT_META に MSSQ_group 列が必要です。\")\n",
    "\n",
    "rank_df = pd.read_csv(outpath(\"SHAP_FEATURE_RANKING.CSV\"), index_col=0, encoding=\"utf-8-sig\")\n",
    "feature_order = [f for f in rank_df.index if f in X_all.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[Cell3D] ランキング上位特徴が存在しません。\")\n",
    "\n",
    "fair_groups = groups.astype(str).map(SUBJECT_META[\"MSSQ_group\"])\n",
    "if fair_groups.isna().any():\n",
    "    raise RuntimeError(\"[Cell3D] SUBJECT_META に存在しない subject_id があります。\")\n",
    "\n",
    "ks = list(range(len(feature_order), 0, -1))\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "def _in_group_auc(mask):\n",
    "    aucs = []\n",
    "    for k in ks:\n",
    "        feats = feature_order[:k]\n",
    "        X = X_all.loc[mask, feats].astype(np.float32)\n",
    "        y = y_all.loc[mask].values\n",
    "        g = groups.loc[mask].values\n",
    "        if len(np.unique(y)) < 2:\n",
    "            aucs.append(np.nan); continue\n",
    "        y_true_all, proba_all = [], []\n",
    "        for tr_idx, te_idx in logo.split(X, y, g):\n",
    "            X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "            y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "            if len(np.unique(y_tr)) < 2:\n",
    "                aucs.append(np.nan); break\n",
    "            model = fit_classifier(X_tr, pd.Series(y_tr))\n",
    "            proba = predict_positive_score(model, X_te)\n",
    "            y_true_all.append(y_te); proba_all.append(proba)\n",
    "        else:\n",
    "            y_true = np.concatenate(y_true_all)\n",
    "            proba = np.concatenate(proba_all)\n",
    "            aucs.append(float(roc_auc_score(y_true, proba)) if len(np.unique(y_true)) > 1 else np.nan)\n",
    "            continue\n",
    "        # breakした場合\n",
    "        if len(aucs) < len(ks):\n",
    "            aucs.extend([np.nan] * (len(ks) - len(aucs)))\n",
    "    return aucs\n",
    "\n",
    "mask_H = (fair_groups == \"High\")\n",
    "mask_L = (fair_groups == \"Low\")\n",
    "auc_high = _in_group_auc(mask_H)\n",
    "auc_low = _in_group_auc(mask_L)\n",
    "\n",
    "def _best_k(ks_list, auc_vals):\n",
    "    arr = np.asarray(auc_vals, dtype=float)\n",
    "    if np.all(~np.isfinite(arr)):\n",
    "        return None, np.nan\n",
    "    maxv = np.nanmax(arr)\n",
    "    cand = np.array(ks_list)[np.isclose(arr, maxv, rtol=1e-6, atol=1e-12)]\n",
    "    return int(np.min(cand)), float(maxv)\n",
    "\n",
    "best_k_high, best_auc_high = _best_k(ks, auc_high)\n",
    "best_k_low, best_auc_low = _best_k(ks, auc_low)\n",
    "\n",
    "with open(outpath(\"BEST_K_BY_GROUP.JSON\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"BEST_K_HIGH\": best_k_high,\n",
    "        \"BEST_AUC_HIGH\": best_auc_high,\n",
    "        \"BEST_K_LOW\": best_k_low,\n",
    "        \"BEST_AUC_LOW\": best_auc_low,\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = plt.gca()\n",
    "ax.plot(ks, auc_high, marker=\"o\", label=\"MSSQ High\")\n",
    "ax.plot(ks, auc_low, marker=\"s\", label=\"MSSQ Low\")\n",
    "if best_k_high is not None:\n",
    "    ax.scatter([best_k_high], [best_auc_high], s=160, zorder=5)\n",
    "    ax.annotate(f\"High max={best_auc_high:.3f} (k={best_k_high})\",\n",
    "                xy=(best_k_high, best_auc_high),\n",
    "                xytext=(best_k_high, best_auc_high + 0.02),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=18)\n",
    "if best_k_low is not None:\n",
    "    ax.scatter([best_k_low], [best_auc_low], s=160, zorder=5)\n",
    "    ax.annotate(f\"Low max={best_auc_low:.3f} (k={best_k_low})\",\n",
    "                xy=(best_k_low, best_auc_low),\n",
    "                xytext=(best_k_low, best_auc_low + 0.02),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=18)\n",
    "\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlabel(\"Number of Features (k)\")\n",
    "ax.set_ylabel(\"ROC AUC (pooled, in-group LOSO)\")\n",
    "ax.set_title(\"AUC vs Number of Features by MSSQ Group\")\n",
    "ax.grid(True, alpha=0.4)\n",
    "ax.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"AUC_VS_K_BY_GROUP.PNG\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"[Cell3D] BEST_K_HIGH={best_k_high}, BEST_K_LOW={best_k_low}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8034b38",
   "metadata": {},
   "source": [
    "# ===== Section: 固定ハイパラ・診断 =====\n",
    "\n",
    "| Cell | 目的 | 主な出力 |\n",
    "| ---- | ---- | -------- |\n",
    "| 4A | best_k を確定し、閾値探索・検証ポリシー・スケール設定など実験パラメータを一括定義 | ー（設定ログのみ） |\n",
    "| 4B | 被験者ごとのラベル分布（件数/陽性/陰性/MSSQ群）を集計 | `SUBJECT_LABEL_STATS.CSV` |\n",
    "| 4C | 外側LOSOの学習被験者から inner-LOSO 検証者を1名ずつ回す fold リストを構築 | 関数 `choose_inner_folds_loso` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a654a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4A: 実験設定（best_k fix / 閾値探索ハイパラ）=====\n",
    "\n",
    "assert \"best_k\" in globals(), \"[Cell4A] best_k が未定義です。特徴選択セクションを先に実行してください。\"\n",
    "\n",
    "USE_GLOBAL_BESTK: bool = True\n",
    "BEST_K: int = int(best_k)\n",
    "\n",
    "THRESH_COARSE_STEP: float = 0.01   # 粗探索刻み\n",
    "THRESH_FINE_STEP:   float = 0.001  # 細探索刻み\n",
    "THRESH_MARGIN:      float = 0.03   # 細探索範囲の±幅\n",
    "THRESH_SEARCH_MODE: str = \"exact\"  # 閾値探索モード（例: \"exact\"）\n",
    "THRESH_WG_MODE:     str = \"min\"    # Worst-group最適化時の目的（\"min\": min(TPR/TNR)を最大化等）\n",
    "THRESH_CALIB:       str = \"none\"   # 校正方式（\"none\"/\"platt\"/\"isotonic\"など）\n",
    "\n",
    "VAL_RETRY_MAX: int = 30                # inner検証用HLペア抽選の最大リトライ\n",
    "VAL_REQUIRE_BOTH_CLASSES: bool = True  # 検証データ内で陽性/陰性の両方を要求\n",
    "VAL_MIN_SAMPLES: int | None = None     # 検証総サンプルの下限（不要なら None）\n",
    "\n",
    "def _scale_pos_weight_from_y(y_binary: np.ndarray) -> float:\n",
    "    pos = int(np.sum(y_binary == 1))\n",
    "    neg = int(np.sum(y_binary == 0))\n",
    "    if pos == 0:\n",
    "        print(\"[Cell4A][WARN] y に陽性が存在しないため scale_pos_weight=1.0\")\n",
    "        return 1.0\n",
    "    return float(neg / max(pos, 1))\n",
    "\n",
    "print(\"[Cell4A] Experiment settings:\")\n",
    "print(f\"  BEST_K = {BEST_K} (USE_GLOBAL_BESTK={USE_GLOBAL_BESTK})\")\n",
    "print(f\"  Threshold search: coarse={THRESH_COARSE_STEP}, fine={THRESH_FINE_STEP}, margin=±{THRESH_MARGIN}\")\n",
    "print(f\"  Search mode={THRESH_SEARCH_MODE}, WG mode={THRESH_WG_MODE}, calib={THRESH_CALIB}\")\n",
    "print(f\"  HL validator: retry_max={VAL_RETRY_MAX}, both_classes={VAL_REQUIRE_BOTH_CLASSES}, min_samples={VAL_MIN_SAMPLES}\")\n",
    "print(f\"  XGB_PARAMS = {XGB_PARAMS}\")\n",
    "print(f\"  SEED_BASE = {SEED_BASE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4B: 被験者ラベル分布の集計 =====\n",
    "\n",
    "if groups.dtype != \"O\":\n",
    "    groups = groups.astype(str)\n",
    "y_bin = y_all.astype(int)\n",
    "\n",
    "subj_stats = (\n",
    "    pd.DataFrame({\"subject_id\": groups.values, \"label\": y_bin.values})\n",
    "      .groupby(\"subject_id\")[\"label\"]\n",
    "      .agg(n_total=\"count\", pos=\"sum\")\n",
    "      .reset_index()\n",
    ")\n",
    "subj_stats[\"neg\"] = subj_stats[\"n_total\"] - subj_stats[\"pos\"]\n",
    "\n",
    "if \"SUBJECT_META\" in globals() and \"MSSQ_group\" in SUBJECT_META.columns:\n",
    "    subj_stats = subj_stats.merge(\n",
    "        SUBJECT_META[[\"MSSQ_group\"]].reset_index(),\n",
    "        on=\"subject_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "SUBJECT_LABEL_STATS = subj_stats.set_index(\"subject_id\").sort_index()\n",
    "label_stats_path = outpath(\"SUBJECT_LABEL_STATS.CSV\")\n",
    "SUBJECT_LABEL_STATS.to_csv(label_stats_path, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell4B] SUBJECT_LABEL_STATS saved -> {label_stats_path} (subjects={len(SUBJECT_LABEL_STATS)})\")\n",
    "\n",
    "display(SUBJECT_LABEL_STATS.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4C: inner-LOSO folds builder =====\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def choose_inner_folds_loso(train_subject_ids: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    外側LOSOで得た学習被験者（例: 16名）から、内側LOSOの検証者を1名ずつ回すfoldリストを返す。\n",
    "    戻り値: [[sid1], [sid2], ..., [sidN]] （昇順ソート済み）\n",
    "    \"\"\"\n",
    "    if not isinstance(train_subject_ids, (list, tuple)):\n",
    "        raise RuntimeError(\"[Cell4C] train_subject_ids は list/tuple である必要があります。\")\n",
    "    uniq = list(pd.unique(pd.Series([str(sid) for sid in train_subject_ids])))\n",
    "    if len(uniq) == 0:\n",
    "        raise RuntimeError(\"[Cell4C] train_subject_ids が空です。\")\n",
    "    uniq_sorted = sorted(uniq, key=lambda x: (len(x), x))\n",
    "    folds = [[sid] for sid in uniq_sorted]\n",
    "    print(f\"[Cell4C] inner-LOSO folds: {len(folds)} splits -> val subjects = {', '.join(uniq_sorted)}\")\n",
    "    return folds\n",
    "\n",
    "print(\"[Cell4C] Function choose_inner_folds_loso ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd5edf",
   "metadata": {},
   "source": [
    "# ===== Section: モデリング本体 =====\n",
    "\n",
    "| Cell | 位置づけ | 目的 | 主な出力 |\n",
    "| ---- | -------- | ---- | -------- |\n",
    "| 5A | メイン実験 | inner-LOSOで τ を探索し、outer-LOSOで Single / Group-GLOBAL / WG-1D / WG-2D の予測と評価を取得 | `GROUP_AWARE_THRESH_BY_FOLD.CSV`, `GROUP_AWARE_PREDICTIONS.CSV`, `GROUP_AWARE_SUMMARY.CSV` |\n",
    "| 5B | 派生1 (post-hoc) | outer予測のみを使って post-hoc exact thresholding を実施 | `PREDICTIONS_OUTERONLY.CSV`, `FINAL_THRESHOLDS_POSTHOC.CSV`, `METRICS_POSTHOC.CSV` |\n",
    "| 5C | 派生2 (群別best_k) | MSSQ High/Low で best_k を変えて Single τ を最適化し、群別指標と混同行列を出力 | `METRICS_SINGLE_BY_GROUPK.CSV`, `CONFMAT_SINGLE_GROUPK_*.png` |\n",
    "| 5D | 派生3 (被験者ブートストラップ) | best_k の pooled 予測(OoF)を使い、被験者単位ブートストラップで AUC のCIを推定 | `OOF_PRED_BESTK.CSV`, `AUC_BOOTSTRAP_SUBJECT.csv`, `AUC_BOOTSTRAP_SUMMARY.csv`, `AUC_BOOTSTRAP_{HIST,ECDF}.png` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378d699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5A: inner-LOSO τ最適化 → outer予測・評価 =====\n",
    "# 前提: Cell1〜4 まで実行済み（X_all, y_all, groups, SUBJECT_META, BEST_K, choose_inner_folds_loso などが存在）\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# --- 必須オブジェクト確認 ---\n",
    "req = [\"X_all\", \"y_all\", \"groups\", \"SUBJECT_META\", \"BEST_K\", \"METRIC\", \"METRIC_NAME\", \"choose_inner_folds_loso\"]\n",
    "missing = [v for v in req if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell5A] 未定義の変数/関数があります: {missing}\")\n",
    "if \"MSSQ_group\" not in SUBJECT_META.columns and \"MSSQ_group\" not in SUBJECT_META.index.names:\n",
    "    raise RuntimeError(\"[Cell5A] SUBJECT_META に MSSQ_group 列（またはインデックス）が必要です。\")\n",
    "\n",
    "X_base = X_all.astype(np.float32)\n",
    "y_base = y_all.astype(int)\n",
    "g_base = groups.astype(str)\n",
    "\n",
    "# MSSQ group マッピング\n",
    "if \"subject_id\" in SUBJECT_META.columns:\n",
    "    mapper = SUBJECT_META.set_index(\"subject_id\")[\"MSSQ_group\"].astype(str).to_dict()\n",
    "else:\n",
    "    mapper = SUBJECT_META[\"MSSQ_group\"].astype(str).to_dict()\n",
    "fair_groups = g_base.map(mapper).str.strip().str.lower().map({\"high\": \"High\", \"low\": \"Low\"})\n",
    "if fair_groups.isna().any():\n",
    "    raise RuntimeError(f\"[Cell5A] MSSQ_group 未割当ID: {fair_groups[fair_groups.isna()].index.tolist()}\")\n",
    "\n",
    "# 特徴選抜（SHAP順の上位K）\n",
    "rank_df = pd.read_csv(outpath(\"SHAP_FEATURE_RANKING.CSV\"), index_col=0, encoding=\"utf-8-sig\")\n",
    "feature_order = [f for f in rank_df.index if f in X_base.columns]\n",
    "feats_k = feature_order[:BEST_K]\n",
    "if len(feats_k) < BEST_K:\n",
    "    print(f\"[Cell5A][WARN] ランキング上位に X に無い特徴が混在 → {len(feats_k)} 列で実行。\")\n",
    "X_k = X_base[feats_k]\n",
    "\n",
    "# ---------- 内部ユーティリティ ----------\n",
    "def _prep_sorted(scores, labels):\n",
    "    order = np.argsort(-scores)\n",
    "    s = scores[order]; yb = labels[order]\n",
    "    pos = (yb == 1); neg = (yb == 0)\n",
    "    cpos = np.cumsum(pos); cneg = np.cumsum(neg)\n",
    "    return s, yb, cpos, cneg, int(pos.sum()), int(neg.sum())\n",
    "\n",
    "def _conf_stats(tab, taus):\n",
    "    taus = np.atleast_1d(taus).astype(float)\n",
    "    sort_scores, _, cpos, cneg, P, N = tab\n",
    "    k = np.searchsorted(-sort_scores, -taus, side=\"right\")\n",
    "    TP = np.where(k > 0, cpos[k-1], 0); FP = np.where(k > 0, cneg[k-1], 0)\n",
    "    FN = P - TP; TN = N - FP\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _score_from_conf(metric, TP, FP, FN, TN):\n",
    "    metric = metric.lower()\n",
    "    TP = np.asarray(TP, float); FP = np.asarray(FP, float)\n",
    "    FN = np.asarray(FN, float); TN = np.asarray(TN, float)\n",
    "    if metric == \"ba\":\n",
    "        TPR = np.divide(TP, TP+FN, out=np.zeros_like(TP), where=(TP+FN) > 0)\n",
    "        TNR = np.divide(TN, TN+FP, out=np.zeros_like(TN), where=(TN+FP) > 0)\n",
    "        return 0.5 * (TPR + TNR)\n",
    "    if metric == \"f1\":\n",
    "        PREC = np.divide(TP, TP+FP, out=np.zeros_like(TP), where=(TP+FP) > 0)\n",
    "        REC  = np.divide(TP, TP+FN, out=np.zeros_like(TP), where=(TP+FN) > 0)\n",
    "        return np.divide(2*PREC*REC, PREC+REC, out=np.zeros_like(PREC), where=(PREC+REC) > 0)\n",
    "    raise ValueError(f\"[Cell5A] metric '{metric}' 未対応\")\n",
    "\n",
    "def _best_single_threshold(tab):\n",
    "    uniq = np.unique(tab[0])\n",
    "    cands = np.concatenate([[np.nextafter(uniq.max(), np.inf)], uniq[::-1],\n",
    "                            [np.nextafter(uniq.min(), -np.inf)]])\n",
    "    TP, FP, FN, TN = _conf_stats(tab, cands)\n",
    "    scores = _score_from_conf(METRIC, TP, FP, FN, TN)\n",
    "    idx = int(np.nanargmax(scores))\n",
    "    return float(cands[idx])\n",
    "\n",
    "def _grid_search_group_thresholds_exact(sH, yH, sL, yL):\n",
    "    tabH = _prep_sorted(sH, yH); tabL = _prep_sorted(sL, yL)\n",
    "    uniqH = np.unique(tabH[0]); uniqL = np.unique(tabL[0])\n",
    "    candsH = np.concatenate([[np.nextafter(uniqH.max(), np.inf)], uniqH[::-1],\n",
    "                             [np.nextafter(uniqH.min(), -np.inf)]])\n",
    "    candsL = np.concatenate([[np.nextafter(uniqL.max(), np.inf)], uniqL[::-1],\n",
    "                             [np.nextafter(uniqL.min(), -np.inf)]])\n",
    "    TP_H, FP_H, FN_H, TN_H = _conf_stats(tabH, candsH)\n",
    "    TP_L, FP_L, FN_L, TN_L = _conf_stats(tabL, candsL)\n",
    "    scrH = _score_from_conf(METRIC, TP_H, FP_H, FN_H, TN_H)\n",
    "    scrL = _score_from_conf(METRIC, TP_L, FP_L, FN_L, TN_L)\n",
    "\n",
    "    best_pooled = {\"BA\": -np.inf, \"tauH\": None, \"tauL\": None}\n",
    "    best_wg = {\"WG\": -np.inf, \"tauH\": None, \"tauL\": None}\n",
    "\n",
    "    for i, tauH in enumerate(candsH):\n",
    "        TP_all = TP_H[i] + TP_L\n",
    "        FP_all = FP_H[i] + FP_L\n",
    "        FN_all = FN_H[i] + FN_L\n",
    "        TN_all = TN_H[i] + TN_L\n",
    "        BA_all = _score_from_conf(\"ba\", TP_all, FP_all, FN_all, TN_all)\n",
    "        j = int(np.nanargmax(BA_all))\n",
    "        if BA_all[j] > best_pooled[\"BA\"]:\n",
    "            best_pooled.update({\"BA\": float(BA_all[j]),\n",
    "                                \"tauH\": float(tauH), \"tauL\": float(candsL[j])})\n",
    "        WG_vec = np.minimum(float(scrH[i]), scrL)\n",
    "        jwg = int(np.nanargmax(WG_vec))\n",
    "        if WG_vec[jwg] > best_wg[\"WG\"]:\n",
    "            best_wg.update({\"WG\": float(WG_vec[jwg]),\n",
    "                            \"tauH\": float(tauH), \"tauL\": float(candsL[jwg])})\n",
    "    return best_pooled, best_wg\n",
    "\n",
    "def _fold_score(y_true, y_pred):\n",
    "    TN, FP, FN, TP = skm.confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return float(_score_from_conf(METRIC, TP, FP, FN, TN))\n",
    "\n",
    "# ---------- outer LOSO ----------\n",
    "logo_outer = LeaveOneGroupOut()\n",
    "rows, pred_rows = [], []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo_outer.split(X_k, y_base.values, g_base.values), start=1):\n",
    "    train_mask = pd.Series(False, index=g_base.index)\n",
    "    test_mask  = pd.Series(False, index=g_base.index)\n",
    "    train_mask.iloc[tr_idx] = True\n",
    "    test_mask.iloc[te_idx] = True\n",
    "    test_sid = g_base.iloc[te_idx].iloc[0]\n",
    "\n",
    "    inner_ids = sorted(g_base[train_mask].unique())\n",
    "    inner_folds = choose_inner_folds_loso(inner_ids)\n",
    "\n",
    "    tau_single_list = []\n",
    "    tauH_BA_list, tauL_BA_list = [], []\n",
    "    tauH_WG_list, tauL_WG_list = [], []\n",
    "\n",
    "    for inner_val in inner_folds:\n",
    "        val_mask = g_base.isin(inner_val) & train_mask\n",
    "        inner_train = train_mask & (~val_mask)\n",
    "        if not inner_train.any() or not val_mask.any():\n",
    "            continue\n",
    "\n",
    "        model_inner = fit_classifier(X_k[inner_train], y_base[inner_train])\n",
    "        scores_val = predict_positive_score(model_inner, X_k[val_mask]).astype(float)\n",
    "        y_val = y_base[val_mask]\n",
    "        grp_val = fair_groups[val_mask].to_numpy()\n",
    "\n",
    "        tau_single_list.append(_best_single_threshold(_prep_sorted(scores_val, y_val.to_numpy())))\n",
    "\n",
    "        sH = scores_val[grp_val == \"High\"]; yH = y_val[grp_val == \"High\"].to_numpy()\n",
    "        sL = scores_val[grp_val == \"Low\"];  yL = y_val[grp_val == \"Low\"].to_numpy()\n",
    "        if len(sH) == 0 or len(sL) == 0:\n",
    "            continue\n",
    "        best_pooled, best_wg = _grid_search_group_thresholds_exact(sH, yH, sL, yL)\n",
    "        tauH_BA_list.append(best_pooled[\"tauH\"]); tauL_BA_list.append(best_pooled[\"tauL\"])\n",
    "        tauH_WG_list.append(best_wg[\"tauH\"]);     tauL_WG_list.append(best_wg[\"tauL\"])\n",
    "\n",
    "    if not tau_single_list:\n",
    "        raise RuntimeError(f\"[Cell5A] fold{fold_id}: inner validation が空です。\")\n",
    "\n",
    "    tau_single = float(np.nanmedian(tau_single_list))\n",
    "    if tauH_BA_list:\n",
    "        tauH_BA = float(np.nanmedian(tauH_BA_list))\n",
    "        tauL_BA = float(np.nanmedian(tauL_BA_list))\n",
    "    else:\n",
    "        tauH_BA = tauL_BA = tau_single\n",
    "    if tauH_WG_list:\n",
    "        tauH_WG = float(np.nanmedian(tauH_WG_list))\n",
    "        tauL_WG = float(np.nanmedian(tauL_WG_list))\n",
    "    else:\n",
    "        tauH_WG = tauL_WG = tau_single\n",
    "\n",
    "    model_outer = fit_classifier(X_k[train_mask], y_base[train_mask])\n",
    "    scores_test = predict_positive_score(model_outer, X_k[test_mask]).astype(float)\n",
    "    y_test = y_base[test_mask]\n",
    "    grp_test = fair_groups[test_mask].to_numpy()\n",
    "\n",
    "    y_pred_single   = (scores_test >= tau_single).astype(int)\n",
    "    y_pred_group_BA = (scores_test >= np.where(grp_test==\"High\", tauH_BA, tauL_BA)).astype(int)\n",
    "    y_pred_group_WG = (scores_test >= np.where(grp_test==\"High\", tauH_WG, tauL_WG)).astype(int)\n",
    "\n",
    "    rows.append({\n",
    "        \"fold_id\": fold_id,\n",
    "        \"test_id\": test_sid,\n",
    "        \"best_k\": BEST_K,\n",
    "        \"tau_single\": tau_single,\n",
    "        \"tau_high_BA\": tauH_BA, \"tau_low_BA\": tauL_BA,\n",
    "        \"tau_high_WG\": tauH_WG, \"tau_low_WG\": tauL_WG,\n",
    "        \"BA_single\": _fold_score(y_test, y_pred_single),\n",
    "        \"BA_group\": _fold_score(y_test, y_pred_group_BA),\n",
    "        \"BA_group_WG\": _fold_score(y_test, y_pred_group_WG),\n",
    "        \"n_test\": len(y_test),\n",
    "    })\n",
    "\n",
    "    for yy, ss, gg, ys, yb, yw in zip(y_test, scores_test, grp_test,\n",
    "                                      y_pred_single, y_pred_group_BA, y_pred_group_WG):\n",
    "        pred_rows.append({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_id\": test_sid,\n",
    "            \"y_true\": int(yy),\n",
    "            \"proba\": float(ss),\n",
    "            \"group\": str(gg),\n",
    "            \"y_pred_single\": int(ys),\n",
    "            \"y_pred_group_BA\": int(yb),\n",
    "            \"y_pred_group_WG\": int(yw),\n",
    "        })\n",
    "\n",
    "df_fold = pd.DataFrame(rows)\n",
    "df_pred = pd.DataFrame(pred_rows)\n",
    "df_fold.to_csv(outpath(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_pred.to_csv(outpath(\"GROUP_AWARE_PREDICTIONS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "y_pool = df_pred[\"y_true\"].to_numpy()\n",
    "s_pool = df_pred[\"proba\"].to_numpy()\n",
    "auc_pool = float(roc_auc_score(y_pool, s_pool))\n",
    "\n",
    "def _pooled_ba(col):\n",
    "    yhat = df_pred[col].to_numpy().astype(int)\n",
    "    TN, FP, FN, TP = skm.confusion_matrix(y_pool, yhat, labels=[0,1]).ravel()\n",
    "    return float(_score_from_conf(\"ba\", TP, FP, FN, TN))\n",
    "\n",
    "summary = {\n",
    "    \"best_k\": BEST_K,\n",
    "    \"AUC_pooled\": auc_pool,\n",
    "    \"BA_pooled_single\": _pooled_ba(\"y_pred_single\"),\n",
    "    \"BA_pooled_group_BA\": _pooled_ba(\"y_pred_group_BA\"),\n",
    "    \"BA_pooled_group_WG\": _pooled_ba(\"y_pred_group_WG\"),\n",
    "    \"metric\": METRIC_NAME,\n",
    "    \"n_samples\": len(df_pred),\n",
    "    \"n_pos\": int((df_pred[\"y_true\"] == 1).sum()),\n",
    "    \"n_neg\": int((df_pred[\"y_true\"] == 0).sum()),\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(outpath(\"GROUP_AWARE_SUMMARY.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell5A] Done: outer folds={len(df_fold)}, pooled AUC={auc_pool:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efea189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5C: MSSQ群別 best_k で Single τ を最適化 =====\n",
    "# 依存: Cell3D (BEST_K_BY_GROUP.JSON)\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bestk_json = outpath(\"BEST_K_BY_GROUP.JSON\")\n",
    "if os.path.exists(bestk_json):\n",
    "    with open(bestk_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        best_k_meta = json.load(f)\n",
    "    BEST_K_HIGH = int(best_k_meta.get(\"BEST_K_HIGH\", BEST_K))\n",
    "    BEST_K_LOW  = int(best_k_meta.get(\"BEST_K_LOW\",  BEST_K))\n",
    "else:\n",
    "    BEST_K_HIGH = BEST_K_LOW = BEST_K\n",
    "    print(\"[Cell5C][WARN] BEST_K_BY_GROUP.JSON が無いため両群で共通BEST_Kを使用\")\n",
    "\n",
    "def _feats_for_k(k):\n",
    "    return feature_order[:max(1, min(int(k), len(feature_order)))]\n",
    "\n",
    "def _single_tau_preds(feats, group_mask):\n",
    "    X = X_all[feats].astype(np.float32)\n",
    "    y = y_all.values\n",
    "    g = groups.values\n",
    "    logo = LeaveOneGroupOut()\n",
    "    y_true_all, proba_all, group_all = [], [], []\n",
    "    for tr_idx, te_idx in logo.split(X, y, g):\n",
    "        X_tr, X_te = X.iloc[tr_idx], X.iloc[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            continue\n",
    "        model = fit_classifier(X_tr, pd.Series(y_tr))\n",
    "        proba = predict_positive_score(model, X_te)\n",
    "        y_true_all.append(y_te); proba_all.append(proba); group_all.append(g[te_idx])\n",
    "    return np.concatenate(y_true_all), np.concatenate(proba_all), np.concatenate(group_all)\n",
    "\n",
    "y_true_high, proba_high, group_high = _single_tau_preds(_feats_for_k(BEST_K_HIGH), fair_groups.values==\"High\")\n",
    "tau_single_high, BA_high = _best_tau(proba_high, y_true_high)\n",
    "y_pred_high = (proba_high >= tau_single_high).astype(int)\n",
    "\n",
    "y_true_low, proba_low, group_low = _single_tau_preds(_feats_for_k(BEST_K_LOW), fair_groups.values==\"Low\")\n",
    "tau_single_low, BA_low = _best_tau(proba_low, y_true_low)\n",
    "y_pred_low = (proba_low >= tau_single_low).astype(int)\n",
    "\n",
    "df_single = pd.DataFrame({\n",
    "    \"y_true\": np.concatenate([y_true_high, y_true_low]),\n",
    "    \"y_pred\": np.concatenate([y_pred_high, y_pred_low]),\n",
    "    \"group\": np.concatenate([np.repeat(\"High\", len(y_true_high)), np.repeat(\"Low\", len(y_true_low))]),\n",
    "})\n",
    "cm_all = confusion_matrix(df_single[\"y_true\"], df_single[\"y_pred\"], labels=[0,1])\n",
    "TN, FP, FN, TP = cm_all.ravel()\n",
    "BA_all = _score_from_conf(METRIC, TP, FP, FN, TN)\n",
    "summary = {\n",
    "    \"best_k_high\": BEST_K_HIGH, \"tau_high\": tau_single_high, \"BA_high\": BA_high,\n",
    "    \"best_k_low\": BEST_K_LOW, \"tau_low\": tau_single_low, \"BA_low\": BA_low,\n",
    "    \"BA_all\": BA_all,\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(outpath(\"METRICS_SINGLE_BY_GROUPK.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell5C] Saved METRICS_SINGLE_BY_GROUPK (BA_all={BA_all:.3f})\")\n",
    "\n",
    "def _draw_cm(ax, cm, title):\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    mat = np.array([[TN, FP],[FN, TP]])\n",
    "    vmax = max(mat.max(), 1)\n",
    "    ax.imshow(mat, cmap=\"Blues\", vmin=0, vmax=vmax)\n",
    "    labels = np.array([[\"TN\",\"FP\"],[\"FN\",\"TP\"]])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = mat[i,j]\n",
    "            color = \"white\" if val > 0.6*vmax else \"black\"\n",
    "            ax.text(j, i, f\"{labels[i,j]} {val}\", ha=\"center\", va=\"center\",\n",
    "                    fontsize=22, fontweight=\"bold\", color=color)\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Pred: Non-Sick\",\"Pred: Sick\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"True: Non-Sick\",\"True: Sick\"], rotation=90, va=\"center\")\n",
    "    ax.set_title(title); ax.grid(False)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "_draw_cm(plt.gca(), confusion_matrix(df_single[\"y_true\"], df_single[\"y_pred\"], labels=[0,1]),\n",
    "         f\"Single τ (All) — {METRIC_NAME}={BA_all:.3f}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"CONFMAT_SINGLE_GROUPK_ALL.png\"), dpi=300); plt.close()\n",
    "\n",
    "print(\"[Cell5C] Confusion matrix saved (All). High/Low個別も必要なら追加描画してください。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
