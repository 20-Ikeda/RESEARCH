{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 0: 環境設定（全セル共通で利用）=====\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib           # 追加\n",
    "import matplotlib.backends  # 追加（←これがポイント）\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ------------------------\n",
    "# 実験スイッチ（Notebook全体で共有）\n",
    "# ------------------------\n",
    "FMS_THRESHOLD: int = 1            # FMS >= 1 を陽性ラベルとみなす\n",
    "EPOCH_LEN: int = 30               # 30 / 60 / 120 のいずれか（本実験では 30 固定運用）\n",
    "MODEL_BACKEND: str = \"xgb\"        # \"xgb\" / \"rf\" / \"svm\"\n",
    "\n",
    "USE_MSSQ_FEATURE: bool = False    # True: MSSQ を特徴量に含める\n",
    "USE_VIMSSQ_FEATURE: bool = False  # True: VIMSSQ を特徴量に含める\n",
    "\n",
    "SEED_BASE: int = 20251101\n",
    "TOP_SUBSET_K: int = 15            # subset探索で使う上位特徴数\n",
    "\n",
    "# ---- 時系列特徴（履歴連結）の設定 ----\n",
    "# HISTORY_N_EPOCHS = 1: 従来どおり履歴なし（f_k）確認済み\n",
    "# HISTORY_N_EPOCHS = H>1: 直近 H エポック分を連結 [f_{k-H+1}, ..., f_k]\n",
    "HISTORY_N_EPOCHS: int = 1\n",
    "\n",
    "if HISTORY_N_EPOCHS < 1:\n",
    "    raise ValueError(\"HISTORY_N_EPOCHS は 1 以上で指定してください。\")\n",
    "\n",
    "# ---- MSSQ / VIMSSQ の High / Low 閾値 ----\n",
    "MSSQ_THRESHOLD_FIXED: float   = 11.0  # 例：MSSQ >= 11 を High\n",
    "VIMSSQ_THRESHOLD_FIXED: float = 3     # 例：VIMSSQ >= 3 を High\n",
    "\n",
    "# ---- グループ分けの基準（FMS推移プロットなどで使用）----\n",
    "# \"MSSQ\" または \"VIMSSQ\" を指定\n",
    "GROUPING_BASIS_FOR_PLOTS: str = \"VIMSSQ\"\n",
    "\n",
    "if EPOCH_LEN not in (30, 60, 120):\n",
    "    raise ValueError(\"EPOCH_LEN は 30/60/120 から選択してください。\")\n",
    "\n",
    "# ---------------- 設定（等間隔グリッド＋近傍再探索） ----------------\n",
    "COARSE_STEPS = 101      # 0.0〜1.0 を等間隔\n",
    "FINE_STEPS   = 101      # 近傍再探索の細かさ\n",
    "FINE_MARGIN  = 0.01     # 近傍幅（±0.01）\n",
    "CORR_THRESHOLD = 0.70   # 相関除去の閾値（Cell3A-pre でも使用）\n",
    "VERBOSE      = True\n",
    "\n",
    "# ------------------------\n",
    "# ファイル入出力ルート\n",
    "# ------------------------\n",
    "BASE_INPUT_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "BASE_ANALYSIS_DIR = os.path.join(BASE_INPUT_DIR, \"ANALYSIS\")\n",
    "BASE_LEVEL0_DIR = os.path.join(BASE_ANALYSIS_DIR, \"機械学習\")  # 階層0\n",
    "\n",
    "# --- 階層1パラメータタグ（可変要素）---\n",
    "FEATURE_SELECTION_METHOD = \"TreeSHAP-group\"  # 特徴量選定方法（SHAPのやり方）\n",
    "\n",
    "\n",
    "def build_level1_dir(feature_selection: str, history_epochs: int, coarse_steps: int, fine_steps: int, corr_threshold: float) -> str:\n",
    "    parts = [\n",
    "        \"FMS1\",           # 固定\n",
    "        \"EPOCH30s\",       # 固定\n",
    "        \"MODEL-XGB\",      # 固定\n",
    "        f\"FS-{feature_selection}\",\n",
    "        f\"H{history_epochs:02d}\",\n",
    "        f\"GridC{coarse_steps}_F{fine_steps}\",\n",
    "        f\"Corr{corr_threshold:.2f}\",\n",
    "    ]\n",
    "    return os.path.join(BASE_LEVEL0_DIR, \"__\".join(parts))\n",
    "\n",
    "LEVEL1_DIR = build_level1_dir(\n",
    "    FEATURE_SELECTION_METHOD,\n",
    "    HISTORY_N_EPOCHS,\n",
    "    COARSE_STEPS,\n",
    "    FINE_STEPS,\n",
    "    CORR_THRESHOLD,\n",
    ")\n",
    "os.makedirs(LEVEL1_DIR, exist_ok=True)\n",
    "\n",
    "CURRENT_CELL_ID = None\n",
    "OUT_DIR = None\n",
    "\n",
    "\n",
    "def set_cell_output(cell_id: int) -> None:\n",
    "    \"\"\"階層2: Cellごとの出力先をセット（Cell0, Cell1, ...）\"\"\"\n",
    "    global CURRENT_CELL_ID, OUT_DIR\n",
    "    CURRENT_CELL_ID = cell_id\n",
    "    OUT_DIR = os.path.join(LEVEL1_DIR, f\"Cell{cell_id}\")\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(f\"[Cell{cell_id}] OUT_DIR -> {OUT_DIR}\")\n",
    "\n",
    "\n",
    "def outpath(filename: str) -> str:\n",
    "    if OUT_DIR is None:\n",
    "        raise RuntimeError(\"先に set_cell_output(cell_id) を呼んでください。\")\n",
    "    return os.path.join(OUT_DIR, filename)\n",
    "\n",
    "\n",
    "def cell_output_path(cell_id: int, filename: str) -> str:\n",
    "    \"\"\"前セルのCSVや図を参照するときに使う\"\"\"\n",
    "    return os.path.join(LEVEL1_DIR, f\"Cell{cell_id}\", filename)\n",
    "\n",
    "\n",
    "print(f\"[LEVEL1_DIR] {LEVEL1_DIR}  |  EPOCH_LEN={EPOCH_LEN}s | HISTORY_N_EPOCHS={HISTORY_N_EPOCHS}\")\n",
    "set_cell_output(0)  # Cell0 の出力先をセット\n",
    "\n",
    "# ------------------------\n",
    "# 対象被験者・時間窓\n",
    "# ------------------------\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "BASELINE_EPOCH = 1770               # ベースライン行（必須）\n",
    "ML_START, ML_END = 1800, 2400       # 学習に使う epoch_start 範囲 [start, end)\n",
    "\n",
    "# ------------------------\n",
    "# 描画スタイル\n",
    "# ------------------------\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"font.size\": 20, \"axes.titlesize\": 26, \"axes.labelsize\": 22,\n",
    "    \"xtick.labelsize\": 20, \"ytick.labelsize\": 20, \"legend.fontsize\": 20,\n",
    "})\n",
    "\n",
    "# ------------------------\n",
    "# FMS二値化ヘルパ\n",
    "# ------------------------\n",
    "def binarize_fms(series: pd.Series, threshold: Optional[int] = None) -> pd.Series:\n",
    "    th = FMS_THRESHOLD if threshold is None else int(threshold)\n",
    "    return (series >= th).astype(int)\n",
    "\n",
    "# ------------------------\n",
    "# モデルレジストリ\n",
    "# ------------------------\n",
    "ModelBuilder = Callable[..., Any]\n",
    "MODEL_REGISTRY: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "def register_backend(name: str, params: Dict[str, Any], builder: ModelBuilder) -> None:\n",
    "    MODEL_REGISTRY[name] = {\"params\": params, \"builder\": builder}\n",
    "\n",
    "def _build_xgb(params: Dict[str, Any], *, scale_pos_weight: Optional[float] = None):\n",
    "    cfg = params.copy()\n",
    "    if scale_pos_weight is not None:\n",
    "        cfg[\"scale_pos_weight\"] = float(scale_pos_weight)\n",
    "    return xgb.XGBClassifier(**cfg)\n",
    "\n",
    "def _build_rf(params: Dict[str, Any], **_):\n",
    "    return RandomForestClassifier(**params)\n",
    "\n",
    "def _build_svm(params: Dict[str, Any], **_):\n",
    "    return SVC(**params)\n",
    "\n",
    "# ---- XGBoost ハイパーパラメータ ----\n",
    "XGB_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=100,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=1,\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cpu\",\n",
    "    seed=0,\n",
    "    random_state=0,\n",
    "\n",
    "    # ---- 学習率（少しだけ低く） ----\n",
    "    learning_rate=0.1,      # デフォルト0.3 → 0.1 にして一歩ずつ学習\n",
    "\n",
    "    # ---- 木の複雑さ（軽く制限）----\n",
    "    max_depth=3,            # デフォルト6 → 3（浅めの木に）\n",
    "    min_child_weight=5,     # デフォルト1 → 5（少数サンプルでの分割を禁止気味に）\n",
    "    gamma=0.5,              # デフォルト0 → 0.5（ショボい分割は切り捨て）\n",
    "\n",
    "    # ---- サブサンプリング（軽く正則化）----\n",
    "    subsample=0.8,          # 1.0 → 0.8（各木が見るデータを8割に）\n",
    "    colsample_bytree=0.6,   # 1.0 → 0.6（各木が見る特徴量を6割に）\n",
    "\n",
    "    # ---- L2 / L1 正則化（本命）----\n",
    "    reg_lambda=20.0,        # L2 正則化\n",
    "    reg_alpha=2.0,          # L1 正則化で“自動特徴選択”を少し効かせる\n",
    ")\n",
    "\n",
    "RF_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=100,      # 論文：決定木100本\n",
    "    max_features=1,        # 論文：max feature of one\n",
    "\n",
    "    # 以下は論文に記載がないので，ほぼデフォルト＋再現性用\n",
    "    bootstrap=True,        # scikit-learn のデフォルト\n",
    "    random_state=SEED_BASE,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "SVM_PARAMS: Dict[str, Any] = dict(\n",
    "    C=1.0,\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"scale\",\n",
    "    probability=True,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED_BASE,\n",
    ")\n",
    "\n",
    "register_backend(\"xgb\", XGB_PARAMS, _build_xgb)\n",
    "register_backend(\"rf\",  RF_PARAMS,  _build_rf)\n",
    "register_backend(\"svm\", SVM_PARAMS, _build_svm)\n",
    "\n",
    "def set_model_backend(name: str) -> None:\n",
    "    name = name.lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録: {list(MODEL_REGISTRY.keys())}\")\n",
    "    global MODEL_BACKEND\n",
    "    MODEL_BACKEND = name\n",
    "\n",
    "def build_estimator(\n",
    "    backend: Optional[str] = None,\n",
    "    *,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    name = (backend or MODEL_BACKEND).lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録。\")\n",
    "    base = MODEL_REGISTRY[name][\"params\"].copy()\n",
    "    if overrides:\n",
    "        base.update(overrides)\n",
    "    builder = MODEL_REGISTRY[name][\"builder\"]\n",
    "    return builder(base, scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "def fit_estimator(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    model = build_estimator(\n",
    "        backend=backend, scale_pos_weight=scale_pos_weight, overrides=overrides\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def predict_positive_score(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return np.asarray(model.decision_function(X), dtype=float)\n",
    "    return model.predict(X).astype(float)\n",
    "\n",
    "MODEL_ID = MODEL_BACKEND.upper()\n",
    "print(f\"[INFO] MODEL_BACKEND={MODEL_ID} / SEED={SEED_BASE} / backends={list(MODEL_REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: データ準備（CSV読込 → EPOCH合成 → SUBJECT_META → 行列出力）=====\n",
    "set_cell_output(1)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --------------------------------------------\n",
    "# ① 30秒EPOCH CSVの読み込み・検証\n",
    "# --------------------------------------------\n",
    "def subject_csv_path(sid: str) -> str:\n",
    "    path = os.path.join(BASE_INPUT_DIR, sid, \"EPOCH\", f\"{sid}_epoch.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"[Cell1] CSV missing for subject {sid}: {path}\")\n",
    "    return path\n",
    "\n",
    "dfs = []\n",
    "for sid in SUBJECT_IDS:\n",
    "    df = pd.read_csv(subject_csv_path(sid))\n",
    "    if df.shape[1] < 4:\n",
    "        raise ValueError(f\"[Cell1] {sid}: 列数が不足（>=4 必須）\")\n",
    "    df = df.copy()\n",
    "\n",
    "    # 4列目以降の列名を文字列化（数値列名対策）\n",
    "    df.columns = list(df.columns[:3]) + [str(c) for c in df.columns[3:]]\n",
    "    c1, c2, c3 = df.columns[:3]\n",
    "    df = df.rename(columns={c1: \"epoch_start\", c2: \"epoch_end\", c3: \"FMS\"})\n",
    "\n",
    "    df[\"epoch_start\"] = pd.to_numeric(df[\"epoch_start\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"epoch_end\"]   = pd.to_numeric(df[\"epoch_end\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"FMS\"]         = pd.to_numeric(df[\"FMS\"],         errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    if df[[\"epoch_start\", \"epoch_end\", \"FMS\"]].isna().any().any():\n",
    "        raise ValueError(f\"[Cell1] {sid}: epoch_start/epoch_end/FMS に NaN があります。\")\n",
    "\n",
    "    df.insert(0, \"subject_id\", sid)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_raw = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# 除外する特徴量（周波数領域など）\n",
    "exclude_feats = {\"HF_power\", \"LF_power\", \"LF_HF_ratio\"}\n",
    "feature_cols_all = [\n",
    "    c for c in combined_raw.columns\n",
    "    if c not in {\"subject_id\", \"epoch_start\", \"epoch_end\", \"FMS\"} and c not in exclude_feats\n",
    "]\n",
    "if not feature_cols_all:\n",
    "    raise RuntimeError(\"[Cell1] 特徴量列が0です。列名や除外設定を確認してください。\")\n",
    "\n",
    "print(f\"[Cell1] Loaded subjects={len(SUBJECT_IDS)}, rows={len(combined_raw)}, \"\n",
    "      f\"features(after drop)={len(feature_cols_all)}\")\n",
    "\n",
    "print(\"[Cell1] Physiological feature columns (feature_cols_all):\")\n",
    "for col in feature_cols_all:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ② EPOCH_LEN 秒への合成 + baseline差分 + ラベル生成\n",
    "# --------------------------------------------\n",
    "if (ML_END - ML_START) % EPOCH_LEN != 0:\n",
    "    raise ValueError(f\"[Cell1] ML window {ML_END-ML_START} が EPOCH_LEN={EPOCH_LEN} で割り切れません。\")\n",
    "\n",
    "rows_per_bin = EPOCH_LEN // 30  # 30秒エポックを何個まとめるか\n",
    "df_out_list = []\n",
    "\n",
    "# デバッグ用ディレクトリ\n",
    "DEBUG_DIR = os.path.join(OUT_DIR, \"Cell1_デバッグ\")\n",
    "os.makedirs(DEBUG_DIR, exist_ok=True)\n",
    "\n",
    "for sid, sdf in combined_raw.groupby(\"subject_id\", sort=False):\n",
    "    # baseline 行（BASELINE_EPOCH）の取得\n",
    "    base_row = sdf.loc[sdf[\"epoch_start\"] == BASELINE_EPOCH]\n",
    "    if len(base_row) != 1:\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline epoch_start=={BASELINE_EPOCH} が見つからない（{len(base_row)}件）\")\n",
    "\n",
    "    base_vals = base_row[feature_cols_all].astype(float).iloc[0]\n",
    "    if base_vals.isna().any():\n",
    "        bad_cols = base_vals.index[base_vals.isna()].tolist()\n",
    "        raise ValueError(f\"[Cell1] {sid}: baselineにNaN -> {bad_cols}\")\n",
    "\n",
    "    # 学習に使う時間窓だけ抽出\n",
    "    sdf_ml = sdf[(sdf[\"epoch_start\"] >= ML_START) & (sdf[\"epoch_start\"] < ML_END)].copy()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: ML window [{ML_START},{ML_END}) が空です。\")\n",
    "\n",
    "    # 30秒epochを EPOCH_LEN 秒にまとめるためのbin\n",
    "    sdf_ml[\"bin_start\"] = ML_START + ((sdf_ml[\"epoch_start\"] - ML_START) // EPOCH_LEN) * EPOCH_LEN\n",
    "    sdf_ml[\"bin_end\"]   = sdf_ml[\"bin_start\"] + EPOCH_LEN\n",
    "\n",
    "    # 行数が揃っている bin のみ採用\n",
    "    bin_counts = sdf_ml.groupby([\"bin_start\", \"bin_end\"]).size()\n",
    "    complete_bins = bin_counts[bin_counts == rows_per_bin].index\n",
    "    sdf_ml = sdf_ml.set_index([\"bin_start\", \"bin_end\"]).loc[complete_bins].reset_index()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: EPOCH_LEN={EPOCH_LEN} で完全なbinが無い\")\n",
    "\n",
    "    # 各 bin で平均を取る（FMS も平均）\n",
    "    agg_dict = {c: \"mean\" for c in feature_cols_all}\n",
    "    agg_dict[\"FMS\"] = \"mean\"\n",
    "    g = sdf_ml.groupby([\"subject_id\", \"bin_start\", \"bin_end\"], as_index=False).agg(agg_dict)\n",
    "\n",
    "    # baseline 差分（生理特徴量のみ）\n",
    "    g_features = g[feature_cols_all].astype(float) - base_vals.values\n",
    "    if g_features.isna().any().any():\n",
    "        bad = g_features.columns[g_features.isna().any()].tolist()\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline差分後にNaN -> {bad}\")\n",
    "\n",
    "    # 出力用に整形\n",
    "    g_out = pd.concat(\n",
    "        [g[[\"subject_id\", \"bin_start\", \"bin_end\", \"FMS\"]], g_features],\n",
    "        axis=1\n",
    "    )\n",
    "    g_out = g_out.rename(columns={\"bin_start\": \"epoch_start\", \"bin_end\": \"epoch_end\"})\n",
    "\n",
    "    # FMS を二値化\n",
    "    g_out[\"label\"] = binarize_fms(g_out[\"FMS\"])\n",
    "\n",
    "    # 列順を整える\n",
    "    g_out = g_out[[\"subject_id\", \"epoch_start\", \"epoch_end\", \"FMS\", \"label\"] + feature_cols_all]\n",
    "\n",
    "    # デバッグ: この被験者のベースライン差分後データをCSVに保存\n",
    "    debug_path = os.path.join(DEBUG_DIR, f\"Cell1_debug_{sid}_E{EPOCH_LEN}s.csv\")\n",
    "    g_out.to_csv(debug_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell1-DEBUG] Saved baseline-diff data for subject {sid} -> {debug_path}\")\n",
    "\n",
    "    df_out_list.append(g_out)\n",
    "\n",
    "df_ml_epoch = pd.concat(df_out_list, ignore_index=True)\n",
    "\n",
    "print(f\"[Cell1] df_ml_epoch shape={df_ml_epoch.shape}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ③ SUBJECT_META & MSSQ / VIMSSQ group（被験者属性読み込み）\n",
    "# --------------------------------------------\n",
    "CANDIDATE_SCORE_PATHS = [\n",
    "    \"/mnt/data/summary_scores.xlsx\",\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"機械学習\", \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_INPUT_DIR, \"summary_scores.xlsx\"),\n",
    "]\n",
    "score_path = next((p for p in CANDIDATE_SCORE_PATHS if os.path.exists(p)), None)\n",
    "if score_path is None:\n",
    "    raise FileNotFoundError(\"[Cell1] summary_scores.xlsx が見つかりません。\")\n",
    "\n",
    "meta_raw = pd.read_excel(score_path, sheet_name=\"Summary\")\n",
    "\n",
    "required = [\"ID\", \"MSSQ\", \"VIMSSQ\"]\n",
    "missing = [c for c in required if c not in meta_raw.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"[Cell1] summary_scores.xlsx に必須列がありません -> {missing}\")\n",
    "\n",
    "meta = meta_raw[required].copy()\n",
    "meta[\"ID\"] = (\n",
    "    meta[\"ID\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "for c in [\"MSSQ\", \"VIMSSQ\"]:\n",
    "    meta[c] = pd.to_numeric(meta[c], errors=\"raise\")\n",
    "\n",
    "sid_set = set(map(str, SUBJECT_IDS))\n",
    "meta = meta[meta[\"ID\"].isin(sid_set)].copy()\n",
    "if meta[\"ID\"].duplicated().any():\n",
    "    dup_ids = meta.loc[meta[\"ID\"].duplicated(), \"ID\"].tolist()\n",
    "    raise ValueError(f\"[Cell1] ID 重複 -> {dup_ids}\")\n",
    "\n",
    "# MSSQ_group / VIMSSQ_group を作成\n",
    "meta[\"MSSQ_group\"]   = np.where(meta[\"MSSQ\"]   >= MSSQ_THRESHOLD_FIXED,   \"High\", \"Low\")\n",
    "meta[\"VIMSSQ_group\"] = np.where(meta[\"VIMSSQ\"] >= VIMSSQ_THRESHOLD_FIXED, \"High\", \"Low\")\n",
    "\n",
    "SUBJECT_META = (\n",
    "    meta.rename(columns={\"ID\": \"subject_id\"})\n",
    "        .set_index(\"subject_id\")[[\"MSSQ\", \"VIMSSQ\", \"MSSQ_group\", \"VIMSSQ_group\"]]\n",
    "        .copy()\n",
    ")\n",
    "\n",
    "SUBJECT_META.to_csv(outpath(\"subject_meta.csv\"), encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell1] SUBJECT_META saved -> {outpath('subject_meta.csv')} (source='{score_path}')\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ④ MSSQ / VIMSSQ をフラグに応じて特徴量に追加（1回だけ）\n",
    "# --------------------------------------------\n",
    "trait_cols_to_use: list[str] = []\n",
    "if USE_MSSQ_FEATURE:\n",
    "    trait_cols_to_use.append(\"MSSQ\")\n",
    "if USE_VIMSSQ_FEATURE:\n",
    "    trait_cols_to_use.append(\"VIMSSQ\")\n",
    "\n",
    "if trait_cols_to_use:\n",
    "    merge_cols = [\"subject_id\"] + trait_cols_to_use\n",
    "    df_ml_epoch = df_ml_epoch.merge(\n",
    "        SUBJECT_META.reset_index()[merge_cols],\n",
    "        on=\"subject_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    if df_ml_epoch[trait_cols_to_use].isna().any().any():\n",
    "        bad_sids = df_ml_epoch.loc[\n",
    "            df_ml_epoch[trait_cols_to_use].isna().any(axis=1), \"subject_id\"\n",
    "        ].unique().tolist()\n",
    "        raise ValueError(f\"[Cell1] MSSQ/VIMSSQ が欠損の subject_id があります -> {bad_sids}\")\n",
    "\n",
    "    print(\"[Cell1] df_ml_epoch with trait features (MSSQ/VIMSSQ):\")\n",
    "    print(df_ml_epoch[[\"subject_id\"] + trait_cols_to_use].drop_duplicates().head())\n",
    "else:\n",
    "    print(\"[Cell1] Trait features (MSSQ/VIMSSQ) are disabled by flags.\")\n",
    "\n",
    "# 生理特徴量 + オプションtraitsをまとめた最終的な特徴量リスト\n",
    "feature_cols_full = feature_cols_all + trait_cols_to_use\n",
    "\n",
    "print(f\"[Cell1] Final feature columns (n={len(feature_cols_full)}):\")\n",
    "for col in feature_cols_full:\n",
    "    print(\"  -\", col)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ⑤ 履歴導入「前」のラベル分布チェック（診断用）\n",
    "# --------------------------------------------\n",
    "label_counts = (\n",
    "    df_ml_epoch\n",
    "    .groupby(\"subject_id\")[\"label\"]\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# 列 0/1 が必ず存在するようにしてから rename\n",
    "for val in (0, 1):\n",
    "    if val not in label_counts.columns:\n",
    "        label_counts[val] = 0\n",
    "label_counts = label_counts[[0, 1]].rename(columns={0: \"neg_before\", 1: \"pos_before\"})\n",
    "\n",
    "LABEL_BEFORE_PATH = outpath(\"LABEL_DIST_BEFORE_HISTORY.csv\")\n",
    "label_counts.to_csv(LABEL_BEFORE_PATH, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell1] Saved label distribution BEFORE history -> {LABEL_BEFORE_PATH}\")\n",
    "\n",
    "print(\"[Cell1] Label distribution BEFORE history (per subject):\")\n",
    "print(label_counts)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ⑥ 学習行列（従来仕様）を一旦作成\n",
    "#    ※ Cell2 で時系列変換後に X_all / y_all / groups は上書きされる\n",
    "# --------------------------------------------\n",
    "fname_raw = f\"ML_DATA_DELTA_{EPOCH_LEN}S_RAW.CSV\"\n",
    "df_ml_epoch.to_csv(outpath(fname_raw), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "X_all = df_ml_epoch[feature_cols_full].copy().astype(float)\n",
    "y_all = df_ml_epoch[\"label\"].copy().astype(int)\n",
    "groups = df_ml_epoch[\"subject_id\"].copy()\n",
    "\n",
    "X_all.to_csv(outpath(f\"X_RAW_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "X_all.to_csv(outpath(f\"X_SCALED_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")  # 木系でスケーリング不要\n",
    "pd.DataFrame({\n",
    "    \"subject_id\": groups,\n",
    "    \"label\": y_all,\n",
    "    \"FMS_mean\": df_ml_epoch[\"FMS\"],\n",
    "}).to_csv(outpath(f\"Y_AND_GROUPS_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[Cell1] Saved -> {outpath(fname_raw)} / X_RAW_ALL / X_SCALED_ALL / Y_AND_GROUPS\")\n",
    "print(f\"[Cell1] Matrices ready (pre-history): X_all={X_all.shape}, y_all={y_all.shape}, \"\n",
    "      f\"SUBJECT_META={SUBJECT_META.shape}\")\n",
    "print(f\"[Cell1] n_features(physio)={len(feature_cols_all)}, \"\n",
    "      f\"+ traits({len(trait_cols_to_use)}) -> {len(feature_cols_full)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb1fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1A: MSSQ / VIMSSQ 群別の FMS 推移プロット =====\n",
    "set_cell_output(2)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 出力ディレクトリ\n",
    "FMS_PLOT_DIR = os.path.join(OUT_DIR, \"Cell1A_FMS_trajectory\")\n",
    "os.makedirs(FMS_PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# 描画スタイル（ユーザー規約）\n",
    "LW = 1.5\n",
    "FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "\n",
    "COLOR_HIGH = \"red\"\n",
    "COLOR_LOW  = \"blue\"\n",
    "\n",
    "\n",
    "def _prepare_fms_long_use_epoch_end():\n",
    "    \"\"\"\n",
    "    df_ml_epoch から FMS 時系列を取り出し、\n",
    "    ML_START からの経過時間（分）を epoch_end 基準で付与した長データを返す。\n",
    "    \"\"\"\n",
    "    required_cols = {\"subject_id\", \"epoch_end\", \"FMS\"}\n",
    "    missing = required_cols - set(df_ml_epoch.columns)\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell1A] df_ml_epoch に必須列がありません -> {missing}\")\n",
    "\n",
    "    df = df_ml_epoch[[\"subject_id\", \"epoch_end\", \"FMS\"]].copy()\n",
    "\n",
    "    # ML_START からの経過時間（秒 → 分）を epoch_end 基準で計算\n",
    "    df[\"t_min\"] = (df[\"epoch_end\"] - ML_START).astype(float) / 60.0\n",
    "    return df\n",
    "\n",
    "\n",
    "def _plot_fms_by_group_min_axis(group_col: str, title_prefix: str, save_name: str):\n",
    "    \"\"\"\n",
    "    group_col: \"MSSQ_group\" または \"VIMSSQ_group\"\n",
    "    title_prefix: 図タイトルのプレフィックス（\"MSSQ group\" など）\n",
    "    save_name: 保存ファイル名\n",
    "    \"\"\"\n",
    "    if group_col not in SUBJECT_META.columns:\n",
    "        raise KeyError(\n",
    "            f\"[Cell1A] SUBJECT_META に {group_col} 列がありません。\"\n",
    "            \"Cell1 の SUBJECT_META 作成部を確認してください。\"\n",
    "        )\n",
    "\n",
    "    # 群に属する被験者 ID をプリント\n",
    "    for level in [\"High\", \"Low\"]:\n",
    "        sids = SUBJECT_META.index[SUBJECT_META[group_col] == level].tolist()\n",
    "        print(f\"[Cell1A] {group_col} = {level}: subjects = {sorted(sids)}\")\n",
    "\n",
    "    # FMS 長データに group_col をマージ（epoch_end 基準）\n",
    "    df_long = _prepare_fms_long_use_epoch_end()\n",
    "    df_long = df_long.merge(\n",
    "        SUBJECT_META.reset_index()[[\"subject_id\", group_col]],\n",
    "        on=\"subject_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    if df_long[group_col].isna().any():\n",
    "        bad = df_long.loc[df_long[group_col].isna(), \"subject_id\"].unique().tolist()\n",
    "        raise ValueError(f\"[Cell1A] {group_col} が欠損の subject_id があります -> {bad}\")\n",
    "\n",
    "    # group × t_min ごとに FMS の平均・標準偏差\n",
    "    agg = (\n",
    "        df_long\n",
    "        .groupby([group_col, \"t_min\"])[\"FMS\"]\n",
    "        .agg([\"mean\", \"std\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "    agg[\"std\"] = agg[\"std\"].fillna(0.0)\n",
    "\n",
    "    # プロット\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for level, color in [(\"High\", COLOR_HIGH), (\"Low\", COLOR_LOW)]:\n",
    "        sub = agg[agg[group_col] == level].sort_values(\"t_min\")\n",
    "        if sub.empty:\n",
    "            print(f\"[Cell1A] 注意: {group_col}={level} のデータがありません。\")\n",
    "            continue\n",
    "\n",
    "        t = sub[\"t_min\"].values  # [分] 単位\n",
    "        m = sub[\"mean\"].values\n",
    "        s = sub[\"std\"].values\n",
    "\n",
    "        # 平均（太線）\n",
    "        ax.plot(t, m, label=f\"{level} (mean)\", linewidth=LW * 2.0, color=color)\n",
    "        # ±1 SD バンド\n",
    "        ax.fill_between(t, m - s, m + s, alpha=0.2, color=color, linewidth=0)\n",
    "\n",
    "    # ----- 軸設定 -----\n",
    "\n",
    "    # 横軸：0〜10分（または ML 窓長に応じて自動）\n",
    "    duration_min = (ML_END - ML_START) / 60.0\n",
    "    # 安全側で 0〜duration_min、整数目盛（0,1,2,...）\n",
    "    max_tick = int(np.floor(duration_min))\n",
    "    xticks = np.arange(0, max_tick + 1, 1)\n",
    "    ax.set_xlim(0.0, duration_min)\n",
    "    ax.set_xticks(xticks)\n",
    "    # ラベルは整数表示（0,1,2,...）\n",
    "    ax.set_xlabel(\"Time [min]\", fontsize=FS_LABEL)\n",
    "\n",
    "    # 縦軸：FMS 0〜4、整数目盛\n",
    "    ax.set_ylim(0, 4)\n",
    "    ax.set_yticks([0, 1, 2, 3, 4])\n",
    "    ax.set_ylabel(\"FMS\", fontsize=FS_LABEL)\n",
    "\n",
    "    # タイトル\n",
    "    ax.set_title(f\"{title_prefix}別 FMS 推移\", fontsize=FS_TITLE)\n",
    "\n",
    "    # グリッド\n",
    "    ax.grid(True)\n",
    "\n",
    "    # 3分 と 6分30秒 に縦の点線\n",
    "    ax.axvline(3.0,  linestyle=\"--\", linewidth=LW, color=\"gray\")\n",
    "    ax.axvline(6.5,  linestyle=\"--\", linewidth=LW, color=\"gray\")\n",
    "\n",
    "    # 目盛フォント\n",
    "    ax.tick_params(axis=\"both\", labelsize=FS_TICK)\n",
    "\n",
    "    # 凡例\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_path = os.path.join(FMS_PLOT_DIR, save_name)\n",
    "    fig.savefig(save_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[Cell1A] Saved FMS trajectory plot -> {save_path}\")\n",
    "\n",
    "\n",
    "# ---- 基準切り替え：MSSQ / VIMSSQ ----\n",
    "basis = GROUPING_BASIS_FOR_PLOTS.upper()\n",
    "\n",
    "if basis == \"MSSQ\":\n",
    "    group_col    = \"MSSQ_group\"\n",
    "    title_prefix = \"MSSQ group\"\n",
    "    save_name    = f\"FMS_MSSQ_group_E{EPOCH_LEN}s.png\"\n",
    "\n",
    "elif basis == \"VIMSSQ\":\n",
    "    group_col    = \"VIMSSQ_group\"\n",
    "    title_prefix = \"VIMSSQ group\"\n",
    "    save_name    = f\"FMS_VIMSSQ_group_E{EPOCH_LEN}s.png\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"[Cell1A] GROUPING_BASIS_FOR_PLOTS は 'MSSQ' か 'VIMSSQ' を指定してください \"\n",
    "        f\"（現在: {GROUPING_BASIS_FOR_PLOTS}）\"\n",
    "    )\n",
    "\n",
    "print(f\"[Cell1A] GROUPING_BASIS_FOR_PLOTS = {GROUPING_BASIS_FOR_PLOTS} で FMS 推移を描画します。\")\n",
    "\n",
    "_plot_fms_by_group_min_axis(\n",
    "    group_col=group_col,\n",
    "    title_prefix=title_prefix,\n",
    "    save_name=save_name,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45007fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2: モデリング共通ヘルパ（fit / SHAP / 評価）=====\n",
    "set_cell_output(3)\n",
    "\n",
    "\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 学習ラッパー（Cell0のレジストリAPIを利用）\n",
    "# --------------------------------------------\n",
    "def fit_classifier(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cell0 の fit_estimator を直接包む薄いラッパ。\n",
    "    - SHAP/評価セルから backend を差し替えたい場合のみ backend / overrides を指定する。\n",
    "    \"\"\"\n",
    "    if \"fit_estimator\" not in globals():\n",
    "        raise RuntimeError(\"[Cell2] fit_estimator が未定義です。Cell0 を先に実行してください。\")\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    return fit_estimator(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        backend=backend,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# TreeSHAP ベースの特徴重要度算出\n",
    "# --------------------------------------------\n",
    "def compute_train_shap_abs_mean(model, X_ref: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    学習データ X_ref 上での平均絶対SHAP値（降順）。\n",
    "    - XGB/RF 等の木モデルを想定（TreeSHAP）。\n",
    "    - SVM など非対応モデルでは ValueError を送出する。\n",
    "    \"\"\"\n",
    "    X_ref = X_ref.astype(np.float32, copy=False)\n",
    "\n",
    "    # 背景データ（最大128行）\n",
    "    bg_n = min(128, len(X_ref))\n",
    "    X_bg = X_ref.sample(n=bg_n, random_state=SEED_BASE) if bg_n >= 2 else X_ref\n",
    "\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            data=X_bg,\n",
    "            model_output=\"probability\",\n",
    "            feature_perturbation=\"interventional\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "    except Exception:\n",
    "        # probability指定が非対応な場合に raw へフォールバック\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            model_output=\"raw\",\n",
    "            feature_perturbation=\"tree_path_dependent\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "\n",
    "    # shap_values の戻り値形状を統一（2D: n_samples × n_features）\n",
    "    classes = getattr(model, \"classes_\", None)\n",
    "    pos_idx = int(np.where(classes == 1)[0][0]) if classes is not None and 1 in list(classes) else -1\n",
    "\n",
    "    if isinstance(sv_any, list):\n",
    "        sv = sv_any[pos_idx]\n",
    "    else:\n",
    "        sv = getattr(sv_any, \"values\", sv_any)\n",
    "        sv = np.asarray(sv)\n",
    "        if sv.ndim == 3:\n",
    "            sv = sv[..., pos_idx]\n",
    "        elif sv.ndim == 1:\n",
    "            sv = sv.reshape(-1, 1)\n",
    "\n",
    "    if sv.shape[1] != X_ref.shape[1]:\n",
    "        raise RuntimeError(\n",
    "            f\"[Cell2] SHAP shape mismatch: sv.shape={sv.shape}, X_ref.shape={X_ref.shape}\"\n",
    "        )\n",
    "\n",
    "    abs_mean = np.mean(np.abs(sv), axis=0)\n",
    "    return pd.Series(abs_mean, index=X_ref.columns, name=\"mean_abs\").sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 評価ユーティリティ\n",
    "# --------------------------------------------\n",
    "def _is_probability_like(scores: np.ndarray) -> bool:\n",
    "    return np.isfinite(scores).all() and 0.0 <= scores.min() and scores.max() <= 1.0\n",
    "\n",
    "\n",
    "def evaluate_fold(model, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - ROC AUC: 2クラス時のみ。\n",
    "    - Accuracy: 確率なら 0.5、スコアなら 0.0 を閾値とする（詳細な最適化は別セル）。\n",
    "    \"\"\"\n",
    "    X_test = X_test.astype(np.float32, copy=False)\n",
    "    scores = predict_positive_score(model, X_test)\n",
    "\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, scores)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "\n",
    "    thr = 0.5 if _is_probability_like(scores) else 0.0\n",
    "    pred = (scores >= thr).astype(int)\n",
    "    acc = accuracy_score(y_test.astype(int), pred)\n",
    "\n",
    "    return {\"roc_auc\": float(roc_auc), \"accuracy\": float(acc)}\n",
    "\n",
    "\n",
    "print(\"[Cell2] Modeling helpers ready (fit_classifier / compute_train_shap_abs_mean / evaluate_fold)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab12b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3: 時系列特徴量変換（固定幅 H エポック）＋統計ログ =====\n",
    "set_cell_output(4)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 必須オブジェクトの存在チェック\n",
    "required = [\n",
    "    \"df_ml_epoch\",\n",
    "    \"feature_cols_all\",\n",
    "    \"trait_cols_to_use\",\n",
    "    \"HISTORY_N_EPOCHS\",\n",
    "    \"EPOCH_LEN\",\n",
    "    \"outpath\",\n",
    "]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3] 未定義の変数があります: {missing}\")\n",
    "\n",
    "H = int(HISTORY_N_EPOCHS)\n",
    "if H < 1:\n",
    "    raise ValueError(\"[Cell3] HISTORY_N_EPOCHS は 1 以上である必要があります。\")\n",
    "\n",
    "print(f\"[Cell3] 時系列特徴量変換を開始: HISTORY_N_EPOCHS={H}, EPOCH_LEN={EPOCH_LEN}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ① 履歴導入「前」のラベル分布を再計算（保険）\n",
    "#    ※ Cell1でも保存しているが、ここでも再作成しておく\n",
    "# --------------------------------------------\n",
    "label_before = (\n",
    "    df_ml_epoch\n",
    "    .groupby(\"subject_id\")[\"label\"]\n",
    "    .value_counts()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "for val in (0, 1):\n",
    "    if val not in label_before.columns:\n",
    "        label_before[val] = 0\n",
    "label_before = label_before[[0, 1]].rename(columns={0: \"neg_before\", 1: \"pos_before\"})\n",
    "\n",
    "print(\"[Cell3] Label distribution BEFORE history (recomputed):\")\n",
    "print(label_before)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ② HISTORY_N_EPOCHS == 1 の場合は変換せず、そのまま利用\n",
    "# --------------------------------------------\n",
    "if H == 1:\n",
    "    print(\"[Cell3] HISTORY_N_EPOCHS == 1 のため、履歴連結は行わず df_ml_epoch をそのまま使用します。\")\n",
    "\n",
    "    df_ml_ts = df_ml_epoch.copy()\n",
    "\n",
    "    # サンプル統計（履歴導入後＝beforeと同じ）\n",
    "    per_subject_stats = []\n",
    "    for sid, g in df_ml_epoch.groupby(\"subject_id\", sort=False):\n",
    "        n_raw = len(g)\n",
    "        pos_after = int((g[\"label\"] == 1).sum())\n",
    "        neg_after = int((g[\"label\"] == 0).sum())\n",
    "        per_subject_stats.append({\n",
    "            \"subject_id\": sid,\n",
    "            \"n_raw\": n_raw,\n",
    "            \"n_ts\": n_raw,\n",
    "            \"pos_after\": pos_after,\n",
    "            \"neg_after\": neg_after,\n",
    "        })\n",
    "\n",
    "    subject_stats_df = pd.DataFrame(per_subject_stats).set_index(\"subject_id\")\n",
    "    subject_stats_df = subject_stats_df.join(label_before, how=\"left\")\n",
    "\n",
    "    # 比率なども一応追加\n",
    "    subject_stats_df[\"ratio_n_ts\"] = subject_stats_df[\"n_ts\"] / subject_stats_df[\"n_raw\"].replace(0, np.nan)\n",
    "\n",
    "    STATS_PATH = outpath(\"SUBJECT_TS_STATS.csv\")\n",
    "    subject_stats_df.to_csv(STATS_PATH, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3] SUBJECT_TS_STATS saved -> {STATS_PATH}\")\n",
    "    print(subject_stats_df)\n",
    "\n",
    "    # 特徴量リストは Cell1 での feature_cols_full をそのまま使用\n",
    "    if \"feature_cols_full\" not in globals():\n",
    "        raise RuntimeError(\"[Cell3] feature_cols_full が未定義です。Cell1 を先に実行してください。\")\n",
    "\n",
    "    ts_feature_cols = feature_cols_full\n",
    "\n",
    "    # グローバル行列を設定（従来仕様）\n",
    "    X_all = df_ml_ts[ts_feature_cols].astype(float)\n",
    "    y_all = df_ml_ts[\"label\"].astype(int)\n",
    "    groups = df_ml_ts[\"subject_id\"].copy()\n",
    "\n",
    "    df_ml_ts.to_csv(outpath(f\"ML_DATA_TS_{EPOCH_LEN}S_H{H}.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3] df_ml_ts saved -> {outpath(f'ML_DATA_TS_{EPOCH_LEN}S_H{H}.CSV')}\")\n",
    "    print(f\"[Cell3] Matrices ready (H=1): X_all={X_all.shape}, y_all={y_all.shape}\")\n",
    "else:\n",
    "    # --------------------------------------------\n",
    "    # ③ HISTORY_N_EPOCHS >= 2: 固定幅 H エポック履歴に変換\n",
    "    # --------------------------------------------\n",
    "    # 物理特徴量部分の lag付き列名\n",
    "    physio_lag_cols = [\n",
    "        f\"{col}_lag{lag}\"\n",
    "        for lag in range(H - 1, -1, -1)  # 例: H=3 -> lag2, lag1, lag0\n",
    "        for col in feature_cols_all\n",
    "    ]\n",
    "    ts_feature_cols = physio_lag_cols + trait_cols_to_use\n",
    "\n",
    "    print(f\"[Cell3] physio_lag_cols: {len(physio_lag_cols)} 列, traits: {trait_cols_to_use}\")\n",
    "\n",
    "    df_ts_list = []\n",
    "    per_subject_stats = []\n",
    "\n",
    "    for sid, g in df_ml_epoch.groupby(\"subject_id\", sort=False):\n",
    "        g = g.sort_values(\"epoch_start\").reset_index(drop=True)\n",
    "        n_raw = len(g)\n",
    "\n",
    "        rows_ts = []\n",
    "\n",
    "        # i: 履歴ウィンドウの末尾インデックス\n",
    "        for i in range(H - 1, n_raw):\n",
    "            block = g.iloc[i - H + 1: i + 1]\n",
    "\n",
    "            # epoch_start が EPOCH_LEN 刻みで連続しているか確認\n",
    "            starts = block[\"epoch_start\"].to_numpy()\n",
    "            diffs = np.diff(starts)\n",
    "            if not np.all(diffs == EPOCH_LEN):\n",
    "                # 欠損をまたぐウィンドウは使わない\n",
    "                continue\n",
    "\n",
    "            # 直近Hエポック分の生理特徴量を連結\n",
    "            features_seq = []\n",
    "            for row_idx in range(i - H + 1, i + 1):\n",
    "                features_seq.append(\n",
    "                    g.loc[row_idx, feature_cols_all].to_numpy(dtype=float)\n",
    "                )\n",
    "            features_concat = np.concatenate(features_seq, axis=0)\n",
    "\n",
    "            if features_concat.shape[0] != len(physio_lag_cols):\n",
    "                raise RuntimeError(\n",
    "                    f\"[Cell3] features_concat length mismatch: \"\n",
    "                    f\"{features_concat.shape[0]} vs {len(physio_lag_cols)}\"\n",
    "                )\n",
    "\n",
    "            row = {\n",
    "                \"subject_id\": g.loc[i, \"subject_id\"],\n",
    "                \"epoch_start\": int(g.loc[i, \"epoch_start\"]),\n",
    "                \"epoch_end\": int(g.loc[i, \"epoch_end\"]),\n",
    "                \"FMS\": float(g.loc[i, \"FMS\"]),\n",
    "                \"label\": int(g.loc[i, \"label\"]),\n",
    "            }\n",
    "\n",
    "            # lag付き物理特徴\n",
    "            for c_idx, col_name in enumerate(physio_lag_cols):\n",
    "                row[col_name] = float(features_concat[c_idx])\n",
    "\n",
    "            # trait（MSSQ/VIMSSQ）はlagなしでそのまま\n",
    "            for tcol in trait_cols_to_use:\n",
    "                row[tcol] = float(g.loc[i, tcol])\n",
    "\n",
    "            rows_ts.append(row)\n",
    "\n",
    "        df_sub_ts = pd.DataFrame(rows_ts)\n",
    "        n_ts = len(df_sub_ts)\n",
    "\n",
    "        if n_ts > 0:\n",
    "            pos_after = int((df_sub_ts[\"label\"] == 1).sum())\n",
    "            neg_after = int((df_sub_ts[\"label\"] == 0).sum())\n",
    "        else:\n",
    "            pos_after = 0\n",
    "            neg_after = 0\n",
    "\n",
    "        per_subject_stats.append({\n",
    "            \"subject_id\": sid,\n",
    "            \"n_raw\": n_raw,\n",
    "            \"n_ts\": n_ts,\n",
    "            \"pos_after\": pos_after,\n",
    "            \"neg_after\": neg_after,\n",
    "        })\n",
    "\n",
    "        df_ts_list.append(df_sub_ts)\n",
    "\n",
    "        print(f\"[Cell3] subject {sid}: n_raw={n_raw}, n_ts={n_ts}, \"\n",
    "              f\"pos_after={pos_after}, neg_after={neg_after}\")\n",
    "\n",
    "    # 被験者ごと時系列ウィンドウを縦結合\n",
    "    df_ml_ts = pd.concat(df_ts_list, ignore_index=True) if df_ts_list else pd.DataFrame()\n",
    "\n",
    "    print(f\"[Cell3] df_ml_ts shape={df_ml_ts.shape}\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # ④ サンプル数・ラベル分布統計（before/after）を集計\n",
    "    # --------------------------------------------\n",
    "    subject_stats_df = pd.DataFrame(per_subject_stats).set_index(\"subject_id\")\n",
    "    subject_stats_df = subject_stats_df.join(label_before, how=\"left\")\n",
    "\n",
    "    # 比率などを追加\n",
    "    subject_stats_df[\"ratio_n_ts\"] = subject_stats_df[\"n_ts\"] / subject_stats_df[\"n_raw\"].replace(0, np.nan)\n",
    "\n",
    "    STATS_PATH = outpath(\"SUBJECT_TS_STATS.csv\")\n",
    "    subject_stats_df.to_csv(STATS_PATH, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3] SUBJECT_TS_STATS saved -> {STATS_PATH}\")\n",
    "    print(subject_stats_df)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # ⑤ グローバル行列 X_all / y_all / groups を履歴版に差し替え\n",
    "    # --------------------------------------------\n",
    "    if df_ml_ts.empty:\n",
    "        raise RuntimeError(\"[Cell3] df_ml_ts が空です。履歴ウィンドウ条件が厳しすぎる可能性があります。\")\n",
    "\n",
    "    X_all = df_ml_ts[ts_feature_cols].astype(float)\n",
    "    y_all = df_ml_ts[\"label\"].astype(int)\n",
    "    groups = df_ml_ts[\"subject_id\"].copy()\n",
    "\n",
    "    df_ml_ts.to_csv(outpath(f\"ML_DATA_TS_{EPOCH_LEN}S_H{H}.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3] df_ml_ts saved -> {outpath(f'ML_DATA_TS_{EPOCH_LEN}S_H{H}.CSV')}\")\n",
    "    print(f\"[Cell3] Matrices ready (H={H}): X_all={X_all.shape}, y_all={y_all.shape}\")\n",
    "    print(f\"[Cell3] ts_feature_cols n={len(ts_feature_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6751dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-pre: 高相関特徴の事前除去（グループ単位） =====\n",
    "set_cell_output(5)\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "required = [\"X_all\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-pre] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "MIN_VARIANCE = 1e-8\n",
    "FEATURE_LIST_PATH = outpath(\"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "# ---------- グループ名取得ヘルパ ----------\n",
    "def get_feature_group(col: str) -> str:\n",
    "    \"\"\"\n",
    "    ベース特徴名を返す:\n",
    "      - 'xxx_lag0', 'xxx_lag1', ... → 'xxx'\n",
    "      - それ以外（MSSQ, VIMSSQ 等）は列名そのまま\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "    return m.group(1) if m else col\n",
    "\n",
    "# ---------- 数値列だけ抽出 ----------\n",
    "X_num = X_all.select_dtypes(include=[np.number]).copy()\n",
    "if X_num.empty:\n",
    "    raise RuntimeError(\"[Cell3A-pre] 数値列がありません。\")\n",
    "\n",
    "# ---------- 列 → グループ / グループ → 列 ----------\n",
    "col_to_group: dict[str, str] = {}\n",
    "group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "\n",
    "for col in X_num.columns:\n",
    "    g = get_feature_group(col)\n",
    "    col_to_group[col] = g\n",
    "    group_to_cols[g].append(col)\n",
    "\n",
    "group_names_in_order: list[str] = []\n",
    "seen = set()\n",
    "for col in X_num.columns:\n",
    "    g = col_to_group[col]\n",
    "    if g not in seen:\n",
    "        group_names_in_order.append(g)\n",
    "        seen.add(g)\n",
    "\n",
    "# ---------- グループ代表系列（lagの平均） ----------\n",
    "X_group = pd.DataFrame(index=X_num.index)\n",
    "for g, cols in group_to_cols.items():\n",
    "    X_group[g] = X_num[cols].mean(axis=1)\n",
    "\n",
    "# ---------- 分散がほぼゼロのグループを除外 ----------\n",
    "var = X_group.var(axis=0, ddof=1).fillna(0.0)\n",
    "valid_groups = var[var > MIN_VARIANCE].index.tolist()\n",
    "if not valid_groups:\n",
    "    raise RuntimeError(\"[Cell3A-pre] 分散がほぼゼロのため使用可能なグループがありません。\")\n",
    "\n",
    "# 優先順は X_all の列順に従ってグループ順序を決定\n",
    "priority_groups = [g for g in group_names_in_order if g in valid_groups]\n",
    "X_use = X_group[priority_groups]\n",
    "\n",
    "# ---------- グループ代表同士の相関行列 ----------\n",
    "corr = X_use.corr(method=\"pearson\").abs()\n",
    "\n",
    "keep_groups: list[str] = []\n",
    "dropped_groups_detail: list[dict] = []\n",
    "\n",
    "for g in priority_groups:\n",
    "    conflict = None\n",
    "    for kept in keep_groups:\n",
    "        if corr.loc[g, kept] >= CORR_THRESHOLD:\n",
    "            conflict = kept\n",
    "            break\n",
    "    if conflict is None:\n",
    "        # まだどの kept とも高相関でない → 代表として残す\n",
    "        keep_groups.append(g)\n",
    "    else:\n",
    "        # すでに keep に入っている代表 (conflict) を残し、後から出てきた g を除去\n",
    "        dropped_groups_detail.append({\n",
    "            \"group\": g,\n",
    "            \"representative_group\": conflict,\n",
    "            \"abs_corr\": float(corr.loc[g, conflict]),\n",
    "            \"dropped_columns\": group_to_cols[g],\n",
    "            \"representative_columns\": group_to_cols[conflict],\n",
    "        })\n",
    "\n",
    "# ---------- グループ→列 への展開 ----------\n",
    "keep_columns = [c for c in X_num.columns if col_to_group[c] in keep_groups]\n",
    "dropped_columns = [c for c in X_num.columns if col_to_group[c] not in keep_groups]\n",
    "\n",
    "payload = {\n",
    "    # 既存セル互換：ここは「残す列名」のリスト\n",
    "    \"keep\": keep_columns,\n",
    "\n",
    "    # 追加情報：グループ単位の情報\n",
    "    \"keep_groups\": keep_groups,\n",
    "    \"dropped_groups\": dropped_groups_detail,\n",
    "    \"threshold\": CORR_THRESHOLD,\n",
    "    \"total_groups\": len(priority_groups),\n",
    "    \"total_columns\": len(X_num.columns),\n",
    "}\n",
    "with open(FEATURE_LIST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[Cell3A-pre] group_keep={len(keep_groups)} / total_groups={len(priority_groups)}, \"\n",
    "      f\"drop_groups={len(dropped_groups_detail)}\")\n",
    "print(f\"[Cell3A-pre] keep_columns={len(keep_columns)} / total_columns={len(X_num.columns)}\")\n",
    "print(f\"[Cell3A-pre] JSON -> {FEATURE_LIST_PATH}\")\n",
    "\n",
    "# ---- ログ出力 ----\n",
    "if dropped_groups_detail:\n",
    "    print(f\"[Cell3A-pre] |r| >= {CORR_THRESHOLD:.2f} のグループペア（後から出てきた方を DROP）：\")\n",
    "    for d in dropped_groups_detail:\n",
    "        print(\n",
    "            f\"  [KEEP-G] {d['representative_group']}  \"\n",
    "            f\"[DROP-G] {d['group']}  \"\n",
    "            f\"(abs_corr={d['abs_corr']:.3f})  \"\n",
    "            f\"cols_keep={len(d['representative_columns'])}, \"\n",
    "            f\"cols_drop={len(d['dropped_columns'])}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"[Cell3A-pre] 高相関によるグループ除去はありませんでした。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a061c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A: SHAPベース順位付け（LOSO, TreeSHAP・グループ単位） =====\n",
    "set_cell_output(6)\n",
    "\n",
    "RUN_CELL_3A_SHAP = True  # デフォルトはTrue\n",
    "\n",
    "if RUN_CELL_3A_SHAP:\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    import json\n",
    "    import os\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import matplotlib.backends\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    required = [\n",
    "        \"X_all\", \"y_all\", \"groups\",\n",
    "        \"fit_classifier\", \"evaluate_fold\",\n",
    "        \"outpath\", \"compute_train_shap_abs_mean\"\n",
    "    ]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-SHAP-RANK] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    # ---------- グループ名取得ヘルパ ----------\n",
    "    def get_feature_group(col: str) -> str:\n",
    "        m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "        return m.group(1) if m else col\n",
    "\n",
    "    # TreeSHAP のバックエンド（木モデル）\n",
    "    SHAP_BACKEND = \"xgb\"\n",
    "\n",
    "    FEATURE_LIST_PATH = cell_output_path(5, \"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "    # --- 特徴量プールの決定（相関事前除去の結果があれば利用） ---\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        # 事前除去で KEEP された列だけ使う\n",
    "        keep_cols = keep_payload.get(\"keep\", [])\n",
    "        feature_pool = [c for c in keep_cols if c in X_all.columns]\n",
    "        print(f\"[Cell3A-SHAP-RANK] correlation-pruned columns loaded ({len(feature_pool)} cols)\")\n",
    "    else:\n",
    "        feature_pool = list(X_all.columns)\n",
    "        print(\"[Cell3A-SHAP-RANK] correlation-pruned list not found. Using all columns.\")\n",
    "\n",
    "    if not feature_pool:\n",
    "        raise RuntimeError(\"[Cell3A-SHAP-RANK] feature_pool が空です。Cell3A-pre の結果を確認してください。\")\n",
    "\n",
    "    X_source = X_all[feature_pool].copy()\n",
    "\n",
    "    # 列→グループ / グループ→列\n",
    "    col_to_group: dict[str, str] = {}\n",
    "    group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "    for col in X_source.columns:\n",
    "        g = get_feature_group(col)\n",
    "        col_to_group[col] = g\n",
    "        group_to_cols[g].append(col)\n",
    "\n",
    "    group_names = list(group_to_cols.keys())\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    # 列単位の fold別ランキング（従来互換）\n",
    "    col_ranking_frames = []\n",
    "    # グループ単位の fold別ランキング（新仕様）\n",
    "    group_ranking_frames = []\n",
    "    # foldごとの性能\n",
    "    metrics_rows = []\n",
    "\n",
    "    # --- LOSO ループ ---\n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_source, y_all, groups), start=1):\n",
    "        X_tr = X_source.iloc[tr_idx].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_source.iloc[te_idx].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(f\"[Cell3A-SHAP-RANK] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "        # --- 木モデルで学習（TreeSHAP 対応） ---\n",
    "        model = fit_classifier(X_tr, y_tr, backend=SHAP_BACKEND)\n",
    "\n",
    "        # --- 列単位の mean(|SHAP|) を計算 ---\n",
    "        shap_mean_col = compute_train_shap_abs_mean(model, X_tr)\n",
    "        # 全列順に並べ直し（fold間でインデックス整合のため）\n",
    "        shap_mean_col = shap_mean_col.reindex(X_tr.columns)\n",
    "\n",
    "        # --- 列単位の順位（互換用） ---\n",
    "        col_ranks = shap_mean_col.rank(ascending=False, method=\"min\").astype(int)\n",
    "        col_ranks.name = f\"fold{fold_id}\"\n",
    "        col_ranking_frames.append(col_ranks)\n",
    "\n",
    "        # --- グループ単位への集約（mean_abs を「合計」） ---\n",
    "        group_importance: dict[str, float] = defaultdict(float)\n",
    "        for col, val in shap_mean_col.items():\n",
    "            g = col_to_group[col]\n",
    "            group_importance[g] += float(val)\n",
    "\n",
    "        shap_mean_group = pd.Series(group_importance)\n",
    "        # グループ単位の順位\n",
    "        group_ranks = shap_mean_group.rank(ascending=False, method=\"min\").astype(int)\n",
    "        group_ranks.name = f\"fold{fold_id}\"\n",
    "        group_ranking_frames.append(group_ranks)\n",
    "\n",
    "        # --- Fold ごとの性能評価 ---\n",
    "        metrics = evaluate_fold(model, X_te, y_te)\n",
    "        metrics.update({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "        })\n",
    "        metrics_rows.append(metrics)\n",
    "\n",
    "        preview_groups = shap_mean_group.sort_values(ascending=False).head(5).index.tolist()\n",
    "        print(f\"[Cell3A-SHAP-RANK] fold{fold_id}: ranked groups={len(shap_mean_group)} (top5 groups={preview_groups})\")\n",
    "\n",
    "    # ---------- 列単位ランキング（互換用） ----------\n",
    "    col_rank_df = pd.concat(col_ranking_frames, axis=1)\n",
    "    col_rank_df[\"rank_mean\"] = col_rank_df.mean(axis=1)\n",
    "    col_rank_df[\"rank_median\"] = col_rank_df.median(axis=1)\n",
    "    col_rank_df = col_rank_df.sort_values(\"rank_mean\")\n",
    "\n",
    "    rank_path_cols = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "    col_rank_df.to_csv(rank_path_cols, encoding=\"utf-8-sig\")\n",
    "    col_rank_df.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] 列単位ランキング saved -> {rank_path_cols}\")\n",
    "\n",
    "    # ---------- グループ単位ランキング（新仕様・メイン） ----------\n",
    "    group_rank_df = pd.concat(group_ranking_frames, axis=1)\n",
    "    group_rank_df[\"rank_mean\"] = group_rank_df.mean(axis=1)\n",
    "    group_rank_df[\"rank_median\"] = group_rank_df.median(axis=1)\n",
    "    group_rank_df = group_rank_df.sort_values(\"rank_mean\")\n",
    "\n",
    "    rank_path_groups = outpath(\"SHAP_GROUP_RANKING.CSV\")\n",
    "    group_rank_df.to_csv(rank_path_groups, encoding=\"utf-8-sig\")\n",
    "    group_rank_df.to_csv(outpath(\"SHAP_GROUP_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] グループ単位ランキング saved -> {rank_path_groups}\")\n",
    "\n",
    "    # ---------- Foldごとの性能 ----------\n",
    "    metrics_path = outpath(\"LOSO_METRICS.CSV\")\n",
    "    pd.DataFrame(metrics_rows).to_csv(metrics_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-SHAP-RANK] LOSO metrics saved -> {metrics_path}\")\n",
    "\n",
    "    # ---------- グループランキングの可視化 ----------\n",
    "    TOP_K = 8\n",
    "\n",
    "    # 全グループ\n",
    "    plt.figure(figsize=(10, max(5, len(group_rank_df)//3)))\n",
    "    plt.barh(group_rank_df.index[::-1], group_rank_df[\"rank_mean\"][::-1])\n",
    "    plt.xlabel(\"Average rank (lower=better)\")\n",
    "    plt.ylabel(\"Feature group\")\n",
    "    plt.title(\"SHAP-based Feature Group Ranking (All)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_GROUP_RANKING_ALL.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 上位 TOP_K グループ\n",
    "    topk = group_rank_df.head(TOP_K).iloc[::-1]\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = plt.gca()\n",
    "    ax.barh(topk.index, topk[\"rank_mean\"])\n",
    "    ax.set_xlabel(\"Average rank (lower=better)\")\n",
    "    ax.set_ylabel(\"Feature group\")\n",
    "    ax.set_title(f\"Top-{TOP_K} SHAP-based Feature Group Ranking\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"SHAP_GROUP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[Cell3A-SHAP-RANK] 図を保存 -> \"\n",
    "          f\"{outpath('SHAP_GROUP_RANKING_ALL.PNG')} / {outpath('SHAP_GROUP_TOP8_RANKING.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-RFE: RFE特徴量ランキング（LOSO, グループ単位） =====\n",
    "set_cell_output(7)\n",
    "\n",
    "\n",
    "RUN_CELL_3A_RFE = False  # デフォルトでは使わない\n",
    "\n",
    "if RUN_CELL_3A_RFE:\n",
    "\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "    import json\n",
    "    import os\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib\n",
    "    import matplotlib.backends\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    required = [\"X_all\", \"y_all\", \"groups\", \"build_estimator\", \"fit_classifier\", \"evaluate_fold\", \"outpath\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-RFE] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    # ---------- グループ名取得ヘルパ ----------\n",
    "    def get_feature_group(col: str) -> str:\n",
    "        \"\"\"\n",
    "        ベース特徴名:\n",
    "          - 'xxx_lag0', 'xxx_lag1', ... → 'xxx'\n",
    "          - それ以外 → 列名そのまま\n",
    "        \"\"\"\n",
    "        m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "        return m.group(1) if m else col\n",
    "\n",
    "    RFE_BACKEND = \"xgb\"   # RFE では XGB 固定\n",
    "    RFE_STEP = 1          # 1本ずつ削除\n",
    "    RFE_MIN_FEATURES = 1  # 最低1列まで落としてフルランキングを得る\n",
    "\n",
    "    FEATURE_LIST_PATH = cell_output_path(5, \"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "    # --- 特徴量プールの決定（相関事前除去の結果があれば利用） ---\n",
    "    if os.path.exists(FEATURE_LIST_PATH):\n",
    "        with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "            keep_payload = json.load(f)\n",
    "        feature_pool = [c for c in keep_payload.get(\"keep\", []) if c in X_all.columns]\n",
    "        print(f\"[Cell3A-RFE] correlation-pruned columns loaded ({len(feature_pool)} cols)\")\n",
    "    else:\n",
    "        feature_pool = list(X_all.columns)\n",
    "        print(\"[Cell3A-RFE] correlation-pruned list not found. Using all columns.\")\n",
    "\n",
    "    if not feature_pool:\n",
    "        raise RuntimeError(\"[Cell3A-RFE] feature_pool が空です。Cell3A-pre の結果を確認してください。\")\n",
    "\n",
    "    X_source = X_all[feature_pool].copy()\n",
    "\n",
    "    # 列→グループ / グループ→列\n",
    "    col_to_group: dict[str, str] = {}\n",
    "    group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "    for col in X_source.columns:\n",
    "        g = get_feature_group(col)\n",
    "        col_to_group[col] = g\n",
    "        group_to_cols[g].append(col)\n",
    "\n",
    "    group_names = list(group_to_cols.keys())\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    # 列単位RFEランキング（参考用）\n",
    "    col_ranking_frames = []\n",
    "    # グループ単位RFEランキング（メイン）\n",
    "    group_ranking_frames = []\n",
    "    # Foldごとの性能指標\n",
    "    metrics_rows = []\n",
    "\n",
    "    # --- LOSO ループ ---\n",
    "    for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_source, y_all, groups), start=1):\n",
    "        X_tr = X_source.iloc[tr_idx].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_source.iloc[te_idx].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            raise RuntimeError(f\"[Cell3A-RFE] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "        # 元の実装と同じく「雛形モデル」を作る\n",
    "        base_estimator = build_estimator(backend=RFE_BACKEND)\n",
    "\n",
    "        selector = RFE(\n",
    "            estimator=base_estimator,\n",
    "            step=max(1, int(RFE_STEP)),\n",
    "            n_features_to_select=max(1, int(RFE_MIN_FEATURES)),\n",
    "        )\n",
    "        selector.fit(X_tr, y_tr)\n",
    "\n",
    "        # --- 列単位のRFE順位（1=最重要） ---\n",
    "        ranks_col = pd.Series(selector.ranking_, index=X_tr.columns, name=f\"fold{fold_id}\")\n",
    "        col_ranking_frames.append(ranks_col)\n",
    "\n",
    "        # --- 列順位 → グループ順位へ集約 ---\n",
    "        #   各グループに属する列の「最小rank」をそのグループのrankとする\n",
    "        group_rank_dict: dict[str, int] = {}\n",
    "        for col, r in ranks_col.items():\n",
    "            g = col_to_group[col]\n",
    "            if g not in group_rank_dict:\n",
    "                group_rank_dict[g] = int(r)\n",
    "            else:\n",
    "                group_rank_dict[g] = min(group_rank_dict[g], int(r))\n",
    "\n",
    "        group_ranks = pd.Series(group_rank_dict, name=f\"fold{fold_id}\")\n",
    "        group_ranking_frames.append(group_ranks)\n",
    "\n",
    "        # --- Foldごとの性能評価 ---\n",
    "        # ここがバグっていたので修正：\n",
    "        #   selector で「どの列を残すか」だけ決めておいて、\n",
    "        #   選ばれた列だけで改めてモデルを学習 → それを評価に使う\n",
    "        selected_cols = list(X_tr.columns[selector.support_])\n",
    "\n",
    "        # ★FIX: ここでちゃんと学習し直す\n",
    "        model = fit_classifier(\n",
    "            X_tr[selected_cols],\n",
    "            y_tr,\n",
    "            backend=RFE_BACKEND,\n",
    "        )\n",
    "\n",
    "        X_te_sel = X_te[selected_cols]\n",
    "        metrics = evaluate_fold(model, X_te_sel, y_te)\n",
    "        metrics.update({\n",
    "            \"fold_id\": fold_id,\n",
    "            \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "            \"n_selected_features\": len(selected_cols),\n",
    "        })\n",
    "        metrics_rows.append(metrics)\n",
    "\n",
    "        # ログ表示\n",
    "        preview_groups = group_ranks.sort_values().head(5).index.tolist()\n",
    "        print(f\"[Cell3A-RFE] fold{fold_id}: groups={len(group_ranks)}, \"\n",
    "              f\"top5_groups(by rank)={preview_groups}\")\n",
    "\n",
    "    # ---------- 列単位RFEランキング（参考・互換用） ----------\n",
    "    rfe_col_rank = pd.concat(col_ranking_frames, axis=1)\n",
    "    rfe_col_rank[\"rank_mean\"] = rfe_col_rank.mean(axis=1)\n",
    "    rfe_col_rank[\"rank_median\"] = rfe_col_rank.median(axis=1)\n",
    "    rfe_col_rank = rfe_col_rank.sort_values(\"rank_mean\")\n",
    "\n",
    "    col_rank_path = outpath(\"RFE_FEATURE_RANKING.CSV\")\n",
    "    rfe_col_rank.to_csv(col_rank_path, encoding=\"utf-8-sig\")\n",
    "    rfe_col_rank.to_csv(outpath(\"RFE_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-RFE] 列単位RFEランキング saved -> {col_rank_path}\")\n",
    "\n",
    "    # ---------- グループ単位RFEランキング（メイン） ----------\n",
    "    rfe_group_rank = pd.concat(group_ranking_frames, axis=1)\n",
    "    rfe_group_rank[\"rank_mean\"] = rfe_group_rank.mean(axis=1)\n",
    "    rfe_group_rank[\"rank_median\"] = rfe_group_rank.median(axis=1)\n",
    "    rfe_group_rank = rfe_group_rank.sort_values(\"rank_mean\")\n",
    "\n",
    "    group_rank_path = outpath(\"RFE_GROUP_RANKING.CSV\")\n",
    "    rfe_group_rank.to_csv(group_rank_path, encoding=\"utf-8-sig\")\n",
    "    rfe_group_rank.to_csv(outpath(\"RFE_GROUP_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-RFE] グループ単位RFEランキング saved -> {group_rank_path}\")\n",
    "\n",
    "    # ---------- LOSO性能ログ ----------\n",
    "    metrics_path = outpath(\"RFE_LOSO_METRICS.CSV\")\n",
    "    pd.DataFrame(metrics_rows).to_csv(metrics_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[Cell3A-RFE] LOSO metrics saved -> {metrics_path}\")\n",
    "\n",
    "    # ---------- グループRFEランキングの可視化 ----------\n",
    "    TOP_K = 8\n",
    "\n",
    "    plt.figure(figsize=(10, max(5, len(rfe_group_rank)//3)))\n",
    "    plt.barh(rfe_group_rank.index[::-1], rfe_group_rank[\"rank_mean\"][::-1])\n",
    "    plt.xlabel(\"Average RFE rank (lower=better)\")\n",
    "    plt.ylabel(\"Feature group\")\n",
    "    plt.title(\"RFE-based Feature Group Ranking (All)\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"RFE_GROUP_RANKING_ALL.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    topk = rfe_group_rank.head(TOP_K).iloc[::-1]\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    ax = plt.gca()\n",
    "    ax.barh(topk.index, topk[\"rank_mean\"])\n",
    "    ax.set_xlabel(\"Average RFE rank (lower=better)\")\n",
    "    ax.set_ylabel(\"Feature group\")\n",
    "    ax.set_title(f\"Top-{TOP_K} RFE-based Feature Group Ranking\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outpath(\"RFE_GROUP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[Cell3A-RFE] 図を保存 -> \"\n",
    "          f\"{outpath('RFE_GROUP_RANKING_ALL.PNG')} / {outpath('RFE_GROUP_TOP8_RANKING.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-Subset: グループ単位 Top-k で ROC-AUC 評価 =====\n",
    "set_cell_output(8)\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "required = [\"X_all\", \"y_all\", \"groups\",\n",
    "            \"fit_classifier\", \"predict_positive_score\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-Subset] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "# ---------- グループ名取得ヘルパ（lag付き列を1つのグループにまとめる） ----------\n",
    "def get_feature_group(col: str) -> str:\n",
    "    \"\"\"\n",
    "    ベース特徴名:\n",
    "      - 'xxx_lag0', 'xxx_lag1', ... → 'xxx'\n",
    "      - それ以外 → 列名そのまま\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "    return m.group(1) if m else col\n",
    "\n",
    "# 列→グループ / グループ→列 の対応を作成\n",
    "col_to_group: dict[str, str] = {}\n",
    "group_to_cols: dict[str, list[str]] = defaultdict(list)\n",
    "for col in X_all.columns:\n",
    "    g = get_feature_group(col)\n",
    "    col_to_group[col] = g\n",
    "    group_to_cols[g].append(col)\n",
    "\n",
    "print(f\"[Cell3A-Subset] feature groups = {len(group_to_cols)} \"\n",
    "      f\"(columns={len(X_all.columns)})\")\n",
    "\n",
    "# ---------- SHAPランキングの読み込み（行＝グループまたは列） ----------\n",
    "rank_csv = cell_output_path(6, \"SHAP_FEATURE_RANKING.CSV\")\n",
    "if not os.path.exists(rank_csv):\n",
    "    raise FileNotFoundError(\"[Cell3A-Subset] SHAP_FEATURE_RANKING.CSV がありません。Cell3A を実行してください。\")\n",
    "\n",
    "rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "\n",
    "# 「小さいほど重要」なスコアを作る（rank_mean 優先, なければ mean_abs に基づく）\n",
    "if \"rank_mean\" in rank_df.columns:\n",
    "    base_score = rank_df[\"rank_mean\"].astype(float)\n",
    "elif \"mean_abs\" in rank_df.columns:\n",
    "    # mean_abs が大きいほど重要 → 符号を反転して「小さいほど重要」にする\n",
    "    base_score = (-rank_df[\"mean_abs\"].astype(float))\n",
    "else:\n",
    "    # 何もなければ行順そのものをスコアにする（最初がもっとも重要）\n",
    "    base_score = pd.Series(\n",
    "        np.arange(len(rank_df), dtype=float),\n",
    "        index=rank_df.index,\n",
    "    )\n",
    "\n",
    "# ---------- 行（特徴orグループ）→ グループスコアへ集約 ----------\n",
    "group_score: dict[str, float] = {}\n",
    "for name, score in base_score.items():\n",
    "    g = get_feature_group(str(name))  # すでにグループ名ならそのまま\n",
    "    if g not in group_score:\n",
    "        group_score[g] = float(score)\n",
    "    else:\n",
    "        # より重要な列が1つでもあれば、そのグループはそのスコアを採用（min）\n",
    "        group_score[g] = min(group_score[g], float(score))\n",
    "\n",
    "group_rank = pd.Series(group_score, name=\"score\").sort_values(ascending=True)\n",
    "\n",
    "# 実際に X_all に存在するグループだけに絞る\n",
    "group_order = [g for g in group_rank.index if g in group_to_cols]\n",
    "if not group_order:\n",
    "    raise RuntimeError(\"[Cell3A-Subset] ランキングに該当するグループが X_all に存在しません。\")\n",
    "\n",
    "total_groups = len(group_order)\n",
    "print(f\"[Cell3A-Subset] Using SHAP group ranking ({total_groups} groups):\")\n",
    "print(group_order)\n",
    "\n",
    "# 旧コードとの互換用：TOP_SUBSET_K があればそれを使い，なければ total_groups で上書き\n",
    "TOP_SUBSET_K = int(globals().get(\"TOP_SUBSET_K\", 0))\n",
    "if TOP_SUBSET_K <= 0 or TOP_SUBSET_K > total_groups:\n",
    "    TOP_SUBSET_K = total_groups\n",
    "globals()[\"TOP_SUBSET_K\"] = TOP_SUBSET_K\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "results = []\n",
    "\n",
    "# ---- 上位kグループの累積セットで評価（k=1..total_groups）----\n",
    "for k in range(1, total_groups + 1):\n",
    "    use_groups = group_order[:k]\n",
    "\n",
    "    # この k グループに属する全ての列を集約\n",
    "    feats: list[str] = []\n",
    "    for g in use_groups:\n",
    "        feats.extend(group_to_cols[g])\n",
    "    # 念のため重複を除去（順序はグループ順→列順を維持）\n",
    "    seen = set()\n",
    "    feats_unique = []\n",
    "    for f in feats:\n",
    "        if f not in seen:\n",
    "            feats_unique.append(f)\n",
    "            seen.add(f)\n",
    "    feats = feats_unique\n",
    "\n",
    "    y_true_all = []\n",
    "    y_score_all = []\n",
    "\n",
    "    for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "        X_tr = X_all.iloc[tr_idx][feats].astype(np.float32)\n",
    "        y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "        X_te = X_all.iloc[te_idx][feats].astype(np.float32)\n",
    "        y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "        # 学習側が単一クラスならこのfoldはスキップ\n",
    "        if len(np.unique(y_tr)) < 2:\n",
    "            continue\n",
    "\n",
    "        model = fit_classifier(X_tr, y_tr)\n",
    "        proba = predict_positive_score(model, X_te)\n",
    "\n",
    "        y_true_all.append(y_te)\n",
    "        y_score_all.append(proba)\n",
    "\n",
    "    if not y_true_all:\n",
    "        auc = float(\"nan\")\n",
    "    else:\n",
    "        y_true = np.concatenate(y_true_all)\n",
    "        y_score = np.concatenate(y_score_all)\n",
    "        if len(np.unique(y_true)) < 2:\n",
    "            auc = float(\"nan\")\n",
    "        else:\n",
    "            auc = float(roc_auc_score(y_true, y_score))\n",
    "\n",
    "    results.append({\n",
    "        \"size\": k,                 # 使用した「グループ数」k\n",
    "        \"n_features\": len(feats),  # 実際に使った列数\n",
    "        \"groups\": use_groups,      # 使用したグループ名\n",
    "        \"features\": feats,         # 使用した列名\n",
    "        \"auc\": auc,\n",
    "    })\n",
    "    print(f\"[Cell3A-Subset] k(groups)={k}/{total_groups}, \"\n",
    "          f\"n_features={len(feats)}, AUC={auc:.4f}, groups={use_groups}\")\n",
    "\n",
    "# ---- 結果の整形・保存 ----\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(\"size\")  # k昇順\n",
    "\n",
    "results_df[\"groups_str\"] = results_df[\"groups\"].apply(lambda lst: \",\".join(lst))\n",
    "results_df[\"features_str\"] = results_df[\"features\"].apply(lambda lst: \",\".join(lst))\n",
    "\n",
    "# 1) このセル独自のファイル（ALLK_*）\n",
    "subset_csv_name_allk = \"ALLK_TOPORDER_AUC.csv\"\n",
    "subset_path_allk = outpath(subset_csv_name_allk)\n",
    "results_df[[\"size\", \"n_features\", \"groups_str\", \"features_str\", \"auc\"]].to_csv(\n",
    "    subset_path_allk, index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[Cell3A-Subset] 保存 (ALLK, group-based) -> {subset_path_allk}\")\n",
    "\n",
    "# 最良のk（AUC最大、同点なら小さいk）を取得\n",
    "best_row = results_df.sort_values([\"auc\", \"size\"], ascending=[False, True]).iloc[0]\n",
    "print(f\"[Cell3A-Subset] best k(groups)={int(best_row['size'])}, \"\n",
    "      f\"n_features={int(best_row['n_features'])}, \"\n",
    "      f\"auc={best_row['auc']:.4f}, \"\n",
    "      f\"groups={best_row['groups']}\")\n",
    "\n",
    "# 2) このセル独自の JSON（ALLK_SUBSET_BEST.json）\n",
    "best_json_name_allk = \"ALLK_SUBSET_BEST.json\"\n",
    "with open(outpath(best_json_name_allk), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        # 互換用：従来の \"size\" は「グループ数」として扱う\n",
    "        \"size\": int(best_row[\"size\"]),\n",
    "        \"n_features\": int(best_row[\"n_features\"]),\n",
    "        \"auc\": float(best_row[\"auc\"]),\n",
    "        \"groups\": best_row[\"groups\"],\n",
    "        \"features\": best_row[\"features\"],\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[Cell3A-Subset] BEST (ALLK, group-based) -> {outpath(best_json_name_allk)}\")\n",
    "\n",
    "# 3) 旧「組合せ探索版」と同じ名前・形式でも保存（互換用）\n",
    "subset_csv_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_AUC.csv\"\n",
    "subset_path_compat = outpath(subset_csv_name_compat)\n",
    "results_df[[\"size\", \"n_features\", \"groups_str\", \"features_str\", \"auc\"]].to_csv(\n",
    "    subset_path_compat, index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[Cell3A-Subset] 互換CSV (group-based) -> {subset_path_compat}\")\n",
    "\n",
    "best_json_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST.json\"\n",
    "with open(outpath(best_json_name_compat), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"size\": int(best_row[\"size\"]),             # グループ数\n",
    "        \"n_features\": int(best_row[\"n_features\"]), # 列数\n",
    "        \"auc\": float(best_row[\"auc\"]),\n",
    "        \"groups\": best_row[\"groups\"],\n",
    "        \"features\": best_row[\"features\"],\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "print(f\"[Cell3A-Subset] 互換BEST JSON (group-based) -> {outpath(best_json_name_compat)}\")\n",
    "\n",
    "# グローバル変数も旧仕様に合わせて更新\n",
    "globals()[\"BEST_SUBSET_FEATURES\"] = best_row[\"features\"]   # 実際に使う列\n",
    "globals()[\"BEST_SUBSET_GROUPS\"] = best_row[\"groups\"]       # 追加：使ったグループ名\n",
    "globals()[\"BEST_SUBSET_K\"] = int(best_row[\"size\"])         # グループ数として解釈\n",
    "\n",
    "# ---- グラフ描画：横軸=グループ数k（右ほど少ない）, 縦軸=ROC-AUC ----\n",
    "FS_TITLE, FS_LABEL, FS_TICK = 30, 24, 20\n",
    "LW = 1.5\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(results_df[\"size\"], results_df[\"auc\"], marker=\"o\", linewidth=LW)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(\"Number of feature groups (k)\", fontsize=FS_LABEL)\n",
    "ax.set_ylabel(\"ROC-AUC (pooled LOSO)\", fontsize=FS_LABEL)\n",
    "ax.set_title(\"ROC-AUC vs. number of feature groups (all SHAP-ranked groups)\", fontsize=FS_TITLE)\n",
    "\n",
    "# 横軸：左が k = total_groups（全部）、右が k = 1（1グループだけ）\n",
    "ax.set_xlim(total_groups, 1)\n",
    "ax.set_xticks(range(1, total_groups + 1))\n",
    "\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "ax.tick_params(axis=\"x\", labelsize=FS_TICK)\n",
    "ax.tick_params(axis=\"y\", labelsize=FS_TICK)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"ALLK_TOPORDER_AUC.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"[Cell3A-Subset] 図を保存 -> {outpath('ALLK_TOPORDER_AUC.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e8d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-Subset-dummy: 全特徴を TOP{TOP_SUBSET_K}_SUBSET_BEST.json に書き込むだけ =====\n",
    "set_cell_output(9)\n",
    "\n",
    "RUN_CELL_3A_ALL_FEATURE= False\n",
    "\n",
    "if RUN_CELL_3A_ALL_FEATURE:\n",
    "\n",
    "    RUN_CELL_3A_SHAP\n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    required = [\"X_all\", \"outpath\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-Subset-dummy] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    # ---- SHAPランキングの読み込み ----\n",
    "    rank_csv = cell_output_path(6, \"SHAP_FEATURE_RANKING.CSV\")\n",
    "    if not os.path.exists(rank_csv):\n",
    "        raise FileNotFoundError(\"[Cell3A-Subset-dummy] SHAP_FEATURE_RANKING.CSV がありません。Cell3A を実行してください。\")\n",
    "\n",
    "    rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "\n",
    "    # rank_mean（小さいほど重要） > mean_abs（大きいほど重要） > index の優先順位で並べ替え\n",
    "    if \"rank_mean\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "    elif \"mean_abs\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"mean_abs\", ascending=False).index.tolist()\n",
    "    else:\n",
    "        feature_order = list(rank_df.index)\n",
    "\n",
    "    # X_all に存在する列だけに制限（＝相関除去後＋実際にある列）\n",
    "    feature_order = [f for f in feature_order if f in X_all.columns]\n",
    "    if not feature_order:\n",
    "        raise RuntimeError(\"[Cell3A-Subset-dummy] ランキングに該当する特徴が X_all に存在しません。\")\n",
    "\n",
    "    total_feats = len(feature_order)\n",
    "    top_features = feature_order  # ★ 全特徴そのまま\n",
    "\n",
    "    print(f\"[Cell3A-Subset-dummy] Using ALL correlation-pruned features ({total_feats}):\")\n",
    "    print(top_features)\n",
    "\n",
    "    # TOP_SUBSET_K を「全特徴数」に強制セット\n",
    "    TOP_SUBSET_K = total_feats\n",
    "    globals()[\"TOP_SUBSET_K\"] = TOP_SUBSET_K\n",
    "\n",
    "    # ---- JSONを書き出し（構造は既存セルと互換）----\n",
    "    best_json_name_compat = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST.json\"\n",
    "    best_json_path = outpath(best_json_name_compat)\n",
    "\n",
    "    payload = {\n",
    "        \"size\": int(TOP_SUBSET_K),\n",
    "        # AUC はここでは計算しないので NaN または None で埋める\n",
    "        # 既存コードと同じく float('nan') を使っておく\n",
    "        \"auc\": float(\"nan\"),\n",
    "        \"features\": top_features,\n",
    "    }\n",
    "\n",
    "    with open(best_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[Cell3A-Subset-dummy] 全特徴を書き込み -> {best_json_path}\")\n",
    "\n",
    "    # 互換用にグローバル変数も更新\n",
    "    globals()[\"BEST_SUBSET_FEATURES\"] = top_features\n",
    "    globals()[\"BEST_SUBSET_K\"] = TOP_SUBSET_K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-Subset: Top-k 組合せ探索 =====\n",
    "set_cell_output(10)\n",
    "\n",
    "RUN_CELL_3A_Topk_FEATURE=False\n",
    "\n",
    "if RUN_CELL_3A_Topk_FEATURE:\n",
    "    import os\n",
    "    from itertools import combinations\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "    required = [\"X_all\", \"y_all\", \"groups\", \"fit_classifier\", \"predict_positive_score\", \"outpath\"]\n",
    "    missing = [name for name in required if name not in globals()]\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"[Cell3A-Subset] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "    TOP_SUBSET_K = int(globals().get(\"TOP_SUBSET_K\", 15))\n",
    "    if TOP_SUBSET_K <= 0:\n",
    "        raise ValueError(\"TOP_SUBSET_K must be positive\")\n",
    "\n",
    "    rank_csv = cell_output_path(6, \"SHAP_FEATURE_RANKING.CSV\")\n",
    "    if not os.path.exists(rank_csv):\n",
    "        raise FileNotFoundError(\"[Cell3A-Subset] SHAP_FEATURE_RANKING.CSV がありません。Cell3A を実行してください。\")\n",
    "\n",
    "    rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "    if \"rank_mean\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "    elif \"mean_abs\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"mean_abs\", ascending=False).index.tolist()\n",
    "    else:\n",
    "        feature_order = list(rank_df.index)\n",
    "\n",
    "    feature_order = [f for f in feature_order if f in X_all.columns]\n",
    "    if not feature_order:\n",
    "        raise RuntimeError(\"[Cell3A-Subset] ランキングに該当する特徴が X_all に存在しません。\")\n",
    "\n",
    "    limit = min(TOP_SUBSET_K, len(feature_order))\n",
    "    top_features = feature_order[:limit]\n",
    "    print(f\"[Cell3A-Subset] Top features ({len(top_features)}): {top_features}\")\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "    results = []\n",
    "\n",
    "    for r in range(1, len(top_features) + 1):\n",
    "        for comb in combinations(top_features, r):\n",
    "            feats = list(comb)\n",
    "            y_true_all = []\n",
    "            y_score_all = []\n",
    "            for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "                X_tr = X_all.iloc[tr_idx][feats].astype(np.float32)\n",
    "                y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "                X_te = X_all.iloc[te_idx][feats].astype(np.float32)\n",
    "                y_te = y_all.iloc[te_idx].astype(int)\n",
    "                if len(np.unique(y_tr)) < 2:\n",
    "                    continue\n",
    "                model = fit_classifier(X_tr, y_tr)\n",
    "                proba = predict_positive_score(model, X_te)\n",
    "                y_true_all.append(y_te)\n",
    "                y_score_all.append(proba)\n",
    "            if not y_true_all:\n",
    "                auc = float(\"nan\")\n",
    "            else:\n",
    "                y_true = np.concatenate(y_true_all)\n",
    "                y_score = np.concatenate(y_score_all)\n",
    "                if len(np.unique(y_true)) < 2:\n",
    "                    auc = float(\"nan\")\n",
    "                else:\n",
    "                    auc = float(roc_auc_score(y_true, y_score))\n",
    "            results.append({\n",
    "                \"size\": r,\n",
    "                \"features\": feats,\n",
    "                \"auc\": auc,\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values([\"auc\", \"size\"], ascending=[False, True])\n",
    "    results_df[\"features_str\"] = results_df[\"features\"].apply(lambda lst: \",\".join(lst))\n",
    "    subset_csv_name = f\"TOP{TOP_SUBSET_K}_SUBSET_AUC.csv\"\n",
    "    subset_path = outpath(subset_csv_name)\n",
    "    results_df[[\"size\", \"features_str\", \"auc\"]].to_csv(subset_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    best_row = results_df.iloc[0]\n",
    "    print(f\"[Cell3A-Subset] best size={int(best_row['size'])}, auc={best_row['auc']:.4f}, features={best_row['features']}\")\n",
    "\n",
    "    best_json_name = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST.json\"\n",
    "    with open(outpath(best_json_name), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"size\": int(best_row[\"size\"]),\n",
    "            \"auc\": float(best_row[\"auc\"]),\n",
    "            \"features\": best_row[\"features\"],\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    globals()[\"BEST_SUBSET_FEATURES\"] = best_row[\"features\"]\n",
    "    globals()[\"BEST_SUBSET_K\"] = len(best_row[\"features\"])\n",
    "\n",
    "    print(f\"[Cell3A-Subset] 保存 -> {subset_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae819ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-SHAP: 全特徴グループの SHAP 可視化 =====\n",
    "set_cell_output(11)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "required = [\"X_all\", \"y_all\", \"fit_classifier\", \"outpath\", \"SEED_BASE\", \"OUT_DIR\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-SHAP] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "# ---------- グループ名取得ヘルパ ----------\n",
    "def get_feature_group(col: str) -> str:\n",
    "    \"\"\"\n",
    "    ベース特徴名:\n",
    "      - 'xxx_lag0', 'xxx_lag1', ... → 'xxx'\n",
    "      - それ以外 → 列名そのまま\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(.+)_lag\\d+$\", col)\n",
    "    return m.group(1) if m else col\n",
    "\n",
    "# ---------- 相関事前除去リスト（列） ----------\n",
    "FEATURE_LIST_PATH = cell_output_path(5, \"FEATURES_AFTER_CORR.json\")\n",
    "if os.path.exists(FEATURE_LIST_PATH):\n",
    "    with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        keep_payload = json.load(f)\n",
    "    keep_cols = [c for c in keep_payload.get(\"keep\", []) if c in X_all.columns]\n",
    "    if not keep_cols:\n",
    "        raise RuntimeError(\"[Cell3A-SHAP] FEATURES_AFTER_CORR.json の keep に有効な列がありません。\")\n",
    "    cols_for_model = keep_cols\n",
    "    print(f\"[Cell3A-SHAP] Using correlation-pruned columns ({len(cols_for_model)})\")\n",
    "else:\n",
    "    cols_for_model = list(X_all.columns)\n",
    "    print(\"[Cell3A-SHAP] FEATURES_AFTER_CORR.json が無いため、全列を使用します。\")\n",
    "\n",
    "# ---------- 列→グループ / グループ→列 ----------\n",
    "col_to_group: dict[str, str] = {}\n",
    "group_to_cols: dict[str, list[str]] = {}\n",
    "for col in cols_for_model:\n",
    "    g = get_feature_group(col)\n",
    "    col_to_group[col] = g\n",
    "    group_to_cols.setdefault(g, []).append(col)\n",
    "\n",
    "group_names = list(group_to_cols.keys())\n",
    "\n",
    "# ---------- SHAPグループランキング（順序を決める） ----------\n",
    "group_rank_csv = cell_output_path(6, \"SHAP_GROUP_RANKING.CSV\")\n",
    "if os.path.exists(group_rank_csv):\n",
    "    group_rank_df = pd.read_csv(group_rank_csv, index_col=0)\n",
    "    if \"rank_mean\" in group_rank_df.columns:\n",
    "        group_order = group_rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "    else:\n",
    "        group_order = list(group_rank_df.index)\n",
    "    print(f\"[Cell3A-SHAP] グループランキング順に従って描画します。groups={len(group_order)}\")\n",
    "else:\n",
    "    group_order = group_names\n",
    "    print(\"[Cell3A-SHAP] SHAP_GROUP_RANKING.CSV が無いため、グループ名の順で描画します。\")\n",
    "\n",
    "# 念のため、現在の group_to_cols に存在するグループだけに制限\n",
    "group_order = [g for g in group_order if g in group_to_cols]\n",
    "\n",
    "# ---------- SHAP計算用データ（列レベル） ----------\n",
    "X_shap = X_all[cols_for_model].astype(np.float32)\n",
    "y_shap = y_all.astype(int)\n",
    "\n",
    "print(f\"[Cell3A-SHAP] samples={X_shap.shape[0]}, columns={X_shap.shape[1]}\")\n",
    "\n",
    "# ---------- モデル学習 ----------\n",
    "model = fit_classifier(X_shap, y_shap)\n",
    "\n",
    "# ---------- TreeExplainer で列レベル SHAP 計算 ----------\n",
    "background = shap.sample(X_shap, min(256, len(X_shap)), random_state=SEED_BASE)\n",
    "explainer = shap.TreeExplainer(\n",
    "    model,\n",
    "    data=background,\n",
    "    model_output=\"probability\",\n",
    "    feature_perturbation=\"interventional\",\n",
    ")\n",
    "\n",
    "shap_values_any = explainer.shap_values(X_shap)\n",
    "if isinstance(shap_values_any, list):\n",
    "    # 2値分類などで [neg, pos] みたいなリストになる場合は「陽性クラス(1)」を採用\n",
    "    if hasattr(model, \"classes_\") and 1 in list(model.classes_):\n",
    "        class_idx = list(model.classes_).index(1)\n",
    "    else:\n",
    "        class_idx = -1  # 最後のクラス\n",
    "    shap_values_col = shap_values_any[class_idx]\n",
    "else:\n",
    "    shap_values_col = shap_values_any\n",
    "\n",
    "shap_values_col = np.asarray(shap_values_col)\n",
    "if shap_values_col.ndim == 3:\n",
    "    # (n_samples, n_features, n_classes) → 陽性クラスを取り出す\n",
    "    if hasattr(model, \"classes_\") and 1 in list(model.classes_):\n",
    "        pos_idx = list(model.classes_).index(1)\n",
    "    else:\n",
    "        pos_idx = -1\n",
    "    shap_values_col = shap_values_col[:, :, pos_idx]\n",
    "elif shap_values_col.ndim == 1:\n",
    "    shap_values_col = shap_values_col.reshape(-1, 1)\n",
    "\n",
    "if shap_values_col.shape[1] != X_shap.shape[1]:\n",
    "    raise RuntimeError(f\"[Cell3A-SHAP] shap_values 形状が一致しません: {shap_values_col.shape} vs {X_shap.shape}\")\n",
    "\n",
    "print(\"[Cell3A-SHAP] 列レベルの SHAP 値を計算しました。\")\n",
    "\n",
    "# ---------- 列レベル SHAP → グループ SHAP への集約 ----------\n",
    "n_samples = shap_values_col.shape[0]\n",
    "n_groups = len(group_order)\n",
    "\n",
    "# 列名→インデックス\n",
    "col_index_map = {col: idx for idx, col in enumerate(X_shap.columns)}\n",
    "\n",
    "# グループ SHAP行列とグループ値行列を作成\n",
    "shap_values_group = np.zeros((n_samples, n_groups), dtype=float)\n",
    "X_group_df = pd.DataFrame(index=X_shap.index, columns=group_order, dtype=float)\n",
    "\n",
    "for g_idx, g in enumerate(group_order):\n",
    "    cols = group_to_cols[g]\n",
    "    # 現在の X_shap に存在する列だけ\n",
    "    cols = [c for c in cols if c in X_shap.columns]\n",
    "    if not cols:\n",
    "        continue\n",
    "    col_indices = [col_index_map[c] for c in cols]\n",
    "\n",
    "    # SHAP: 各サンプルにおいて、当該グループに属する列の SHAP を合計\n",
    "    shap_values_group[:, g_idx] = shap_values_col[:, col_indices].sum(axis=1)\n",
    "\n",
    "    # グループの代表値（色付け用）は、当該列の平均値とする\n",
    "    X_group_df[g] = X_shap[cols].mean(axis=1)\n",
    "\n",
    "print(f\"[Cell3A-SHAP] グループ数={n_groups}, shap_values_group.shape={shap_values_group.shape}\")\n",
    "\n",
    "# ---------- Top-K の設定 ----------\n",
    "TOP_K = int(globals().get(\"TOP_SUBSET_K\", 15))\n",
    "TOP_K = max(1, min(TOP_K, n_groups))  # 1〜グループ数にクリップ\n",
    "\n",
    "# ---------- Top-k subset をグループにマップ（ハイライト用） ----------\n",
    "highlight_groups: list[str] = []\n",
    "try:\n",
    "    subset_json_candidates = [\n",
    "        cell_output_path(8, f\"TOP{TOP_K}_SUBSET_BEST.json\"),\n",
    "        cell_output_path(8, \"TOP10_SUBSET_BEST.json\"),\n",
    "        cell_output_path(10, f\"TOP{TOP_K}_SUBSET_BEST.json\"),\n",
    "        cell_output_path(10, \"TOP10_SUBSET_BEST.json\"),\n",
    "    ]\n",
    "    subset_json_path = next((p for p in subset_json_candidates if os.path.exists(p)), None)\n",
    "\n",
    "    if subset_json_path is not None:\n",
    "        with open(subset_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            info = json.load(f)\n",
    "        feature_cols_subset = info.get(\"features\", [])\n",
    "        # 列名 -> グループ名 に変換して一意集合に\n",
    "        highlight_groups = sorted({get_feature_group(c) for c in feature_cols_subset if c in col_to_group})\n",
    "        print(f\"[Cell3A-SHAP] subset highlight groups (from {os.path.basename(subset_json_path)}):\")\n",
    "        print(f\"  {highlight_groups}\")\n",
    "    else:\n",
    "        print(\"[Cell3A-SHAP] subset JSON が見つからなかったため、ハイライトなし。\")\n",
    "except Exception as e:\n",
    "    print(f\"[Cell3A-SHAP][WARN] subset読み込み失敗: {e}\")\n",
    "    highlight_groups = []\n",
    "\n",
    "# ---------- 出力ディレクトリ ----------\n",
    "shap_dir = os.path.join(OUT_DIR, \"SHAP_GROUP\")\n",
    "os.makedirs(shap_dir, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Summary plot（Top-K グループ）\n",
    "# =============================================================================\n",
    "# Top-K グループを group_order から切り出し\n",
    "top_groups = group_order[:TOP_K]\n",
    "top_idx = [group_order.index(g) for g in top_groups]\n",
    "\n",
    "X_top = X_group_df[top_groups]\n",
    "shap_top = shap_values_group[:, top_idx]\n",
    "\n",
    "plt.figure()\n",
    "shap.summary_plot(\n",
    "    shap_top,\n",
    "    X_top,\n",
    "    show=False,\n",
    "    plot_type=\"dot\",\n",
    "    max_display=TOP_K,\n",
    "    sort=False,  # こちらで列順を制御する\n",
    ")\n",
    "ax = plt.gca()\n",
    "for label in ax.get_yticklabels():\n",
    "    if label.get_text() in highlight_groups:\n",
    "        label.set_color(\"red\")\n",
    "plt.tight_layout()\n",
    "summary_top_path = os.path.join(shap_dir, f\"SHAP_GROUP_SUMMARY_TOP{TOP_K}.png\")\n",
    "plt.savefig(summary_top_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"[Cell3A-SHAP] Summary plot (Top-{TOP_K} groups) -> {summary_top_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Summary plot（全グループ）\n",
    "# =============================================================================\n",
    "plt.figure()\n",
    "shap.summary_plot(\n",
    "    shap_values_group,\n",
    "    X_group_df[group_order],\n",
    "    show=False,\n",
    "    plot_type=\"dot\",\n",
    "    max_display=len(group_order),\n",
    "    sort=False,\n",
    ")\n",
    "ax = plt.gca()\n",
    "for label in ax.get_yticklabels():\n",
    "    if label.get_text() in highlight_groups:\n",
    "        label.set_color(\"red\")\n",
    "plt.tight_layout()\n",
    "summary_all_path = os.path.join(shap_dir, f\"SHAP_GROUP_SUMMARY_ALL.png\")\n",
    "plt.savefig(summary_all_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"[Cell3A-SHAP] Summary plot (ALL groups) -> {summary_all_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 各グループごとの dependence plot\n",
    "# =============================================================================\n",
    "for g in group_order:\n",
    "    plt.figure()\n",
    "    shap.dependence_plot(\n",
    "        g,\n",
    "        shap_values_group,\n",
    "        X_group_df[group_order],  # 全グループを渡す\n",
    "        show=False,\n",
    "        interaction_index=None,   # 一変数の関係だけを描画\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    safe_name = g.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    dep_path = os.path.join(shap_dir, f\"SHAP_GROUP_DEP_{safe_name}.png\")\n",
    "    plt.savefig(dep_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[Cell3A-SHAP] Dependence plot (group={g}) -> {dep_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 4A: ROC曲線（TOPサブセット） =====\n",
    "set_cell_output(12)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "\n",
    "\n",
    "subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "subset_candidates = [\n",
    "    cell_output_path(8, subset_primary),\n",
    "    cell_output_path(8, \"TOP10_SUBSET_BEST.json\"),\n",
    "    cell_output_path(10, subset_primary),\n",
    "    cell_output_path(10, \"TOP10_SUBSET_BEST.json\"),\n",
    "]\n",
    "subset_json = next((p for p in subset_candidates if os.path.exists(p)), None)\n",
    "if subset_json is None:\n",
    "    raise FileNotFoundError(\"[Cell4A] TOP*_SUBSET_BEST.json がありません。Cell3A-Subset を実行してください。\")\n",
    "\n",
    "with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    subset_info = json.load(f)\n",
    "\n",
    "best_features = subset_info.get(\"features\", [])\n",
    "if not best_features:\n",
    "    raise RuntimeError(\"[Cell4A] JSON 内に features がありません。\")\n",
    "\n",
    "# X_all に存在するものだけに絞る\n",
    "best_features = [f for f in best_features if f in X_all.columns]\n",
    "if not best_features:\n",
    "    raise RuntimeError(\"[Cell4A] X_all に存在する特徴がありません。\")\n",
    "\n",
    "# ★ オプションで MSSQ / VIMSSQ を追加 ★\n",
    "extra_traits = []\n",
    "if globals().get(\"USE_MSSQ_FEATURE\", False) and \"MSSQ\" in X_all.columns:\n",
    "    extra_traits.append(\"MSSQ\")\n",
    "if globals().get(\"USE_VIMSSQ_FEATURE\", False) and \"VIMSSQ\" in X_all.columns:\n",
    "    extra_traits.append(\"VIMSSQ\")\n",
    "\n",
    "extra_traits = [f for f in extra_traits if f not in best_features]\n",
    "if extra_traits:\n",
    "    print(f\"[Cell4A] 追加で使用する属性特徴: {extra_traits}\")\n",
    "    best_features = best_features + extra_traits\n",
    "# ★ ここまで ★\n",
    "\n",
    "best_k = len(best_features)\n",
    "\n",
    "# もともとの表示\n",
    "print(f\"[Cell4A] 使用特徴 ({best_k}) from {os.path.basename(subset_json)}: {best_features}\")\n",
    "\n",
    "# ★ デバッグ用: 実際に使う最終特徴一覧を明示的にプリント ★\n",
    "print(\"[Cell4A][DEBUG] 実際にモデルに渡した特徴量リスト:\")\n",
    "for i, f_name in enumerate(best_features, start=1):\n",
    "    print(f\"  {i:2d}: {f_name}\")\n",
    "# ★ ここまで ★\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "y_true_all, proba_all, subj_all = [], [], []\n",
    "for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "    X_tr = X_all.iloc[tr_idx][best_features].astype(np.float32)\n",
    "    y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "    X_te = X_all.iloc[te_idx][best_features].astype(np.float32)\n",
    "    y_te = y_all.iloc[te_idx].astype(int)\n",
    "    if len(np.unique(y_tr)) < 2:\n",
    "        continue\n",
    "    model = fit_classifier(X_tr, y_tr)\n",
    "    proba = predict_positive_score(model, X_te)\n",
    "    y_true_all.append(y_te)\n",
    "    proba_all.append(proba)\n",
    "    subj_all.append(groups.iloc[te_idx].values)\n",
    "\n",
    "if not y_true_all:\n",
    "    raise RuntimeError(\"[Cell4A] 評価に必要な fold が得られませんでした。\")\n",
    "\n",
    "y_pool = np.concatenate(y_true_all)\n",
    "s_pool = np.concatenate(proba_all)\n",
    "subj_pool = np.concatenate(subj_all)\n",
    "if len(np.unique(y_pool)) < 2:\n",
    "    raise RuntimeError(\"[Cell4A] 真値が単一クラスのため ROC-AUC を計算できません。\")\n",
    "\n",
    "auc_obs = float(roc_auc_score(y_pool, s_pool))\n",
    "\n",
    "rng = np.random.default_rng(20251101)\n",
    "df_pool = pd.DataFrame({\"subject\": subj_pool, \"y_true\": y_pool, \"y_score\": s_pool})\n",
    "subj_ids = df_pool[\"subject\"].unique()\n",
    "auc_boot = []\n",
    "for _ in range(2000):\n",
    "    sampled = rng.choice(subj_ids, size=len(subj_ids), replace=True)\n",
    "    df_boot = pd.concat([df_pool[df_pool[\"subject\"] == sid] for sid in sampled], ignore_index=True)\n",
    "    if df_boot[\"y_true\"].nunique() < 2:\n",
    "        continue\n",
    "    auc_boot.append(float(roc_auc_score(df_boot[\"y_true\"], df_boot[\"y_score\"])))\n",
    "if auc_boot:\n",
    "    ci_low = float(np.quantile(auc_boot, 0.025))\n",
    "    ci_high = float(np.quantile(auc_boot, 0.975))\n",
    "else:\n",
    "    ci_low = ci_high = float(\"nan\")\n",
    "\n",
    "pd.DataFrame([{\"k\": best_k, \"auc\": auc_obs, \"ci_low\": ci_low, \"ci_high\": ci_high}]).to_csv(\n",
    "    outpath(\"AUC_K_CI.csv\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[Cell4A] AUC={auc_obs:.4f} (95% CI [{ci_low:.4f}, {ci_high:.4f}])\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_pool, s_pool)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_obs:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Chance\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Best Subset)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"AUC_K_CI.png\"), dpi=300)\n",
    "plt.close()\n",
    "print(f\"[Cell4A] ROC 図を保存 -> {outpath('AUC_K_CI.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ced576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Inner LOSO folds builder =====\n",
    "set_cell_output(13)\n",
    "\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def choose_inner_folds_loso(train_subject_ids: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    外側LOSOで得た “学習側の被験者ID” リストを受け取り、\n",
    "    1名ずつ検証に回す inner-LOSO のfoldリスト（[[sid1], [sid2], ...]）を返す。\n",
    "    \"\"\"\n",
    "    if not isinstance(train_subject_ids, (list, tuple)):\n",
    "        raise RuntimeError(\"[inner folds] train_subject_ids は list/tuple を想定しています。\")\n",
    "    uniq = list(pd.unique(pd.Series([str(sid) for sid in train_subject_ids])))\n",
    "    if len(uniq) == 0:\n",
    "        raise RuntimeError(\"[inner folds] train_subject_ids が空です。\")\n",
    "    uniq_sorted = sorted(uniq, key=lambda x: (len(x), x))\n",
    "    folds = [[sid] for sid in uniq_sorted]\n",
    "    print(f\"[inner folds] {len(folds)} splits -> val subjects = {', '.join(uniq_sorted)}\")\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e41e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Cell 5A: inner-LOSO τ最適化（F1/BA 切替） =====\n",
    "set_cell_output(14)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- モード切替 ----------------\n",
    "CELL5_MODE = str(globals().get(\"CELL5_MODE\", \"F1\")).upper()\n",
    "if CELL5_MODE not in {\"F1\", \"BA\"}:\n",
    "    raise ValueError(f\"[Cell5A] CELL5_MODE は 'F1' または 'BA' を指定してください（今: {CELL5_MODE}）\")\n",
    "MODE_TAG = CELL5_MODE\n",
    "METRIC_LABEL = MODE_TAG\n",
    "\n",
    "# ---------------- 出力ディレクトリ（Cell5専用） ----------------\n",
    "CELL5_DIR = os.path.join(LEVEL1_DIR, f\"Cell5A_{MODE_TAG}\")\n",
    "os.makedirs(CELL5_DIR, exist_ok=True)\n",
    "\n",
    "def cell5_out(filename: str) -> str:\n",
    "    path = os.path.join(CELL5_DIR, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "GROUP_AWARE_DIR = os.path.join(CELL5_DIR, \"GROUP_AWARE\", MODE_TAG)\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_out(filename: str) -> str:\n",
    "    path = os.path.join(GROUP_AWARE_DIR, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ---------------- 群分け基準（MSSQ / VIMSSQ 切り替え） ----------------\n",
    "GROUPING_BASIS_FOR_FAIRNESS = globals().get(\n",
    "    \"GROUPING_BASIS_FOR_FAIRNESS\",\n",
    "    globals().get(\"GROUPING_BASIS_FOR_PLOTS\", \"MSSQ\"),\n",
    ")\n",
    "basis = str(GROUPING_BASIS_FOR_FAIRNESS).upper()\n",
    "if basis == \"MSSQ\":\n",
    "    GROUP_COL_NAME = \"MSSQ_group\"\n",
    "elif basis == \"VIMSSQ\":\n",
    "    GROUP_COL_NAME = \"VIMSSQ_group\"\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"[Cell5A] GROUPING_BASIS_FOR_FAIRNESS は 'MSSQ' か 'VIMSSQ' を指定してください（今: {GROUPING_BASIS_FOR_FAIRNESS}）\"\n",
    "    )\n",
    "print(f\"[Cell5A] MODE={MODE_TAG} / group_col={GROUP_COL_NAME}\")\n",
    "\n",
    "# ---------------- 基本チェック ----------------\n",
    "req = [\"X_all\",\"y_all\",\"groups\",\"SUBJECT_META\", \"choose_inner_folds_loso\",\"fit_classifier\",\"predict_positive_score\",\"outpath\"]\n",
    "missing = [v for v in req if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell5A] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "# ---------------- 入力整形 ----------------\n",
    "X_base = X_all.astype(np.float32)\n",
    "y_base = y_all.astype(int)\n",
    "g_base = groups.astype(str)\n",
    "\n",
    "# 群ラベルを High/Low に正規化\n",
    "if \"subject_id\" in SUBJECT_META.columns:\n",
    "    mapper = SUBJECT_META.set_index(\"subject_id\")[GROUP_COL_NAME].astype(str).to_dict()\n",
    "else:\n",
    "    mapper = SUBJECT_META[GROUP_COL_NAME].astype(str).to_dict()\n",
    "\n",
    "fair_groups_raw = g_base.map(mapper)\n",
    "if fair_groups_raw.isna().any():\n",
    "    raise RuntimeError(f\"[Cell5A] {GROUP_COL_NAME} 未割当ID: {sorted(set(g_base[fair_groups_raw.isna()]))}\")\n",
    "\n",
    "fair_groups = (\n",
    "    fair_groups_raw.astype(str).str.strip().str.lower().map({\"high\": \"High\", \"low\": \"Low\"})\n",
    ")\n",
    "if fair_groups.isna().any():\n",
    "    bad_labels = sorted(set(fair_groups_raw[~fair_groups_raw.isin([\"High\",\"Low\",\"high\",\"low\"]) ]))\n",
    "    raise RuntimeError(f\"[Cell5A] {GROUP_COL_NAME} に 'High'/'Low' 以外のラベル: {bad_labels}\")\n",
    "\n",
    "# デバッグ表示\n",
    "if \"subject_id\" in SUBJECT_META.columns:\n",
    "    dbg_meta = SUBJECT_META[[\"subject_id\", GROUP_COL_NAME]].copy()\n",
    "else:\n",
    "    dbg_meta = SUBJECT_META.reset_index()[[\"subject_id\", GROUP_COL_NAME]].copy()\n",
    "print(f\"\\n[Cell5A-DEBUG] SUBJECT_META {GROUP_COL_NAME} 分類一覧\")\n",
    "print(dbg_meta.sort_values([GROUP_COL_NAME, \"subject_id\"]).to_string(index=False))\n",
    "\n",
    "# ---------------- 特徴選抜（Cell3A-Subset の JSON を流用） ----------------\n",
    "subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "subset_candidates = [\n",
    "    cell_output_path(8, subset_primary),\n",
    "    cell_output_path(8, \"TOP10_SUBSET_BEST.json\"),\n",
    "    cell_output_path(10, subset_primary),\n",
    "    cell_output_path(10, \"TOP10_SUBSET_BEST.json\"),\n",
    "]\n",
    "subset_json = next((p for p in subset_candidates if os.path.exists(p)), None)\n",
    "if subset_json is None:\n",
    "    raise FileNotFoundError(\"[Cell5A] TOP*_SUBSET_BEST.json が見つからない．Cell3A-Subset を実行すること．\")\n",
    "\n",
    "with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    subset_info = json.load(f)\n",
    "raw_features = subset_info.get(\"features\", [])\n",
    "if not raw_features:\n",
    "    raise RuntimeError(f\"[Cell5A] JSON 内に 'features' が空です -> {os.path.basename(subset_json)}\")\n",
    "\n",
    "feature_order = [f for f in raw_features if f in X_base.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(f\"[Cell5A] JSON の features が X_all に1つも存在しない: {raw_features}\")\n",
    "\n",
    "extra_traits = []\n",
    "if globals().get(\"USE_MSSQ_FEATURE\", False) and \"MSSQ\" in X_base.columns:\n",
    "    extra_traits.append(\"MSSQ\")\n",
    "if globals().get(\"USE_VIMSSQ_FEATURE\", False) and \"VIMSSQ\" in X_base.columns:\n",
    "    extra_traits.append(\"VIMSSQ\")\n",
    "extra_traits = [f for f in extra_traits if f not in feature_order]\n",
    "\n",
    "feats_k = feature_order + extra_traits\n",
    "print(f\"[Cell5A] Using subset features from {os.path.basename(subset_json)}\")\n",
    "print(f\"[Cell5A] JSON features (base) k={len(feature_order)}: {feature_order}\")\n",
    "if extra_traits:\n",
    "    print(f\"[Cell5A] 追加で使用する属性特徴: {extra_traits}\")\n",
    "print(f\"[Cell5A] 最終的に使用する特徴数 = {len(feats_k)}\")\n",
    "\n",
    "X_k = X_base[feats_k]\n",
    "\n",
    "# ---------------- 指標ユーティリティ（F1/BA をモードで切替） ----------------\n",
    "def _conf_from_preds(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    TN, FP, FN, TP = skm.confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _metric_from_conf(TP, FP, FN, TN) -> float:\n",
    "    TP = float(TP); FP = float(FP); FN = float(FN); TN = float(TN)\n",
    "    if MODE_TAG == \"F1\":\n",
    "        denom = (2*TP + FP + FN)\n",
    "        return (2*TP / denom) if denom > 0 else 0.0\n",
    "    # BA\n",
    "    tpr = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    tnr = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    return 0.5 * (tpr + tnr)\n",
    "\n",
    "def _metric_binary(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    TP, FP, FN, TN = _conf_from_preds(y_true, y_pred)\n",
    "    return _metric_from_conf(TP, FP, FN, TN)\n",
    "\n",
    "def _grid(l, r, steps):\n",
    "    l = float(max(0.0, l)); r = float(min(1.0, r))\n",
    "    if l > r: l, r = r, l\n",
    "    return np.linspace(l, r, int(steps), dtype=float)\n",
    "\n",
    "# ---------------- τ最適化（Single / WG / Group） ----------------\n",
    "def _single_tau_opt(scores: np.ndarray, y: np.ndarray):\n",
    "    cands = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    vals = np.array([_metric_binary(y, (scores >= t).astype(int)) for t in cands])\n",
    "    idx = int(np.nanargmax(vals)); tau0 = float(cands[idx]); best0 = float(vals[idx])\n",
    "\n",
    "    left, right = max(0.0, tau0 - FINE_MARGIN), min(1.0, tau0 + FINE_MARGIN)\n",
    "    cands2 = _grid(left, right, FINE_STEPS)\n",
    "    vals2 = np.array([_metric_binary(y, (scores >= t).astype(int)) for t in cands2])\n",
    "    idx2 = int(np.nanargmax(vals2)); tau = float(cands2[idx2]); best = float(vals2[idx2])\n",
    "    return {\"tau\": tau, f\"{METRIC_LABEL}_val\": best, \"tau_coarse\": tau0, f\"{METRIC_LABEL}_coarse\": best0}\n",
    "\n",
    "def _wg_opt_joint(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[WG] 連結valに High/Low の両群が必要\")\n",
    "\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "\n",
    "    def _metric_vec(s, yy, cands):\n",
    "        return np.array([_metric_binary(yy, (s >= t).astype(int)) for t in cands])\n",
    "\n",
    "    mH = _metric_vec(sH, yH, candH)\n",
    "    mL = _metric_vec(sL, yL, candL)\n",
    "\n",
    "    best = {\"WG\": -np.inf, \"pooled\": -np.inf, \"tH\": 0.5, \"tL\": 0.5, \"mH\": 0.0, \"mL\": 0.0}\n",
    "    for i, tH in enumerate(candH):\n",
    "        wg_row = np.minimum(mH[i], mL)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL[j]).astype(int)\n",
    "        pooled = _metric_binary(np.concatenate([yH, yL]), np.concatenate([yhatH, yhatL]))\n",
    "        cand = {\"WG\": WG, \"pooled\": float(pooled), \"tH\": float(tH), \"tL\": float(candL[j]), \"mH\": float(mH[i]), \"mL\": float(mL[j])}\n",
    "        def _is_better(cur, new):\n",
    "            if new[\"WG\"] > cur[\"WG\"]: return True\n",
    "            if new[\"WG\"] < cur[\"WG\"]: return False\n",
    "            if new[\"pooled\"] > cur[\"pooled\"]: return True\n",
    "            if new[\"pooled\"] < cur[\"pooled\"]: return False\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) < abs(cur[\"tH\"]-cur[\"tL\"]): return True\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) > abs(cur[\"tH\"]-cur[\"tL\"]): return False\n",
    "            if (new[\"tH\"], new[\"tL\"]) < (cur[\"tH\"], cur[\"tL\"]): return True\n",
    "            return False\n",
    "        if _is_better(best, cand):\n",
    "            best = cand\n",
    "\n",
    "    # fine search\n",
    "    lH, rH = max(0.0, best[\"tH\"] - FINE_MARGIN), min(1.0, best[\"tH\"] + FINE_MARGIN)\n",
    "    lL, rL = max(0.0, best[\"tL\"] - FINE_MARGIN), min(1.0, best[\"tL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS)\n",
    "    candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "\n",
    "    best2 = dict(best)\n",
    "    mH2 = _metric_vec(sH, yH, candH2)\n",
    "    mL2 = _metric_vec(sL, yL, candL2)\n",
    "    for i, tH in enumerate(candH2):\n",
    "        wg_row = np.minimum(mH2[i], mL2)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL2[j]).astype(int)\n",
    "        pooled = _metric_binary(np.concatenate([yH, yL]), np.concatenate([yhatH, yhatL]))\n",
    "        cand = {\"WG\": WG, \"pooled\": float(pooled), \"tH\": float(tH), \"tL\": float(candL2[j]), \"mH\": float(mH2[i]), \"mL\": float(mL2[j])}\n",
    "        if (cand[\"WG\"] > best2[\"WG\"] or\n",
    "            (cand[\"WG\"] == best2[\"WG\"] and (cand[\"pooled\"] > best2[\"pooled\"] or\n",
    "                                               (cand[\"pooled\"] == best2[\"pooled\"] and abs(cand[\"tH\"]-cand[\"tL\"]) <= abs(best2[\"tH\"]-best2[\"tL\"])) ))):\n",
    "            best2 = cand\n",
    "\n",
    "    return {\n",
    "        \"tauH\": best2[\"tH\"], \"tauL\": best2[\"tL\"],\n",
    "        f\"{METRIC_LABEL}_H_val\": best2[\"mH\"], f\"{METRIC_LABEL}_L_val\": best2[\"mL\"],\n",
    "        f\"WG_{METRIC_LABEL}_val\": best2[\"WG\"], f\"{METRIC_LABEL}_pooled_val\": best2[\"pooled\"],\n",
    "    }\n",
    "\n",
    "def _group_opt(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[Group] 連結valに High/Low の両群が必要\")\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    def _pooled_for(tH, tL):\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= tL).astype(int)\n",
    "        return _metric_binary(np.concatenate([yH, yL]), np.concatenate([yhatH, yhatL]))\n",
    "    best = {\"val\": -np.inf, \"tH\": None, \"tL\": None}\n",
    "    for tH in candH:\n",
    "        vals = np.array([_pooled_for(tH, tL) for tL in candL])\n",
    "        jmax = int(np.nanargmax(vals))\n",
    "        if float(vals[jmax]) > best[\"val\"]:\n",
    "            best = {\"val\": float(vals[jmax]), \"tH\": float(tH), \"tL\": float(candL[jmax])}\n",
    "    lH, rH = max(0.0, best[\"tH\"] - FINE_MARGIN), min(1.0, best[\"tH\"] + FINE_MARGIN)\n",
    "    lL, rL = max(0.0, best[\"tL\"] - FINE_MARGIN), min(1.0, best[\"tL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS); candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "    best2 = dict(best)\n",
    "    for tH in candH2:\n",
    "        vals2 = np.array([_pooled_for(tH, tL) for tL in candL2])\n",
    "        jmax = int(np.nanargmax(vals2))\n",
    "        if float(vals2[jmax]) > best2[\"val\"]:\n",
    "            best2 = {\"val\": float(vals2[jmax]), \"tH\": float(tH), \"tL\": float(candL2[jmax])}\n",
    "    return {\"tauH\": best2[\"tH\"], \"tauL\": best2[\"tL\"], f\"{METRIC_LABEL}_val\": best2[\"val\"]}\n",
    "\n",
    "# ---------------- outer LOSO with inner concatenation ----------------\n",
    "logo_outer = LeaveOneGroupOut()\n",
    "rows, pred_rows = [], []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo_outer.split(X_k, y_base.values, g_base.values), start=1):\n",
    "    train_mask = pd.Series(False, index=g_base.index); train_mask.iloc[tr_idx] = True\n",
    "    test_mask  = pd.Series(False, index=g_base.index);  test_mask.iloc[te_idx]  = True\n",
    "    test_sid = g_base.iloc[te_idx].iloc[0]\n",
    "\n",
    "    inner_ids   = sorted(g_base[train_mask].unique())\n",
    "    inner_folds = choose_inner_folds_loso(inner_ids)\n",
    "\n",
    "    val_scores_all, val_y_all, val_grp_all = [], [], []\n",
    "\n",
    "    inner_train_groups = fair_groups[train_mask]\n",
    "    if not ((\"High\" in set(inner_train_groups)) and (\"Low\" in set(inner_train_groups))):\n",
    "        raise RuntimeError(f\"[Cell5A] fold{fold_id}: inner-train に両群(High/Low)が必要\")\n",
    "\n",
    "    for inner_val in inner_folds:\n",
    "        val_mask  = g_base.isin(inner_val) & train_mask\n",
    "        trn_mask  = train_mask & (~val_mask)\n",
    "        if not trn_mask.any() or not val_mask.any():\n",
    "            continue\n",
    "        X_tr, y_tr = X_k[trn_mask], y_base[trn_mask]\n",
    "        X_vl, y_vl = X_k[val_mask], y_base[val_mask]\n",
    "        grp_vl     = fair_groups[val_mask].to_numpy()\n",
    "\n",
    "        model_inner = fit_classifier(X_tr, y_tr)\n",
    "        sc_vl = predict_positive_score(model_inner, X_vl).astype(float)\n",
    "\n",
    "        val_scores_all.append(sc_vl)\n",
    "        val_y_all.append(y_vl.to_numpy())\n",
    "        val_grp_all.append(grp_vl)\n",
    "\n",
    "    if not val_scores_all:\n",
    "        raise RuntimeError(f\"[Cell5A] fold{fold_id}: inner val が空です。\")\n",
    "\n",
    "    val_scores = np.concatenate(val_scores_all)\n",
    "    val_y = np.concatenate(val_y_all)\n",
    "    val_grp = np.concatenate(val_grp_all)\n",
    "\n",
    "    res_single = _single_tau_opt(val_scores, val_y)\n",
    "    res_wg = _wg_opt_joint(val_scores, val_y, val_grp)\n",
    "    res_group = _group_opt(val_scores, val_y, val_grp)\n",
    "\n",
    "    # outer train/test\n",
    "    X_tr_out, y_tr_out = X_k[train_mask], y_base[train_mask]\n",
    "    X_te_out, y_te_out = X_k[test_mask], y_base[test_mask]\n",
    "    grp_te_out = fair_groups[test_mask].to_numpy()\n",
    "\n",
    "    model_outer = fit_classifier(X_tr_out, y_tr_out)\n",
    "    sc_te = predict_positive_score(model_outer, X_te_out).astype(float)\n",
    "\n",
    "    tau_single = float(res_single[\"tau\"])\n",
    "    tau_high_group = float(res_group[\"tauH\"])\n",
    "    tau_low_group = float(res_group[\"tauL\"])\n",
    "    tau_high_wg = float(res_wg[\"tauH\"])\n",
    "    tau_low_wg = float(res_wg[\"tauL\"])\n",
    "\n",
    "    yhat_single = (sc_te >= tau_single).astype(int)\n",
    "    yhat_group = np.where(grp_te_out == \"High\", (sc_te >= tau_high_group).astype(int), (sc_te >= tau_low_group).astype(int))\n",
    "    yhat_wg = np.where(grp_te_out == \"High\", (sc_te >= tau_high_wg).astype(int), (sc_te >= tau_low_wg).astype(int))\n",
    "\n",
    "    metric_single = _metric_binary(y_te_out, yhat_single)\n",
    "    metric_group = _metric_binary(y_te_out, yhat_group)\n",
    "    metric_wg = _metric_binary(y_te_out, yhat_wg)\n",
    "\n",
    "    pred_rows.append(pd.DataFrame({\n",
    "        \"fold_id\": fold_id,\n",
    "        \"test_id\": test_sid,\n",
    "        \"group\": grp_te_out,\n",
    "        \"y_true\": y_te_out,\n",
    "        \"proba\": sc_te,\n",
    "        \"tau_single\": tau_single,\n",
    "        \"tau_high_group\": tau_high_group,\n",
    "        \"tau_low_group\": tau_low_group,\n",
    "        \"tau_high_wg\": tau_high_wg,\n",
    "        \"tau_low_wg\": tau_low_wg,\n",
    "        \"y_pred_single\": yhat_single,\n",
    "        \"y_pred_group\": yhat_group,\n",
    "        \"y_pred_wg\": yhat_wg,\n",
    "        f\"metric_single_{MODE_TAG}\": metric_single,\n",
    "        f\"metric_group_{MODE_TAG}\": metric_group,\n",
    "        f\"metric_wg_{MODE_TAG}\": metric_wg,\n",
    "    }))\n",
    "\n",
    "    col_high_group = f\"tau_high_Group{MODE_TAG}\"\n",
    "    col_low_group  = f\"tau_low_Group{MODE_TAG}\"\n",
    "    col_high_wg    = f\"tau_high_WG{MODE_TAG}\"\n",
    "    col_low_wg     = f\"tau_low_WG{MODE_TAG}\"\n",
    "\n",
    "    rows.append({\n",
    "        \"fold_id\": fold_id,\n",
    "        \"test_id\": test_sid,\n",
    "        \"tau_single\": tau_single,\n",
    "        col_high_group: tau_high_group,\n",
    "        col_low_group: tau_low_group,\n",
    "        col_high_wg: tau_high_wg,\n",
    "        col_low_wg: tau_low_wg,\n",
    "        f\"val_single_{MODE_TAG}\": res_single[f\"{METRIC_LABEL}_val\"],\n",
    "        f\"val_group_{MODE_TAG}\": res_group[f\"{METRIC_LABEL}_val\"],\n",
    "        f\"val_wg_{MODE_TAG}\": res_wg[f\"WG_{METRIC_LABEL}_val\"],\n",
    "    })\n",
    "\n",
    "# ---------------- 結果集計 ----------------\n",
    "df_pred = pd.concat(pred_rows, ignore_index=True)\n",
    "df_rows = pd.DataFrame(rows)\n",
    "\n",
    "pred_path = groupaware_out(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "df_pred.to_csv(pred_path, index=False, encoding=\"utf-8-sig\")\n",
    "fold_path = groupaware_out(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "df_rows.to_csv(fold_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell5A] predictions -> {pred_path}\")\n",
    "print(f\"[Cell5A] thresholds -> {fold_path}\")\n",
    "\n",
    "# ---------------- AUCと図（従来互換） ----------------\n",
    "y_pool = df_pred[\"y_true\"].to_numpy()\n",
    "s_pool = df_pred[\"proba\"].to_numpy()\n",
    "subj_pool = df_pred[\"test_id\"].to_numpy()\n",
    "if len(np.unique(y_pool)) < 2:\n",
    "    raise RuntimeError(\"[Cell5A] 真値が単一クラスのため ROC-AUC を計算できません。\")\n",
    "auc_obs = float(roc_auc_score(y_pool, s_pool))\n",
    "\n",
    "rng = np.random.default_rng(20251101)\n",
    "df_pool = pd.DataFrame({\"subject\": subj_pool, \"y_true\": y_pool, \"y_score\": s_pool})\n",
    "subj_ids = df_pool[\"subject\"].unique()\n",
    "auc_boot = []\n",
    "for _ in range(2000):\n",
    "    sampled = rng.choice(subj_ids, size=len(subj_ids), replace=True)\n",
    "    df_boot = pd.concat([df_pool[df_pool[\"subject\"] == sid] for sid in sampled], ignore_index=True)\n",
    "    if df_boot[\"y_true\"].nunique() < 2:\n",
    "        continue\n",
    "    auc_boot.append(float(roc_auc_score(df_boot[\"y_true\"], df_boot[\"y_score\"])))\n",
    "if auc_boot:\n",
    "    ci_low = float(np.quantile(auc_boot, 0.025))\n",
    "    ci_high = float(np.quantile(auc_boot, 0.975))\n",
    "else:\n",
    "    ci_low = ci_high = float(\"nan\")\n",
    "\n",
    "pd.DataFrame([{\"mode\": MODE_TAG, \"k\": len(feats_k), \"auc\": auc_obs, \"ci_low\": ci_low, \"ci_high\": ci_high}]).to_csv(\n",
    "    cell5_out(f\"AUC_K_CI_{MODE_TAG}.csv\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[Cell5A] AUC={auc_obs:.4f} (95% CI [{ci_low:.4f}, {ci_high:.4f}])\")\n",
    "\n",
    "fpr, tpr, _ = skm.roc_curve(y_pool, s_pool)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_obs:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Chance\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC Curve (Best Subset, {MODE_TAG})\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(cell5_out(f\"AUC_K_CI_{MODE_TAG}.png\"), dpi=300)\n",
    "plt.close()\n",
    "print(f\"[Cell5A] ROC 図を保存 -> {cell5_out(f'AUC_K_CI_{MODE_TAG}.png')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Cell 5B++: 確率スコア分布（F1/BA 切替） =====\n",
    "set_cell_output(15)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CELL5_MODE = str(globals().get(\"CELL5_MODE\", \"F1\")).upper()\n",
    "if CELL5_MODE not in {\"F1\", \"BA\"}:\n",
    "    raise ValueError(f\"[Cell5B++] CELL5_MODE は 'F1' または 'BA' を指定してください（今: {CELL5_MODE}）\")\n",
    "\n",
    "# ---------- 設定 ----------\n",
    "BINS = 40\n",
    "LW = 1.5\n",
    "FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "COLOR_SICK = \"red\"\n",
    "COLOR_NON  = \"blue\"\n",
    "COLOR_SINGLE = \"black\"\n",
    "COLOR_GROUP  = \"green\"\n",
    "COLOR_WG     = \"purple\"\n",
    "\n",
    "RUN_ROOT = os.path.join(LEVEL1_DIR, f\"Cell5A_{CELL5_MODE}\")\n",
    "IMG_DIR  = os.path.join(RUN_ROOT, f\"PROBA_DIST_{CELL5_MODE}\")\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "SAVE_OVERALL_SvG   = os.path.join(IMG_DIR, \"OVERALL_SvGroup.png\")\n",
    "SAVE_OVERALL_SvWG  = os.path.join(IMG_DIR, \"OVERALL_SvWG.png\")\n",
    "SAVE_BYGROUP_SvG   = os.path.join(IMG_DIR, \"BYGROUP_SvGroup.png\")\n",
    "SAVE_BYGROUP_SvWG  = os.path.join(IMG_DIR, \"BYGROUP_SvWG.png\")\n",
    "\n",
    "GROUP_AWARE_DIR = os.path.join(RUN_ROOT, \"GROUP_AWARE\", CELL5_MODE)\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_path(filename: str) -> str:\n",
    "    return os.path.join(GROUP_AWARE_DIR, filename)\n",
    "\n",
    "pred_path = groupaware_path(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "fold_path = groupaware_path(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "if not (os.path.exists(pred_path) and os.path.exists(fold_path)):\n",
    "    raise FileNotFoundError(f\"[Cell5B++] 必要CSVが見つからない（Cell5A {CELL5_MODE} を先に実行）\")\n",
    "\n",
    "df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- モード自動判定（F1 or BA、列名で確認） ----------\n",
    "cols_f1 = {\"high\":\"tau_high_GroupF1\", \"low\":\"tau_low_GroupF1\", \"wgh\":\"tau_high_WGF1\", \"wgl\":\"tau_low_WGF1\"}\n",
    "cols_ba = {\"high\":\"tau_high_GroupBA\", \"low\":\"tau_low_GroupBA\", \"wgh\":\"tau_high_WGBA\", \"wgl\":\"tau_low_WGBA\"}\n",
    "\n",
    "if all(c in df_fold.columns for c in [cols_f1[\"high\"], cols_f1[\"low\"]]):\n",
    "    mode = \"F1\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_f1[\"high\"], cols_f1[\"low\"], cols_f1[\"wgh\"], cols_f1[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_F1\"\n",
    "elif all(c in df_fold.columns for c in [cols_ba[\"high\"], cols_ba[\"low\"]]):\n",
    "    mode = \"BA\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_ba[\"high\"], cols_ba[\"low\"], cols_ba[\"wgh\"], cols_ba[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_BA\"\n",
    "else:\n",
    "    raise RuntimeError(\"[Cell5B++] しきい値列が見つからない（F1/BAどちらかのCell5Aの出力が必要）\")\n",
    "\n",
    "# ---------- 集約: 中央値/IQR（Q1〜Q3） ----------\n",
    "def _qstats(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.size == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    med = float(np.nanmedian(s))\n",
    "    q1, q3 = np.nanpercentile(s, [25, 75])\n",
    "    half = float((q3 - q1)/2.0)\n",
    "    return float(med), float(q1), float(q3), half\n",
    "\n",
    "tau_single_med, tau_single_q1, tau_single_q3, _ = _qstats(df_fold[\"tau_single\"])\n",
    "tau_high_med,   tau_high_q1,   tau_high_q3,   _ = _qstats(df_fold[c_high])\n",
    "tau_low_med,    tau_low_q1,    tau_low_q3,    _ = _qstats(df_fold[c_low])\n",
    "tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, _ = _qstats(df_fold[c_wgh]) if c_wgh in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  _ = _qstats(df_fold[c_wgl]) if c_wgl in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "# ---------- データ分解 ----------\n",
    "proba = pd.to_numeric(df_pred[\"proba\"], errors=\"coerce\").values\n",
    "ytrue = pd.to_numeric(df_pred[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "grp   = df_pred[\"group\"].astype(str).str.strip()\n",
    "\n",
    "p_sick = proba[ytrue == 1]\n",
    "p_non  = proba[ytrue == 0]\n",
    "n_sick, n_non = len(p_sick), len(p_non)\n",
    "maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "\n",
    "# ---------- ユーティリティ ----------\n",
    "def _style_axes(ax, title=None):\n",
    "    if title: ax.set_title(title, fontsize=FS_TITLE)\n",
    "    ax.set_xlabel(\"Predicted probability\", fontsize=FS_LABEL)\n",
    "    ax.set_ylabel(\"Density\", fontsize=FS_LABEL)\n",
    "    ax.tick_params(axis=\"both\", labelsize=FS_TICK)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "def _hist_overall(ax):\n",
    "    ax.hist(p_sick, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={n_sick})\", color=COLOR_SICK)\n",
    "    ax.hist(p_non,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={n_non})\", color=COLOR_NON)\n",
    "\n",
    "def _hist_bygroup(axes):\n",
    "    p_sick_H = proba[(ytrue==1) & maskH]; p_non_H = proba[(ytrue==0) & maskH]\n",
    "    axes[0].hist(p_sick_H, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_H)})\", color=COLOR_SICK)\n",
    "    axes[0].hist(p_non_H,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_H)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[0], \"High group\")\n",
    "    p_sick_L = proba[(ytrue==1) & maskL]; p_non_L = proba[(ytrue==0) & maskL]\n",
    "    axes[1].hist(p_sick_L, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_L)})\", color=COLOR_SICK)\n",
    "    axes[1].hist(p_non_L,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_L)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[1], \"Low group\")\n",
    "\n",
    "def _vline_with_iqr(ax, x_med, q1, q3, color, ls, label_core):\n",
    "    if np.isfinite(x_med):\n",
    "        ax.axvline(x_med, color=color, linestyle=ls, linewidth=LW,\n",
    "                   label=f\"{label_core} = {x_med:.3f} ± {(q3-q1)/2:.3f}\" if (np.isfinite(q1) and np.isfinite(q3)) else f\"{label_core} = {x_med:.3f}\")\n",
    "    if np.isfinite(q1) and np.isfinite(q3):\n",
    "        ax.axvspan(q1, q3, color=color, alpha=0.12)\n",
    "\n",
    "# ---------- 1) OVERALL: Single vs Group ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_med,   tau_high_q1,   tau_high_q3,   COLOR_GROUP,  \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(ax, tau_low_med,    tau_low_q1,    tau_low_q3,    COLOR_GROUP,  \"--\", f\"τ_low_{mode}\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs Group [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvG}\")\n",
    "\n",
    "# ---------- 2) OVERALL: Single vs WG ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(ax, tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs WG [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvWG}\")\n",
    "\n",
    "# ---------- 3) BY_GROUP: Single vs Group ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[0], tau_high_med, tau_high_q1, tau_high_q3, COLOR_GROUP, \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(axes[1], tau_low_med,  tau_low_q1,  tau_low_q3,  COLOR_GROUP, \"--\", f\"τ_low_{mode}\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvG}\")\n",
    "\n",
    "# ---------- 4) BY_GROUP: Single vs WG ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[0], tau_high_wg, tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(axes[1], tau_low_wg,  tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvWG}\")\n",
    "\n",
    "# ---------- 5) Fold単位：OVERALL の確率分布と各Foldの τ ----------\n",
    "for _, row in df_fold.iterrows():\n",
    "    fid = int(row.get(\"fold_id\", -1)) if \"fold_id\" in row else None\n",
    "    test_id = str(row.get(\"test_id\", f\"fold{fid}\"))\n",
    "    sub = df_pred[df_pred[\"fold_id\"] == fid] if \"fold_id\" in df_pred.columns and fid is not None else df_pred.copy()\n",
    "\n",
    "    p = pd.to_numeric(sub[\"proba\"], errors=\"coerce\").values\n",
    "    yt = pd.to_numeric(sub[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "    p_s, p_n = p[yt==1], p[yt==0]\n",
    "\n",
    "    t_single = float(row[\"tau_single\"])\n",
    "    t_high   = float(row[c_high]) if c_high in row else np.nan\n",
    "    t_low    = float(row[c_low])  if c_low  in row else np.nan\n",
    "    t_high_w = float(row[c_wgh])  if c_wgh  in row else np.nan\n",
    "    t_low_w  = float(row[c_wgl])  if c_wgl  in row else np.nan\n",
    "\n",
    "    p_sg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvGroup.png\")\n",
    "    p_wg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvWG.png\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high):   ax.axvline(t_high,   color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_high_{mode} = {t_high:.3f}\")\n",
    "    if np.isfinite(t_low):    ax.axvline(t_low,    color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_low_{mode}  = {t_low:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs Group [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_sg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_sg}\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high_w): ax.axvline(t_high_w, color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_high_WG = {t_high_w:.3f}\")\n",
    "    if np.isfinite(t_low_w):  ax.axvline(t_low_w,  color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_low_WG  = {t_low_w:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs WG [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_wg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_wg}\")\n",
    "\n",
    "print(f\"[Cell5B++] All images saved in: {IMG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セル統合済み: 旧 Cell5A-BA は Cell5A (F1/BA切替) に集約しました。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "セル統合済み: 旧 Cell5B++ (BA) は Cell5B++ (F1/BA切替) に集約しました。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}