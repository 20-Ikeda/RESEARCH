{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9975bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 0: 環境設定（全セル共通で利用）=====\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Any, Callable, Dict, Optional\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ------------------------\n",
    "# 実験スイッチ（Notebook全体で共有）\n",
    "# ------------------------\n",
    "FMS_THRESHOLD: int = 1           # FMS >= 1 を陽性ラベルとみなす\n",
    "EPOCH_LEN: int = 30               # 30 / 60 / 120 のいずれか\n",
    "MODEL_BACKEND: str = \"xgb\"        # \"xgb\" / \"rf\" / \"svm\"\n",
    "USE_AP_FOR_K: bool = False         # APベースの best_k で上書きするか      # 表示用\n",
    "SEED_BASE: int = 20251101\n",
    "TOP_SUBSET_K: int = 15           # subset探索で使う上位特徴数\n",
    "\n",
    "if EPOCH_LEN not in (30, 60, 120):\n",
    "    raise ValueError(\"EPOCH_LEN は 30/60/120 から選択してください。\")\n",
    "\n",
    "# ------------------------\n",
    "# ファイル入出力ルート\n",
    "# ------------------------\n",
    "BASE_INPUT_DIR = r\"C:\\Users\\taiki\\OneDrive - Science Tokyo\\デスクトップ\\研究\\本実験結果\"\n",
    "BASE_ANALYSIS_DIR = os.path.join(BASE_INPUT_DIR, \"ANALYSIS\")\n",
    "OUT_DIR = os.path.join(BASE_ANALYSIS_DIR, \"機械学習(SHAP_REF法)\", f\"閾値FMS{FMS_THRESHOLD}_区間{EPOCH_LEN}\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def outpath(filename: str) -> str:\n",
    "    return os.path.join(OUT_DIR, filename)\n",
    "\n",
    "print(f\"[OUT_DIR] {OUT_DIR}  |  EPOCH_LEN={EPOCH_LEN}s\")\n",
    "\n",
    "# ------------------------\n",
    "# 対象被験者・時間窓\n",
    "# ------------------------\n",
    "SUBJECT_IDS = [\n",
    "    \"10061\",\"10063\",\"10064\",\n",
    "    \"10071\",\"10072\",\"10073\",\"10074\",\n",
    "    \"10081\",\"10082\",\"10083\",\n",
    "    \"10091\",\"10092\",\"10093\",\"10094\",\n",
    "    \"10101\",\"10102\",\"10103\",\n",
    "]\n",
    "\n",
    "BASELINE_EPOCH = 1770               # ベースライン行（必須）\n",
    "ML_START, ML_END = 1800, 2400       # 学習に使う epoch_start 範囲 [start, end)\n",
    "\n",
    "# ------------------------\n",
    "# 描画スタイル\n",
    "# ------------------------\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120, \"savefig.dpi\": 300,\n",
    "    \"font.size\": 20, \"axes.titlesize\": 26, \"axes.labelsize\": 22,\n",
    "    \"xtick.labelsize\": 20, \"ytick.labelsize\": 20, \"legend.fontsize\": 20,\n",
    "})\n",
    "\n",
    "# ------------------------\n",
    "# FMS二値化ヘルパ\n",
    "# ------------------------\n",
    "def binarize_fms(series: pd.Series, threshold: Optional[int] = None) -> pd.Series:\n",
    "    th = FMS_THRESHOLD if threshold is None else int(threshold)\n",
    "    return (series >= th).astype(int)\n",
    "\n",
    "# ------------------------\n",
    "# モデルレジストリ\n",
    "# ------------------------\n",
    "ModelBuilder = Callable[..., Any]\n",
    "MODEL_REGISTRY: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "def register_backend(name: str, params: Dict[str, Any], builder: ModelBuilder) -> None:\n",
    "    MODEL_REGISTRY[name] = {\"params\": params, \"builder\": builder}\n",
    "\n",
    "def _build_xgb(params: Dict[str, Any], *, scale_pos_weight: Optional[float] = None):\n",
    "    cfg = params.copy()\n",
    "    if scale_pos_weight is not None:\n",
    "        cfg[\"scale_pos_weight\"] = float(scale_pos_weight)\n",
    "    return xgb.XGBClassifier(**cfg)\n",
    "\n",
    "def _build_rf(params: Dict[str, Any], **_):\n",
    "    return RandomForestClassifier(**params)\n",
    "\n",
    "def _build_svm(params: Dict[str, Any], **_):\n",
    "    return SVC(**params)\n",
    "\n",
    "XGB_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=100,\n",
    "    eval_metric=\"logloss\",\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "    n_jobs=1,\n",
    "    tree_method=\"hist\",\n",
    "    device=\"cpu\",\n",
    "    seed=0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "RF_PARAMS: Dict[str, Any] = dict(\n",
    "    n_estimators=439,\n",
    "    max_depth=14,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=False,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED_BASE,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "SVM_PARAMS: Dict[str, Any] = dict(\n",
    "    C=1.0,\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"scale\",\n",
    "    probability=True,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED_BASE,\n",
    ")\n",
    "\n",
    "register_backend(\"xgb\", XGB_PARAMS, _build_xgb)\n",
    "register_backend(\"rf\",  RF_PARAMS,  _build_rf)\n",
    "register_backend(\"svm\", SVM_PARAMS, _build_svm)\n",
    "\n",
    "def set_model_backend(name: str) -> None:\n",
    "    name = name.lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録: {list(MODEL_REGISTRY.keys())}\")\n",
    "    global MODEL_BACKEND\n",
    "    MODEL_BACKEND = name\n",
    "\n",
    "def build_estimator(\n",
    "    backend: Optional[str] = None,\n",
    "    *,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    name = (backend or MODEL_BACKEND).lower()\n",
    "    if name not in MODEL_REGISTRY:\n",
    "        raise KeyError(f\"[ERROR] backend '{name}' は未登録。\")\n",
    "    base = MODEL_REGISTRY[name][\"params\"].copy()\n",
    "    if overrides:\n",
    "        base.update(overrides)\n",
    "    builder = MODEL_REGISTRY[name][\"builder\"]\n",
    "    return builder(base, scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "def fit_estimator(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    model = build_estimator(\n",
    "        backend=backend, scale_pos_weight=scale_pos_weight, overrides=overrides\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def predict_positive_score(model, X: pd.DataFrame) -> np.ndarray:\n",
    "    X = X.astype(np.float32, copy=False)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        return np.asarray(model.decision_function(X), dtype=float)\n",
    "    return model.predict(X).astype(float)\n",
    "\n",
    "MODEL_ID = MODEL_BACKEND.upper()\n",
    "print(f\"[INFO] MODEL_BACKEND={MODEL_ID} / SEED={SEED_BASE} / backends={list(MODEL_REGISTRY.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: データ準備（CSV読込 → EPOCH合成 → SUBJECT_META → 行列出力）=====\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------------------------\n",
    "# ① 30秒EPOCH CSVの読み込み・検証\n",
    "# --------------------------------------------\n",
    "def subject_csv_path(sid: str) -> str:\n",
    "    path = os.path.join(BASE_INPUT_DIR, sid, \"EPOCH\", f\"{sid}_epoch.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"[Cell1] CSV missing for subject {sid}: {path}\")\n",
    "    return path\n",
    "\n",
    "dfs = []\n",
    "for sid in SUBJECT_IDS:\n",
    "    df = pd.read_csv(subject_csv_path(sid))\n",
    "    if df.shape[1] < 4:\n",
    "        raise ValueError(f\"[Cell1] {sid}: 列数が不足（>=4 必須）\")\n",
    "    df = df.copy()\n",
    "    df.columns = list(df.columns[:3]) + [str(c) for c in df.columns[3:]]\n",
    "    c1, c2, c3 = df.columns[:3]\n",
    "    df = df.rename(columns={c1: \"epoch_start\", c2: \"epoch_end\", c3: \"FMS\"})\n",
    "    df[\"epoch_start\"] = pd.to_numeric(df[\"epoch_start\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"epoch_end\"]   = pd.to_numeric(df[\"epoch_end\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "    df[\"FMS\"]         = pd.to_numeric(df[\"FMS\"],         errors=\"coerce\").astype(\"Int64\")\n",
    "    if df[[\"epoch_start\",\"epoch_end\",\"FMS\"]].isna().any().any():\n",
    "        raise ValueError(f\"[Cell1] {sid}: epoch_start/epoch_end/FMS に NaN\")\n",
    "    df.insert(0, \"subject_id\", sid)\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_raw = pd.concat(dfs, ignore_index=True)\n",
    "exclude_feats = {\"HF_power\", \"LF_power\", \"LF_HF_ratio\"}\n",
    "feature_cols_all = [\n",
    "    c for c in combined_raw.columns\n",
    "    if c not in {\"subject_id\",\"epoch_start\",\"epoch_end\",\"FMS\"} and c not in exclude_feats\n",
    "]\n",
    "if not feature_cols_all:\n",
    "    raise RuntimeError(\"[Cell1] 特徴量列が0です。列名や除外設定を確認してください。\")\n",
    "\n",
    "print(f\"[Cell1] Loaded subjects={len(SUBJECT_IDS)}, rows={len(combined_raw)}, features(after drop)={len(feature_cols_all)}\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ② EPOCH_LEN 秒への合成 + baseline差分 + ラベル生成\n",
    "# --------------------------------------------\n",
    "if (ML_END - ML_START) % EPOCH_LEN != 0:\n",
    "    raise ValueError(f\"[Cell1] ML window {ML_END-ML_START} が EPOCH_LEN={EPOCH_LEN} で割り切れません。\")\n",
    "\n",
    "rows_per_bin = EPOCH_LEN // 30\n",
    "df_out_list = []\n",
    "\n",
    "for sid, sdf in combined_raw.groupby(\"subject_id\", sort=False):\n",
    "    base_row = sdf.loc[sdf[\"epoch_start\"] == BASELINE_EPOCH]\n",
    "    if len(base_row) != 1:\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline epoch_start=={BASELINE_EPOCH} が見つからない\")\n",
    "    base_vals = base_row[feature_cols_all].astype(float).iloc[0]\n",
    "    if base_vals.isna().any():\n",
    "        raise ValueError(f\"[Cell1] {sid}: baselineにNaN -> {base_vals.index[base_vals.isna()].tolist()}\")\n",
    "\n",
    "    sdf_ml = sdf[(sdf[\"epoch_start\"] >= ML_START) & (sdf[\"epoch_start\"] < ML_END)].copy()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: ML window [{ML_START},{ML_END}) が空です。\")\n",
    "\n",
    "    sdf_ml[\"bin_start\"] = ML_START + ((sdf_ml[\"epoch_start\"] - ML_START) // EPOCH_LEN) * EPOCH_LEN\n",
    "    sdf_ml[\"bin_end\"]   = sdf_ml[\"bin_start\"] + EPOCH_LEN\n",
    "\n",
    "    bin_counts = sdf_ml.groupby([\"bin_start\",\"bin_end\"]).size()\n",
    "    complete_bins = bin_counts[bin_counts == rows_per_bin].index\n",
    "    sdf_ml = sdf_ml.set_index([\"bin_start\",\"bin_end\"]).loc[complete_bins].reset_index()\n",
    "    if sdf_ml.empty:\n",
    "        raise ValueError(f\"[Cell1] {sid}: EPOCH_LEN={EPOCH_LEN} で完全なbinが無い\")\n",
    "\n",
    "    agg_dict = {c: \"mean\" for c in feature_cols_all}\n",
    "    agg_dict[\"FMS\"] = \"mean\"\n",
    "    g = sdf_ml.groupby([\"subject_id\",\"bin_start\",\"bin_end\"], as_index=False).agg(agg_dict)\n",
    "\n",
    "    g_features = g[feature_cols_all].astype(float) - base_vals.values\n",
    "    if g_features.isna().any().any():\n",
    "        bad = g_features.columns[g_features.isna().any()].tolist()\n",
    "        raise ValueError(f\"[Cell1] {sid}: baseline差分後にNaN -> {bad}\")\n",
    "\n",
    "    g_out = pd.concat([g[[\"subject_id\",\"bin_start\",\"bin_end\",\"FMS\"]], g_features], axis=1)\n",
    "    g_out = g_out.rename(columns={\"bin_start\":\"epoch_start\",\"bin_end\":\"epoch_end\"})\n",
    "    g_out[\"label\"] = binarize_fms(g_out[\"FMS\"])\n",
    "    g_out = g_out[[\"subject_id\",\"epoch_start\",\"epoch_end\",\"FMS\",\"label\"] + feature_cols_all]\n",
    "    df_out_list.append(g_out)\n",
    "\n",
    "df_ml_epoch = pd.concat(df_out_list, ignore_index=True)\n",
    "\n",
    "# --------------------------------------------\n",
    "# ③ SUBJECT_META & MSSQ group\n",
    "# --------------------------------------------\n",
    "CANDIDATE_SCORE_PATHS = [\n",
    "    \"/mnt/data/summary_scores.xlsx\",\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_ANALYSIS_DIR, \"機械学習\", \"summary_scores.xlsx\"),\n",
    "    os.path.join(BASE_INPUT_DIR, \"summary_scores.xlsx\"),\n",
    "]\n",
    "score_path = next((p for p in CANDIDATE_SCORE_PATHS if os.path.exists(p)), None)\n",
    "if score_path is None:\n",
    "    raise FileNotFoundError(\"[Cell1] summary_scores.xlsx が見つかりません。\")\n",
    "meta_raw = pd.read_excel(score_path, sheet_name=\"Summary\")\n",
    "\n",
    "required = [\"ID\", \"MSSQ\", \"VIMSSQ\"]\n",
    "missing = [c for c in required if c not in meta_raw.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"[Cell1] summary_scores.xlsx に必須列がありません -> {missing}\")\n",
    "\n",
    "meta = meta_raw[required].copy()\n",
    "meta[\"ID\"] = (\n",
    "    meta[\"ID\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    ")\n",
    "for c in [\"MSSQ\", \"VIMSSQ\"]:\n",
    "    meta[c] = pd.to_numeric(meta[c], errors=\"raise\")\n",
    "\n",
    "sid_set = set(map(str, SUBJECT_IDS))\n",
    "meta = meta[meta[\"ID\"].isin(sid_set)].copy()\n",
    "if meta[\"ID\"].duplicated().any():\n",
    "    raise ValueError(f\"[Cell1] ID 重複 -> {meta.loc[meta['ID'].duplicated(), 'ID'].tolist()}\")\n",
    "\n",
    "MSSQ_THRESHOLD_FIXED = 12.0\n",
    "meta[\"MSSQ_group\"] = np.where(meta[\"MSSQ\"] >= MSSQ_THRESHOLD_FIXED, \"High\", \"Low\")\n",
    "SUBJECT_META = (\n",
    "    meta.rename(columns={\"ID\": \"subject_id\"})\n",
    "        .set_index(\"subject_id\")[[\"MSSQ\", \"VIMSSQ\", \"MSSQ_group\"]]\n",
    "        .copy()\n",
    ")\n",
    "SUBJECT_META.to_csv(outpath(\"subject_meta.csv\"), encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell1] SUBJECT_META saved -> {outpath('subject_meta.csv')} (source='{score_path}')\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# ④ 学習行列＆行列保存\n",
    "# --------------------------------------------\n",
    "fname_raw = f\"ML_DATA_DELTA_{EPOCH_LEN}S_RAW.CSV\"\n",
    "df_ml_epoch.to_csv(outpath(fname_raw), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "X_all = df_ml_epoch[feature_cols_all].copy().astype(float)\n",
    "y_all = df_ml_epoch[\"label\"].copy().astype(int)\n",
    "groups = df_ml_epoch[\"subject_id\"].copy()\n",
    "\n",
    "X_all.to_csv(outpath(f\"X_RAW_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "X_all.to_csv(outpath(f\"X_SCALED_ALL_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\")  # 木系でスケーリング不要\n",
    "pd.DataFrame({\"subject_id\": groups, \"label\": y_all, \"FMS_mean\": df_ml_epoch[\"FMS\"]}).to_csv(\n",
    "    outpath(f\"Y_AND_GROUPS_{EPOCH_LEN}S.CSV\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "print(f\"[Cell1] Saved -> {outpath(fname_raw)} / X_RAW_ALL / X_SCALED_ALL / Y_AND_GROUPS\")\n",
    "print(f\"[Cell1] Matrices ready: X_all={X_all.shape}, y_all={y_all.shape}, SUBJECT_META={SUBJECT_META.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45007fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 2: モデリング共通ヘルパ（fit / SHAP / 評価）=====\n",
    "\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 学習ラッパー（Cell0のレジストリAPIを利用）\n",
    "# --------------------------------------------\n",
    "def fit_classifier(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    *,\n",
    "    backend: Optional[str] = None,\n",
    "    scale_pos_weight: Optional[float] = None,\n",
    "    overrides: Optional[Dict[str, Any]] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Cell0 の fit_estimator を直接包む薄いラッパ。\n",
    "    - SHAP/評価セルから backend を差し替えたい場合のみ backend / overrides を指定する。\n",
    "    \"\"\"\n",
    "    if \"fit_estimator\" not in globals():\n",
    "        raise RuntimeError(\"[Cell2] fit_estimator が未定義です。Cell0 を先に実行してください。\")\n",
    "    X_train = X_train.astype(np.float32, copy=False)\n",
    "    y_train = y_train.astype(np.int32, copy=False)\n",
    "    return fit_estimator(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        backend=backend,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        overrides=overrides,\n",
    "    )\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# TreeSHAP ベースの特徴重要度算出\n",
    "# --------------------------------------------\n",
    "def compute_train_shap_abs_mean(model, X_ref: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    学習データ X_ref 上での平均絶対SHAP値（降順）。\n",
    "    - XGB/RF 等の木モデルを想定（TreeSHAP）。\n",
    "    - SVM など非対応モデルでは ValueError を送出する。\n",
    "    \"\"\"\n",
    "    X_ref = X_ref.astype(np.float32, copy=False)\n",
    "\n",
    "    # 背景データ（最大128行）\n",
    "    bg_n = min(128, len(X_ref))\n",
    "    X_bg = X_ref.sample(n=bg_n, random_state=SEED_BASE) if bg_n >= 2 else X_ref\n",
    "\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            data=X_bg,\n",
    "            model_output=\"probability\",\n",
    "            feature_perturbation=\"interventional\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "    except Exception:\n",
    "        # probability指定が非対応な場合に raw へフォールバック\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            model_output=\"raw\",\n",
    "            feature_perturbation=\"tree_path_dependent\",\n",
    "        )\n",
    "        sv_any = explainer.shap_values(X_ref)\n",
    "\n",
    "    # shap_values の戻り値形状を統一（2D: n_samples × n_features）\n",
    "    classes = getattr(model, \"classes_\", None)\n",
    "    pos_idx = int(np.where(classes == 1)[0][0]) if classes is not None and 1 in list(classes) else -1\n",
    "\n",
    "    if isinstance(sv_any, list):\n",
    "        sv = sv_any[pos_idx]\n",
    "    else:\n",
    "        sv = getattr(sv_any, \"values\", sv_any)\n",
    "        sv = np.asarray(sv)\n",
    "        if sv.ndim == 3:\n",
    "            sv = sv[..., pos_idx]\n",
    "        elif sv.ndim == 1:\n",
    "            sv = sv.reshape(-1, 1)\n",
    "\n",
    "    if sv.shape[1] != X_ref.shape[1]:\n",
    "        raise RuntimeError(\n",
    "            f\"[Cell2] SHAP shape mismatch: sv.shape={sv.shape}, X_ref.shape={X_ref.shape}\"\n",
    "        )\n",
    "\n",
    "    abs_mean = np.mean(np.abs(sv), axis=0)\n",
    "    return pd.Series(abs_mean, index=X_ref.columns, name=\"mean_abs\").sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# --------------------------------------------\n",
    "# 評価ユーティリティ\n",
    "# --------------------------------------------\n",
    "def _is_probability_like(scores: np.ndarray) -> bool:\n",
    "    return np.isfinite(scores).all() and 0.0 <= scores.min() and scores.max() <= 1.0\n",
    "\n",
    "\n",
    "def evaluate_fold(model, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - ROC AUC: 2クラス時のみ。\n",
    "    - Accuracy: 確率なら 0.5、スコアなら 0.0 を閾値とする（詳細な最適化は別セル）。\n",
    "    \"\"\"\n",
    "    X_test = X_test.astype(np.float32, copy=False)\n",
    "    scores = predict_positive_score(model, X_test)\n",
    "\n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, scores)\n",
    "    else:\n",
    "        roc_auc = float(\"nan\")\n",
    "\n",
    "    thr = 0.5 if _is_probability_like(scores) else 0.0\n",
    "    pred = (scores >= thr).astype(int)\n",
    "    acc = accuracy_score(y_test.astype(int), pred)\n",
    "\n",
    "    return {\"roc_auc\": float(roc_auc), \"accuracy\": float(acc)}\n",
    "\n",
    "\n",
    "print(\"[Cell2] Modeling helpers ready (fit_classifier / compute_train_shap_abs_mean / evaluate_fold)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75547a3",
   "metadata": {},
   "source": [
    "# ===== Cell 3A-pre: 高相関特徴の事前除去 =====\n",
    "\n",
    "RFE の前に、全特徴の相関をチェックして |r| が閾値以上のペアを間引きます。\n",
    "列順（X_all の元々の並び）を優先し、残す列と除外した列を JSON に保存します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6751dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-pre: 高相関特徴の事前除去 =====\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "required = [\"X_all\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-pre] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "CORR_THRESHOLD = 0.90\n",
    "MIN_VARIANCE = 1e-8\n",
    "FEATURE_LIST_PATH = outpath(\"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "X_num = X_all.select_dtypes(include=[np.number]).copy()\n",
    "if X_num.empty:\n",
    "    raise RuntimeError(\"[Cell3A-pre] 数値列がありません。\")\n",
    "\n",
    "var = X_num.var(axis=0, ddof=1).fillna(0.0)\n",
    "valid_cols = var[var > MIN_VARIANCE].index.tolist()\n",
    "if not valid_cols:\n",
    "    raise RuntimeError(\"[Cell3A-pre] 分散がほぼゼロのため使用可能な列がありません。\")\n",
    "\n",
    "priority = [c for c in X_all.columns if c in valid_cols]\n",
    "X_use = X_num[priority]\n",
    "\n",
    "corr = X_use.corr(method=\"pearson\").abs()\n",
    "keep = []\n",
    "dropped = []\n",
    "\n",
    "for col in priority:\n",
    "    conflict = None\n",
    "    for kept in keep:\n",
    "        if corr.loc[col, kept] >= CORR_THRESHOLD:\n",
    "            conflict = kept\n",
    "            break\n",
    "    if conflict is None:\n",
    "        keep.append(col)\n",
    "    else:\n",
    "        dropped.append({\n",
    "            \"feature\": col,\n",
    "            \"representative\": conflict,\n",
    "            \"abs_corr\": float(corr.loc[col, conflict]),\n",
    "        })\n",
    "\n",
    "payload = {\n",
    "    \"keep\": keep,\n",
    "    \"dropped\": dropped,\n",
    "    \"threshold\": CORR_THRESHOLD,\n",
    "    \"total_columns\": len(priority),\n",
    "}\n",
    "with open(FEATURE_LIST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"[Cell3A-pre] keep={len(keep)} / total={len(priority)}, dropped={len(dropped)}\")\n",
    "print(f\"[Cell3A-pre] JSON -> {FEATURE_LIST_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fea0e",
   "metadata": {},
   "source": [
    "# ===== Cell 3A: RFE特徴量ランキング（LOSO）=====\n",
    "\n",
    "LOSO 各 fold の学習データで XGB を使った再帰的特徴量削除 (RFE) を実行し、得られた\n",
    "順位を平均してグローバルな重要度順を決めます。生成する CSV/PNG 名は従来の SHAP\n",
    "ファイルと同じなので、下流セルの修正は不要です。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A: RFE特徴量ランキング（LOSO） =====\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import json\n",
    "import os\n",
    "\n",
    "required = [\"X_all\", \"y_all\", \"groups\", \"build_estimator\", \"fit_classifier\", \"evaluate_fold\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-RFE] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "RFE_BACKEND = \"xgb\"   # RFE では XGB 固定\n",
    "RFE_STEP = 1          # 1 本ずつ削除\n",
    "RFE_MIN_FEATURES = 1  # 最低残す特徴数\n",
    "FEATURE_LIST_PATH = outpath(\"FEATURES_AFTER_CORR.json\")\n",
    "\n",
    "if os.path.exists(FEATURE_LIST_PATH):\n",
    "    with open(FEATURE_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        keep_payload = json.load(f)\n",
    "    feature_pool = [c for c in keep_payload.get(\"keep\", []) if c in X_all.columns]\n",
    "    print(f\"[Cell3A-RFE] correlation-pruned features loaded ({len(feature_pool)} cols)\")\n",
    "else:\n",
    "    feature_pool = list(X_all.columns)\n",
    "    print(\"[Cell3A-RFE] correlation-pruned list not found. Using all columns.\")\n",
    "\n",
    "if not feature_pool:\n",
    "    raise RuntimeError(\"[Cell3A-RFE] feature_pool が空です。Cell3A-pre の結果を確認してください。\")\n",
    "\n",
    "X_source = X_all[feature_pool].copy()\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "ranking_frames = []\n",
    "metrics_rows = []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo.split(X_source, y_all, groups), start=1):\n",
    "    X_tr = X_source.iloc[tr_idx].astype(np.float32)\n",
    "    y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "    X_te = X_source.iloc[te_idx].astype(np.float32)\n",
    "    y_te = y_all.iloc[te_idx].astype(int)\n",
    "\n",
    "    if len(np.unique(y_tr)) < 2:\n",
    "        raise RuntimeError(f\"[Cell3A-RFE] fold{fold_id}: 学習側が単一クラスです。\")\n",
    "\n",
    "    base_estimator = build_estimator(backend=RFE_BACKEND)\n",
    "    selector = RFE(\n",
    "        estimator=base_estimator,\n",
    "        step=max(1, int(RFE_STEP)),\n",
    "        n_features_to_select=max(1, int(RFE_MIN_FEATURES)),\n",
    "    )\n",
    "    selector.fit(X_tr, y_tr)\n",
    "\n",
    "    ranks = pd.Series(selector.ranking_, index=X_tr.columns, name=f\"fold{fold_id}\")\n",
    "    ranking_frames.append(ranks)\n",
    "\n",
    "    selected_cols = list(X_tr.columns[selector.support_])\n",
    "    model = fit_classifier(X_tr[selected_cols], y_tr, backend=RFE_BACKEND)\n",
    "    metrics = evaluate_fold(model, X_te[selected_cols], y_te)\n",
    "    metrics.update({\n",
    "        \"fold_id\": fold_id,\n",
    "        \"test_subject\": groups.iloc[te_idx].iloc[0],\n",
    "    })\n",
    "    metrics_rows.append(metrics)\n",
    "\n",
    "    preview = selected_cols[:5]\n",
    "    print(f\"[Cell3A-RFE] fold{fold_id}: selected {len(selected_cols)} features (preview={preview})\")\n",
    "\n",
    "rfe_rank = pd.concat(ranking_frames, axis=1)\n",
    "rfe_rank[\"rank_mean\"] = rfe_rank.mean(axis=1)\n",
    "rfe_rank[\"rank_median\"] = rfe_rank.median(axis=1)\n",
    "rfe_rank = rfe_rank.sort_values(\"rank_mean\")\n",
    "\n",
    "rank_path = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "rfe_rank.to_csv(rank_path, encoding=\"utf-8-sig\")\n",
    "rfe_rank.to_csv(outpath(\"SHAP_FEATURE_RANKING_LABELED.CSV\"), encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(metrics_rows).to_csv(outpath(\"LOSO_METRICS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell3A-RFE] Saved ranking -> {rank_path}\")\n",
    "\n",
    "plt.figure(figsize=(10, max(5, len(rfe_rank)//3)))\n",
    "plt.barh(rfe_rank.index[::-1], rfe_rank[\"rank_mean\"][::-1])\n",
    "plt.xlabel(\"Average rank (lower=better)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"RFE Feature Ranking (All)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"SHAP_RANKING_ALL.PNG\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "TOP_K = 8\n",
    "topk = rfe_rank.head(TOP_K).iloc[::-1]\n",
    "plt.figure(figsize=(12, 7))\n",
    "ax = plt.gca()\n",
    "ax.barh(topk.index, topk[\"rank_mean\"])\n",
    "ax.set_xlabel(\"Average rank (lower=better)\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.set_title(f\"Top-{TOP_K} RFE Feature Ranking\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"SHAP_TOP8_RANKING.PNG\"), dpi=300)\n",
    "plt.close()\n",
    "print(f\"[Cell3A-RFE] 図を保存 -> {outpath('SHAP_TOP8_RANKING.PNG')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c998ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Cell 3A-Subset: Top-15 組合せ探索 =====\n",
    "import os\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "required = [\"X_all\", \"y_all\", \"groups\", \"fit_classifier\", \"predict_positive_score\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-Subset] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "TOP_SUBSET_K = int(globals().get(\"TOP_SUBSET_K\", 15))\n",
    "if TOP_SUBSET_K <= 0:\n",
    "    raise ValueError(\"TOP_SUBSET_K must be positive\")\n",
    "\n",
    "rank_csv = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "if not os.path.exists(rank_csv):\n",
    "    raise FileNotFoundError(\"[Cell3A-Subset] SHAP_FEATURE_RANKING.CSV がありません。Cell3A を実行してください。\")\n",
    "\n",
    "rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "if \"rank_mean\" in rank_df.columns:\n",
    "    feature_order = rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "elif \"mean_abs\" in rank_df.columns:\n",
    "    feature_order = rank_df.sort_values(\"mean_abs\", ascending=False).index.tolist()\n",
    "else:\n",
    "    feature_order = list(rank_df.index)\n",
    "\n",
    "feature_order = [f for f in feature_order if f in X_all.columns]\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[Cell3A-Subset] ランキングに該当する特徴が X_all に存在しません。\")\n",
    "\n",
    "limit = min(TOP_SUBSET_K, len(feature_order))\n",
    "top_features = feature_order[:limit]\n",
    "print(f\"[Cell3A-Subset] Top features ({len(top_features)}): {top_features}\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "results = []\n",
    "\n",
    "for r in range(1, len(top_features) + 1):\n",
    "    for comb in combinations(top_features, r):\n",
    "        feats = list(comb)\n",
    "        y_true_all = []\n",
    "        y_score_all = []\n",
    "        for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "            X_tr = X_all.iloc[tr_idx][feats].astype(np.float32)\n",
    "            y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "            X_te = X_all.iloc[te_idx][feats].astype(np.float32)\n",
    "            y_te = y_all.iloc[te_idx].astype(int)\n",
    "            if len(np.unique(y_tr)) < 2:\n",
    "                continue\n",
    "            model = fit_classifier(X_tr, y_tr)\n",
    "            proba = predict_positive_score(model, X_te)\n",
    "            y_true_all.append(y_te)\n",
    "            y_score_all.append(proba)\n",
    "        if not y_true_all:\n",
    "            auc = float(\"nan\")\n",
    "        else:\n",
    "            y_true = np.concatenate(y_true_all)\n",
    "            y_score = np.concatenate(y_score_all)\n",
    "            if len(np.unique(y_true)) < 2:\n",
    "                auc = float(\"nan\")\n",
    "            else:\n",
    "                auc = float(roc_auc_score(y_true, y_score))\n",
    "        results.append({\n",
    "            \"size\": r,\n",
    "            \"features\": feats,\n",
    "            \"auc\": auc,\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values([\"auc\", \"size\"], ascending=[False, True])\n",
    "results_df[\"features_str\"] = results_df[\"features\"].apply(lambda lst: \",\".join(lst))\n",
    "subset_csv_name = f\"TOP{TOP_SUBSET_K}_SUBSET_AUC.csv\"\n",
    "subset_path = outpath(subset_csv_name)\n",
    "results_df[[\"size\", \"features_str\", \"auc\"]].to_csv(subset_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "best_row = results_df.iloc[0]\n",
    "print(f\"[Cell3A-Subset] best size={int(best_row['size'])}, auc={best_row['auc']:.4f}, features={best_row['features']}\")\n",
    "\n",
    "best_json_name = f\"TOP{TOP_SUBSET_K}_SUBSET_BEST.json\"\n",
    "with open(outpath(best_json_name), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"size\": int(best_row[\"size\"]),\n",
    "        \"auc\": float(best_row[\"auc\"]),\n",
    "        \"features\": best_row[\"features\"],\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "globals()[\"BEST_SUBSET_FEATURES\"] = best_row[\"features\"]\n",
    "globals()[\"BEST_SUBSET_K\"] = len(best_row[\"features\"])\n",
    "\n",
    "print(f\"[Cell3A-Subset] 保存 -> {subset_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae819ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3A-SHAP: 全特徴の SHAP 可視化 =====\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "required = [\"X_all\", \"y_all\", \"fit_classifier\", \"outpath\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell3A-SHAP] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "rank_csv = outpath(\"SHAP_FEATURE_RANKING.CSV\")\n",
    "if os.path.exists(rank_csv):\n",
    "    rank_df = pd.read_csv(rank_csv, index_col=0)\n",
    "    if \"rank_mean\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"rank_mean\").index.tolist()\n",
    "    elif \"mean_abs\" in rank_df.columns:\n",
    "        feature_order = rank_df.sort_values(\"mean_abs\", ascending=False).index.tolist()\n",
    "    else:\n",
    "        feature_order = list(rank_df.index)\n",
    "else:\n",
    "    feature_order = list(X_all.columns)\n",
    "\n",
    "if not feature_order:\n",
    "    raise RuntimeError(\"[Cell3A-SHAP] 可視化対象の特徴がありません。\")\n",
    "\n",
    "TOP_K = min(10, len(feature_order))\n",
    "features_for_model = feature_order[:TOP_K]\n",
    "X_shap = X_all[features_for_model].astype(np.float32)\n",
    "y_shap = y_all.astype(int)\n",
    "\n",
    "print(f\"[Cell3A-SHAP] samples={X_shap.shape[0]}, features={X_shap.shape[1]} (top {TOP_K})\")\n",
    "\n",
    "model = fit_classifier(X_shap, y_shap)\n",
    "\n",
    "background = shap.sample(X_shap, min(256, len(X_shap)), random_state=SEED_BASE)\n",
    "explainer = shap.TreeExplainer(\n",
    "    model,\n",
    "    data=background,\n",
    "    model_output=\"probability\",\n",
    "    feature_perturbation=\"interventional\",\n",
    ")\n",
    "\n",
    "shap_values_any = explainer.shap_values(X_shap)\n",
    "if isinstance(shap_values_any, list):\n",
    "    class_idx = getattr(model, \"classes_\", [1]).tolist().index(1) if hasattr(model, \"classes_\") else -1\n",
    "    shap_values = shap_values_any[class_idx]\n",
    "else:\n",
    "    shap_values = shap_values_any\n",
    "\n",
    "shap_values = np.asarray(shap_values)\n",
    "if shap_values.ndim == 3:\n",
    "    shap_values = shap_values[:, :, -1]\n",
    "elif shap_values.ndim == 1:\n",
    "    shap_values = shap_values.reshape(-1, 1)\n",
    "\n",
    "if shap_values.shape[1] != X_shap.shape[1]:\n",
    "    raise RuntimeError(f\"[Cell3A-SHAP] shap_values 形状が一致しません: {shap_values.shape} vs {X_shap.shape}\")\n",
    "\n",
    "print(\"[Cell3A-SHAP] SHAP 値を計算しました。\")\n",
    "\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_shap, show=False, plot_type=\"dot\", max_display=TOP_K)\n",
    "plt.tight_layout()\n",
    "summary_path = outpath(\"SHAP_SUMMARY_TOP10.PNG\")\n",
    "plt.savefig(summary_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"[Cell3A-SHAP] Summary plot -> {summary_path}\")\n",
    "\n",
    "for feature in features_for_model:\n",
    "    plt.figure()\n",
    "    shap.dependence_plot(\n",
    "        feature,\n",
    "        shap_values,\n",
    "        X_shap,\n",
    "        show=False,\n",
    "        interaction_index=None,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    dep_path = outpath(f\"SHAP_DEP_{feature}.png\").replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "    plt.savefig(dep_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"[Cell3A-SHAP] Dependence plot -> {dep_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c3ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Cell 4A: ROC曲線（TOPサブセット） =====\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "primary_subset_file = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "preferred = [primary_subset_file, \"TOP10_SUBSET_BEST.json\"]\n",
    "subset_json = None\n",
    "for name in preferred:\n",
    "    cand = outpath(name)\n",
    "    if os.path.exists(cand):\n",
    "        subset_json = cand\n",
    "        break\n",
    "if subset_json is None:\n",
    "    raise FileNotFoundError(\"[Cell4A] TOP*_SUBSET_BEST.json がありません。Cell3A-Subset を実行してください。\")\n",
    "\n",
    "with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    subset_info = json.load(f)\n",
    "\n",
    "best_features = subset_info.get(\"features\", [])\n",
    "if not best_features:\n",
    "    raise RuntimeError(\"[Cell4A] JSON 内に features がありません。\")\n",
    "\n",
    "best_features = [f for f in best_features if f in X_all.columns]\n",
    "if not best_features:\n",
    "    raise RuntimeError(\"[Cell4A] X_all に存在する特徴がありません。\")\n",
    "\n",
    "best_k = len(best_features)\n",
    "print(f\"[Cell4A] 使用特徴 ({best_k}) from {os.path.basename(subset_json)}: {best_features}\")\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "y_true_all, proba_all, subj_all = [], [], []\n",
    "for tr_idx, te_idx in logo.split(X_all, y_all, groups):\n",
    "    X_tr = X_all.iloc[tr_idx][best_features].astype(np.float32)\n",
    "    y_tr = y_all.iloc[tr_idx].astype(int)\n",
    "    X_te = X_all.iloc[te_idx][best_features].astype(np.float32)\n",
    "    y_te = y_all.iloc[te_idx].astype(int)\n",
    "    if len(np.unique(y_tr)) < 2:\n",
    "        continue\n",
    "    model = fit_classifier(X_tr, y_tr)\n",
    "    proba = predict_positive_score(model, X_te)\n",
    "    y_true_all.append(y_te)\n",
    "    proba_all.append(proba)\n",
    "    subj_all.append(groups.iloc[te_idx].values)\n",
    "\n",
    "if not y_true_all:\n",
    "    raise RuntimeError(\"[Cell4A] 評価に必要な fold が得られませんでした。\")\n",
    "\n",
    "y_pool = np.concatenate(y_true_all)\n",
    "s_pool = np.concatenate(proba_all)\n",
    "subj_pool = np.concatenate(subj_all)\n",
    "if len(np.unique(y_pool)) < 2:\n",
    "    raise RuntimeError(\"[Cell4A] 真値が単一クラスのため ROC-AUC を計算できません。\")\n",
    "\n",
    "auc_obs = float(roc_auc_score(y_pool, s_pool))\n",
    "\n",
    "rng = np.random.default_rng(20251101)\n",
    "df_pool = pd.DataFrame({\"subject\": subj_pool, \"y_true\": y_pool, \"y_score\": s_pool})\n",
    "subj_ids = df_pool[\"subject\"].unique()\n",
    "auc_boot = []\n",
    "for _ in range(2000):\n",
    "    sampled = rng.choice(subj_ids, size=len(subj_ids), replace=True)\n",
    "    df_boot = pd.concat([df_pool[df_pool[\"subject\"] == sid] for sid in sampled], ignore_index=True)\n",
    "    if df_boot[\"y_true\"].nunique() < 2:\n",
    "        continue\n",
    "    auc_boot.append(float(roc_auc_score(df_boot[\"y_true\"], df_boot[\"y_score\"])))\n",
    "if auc_boot:\n",
    "    ci_low = float(np.quantile(auc_boot, 0.025))\n",
    "    ci_high = float(np.quantile(auc_boot, 0.975))\n",
    "else:\n",
    "    ci_low = ci_high = float(\"nan\")\n",
    "\n",
    "pd.DataFrame([{\"k\": best_k, \"auc\": auc_obs, \"ci_low\": ci_low, \"ci_high\": ci_high}]).to_csv(\n",
    "    outpath(\"AUC_K_CI.csv\"), index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"[Cell4A] AUC={auc_obs:.4f} (95% CI [{ci_low:.4f}, {ci_high:.4f}])\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_pool, s_pool)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_obs:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Chance\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Best Subset)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(outpath(\"AUC_K_CI.png\"), dpi=300)\n",
    "plt.close()\n",
    "print(f\"[Cell4A] ROC 図を保存 -> {outpath('AUC_K_CI.png')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ced576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Inner LOSO folds builder =====\n",
    "\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "def choose_inner_folds_loso(train_subject_ids: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    外側LOSOで得た “学習側の被験者ID” リストを受け取り、\n",
    "    1名ずつ検証に回す inner-LOSO のfoldリスト（[[sid1], [sid2], ...]）を返す。\n",
    "    \"\"\"\n",
    "    if not isinstance(train_subject_ids, (list, tuple)):\n",
    "        raise RuntimeError(\"[inner folds] train_subject_ids は list/tuple を想定しています。\")\n",
    "    uniq = list(pd.unique(pd.Series([str(sid) for sid in train_subject_ids])))\n",
    "    if len(uniq) == 0:\n",
    "        raise RuntimeError(\"[inner folds] train_subject_ids が空です。\")\n",
    "    uniq_sorted = sorted(uniq, key=lambda x: (len(x), x))\n",
    "    folds = [[sid] for sid in uniq_sorted]\n",
    "    print(f\"[inner folds] {len(folds)} splits -> val subjects = {', '.join(uniq_sorted)}\")\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf3838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5A (FINAL, F1専用): inner-LOSO τ最適化（連結val）→ outer予測・評価 =====\n",
    "# 前提: Cell1〜4 実行済み（X_all, y_all, groups, SUBJECT_META, BEST_K, choose_inner_folds_loso 等）\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- GROUP_AWARE 出力先（F1） ----------------\n",
    "MODE_TAG = \"F1\"\n",
    "GROUP_AWARE_DIR = os.path.join(OUT_DIR, \"GROUP_AWARE\", MODE_TAG)\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_out(filename: str) -> str:\n",
    "    path = os.path.join(GROUP_AWARE_DIR, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ---------------- 基本チェック ----------------\n",
    "req = [\"X_all\",\"y_all\",\"groups\",\"SUBJECT_META\",\n",
    "       \"choose_inner_folds_loso\",\"fit_classifier\",\"predict_positive_score\",\"outpath\"]\n",
    "missing = [v for v in req if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"[Cell5A] 未定義の変数/関数があります: {missing}\")\n",
    "\n",
    "if \"MSSQ_group\" not in SUBJECT_META.columns and \"MSSQ_group\" not in SUBJECT_META.index.names:\n",
    "    raise RuntimeError(\"[Cell5A] SUBJECT_META に MSSQ_group 列（または index）が必要である．\")\n",
    "\n",
    "# ---------------- 設定（等間隔グリッド＋近傍再探索） ----------------\n",
    "COARSE_STEPS = 1001      # 0.0〜1.0 を等間隔\n",
    "FINE_STEPS   = 101       # 近傍再探索の細かさ\n",
    "FINE_MARGIN  = 0.01      # 近傍幅（±0.01）\n",
    "VERBOSE      = True\n",
    "\n",
    "# ---------------- 入力整形 ----------------\n",
    "X_base = X_all.astype(np.float32)\n",
    "y_base = y_all.astype(int)\n",
    "g_base = groups.astype(str)\n",
    "\n",
    "# MSSQ group マッピング（High/Lowに正規化）\n",
    "if \"subject_id\" in SUBJECT_META.columns:\n",
    "    mapper = SUBJECT_META.set_index(\"subject_id\")[\"MSSQ_group\"].astype(str).to_dict()\n",
    "else:\n",
    "    mapper = SUBJECT_META[\"MSSQ_group\"].astype(str).to_dict()\n",
    "\n",
    "fair_groups = g_base.map(mapper).astype(str).str.strip().str.lower().map({\"high\":\"High\",\"low\":\"Low\"})\n",
    "if fair_groups.isna().any():\n",
    "    raise RuntimeError(f\"[Cell5A] MSSQ_group 未割当ID: {sorted(set(g_base[fair_groups.isna()]))}\")\n",
    "\n",
    "\n",
    "# 特徴選抜（SHAP順の上位K → サブセット優先）\n",
    "subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "subset_candidates = [subset_primary, \"TOP10_SUBSET_BEST.json\"]\n",
    "subset_json = None\n",
    "for name in subset_candidates:\n",
    "    cand = outpath(name)\n",
    "    if os.path.exists(cand):\n",
    "        subset_json = cand\n",
    "        break\n",
    "feature_order = []\n",
    "if subset_json is not None:\n",
    "    try:\n",
    "        with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            subset_info = json.load(f)\n",
    "        feature_order = [f for f in subset_info.get(\"features\", []) if f in X_base.columns]\n",
    "        if feature_order:\n",
    "            best_k = len(feature_order)\n",
    "            print(f\"[Cell5A] Using subset features from {os.path.basename(subset_json)} (k={best_k})\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Cell5A][WARN] subset読み込み失敗: {e}\")\n",
    "if not feature_order:\n",
    "    rank_df = pd.read_csv(outpath(\"SHAP_FEATURE_RANKING.CSV\"), index_col=0, encoding=\"utf-8-sig\")\n",
    "    feature_order = [f for f in rank_df.index if f in X_base.columns]\n",
    "    if not feature_order:\n",
    "        raise RuntimeError(\"[Cell5A] ランキング上位に X に無い特徴があります。\")\n",
    "    if \"best_k\" not in globals():\n",
    "        raise RuntimeError(\"[Cell5A] best_k が未定義です。Cell3B を実行してください。\")\n",
    "    best_k = int(best_k)\n",
    "    feats_k = feature_order[:best_k]\n",
    "else:\n",
    "    feats_k = feature_order\n",
    "X_k = X_base[feats_k]\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"[DBG] 使用特徴数: {len(feats_k)}  (BEST_K={best_k})\")\n",
    "\n",
    "# ---------------- 指標・評価ユーティリティ（F1専用） ----------------\n",
    "def _conf_from_preds(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    TN, FP, FN, TP = skm.confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _f1_from_conf(TP, FP, FN, TN) -> float:\n",
    "    TP = float(TP); FP = float(FP); FN = float(FN)\n",
    "    denom = (2*TP + FP + FN)\n",
    "    return (2*TP / denom) if denom > 0 else 0.0\n",
    "\n",
    "def _f1_binary(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    TP, FP, FN, TN = _conf_from_preds(y_true, y_pred)\n",
    "    return _f1_from_conf(TP, FP, FN, TN)\n",
    "\n",
    "def _grid(l, r, steps):\n",
    "    l = float(max(0.0, l)); r = float(min(1.0, r))\n",
    "    if l > r: l, r = r, l\n",
    "    return np.linspace(l, r, int(steps), dtype=float)\n",
    "\n",
    "# ---------------- τ最適化（Single / WG-F1 / Group-F1） ----------------\n",
    "def _single_tau_opt(scores: np.ndarray, y: np.ndarray):\n",
    "    # coarse\n",
    "    cands = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    f1_vec = []\n",
    "    for t in cands:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        f1_vec.append(_f1_binary(y, yhat))\n",
    "    f1_vec = np.asarray(f1_vec)\n",
    "    idx = int(np.nanargmax(f1_vec)); tau0 = float(cands[idx]); best0 = float(f1_vec[idx])\n",
    "\n",
    "    # fine around tau0\n",
    "    left  = max(0.0, tau0 - FINE_MARGIN)\n",
    "    right = min(1.0, tau0 + FINE_MARGIN)\n",
    "    cands2 = _grid(left, right, FINE_STEPS)\n",
    "    f1_vec2 = []\n",
    "    for t in cands2:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        f1_vec2.append(_f1_binary(y, yhat))\n",
    "    f1_vec2 = np.asarray(f1_vec2)\n",
    "    idx2 = int(np.nanargmax(f1_vec2)); tau = float(cands2[idx2]); best = float(f1_vec2[idx2])\n",
    "\n",
    "    return {\"tau\": tau, \"F1_val\": best, \"tau_coarse\": tau0, \"F1_coarse\": best0}\n",
    "\n",
    "def _wg_f1_opt_joint(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    \"\"\"\n",
    "    Fair-MinF1（最悪群F1の最大化）を 2Dグリッド（τH×τL）で探索する．\n",
    "    戻り値:\n",
    "      {\"tauH\",\"tauL\",\"F1_H_val\",\"F1_L_val\",\"WG_F1_val\",\"F1_pooled_val\"}\n",
    "    \"\"\"\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[WG-F1] 連結valに High/Low の両群が必要（どちらかが0件）\")\n",
    "\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "\n",
    "    def _f1_vec(s, yy, cands):\n",
    "        f1 = np.empty_like(cands)\n",
    "        for i, t in enumerate(cands):\n",
    "            yhat = (s >= t).astype(int)\n",
    "            f1[i] = _f1_binary(yy, yhat)\n",
    "        return f1\n",
    "\n",
    "    f1H = _f1_vec(sH, yH, candH)\n",
    "    f1L = _f1_vec(sL, yL, candL)\n",
    "\n",
    "    best = {\"WG\": -np.inf, \"pooled\": -np.inf, \"tH\": 0.5, \"tL\": 0.5, \"F1H\": 0.0, \"F1L\": 0.0}\n",
    "    for i, tH in enumerate(candH):\n",
    "        wg_row = np.minimum(f1H[i], f1L)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL[j]).astype(int)\n",
    "        F1_pooled = _f1_binary(\n",
    "            np.concatenate([yH, yL]),\n",
    "            np.concatenate([yhatH, yhatL])\n",
    "        )\n",
    "\n",
    "        cand = {\"WG\": WG, \"pooled\": float(F1_pooled),\n",
    "                \"tH\": float(tH), \"tL\": float(candL[j]),\n",
    "                \"F1H\": float(f1H[i]), \"F1L\": float(f1L[j])}\n",
    "\n",
    "        def _is_better(cur, new):\n",
    "            if new[\"WG\"] > cur[\"WG\"]: return True\n",
    "            if new[\"WG\"] < cur[\"WG\"]: return False\n",
    "            if new[\"pooled\"] > cur[\"pooled\"]: return True\n",
    "            if new[\"pooled\"] < cur[\"pooled\"]: return False\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) < abs(cur[\"tH\"]-cur[\"tL\"]): return True\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) > abs(cur[\"tH\"]-cur[\"tL\"]): return False\n",
    "            if (new[\"tH\"], new[\"tL\"]) < (cur[\"tH\"], cur[\"tL\"]): return True\n",
    "            return False\n",
    "\n",
    "        if _is_better(best, cand):\n",
    "            best = cand\n",
    "\n",
    "    # fine (box refine)\n",
    "    lH = max(0.0, best[\"tH\"] - FINE_MARGIN); rH = min(1.0, best[\"tH\"] + FINE_MARGIN)\n",
    "    lL = max(0.0, best[\"tL\"] - FINE_MARGIN); rL = min(1.0, best[\"tL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS)\n",
    "    candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "\n",
    "    f1H2 = _f1_vec(sH, yH, candH2)\n",
    "    f1L2 = _f1_vec(sL, yL, candL2)\n",
    "\n",
    "    best2 = dict(best)\n",
    "    for i, tH in enumerate(candH2):\n",
    "        wg_row = np.minimum(f1H2[i], f1L2)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL2[j]).astype(int)\n",
    "        F1_pooled = _f1_binary(\n",
    "            np.concatenate([yH, yL]),\n",
    "            np.concatenate([yhatH, yhatL])\n",
    "        )\n",
    "        cand = {\"WG\": WG, \"pooled\": float(F1_pooled),\n",
    "                \"tH\": float(tH), \"tL\": float(candL2[j]),\n",
    "                \"F1H\": float(f1H2[i]), \"F1L\": float(f1L2[j])}\n",
    "\n",
    "        if (cand[\"WG\"] > best2[\"WG\"] or\n",
    "            (cand[\"WG\"] == best2[\"WG\"] and (\n",
    "                cand[\"pooled\"] > best2[\"pooled\"] or\n",
    "                (cand[\"pooled\"] == best2[\"pooled\"] and (\n",
    "                    abs(cand[\"tH\"]-cand[\"tL\"]) < abs(best2[\"tH\"]-best2[\"tL\"]) or\n",
    "                    (abs(cand[\"tH\"]-cand[\"tL\"]) == abs(best2[\"tH\"]-best2[\"tL\"]) and\n",
    "                     (cand[\"tH\"], cand[\"tL\"]) < (best2[\"tH\"], best2[\"tL\"]))\n",
    "                ))\n",
    "            ))):\n",
    "            best2 = cand\n",
    "\n",
    "    return {\n",
    "        \"tauH\": best2[\"tH\"], \"tauL\": best2[\"tL\"],\n",
    "        \"F1_H_val\": best2[\"F1H\"], \"F1_L_val\": best2[\"F1L\"],\n",
    "        \"WG_F1_val\": best2[\"WG\"], \"F1_pooled_val\": best2[\"pooled\"],\n",
    "    }\n",
    "\n",
    "def _group_f1_opt(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    # 2Dグリッド（τH×τL）で pooled F1 最大化（coarse→fine）\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[Group-F1] 連結valに High/Low の両群が必要（どちらかが0件）\")\n",
    "\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "\n",
    "    def _pooled_f1_for(tH, tL):\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= tL).astype(int)\n",
    "        y_true = np.concatenate([yH, yL])\n",
    "        y_pred = np.concatenate([yhatH, yhatL])\n",
    "        return _f1_binary(y_true, y_pred)\n",
    "\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    best = {\"F1_val\": -np.inf, \"tauH\": None, \"tauL\": None}\n",
    "    for tH in candH:\n",
    "        f1_vec = np.empty_like(candL)\n",
    "        for j, tL in enumerate(candL):\n",
    "            f1_vec[j] = _pooled_f1_for(tH, tL)\n",
    "        jmax = int(np.nanargmax(f1_vec))\n",
    "        if float(f1_vec[jmax]) > best[\"F1_val\"]:\n",
    "            best.update({\"F1_val\": float(f1_vec[jmax]), \"tauH\": float(tH), \"tauL\": float(candL[jmax])})\n",
    "\n",
    "    # fine\n",
    "    lH = max(0.0, best[\"tauH\"] - FINE_MARGIN); rH = min(1.0, best[\"tauH\"] + FINE_MARGIN)\n",
    "    lL = max(0.0, best[\"tauL\"] - FINE_MARGIN); rL = min(1.0, best[\"tauL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS); candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "\n",
    "    best2 = dict(best)\n",
    "    for tH in candH2:\n",
    "        f1_vec2 = np.empty_like(candL2)\n",
    "        for j, tL in enumerate(candL2):\n",
    "            f1_vec2[j] = _pooled_f1_for(tH, tL)\n",
    "        jmax = int(np.nanargmax(f1_vec2))\n",
    "        if float(f1_vec2[jmax]) > best2[\"F1_val\"]:\n",
    "            best2.update({\"F1_val\": float(f1_vec2[jmax]), \"tauH\": float(tH), \"tauL\": float(candL2[jmax])})\n",
    "\n",
    "    return best2  # {\"tauH\",\"tauL\",\"F1_val\"}\n",
    "\n",
    "# ---------------- outer LOSO with inner concatenation ----------------\n",
    "logo_outer = LeaveOneGroupOut()\n",
    "rows, pred_rows = [], []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo_outer.split(X_k, y_base.values, g_base.values), start=1):\n",
    "    train_mask = pd.Series(False, index=g_base.index); train_mask.iloc[tr_idx] = True\n",
    "    test_mask  = pd.Series(False, index=g_base.index);  test_mask.iloc[te_idx]  = True\n",
    "    test_sid = g_base.iloc[te_idx].iloc[0]\n",
    "\n",
    "    # ===== inner folds =====\n",
    "    inner_ids   = sorted(g_base[train_mask].unique())\n",
    "    inner_folds = choose_inner_folds_loso(inner_ids)\n",
    "    if VERBOSE:\n",
    "        print(f\"[inner folds] {len(inner_folds)} splits -> val subjects = {', '.join(inner_ids)}\")\n",
    "\n",
    "    # 連結valの器\n",
    "    val_scores_all, val_y_all, val_grp_all = [], [], []\n",
    "\n",
    "    # --- inner train に両群が居るか（群別τの前提） ---\n",
    "    inner_train_groups = fair_groups[train_mask]\n",
    "    if not ((\"High\" in set(inner_train_groups)) and (\"Low\" in set(inner_train_groups))):\n",
    "        raise RuntimeError(f\"[Cell5A] fold{fold_id}: inner-train に両群(High/Low)が必要（群別τの前提）\")\n",
    "\n",
    "    # ===== inner: train→val 予測の連結 =====\n",
    "    for inner_val in inner_folds:\n",
    "        val_mask  = g_base.isin(inner_val) & train_mask\n",
    "        trn_mask  = train_mask & (~val_mask)\n",
    "        if not trn_mask.any() or not val_mask.any():\n",
    "            continue\n",
    "\n",
    "        X_tr, y_tr = X_k[trn_mask], y_base[trn_mask]\n",
    "        X_vl, y_vl = X_k[val_mask], y_base[val_mask]\n",
    "        grp_vl     = fair_groups[val_mask].to_numpy()\n",
    "\n",
    "        # inner 学習\n",
    "        model_inner = fit_classifier(X_tr, y_tr)\n",
    "        sc_vl = predict_positive_score(model_inner, X_vl).astype(float)\n",
    "\n",
    "        # 連結（生の val 予測）\n",
    "        val_scores_all.append(sc_vl)\n",
    "        val_y_all.append(y_vl.to_numpy())\n",
    "        val_grp_all.append(grp_vl)\n",
    "\n",
    "        if VERBOSE:\n",
    "            nH = int((grp_vl==\"High\").sum()); nL = int((grp_vl==\"Low\").sum())\n",
    "            print(f\"[DBG] fold{fold_id}: inner_val size={len(y_vl):3d}  High={nH} Low={nL}\")\n",
    "\n",
    "    # 連結\n",
    "    if len(val_scores_all) == 0:\n",
    "        raise RuntimeError(f\"[Cell5A] fold{fold_id}: inner validation が空である．\")\n",
    "    s_val = np.concatenate(val_scores_all)\n",
    "    y_val = np.concatenate(val_y_all)\n",
    "    g_val = np.concatenate(val_grp_all)\n",
    "\n",
    "    # ===== 連結valで一度だけ τ を最適化（F1） =====\n",
    "    res_single = _single_tau_opt(s_val, y_val)\n",
    "    tau_single = float(res_single[\"tau\"])\n",
    "\n",
    "    res_wg = _wg_f1_opt_joint(s_val, y_val, g_val)\n",
    "    tauH_wg, tauL_wg = float(res_wg[\"tauH\"]), float(res_wg[\"tauL\"])\n",
    "\n",
    "    res_group = _group_f1_opt(s_val, y_val, g_val)\n",
    "    tauH_grp, tauL_grp = float(res_group[\"tauH\"]), float(res_group[\"tauL\"])\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"[DBG] fold{fold_id}: tau_single={tau_single:.6f} | \"\n",
    "              f\"Group-F1(tH,tL)=({tauH_grp:.6f},{tauL_grp:.6f}) | \"\n",
    "              f\"WG-F1(tH,tL)=({tauH_wg:.6f},{tauL_wg:.6f})\")\n",
    "\n",
    "    # ===== outer test =====\n",
    "    X_tr_o, y_tr_o = X_k[train_mask], y_base[train_mask]\n",
    "    X_te_o, y_te_o = X_k[test_mask],  y_base[test_mask]\n",
    "    grp_te         = fair_groups[test_mask].to_numpy()\n",
    "\n",
    "    model_outer = fit_classifier(X_tr_o, y_tr_o)\n",
    "    sc_te = predict_positive_score(model_outer, X_te_o).astype(float)\n",
    "\n",
    "    yhat_single = (sc_te >= tau_single).astype(int)\n",
    "    yhat_groupF1 = (sc_te >= np.where(grp_te==\"High\", tauH_grp, tauL_grp)).astype(int)\n",
    "    yhat_wgF1    = (sc_te >= np.where(grp_te==\"High\", tauH_wg,  tauL_wg )).astype(int)\n",
    "\n",
    "    F1_single = _f1_binary(y_te_o.to_numpy(), yhat_single)\n",
    "    F1_group  = _f1_binary(y_te_o.to_numpy(), yhat_groupF1)\n",
    "    F1_wg     = _f1_binary(y_te_o.to_numpy(), yhat_wgF1)\n",
    "\n",
    "    if VERBOSE:\n",
    "        same_grp = bool(np.array_equal(yhat_single, yhat_groupF1))\n",
    "        same_wg  = bool(np.array_equal(yhat_single, yhat_wgF1))\n",
    "        nH_te = int((grp_te==\"High\").sum()); nL_te = int((grp_te==\"Low\").sum())\n",
    "        print(f\"[DBG] fold{fold_id}: TEST High={nH_te} Low={nL_te} | \"\n",
    "              f\"Single==Group-F1: {same_grp}  Single==WG-F1: {same_wg} | \"\n",
    "              f\"F1(single, group, wg)=({F1_single:.3f}, {F1_group:.3f}, {F1_wg:.3f})\")\n",
    "\n",
    "    rows.append({\n",
    "        \"fold_id\": int(fold_id),\n",
    "        \"test_id\": str(test_sid),\n",
    "        \"best_k\": int(best_k),\n",
    "        \"tau_single\": float(tau_single),\n",
    "        \"tau_high_GroupF1\": float(tauH_grp), \"tau_low_GroupF1\": float(tauL_grp),\n",
    "        \"tau_high_WGF1\": float(tauH_wg),     \"tau_low_WGF1\": float(tauL_wg),\n",
    "        \"F1_single\": float(F1_single),\n",
    "        \"F1_group\":  float(F1_group),\n",
    "        \"F1_group_WG\": float(F1_wg),\n",
    "        \"n_test\": int(len(y_te_o)),\n",
    "    })\n",
    "\n",
    "    # 予測詳細（F1専用命名）\n",
    "    for yy, ss, gg, ys, yg, yw in zip(y_te_o, sc_te, grp_te, yhat_single, yhat_groupF1, yhat_wgF1):\n",
    "        pred_rows.append({\n",
    "            \"fold_id\": int(fold_id),\n",
    "            \"test_id\": str(test_sid),\n",
    "            \"y_true\": int(yy),\n",
    "            \"proba\": float(ss),\n",
    "            \"group\": str(gg),\n",
    "            \"y_pred_single\": int(ys),\n",
    "            \"y_pred_group_F1\": int(yg),\n",
    "            \"y_pred_group_WG\": int(yw),\n",
    "        })\n",
    "    \n",
    "    print(f\"[DBG] fold{fold_id}: WG-joint val -> \"\n",
    "          f\"F1_H={res_wg['F1_H_val']:.3f}, F1_L={res_wg['F1_L_val']:.3f}, \"\n",
    "          f\"WG(F1)={res_wg['WG_F1_val']:.3f}, pooled(F1)={res_wg['F1_pooled_val']:.3f}, \"\n",
    "          f\"(tH,tL)=({tauH_wg:.4f},{tauL_wg:.4f})\")\n",
    "\n",
    "# ---------------- 出力 ----------------\n",
    "df_fold = pd.DataFrame(rows)\n",
    "df_pred = pd.DataFrame(pred_rows)\n",
    "df_fold.to_csv(groupaware_out(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_pred.to_csv(groupaware_out(\"GROUP_AWARE_PREDICTIONS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 参考: プールAUC（確率は共通なので1つ）\n",
    "auc_pool = float(roc_auc_score(df_pred[\"y_true\"].to_numpy(), df_pred[\"proba\"].to_numpy()))\n",
    "\n",
    "def _pooled_f1(col):\n",
    "    y_true = df_pred[\"y_true\"].to_numpy()\n",
    "    yhat   = df_pred[col].to_numpy().astype(int)\n",
    "    return _f1_binary(y_true, yhat)\n",
    "\n",
    "summary = {\n",
    "    \"best_k\": int(best_k),\n",
    "    \"AUC_pooled\": auc_pool,  # 不要ならこのキーごと削除可\n",
    "    \"F1_pooled_single\": _pooled_f1(\"y_pred_single\"),\n",
    "    \"F1_pooled_group\":  _pooled_f1(\"y_pred_group_F1\"),\n",
    "    \"F1_pooled_group_WG\": _pooled_f1(\"y_pred_group_WG\"),\n",
    "    \"metric\": \"F1\",\n",
    "    \"n_samples\": int(len(df_pred)),\n",
    "    \"n_pos\": int((df_pred[\"y_true\"]==1).sum()),\n",
    "    \"n_neg\": int((df_pred[\"y_true\"]==0).sum()),\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(groupaware_out(\"GROUP_AWARE_SUMMARY.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell5A] Done: outer folds={len(df_fold)}, pooled AUC={auc_pool:.3f}\")\n",
    "print(f\"[Cell5A] Summary: {summary}\")\n",
    "\n",
    "# ---------------- 混同行列プロット（F1専用ラベル） ----------------\n",
    "def _draw_cm(cm, title, path):\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\", vmin=0, vmax=max(cm.max(),1))\n",
    "    labels = np.array([[\"TN\",\"FP\"],[\"FN\",\"TP\"]])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = int(cm[i,j])\n",
    "            color = \"white\" if val > 0.6*im.get_clim()[1] else \"black\"\n",
    "            ax.text(j, i, f\"{labels[i,j]}\\n{val}\", ha=\"center\", va=\"center\",\n",
    "                    fontsize=18, fontweight=\"bold\", color=color)\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Pred:0\",\"Pred:1\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"True:0\",\"True:1\"], rotation=90, va=\"center\")\n",
    "    ax.set_title(title); ax.grid(False)\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=300); plt.close()\n",
    "\n",
    "y_pool = df_pred[\"y_true\"].to_numpy()\n",
    "cm_single = skm.confusion_matrix(y_pool, df_pred[\"y_pred_single\"],    labels=[0,1])\n",
    "cm_group  = skm.confusion_matrix(y_pool, df_pred[\"y_pred_group_F1\"],  labels=[0,1])\n",
    "cm_wg     = skm.confusion_matrix(y_pool, df_pred[\"y_pred_group_WG\"],  labels=[0,1])\n",
    "\n",
    "_draw_cm(cm_single, f\"Cell5A Single τ (F1={summary['F1_pooled_single']:.3f})\",\n",
    "         outpath(\"CONFMAT_CELL5A_SINGLE(F1).png\"))\n",
    "_draw_cm(cm_group,  f\"Cell5A Group-F1 τ (F1={summary['F1_pooled_group']:.3f})\",\n",
    "         outpath(\"CONFMAT_CELL5A_GROUP(F1).png\"))\n",
    "_draw_cm(cm_wg,     f\"Cell5A WG-F1 τ (F1={summary['F1_pooled_group_WG']:.3f})\",\n",
    "         outpath(\"CONFMAT_CELL5A_WG(F1).png\"))\n",
    "print(\"[Cell5A] Confusion matrices saved -> CONFMAT_CELL5A_*.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5B++: 確率スコア分布（OVERALL/群別/各Fold）＋ τ の中央値/IQR 帯・±表記 =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 設定 ----------\n",
    "BINS = 40\n",
    "LW = 1.5\n",
    "FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "\n",
    "# 色指定（ユーザ指定）\n",
    "COLOR_SICK = \"red\"   # True:Sick\n",
    "COLOR_NON  = \"blue\"  # True:Non-Sick\n",
    "\n",
    "# 線色（しきい値）\n",
    "COLOR_SINGLE = \"black\"\n",
    "COLOR_GROUP  = \"green\"\n",
    "COLOR_WG     = \"purple\"\n",
    "\n",
    "# 出力フォルダ（新規作成）\n",
    "RUN_ROOT = os.path.dirname(outpath(\"__dummy__\"))\n",
    "IMG_DIR  = os.path.join(RUN_ROOT, \"PROBA_DIST_F1\")\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "# ファイルパス\n",
    "SAVE_OVERALL_SvG   = os.path.join(IMG_DIR, \"OVERALL_SvGroup.png\")\n",
    "SAVE_OVERALL_SvWG  = os.path.join(IMG_DIR, \"OVERALL_SvWG.png\")\n",
    "SAVE_BYGROUP_SvG   = os.path.join(IMG_DIR, \"BYGROUP_SvGroup.png\")\n",
    "SAVE_BYGROUP_SvWG  = os.path.join(IMG_DIR, \"BYGROUP_SvWG.png\")\n",
    "\n",
    "# ---------- 入力 ----------\n",
    "GROUP_AWARE_DIR = os.path.join(OUT_DIR, \"GROUP_AWARE\", \"F1\")\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_path(filename: str) -> str:\n",
    "    return os.path.join(GROUP_AWARE_DIR, filename)\n",
    "\n",
    "pred_path = groupaware_path(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "fold_path = groupaware_path(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "if not (os.path.exists(pred_path) and os.path.exists(fold_path)):\n",
    "    raise FileNotFoundError(\"[Cell5B++] 必要CSVが見つからない（F1版 Cell 5A を先に実行）\")\n",
    "\n",
    "df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- モード自動判定（F1 or BA） ----------\n",
    "cols_f1 = {\"high\":\"tau_high_GroupF1\", \"low\":\"tau_low_GroupF1\", \"wgh\":\"tau_high_WGF1\", \"wgl\":\"tau_low_WGF1\"}\n",
    "cols_ba = {\"high\":\"tau_high_GroupBA\", \"low\":\"tau_low_GroupBA\", \"wgh\":\"tau_high_WGBA\", \"wgl\":\"tau_low_WGBA\"}\n",
    "\n",
    "if all(c in df_fold.columns for c in [cols_f1[\"high\"], cols_f1[\"low\"]]):\n",
    "    mode = \"F1\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_f1[\"high\"], cols_f1[\"low\"], cols_f1[\"wgh\"], cols_f1[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_F1\"\n",
    "elif all(c in df_fold.columns for c in [cols_ba[\"high\"], cols_ba[\"low\"]]):\n",
    "    mode = \"BA\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_ba[\"high\"], cols_ba[\"low\"], cols_ba[\"wgh\"], cols_ba[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_BA\"\n",
    "else:\n",
    "    raise RuntimeError(\"[Cell5B++] しきい値列が見つからない（F1/BAどちらかのCell 5Aの出力が必要）\")\n",
    "\n",
    "# ---------- 集約: 中央値/IQR（Q1〜Q3） ----------\n",
    "def _qstats(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.size == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan  # med, q1, q3, half_iqr\n",
    "    med = float(np.nanmedian(s))\n",
    "    q1, q3 = np.nanpercentile(s, [25, 75])\n",
    "    half = float((q3 - q1)/2.0)\n",
    "    return float(med), float(q1), float(q3), half\n",
    "\n",
    "tau_single_med, tau_single_q1, tau_single_q3, tau_single_half = _qstats(df_fold[\"tau_single\"])\n",
    "tau_high_med,   tau_high_q1,   tau_high_q3,   tau_high_half   = _qstats(df_fold[c_high])\n",
    "tau_low_med,    tau_low_q1,    tau_low_q3,    tau_low_half    = _qstats(df_fold[c_low])\n",
    "tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, tau_high_wg_half = _qstats(df_fold[c_wgh]) if c_wgh in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  tau_low_wg_half  = _qstats(df_fold[c_wgl]) if c_wgl in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "# ---------- データ分解 ----------\n",
    "proba = pd.to_numeric(df_pred[\"proba\"], errors=\"coerce\").values\n",
    "ytrue = pd.to_numeric(df_pred[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "grp   = df_pred[\"group\"].astype(str).str.strip()\n",
    "\n",
    "p_sick = proba[ytrue == 1]   # True:Sick\n",
    "p_non  = proba[ytrue == 0]   # True:Non-Sick\n",
    "n_sick, n_non = len(p_sick), len(p_non)\n",
    "\n",
    "maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "\n",
    "# ---------- ユーティリティ ----------\n",
    "def _style_axes(ax, title=None):\n",
    "    if title: ax.set_title(title, fontsize=FS_TITLE)\n",
    "    ax.set_xlabel(\"Predicted probability\", fontsize=FS_LABEL)\n",
    "    ax.set_ylabel(\"Density\", fontsize=FS_LABEL)\n",
    "    ax.tick_params(axis=\"both\", labelsize=FS_TICK)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "def _hist_overall(ax):\n",
    "    ax.hist(p_sick, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={n_sick})\", color=COLOR_SICK)\n",
    "    ax.hist(p_non,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={n_non})\", color=COLOR_NON)\n",
    "\n",
    "def _hist_bygroup(axes):\n",
    "    # High\n",
    "    p_sick_H = proba[(ytrue==1) & maskH]; p_non_H = proba[(ytrue==0) & maskH]\n",
    "    axes[0].hist(p_sick_H, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_H)})\", color=COLOR_SICK)\n",
    "    axes[0].hist(p_non_H,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_H)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[0], \"High group\")\n",
    "    # Low\n",
    "    p_sick_L = proba[(ytrue==1) & maskL]; p_non_L = proba[(ytrue==0) & maskL]\n",
    "    axes[1].hist(p_sick_L, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_L)})\", color=COLOR_SICK)\n",
    "    axes[1].hist(p_non_L,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_L)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[1], \"Low group\")\n",
    "\n",
    "def _vline_with_iqr(ax, x_med, q1, q3, color, ls, label_core):\n",
    "    if np.isfinite(x_med):\n",
    "        ax.axvline(x_med, color=color, linestyle=ls, linewidth=LW,\n",
    "                   label=f\"{label_core} = {x_med:.3f} ± {(q3-q1)/2:.3f}\" if (np.isfinite(q1) and np.isfinite(q3)) else f\"{label_core} = {x_med:.3f}\")\n",
    "    if np.isfinite(q1) and np.isfinite(q3):\n",
    "        ax.axvspan(q1, q3, color=color, alpha=0.12)\n",
    "\n",
    "# ---------- 1) OVERALL: Single vs Group（中央値・IQR帯・±表記） ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_med,   tau_high_q1,   tau_high_q3,   COLOR_GROUP,  \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(ax, tau_low_med,    tau_low_q1,    tau_low_q3,    COLOR_GROUP,  \"--\", f\"τ_low_{mode}\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs Group [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvG}\")\n",
    "\n",
    "# ---------- 2) OVERALL: Single vs WG（中央値・IQR帯・±表記） ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(ax, tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs WG [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvWG}\")\n",
    "\n",
    "# ---------- 3) BY_GROUP: Single vs Group（各群パネルに該当の IQR帯） ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "# Single は両段に表示\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "# High 段に High の Group τ、Low 段に Low の Group τ\n",
    "_vline_with_iqr(axes[0], tau_high_med, tau_high_q1, tau_high_q3, COLOR_GROUP, \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(axes[1], tau_low_med,  tau_low_q1,  tau_low_q3,  COLOR_GROUP, \"--\", f\"τ_low_{mode}\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvG}\")\n",
    "\n",
    "# ---------- 4) BY_GROUP: Single vs WG（各群パネルに該当の IQR帯） ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[0], tau_high_wg, tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(axes[1], tau_low_wg,  tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvWG}\")\n",
    "\n",
    "# ---------- 5) Fold単位：OVERALL の確率分布と各Foldの τ（Single vs Group / Single vs WG） ----------\n",
    "#   - 各Foldのテストサンプルのみでヒストを作図\n",
    "#   - しきい値は「そのFold行」の値を直接表示（±は不要／IQR帯は使わない）\n",
    "for _, row in df_fold.iterrows():\n",
    "    fid = int(row[\"fold_id\"]) if \"fold_id\" in row else None\n",
    "    test_id = str(row.get(\"test_id\", f\"fold{fid}\"))\n",
    "    sub = df_pred[df_pred[\"fold_id\"] == fid] if \"fold_id\" in df_pred.columns and fid is not None else df_pred.copy()\n",
    "\n",
    "    p = pd.to_numeric(sub[\"proba\"], errors=\"coerce\").values\n",
    "    yt = pd.to_numeric(sub[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "    p_s, p_n = p[yt==1], p[yt==0]\n",
    "\n",
    "    # 値（このFoldの τ）\n",
    "    t_single = float(row[\"tau_single\"])\n",
    "    t_high   = float(row[c_high]) if c_high in row else np.nan\n",
    "    t_low    = float(row[c_low])  if c_low  in row else np.nan\n",
    "    t_high_w = float(row[c_wgh])  if c_wgh  in row else np.nan\n",
    "    t_low_w  = float(row[c_wgl])  if c_wgl  in row else np.nan\n",
    "\n",
    "    # 出力パス\n",
    "    p_sg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvGroup.png\")\n",
    "    p_wg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvWG.png\")\n",
    "\n",
    "    # Single vs Group\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high):   ax.axvline(t_high,   color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_high_{mode} = {t_high:.3f}\")\n",
    "    if np.isfinite(t_low):    ax.axvline(t_low,    color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_low_{mode}  = {t_low:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs Group [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_sg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_sg}\")\n",
    "\n",
    "    # Single vs WG\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high_w): ax.axvline(t_high_w, color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_high_WG = {t_high_w:.3f}\")\n",
    "    if np.isfinite(t_low_w):  ax.axvline(t_low_w,  color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_low_WG  = {t_low_w:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs WG [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_wg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_wg}\")\n",
    "\n",
    "print(f\"[Cell5B++] All images saved in: {IMG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5A (FINAL, BA専用): inner-LOSO τ最適化（連結val）→ outer予測・評価 =====\n",
    "# 前提: Cell1〜4 実行済み（X_all, y_all, groups, SUBJECT_META, BEST_K, choose_inner_folds_loso 等）\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- GROUP_AWARE 出力先（BA） ----------------\n",
    "MODE_TAG = \"BA\"\n",
    "GROUP_AWARE_DIR = os.path.join(OUT_DIR, \"GROUP_AWARE\", MODE_TAG)\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_out(filename: str) -> str:\n",
    "    path = os.path.join(GROUP_AWARE_DIR, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "# ---------------- 基本チェック ----------------\n",
    "req = [\"X_all\",\"y_all\",\"groups\",\"SUBJECT_META\",\n",
    "       \"choose_inner_folds_loso\",\"fit_classifier\",\"predict_positive_score\",\"outpath\"]\n",
    "missing = [v for v in req if v not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f(\"[Cell5A] 未定義の変数/関数があります: {missing}\"))\n",
    "\n",
    "if \"MSSQ_group\" not in SUBJECT_META.columns and \"MSSQ_group\" not in SUBJECT_META.index.names:\n",
    "    raise RuntimeError(\"[Cell5A] SUBJECT_META に MSSQ_group 列（または index）が必要である．\")\n",
    "\n",
    "# ---------------- 設定（等間隔グリッド＋近傍再探索） ----------------\n",
    "COARSE_STEPS = 1001      # 0.0〜1.0 を等間隔\n",
    "FINE_STEPS   = 101       # 近傍再探索の細かさ\n",
    "FINE_MARGIN  = 0.01      # 近傍幅（±0.01）\n",
    "VERBOSE      = True\n",
    "\n",
    "# ---------------- 入力整形 ----------------\n",
    "X_base = X_all.astype(np.float32)\n",
    "y_base = y_all.astype(int)\n",
    "g_base = groups.astype(str)\n",
    "\n",
    "# MSSQ group マッピング（High/Lowに正規化）\n",
    "if \"subject_id\" in SUBJECT_META.columns:\n",
    "    mapper = SUBJECT_META.set_index(\"subject_id\")[\"MSSQ_group\"].astype(str).to_dict()\n",
    "else:\n",
    "    mapper = SUBJECT_META[\"MSSQ_group\"].astype(str).to_dict()\n",
    "\n",
    "fair_groups = g_base.map(mapper).astype(str).str.strip().str.lower().map({\"high\":\"High\",\"low\":\"Low\"})\n",
    "if fair_groups.isna().any():\n",
    "    raise RuntimeError(f\"[Cell5A] MSSQ_group 未割当ID: {sorted(set(g_base[fair_groups.isna()]))}\")\n",
    "\n",
    "\n",
    "# 特徴選抜（SHAP順の上位K → サブセット優先）\n",
    "subset_primary = f\"TOP{int(globals().get('TOP_SUBSET_K', 15))}_SUBSET_BEST.json\"\n",
    "subset_candidates = [subset_primary, \"TOP10_SUBSET_BEST.json\"]\n",
    "subset_json = None\n",
    "for name in subset_candidates:\n",
    "    cand = outpath(name)\n",
    "    if os.path.exists(cand):\n",
    "        subset_json = cand\n",
    "        break\n",
    "feature_order = []\n",
    "if subset_json is not None:\n",
    "    try:\n",
    "        with open(subset_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            subset_info = json.load(f)\n",
    "        feature_order = [f for f in subset_info.get(\"features\", []) if f in X_base.columns]\n",
    "        if feature_order:\n",
    "            best_k = len(feature_order)\n",
    "            print(f\"[Cell5A-BA] Using subset features from {os.path.basename(subset_json)} (k={best_k})\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Cell5A-BA][WARN] subset読み込み失敗: {e}\")\n",
    "if not feature_order:\n",
    "    rank_df = pd.read_csv(outpath(\"SHAP_FEATURE_RANKING.CSV\"), index_col=0, encoding=\"utf-8-sig\")\n",
    "    feature_order = [f for f in rank_df.index if f in X_base.columns]\n",
    "    if not feature_order:\n",
    "        raise RuntimeError(\"[Cell5A-BA] ランキング上位に X に無い特徴があります。\")\n",
    "    if \"best_k\" not in globals():\n",
    "        raise RuntimeError(\"[Cell5A-BA] best_k が未定義です。Cell3B を実行してください。\")\n",
    "    best_k = int(best_k)\n",
    "    feats_k = feature_order[:best_k]\n",
    "else:\n",
    "    feats_k = feature_order\n",
    "X_k = X_base[feats_k]\n",
    "\n",
    "if VERBOSE:\n",
    "    print(f\"[DBG] 使用特徴数: {len(feats_k)}  (BEST_K={best_k})\")\n",
    "\n",
    "# ---------------- 指標・評価ユーティリティ（BA専用） ----------------\n",
    "def _conf_from_preds(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    TN, FP, FN, TP = skm.confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    return TP, FP, FN, TN\n",
    "\n",
    "def _ba_from_conf(TP, FP, FN, TN) -> float:\n",
    "    TP = float(TP); FP = float(FP); FN = float(FN); TN = float(TN)\n",
    "    # Sensitivity/TPR と Specificity/TNR\n",
    "    tpr = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "    tnr = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "    return 0.5 * (tpr + tnr)\n",
    "\n",
    "def _ba_binary(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    TP, FP, FN, TN = _conf_from_preds(y_true, y_pred)\n",
    "    return _ba_from_conf(TP, FP, FN, TN)\n",
    "\n",
    "def _grid(l, r, steps):\n",
    "    l = float(max(0.0, l)); r = float(min(1.0, r))\n",
    "    if l > r: l, r = r, l\n",
    "    return np.linspace(l, r, int(steps), dtype=float)\n",
    "\n",
    "# ---------------- τ最適化（Single / WG-BA / Group-BA） ----------------\n",
    "def _single_tau_opt(scores: np.ndarray, y: np.ndarray):\n",
    "    # coarse\n",
    "    cands = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    ba_vec = []\n",
    "    for t in cands:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        ba_vec.append(_ba_binary(y, yhat))\n",
    "    ba_vec = np.asarray(ba_vec)\n",
    "    idx = int(np.nanargmax(ba_vec)); tau0 = float(cands[idx]); best0 = float(ba_vec[idx])\n",
    "\n",
    "    # fine around tau0\n",
    "    left  = max(0.0, tau0 - FINE_MARGIN)\n",
    "    right = min(1.0, tau0 + FINE_MARGIN)\n",
    "    cands2 = _grid(left, right, FINE_STEPS)\n",
    "    ba_vec2 = []\n",
    "    for t in cands2:\n",
    "        yhat = (scores >= t).astype(int)\n",
    "        ba_vec2.append(_ba_binary(y, yhat))\n",
    "    ba_vec2 = np.asarray(ba_vec2)\n",
    "    idx2 = int(np.nanargmax(ba_vec2)); tau = float(cands2[idx2]); best = float(ba_vec2[idx2])\n",
    "\n",
    "    return {\"tau\": tau, \"BA_val\": best, \"tau_coarse\": tau0, \"BA_coarse\": best0}\n",
    "\n",
    "def _wg_ba_opt_joint(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    \"\"\"\n",
    "    Fair-MinBA（最悪群BAの最大化）を 2Dグリッド（τH×τL）で探索する．\n",
    "    戻り値: {\"tauH\",\"tauL\",\"BA_H_val\",\"BA_L_val\",\"WG_BA_val\",\"BA_pooled_val\"}\n",
    "    \"\"\"\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[WG-BA] 連結valに High/Low の両群が必要（どちらかが0件）\")\n",
    "\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "\n",
    "    def _ba_vec(s, yy, cands):\n",
    "        ba = np.empty_like(cands)\n",
    "        for i, t in enumerate(cands):\n",
    "            yhat = (s >= t).astype(int)\n",
    "            ba[i] = _ba_binary(yy, yhat)\n",
    "        return ba\n",
    "\n",
    "    baH = _ba_vec(sH, yH, candH)\n",
    "    baL = _ba_vec(sL, yL, candL)\n",
    "\n",
    "    best = {\"WG\": -np.inf, \"pooled\": -np.inf, \"tH\": 0.5, \"tL\": 0.5, \"BAH\": 0.0, \"BAL\": 0.0}\n",
    "    for i, tH in enumerate(candH):\n",
    "        wg_row = np.minimum(baH[i], baL)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL[j]).astype(int)\n",
    "        BA_pooled = _ba_binary(\n",
    "            np.concatenate([yH, yL]),\n",
    "            np.concatenate([yhatH, yhatL])\n",
    "        )\n",
    "\n",
    "        cand = {\"WG\": WG, \"pooled\": float(BA_pooled),\n",
    "                \"tH\": float(tH), \"tL\": float(candL[j]),\n",
    "                \"BAH\": float(baH[i]), \"BAL\": float(baL[j])}\n",
    "\n",
    "        # tie-break: pooled BA → |τH-τL| 小 → (τH,τL) 辞書順小\n",
    "        def _is_better(cur, new):\n",
    "            if new[\"WG\"] > cur[\"WG\"]: return True\n",
    "            if new[\"WG\"] < cur[\"WG\"]: return False\n",
    "            if new[\"pooled\"] > cur[\"pooled\"]: return True\n",
    "            if new[\"pooled\"] < cur[\"pooled\"]: return False\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) < abs(cur[\"tH\"]-cur[\"tL\"]): return True\n",
    "            if abs(new[\"tH\"]-new[\"tL\"]) > abs(cur[\"tH\"]-cur[\"tL\"]): return False\n",
    "            if (new[\"tH\"], new[\"tL\"]) < (cur[\"tH\"], cur[\"tL\"]): return True\n",
    "            return False\n",
    "\n",
    "        if _is_better(best, cand):\n",
    "            best = cand\n",
    "\n",
    "    # fine (box refine)\n",
    "    lH = max(0.0, best[\"tH\"] - FINE_MARGIN); rH = min(1.0, best[\"tH\"] + FINE_MARGIN)\n",
    "    lL = max(0.0, best[\"tL\"] - FINE_MARGIN); rL = min(1.0, best[\"tL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS)\n",
    "    candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "\n",
    "    baH2 = _ba_vec(sH, yH, candH2)\n",
    "    baL2 = _ba_vec(sL, yL, candL2)\n",
    "\n",
    "    best2 = dict(best)\n",
    "    for i, tH in enumerate(candH2):\n",
    "        wg_row = np.minimum(baH2[i], baL2)\n",
    "        j = int(np.nanargmax(wg_row))\n",
    "        WG = float(wg_row[j])\n",
    "\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= candL2[j]).astype(int)\n",
    "        BA_pooled = _ba_binary(\n",
    "            np.concatenate([yH, yL]),\n",
    "            np.concatenate([yhatH, yhatL])\n",
    "        )\n",
    "        cand = {\"WG\": WG, \"pooled\": float(BA_pooled),\n",
    "                \"tH\": float(tH), \"tL\": float(candL2[j]),\n",
    "                \"BAH\": float(baH2[i]), \"BAL\": float(baL2[j])}\n",
    "\n",
    "        if (cand[\"WG\"] > best2[\"WG\"] or\n",
    "            (cand[\"WG\"] == best2[\"WG\"] and (\n",
    "                cand[\"pooled\"] > best2[\"pooled\"] or\n",
    "                (cand[\"pooled\"] == best2[\"pooled\"] and (\n",
    "                    abs(cand[\"tH\"]-cand[\"tL\"]) < abs(best2[\"tH\"]-best2[\"tL\"]) or\n",
    "                    (abs(cand[\"tH\"]-cand[\"tL\"]) == abs(best2[\"tH\"]-best2[\"tL\"]) and\n",
    "                     (cand[\"tH\"], cand[\"tL\"]) < (best2[\"tH\"], best2[\"tL\"]))\n",
    "                ))\n",
    "            ))):\n",
    "            best2 = cand\n",
    "\n",
    "    return {\n",
    "        \"tauH\": best2[\"tH\"], \"tauL\": best2[\"tL\"],\n",
    "        \"BA_H_val\": best2[\"BAH\"], \"BA_L_val\": best2[\"BAL\"],\n",
    "        \"WG_BA_val\": best2[\"WG\"], \"BA_pooled_val\": best2[\"pooled\"],\n",
    "    }\n",
    "\n",
    "def _group_ba_opt(scores: np.ndarray, y: np.ndarray, grp: np.ndarray):\n",
    "    # 2Dグリッド（τH×τL）で pooled BA 最大化（coarse→fine）\n",
    "    maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "    if (maskH.sum()==0) or (maskL.sum()==0):\n",
    "        raise RuntimeError(\"[Group-BA] 連結valに High/Low の両群が必要（どちらかが0件）\")\n",
    "\n",
    "    sH, yH = scores[maskH], y[maskH]\n",
    "    sL, yL = scores[maskL], y[maskL]\n",
    "\n",
    "    def _pooled_ba_for(tH, tL):\n",
    "        yhatH = (sH >= tH).astype(int)\n",
    "        yhatL = (sL >= tL).astype(int)\n",
    "        y_true = np.concatenate([yH, yL])\n",
    "        y_pred = np.concatenate([yhatH, yhatL])\n",
    "        return _ba_binary(y_true, y_pred)\n",
    "\n",
    "    candH = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    candL = _grid(0.0, 1.0, COARSE_STEPS)\n",
    "    best = {\"BA_val\": -np.inf, \"tauH\": None, \"tauL\": None}\n",
    "    for tH in candH:\n",
    "        ba_vec = np.empty_like(candL)\n",
    "        for j, tL in enumerate(candL):\n",
    "            ba_vec[j] = _pooled_ba_for(tH, tL)\n",
    "        jmax = int(np.nanargmax(ba_vec))\n",
    "        if float(ba_vec[jmax]) > best[\"BA_val\"]:\n",
    "            best.update({\"BA_val\": float(ba_vec[jmax]), \"tauH\": float(tH), \"tauL\": float(candL[jmax])})\n",
    "\n",
    "    # fine\n",
    "    lH = max(0.0, best[\"tauH\"] - FINE_MARGIN); rH = min(1.0, best[\"tauH\"] + FINE_MARGIN)\n",
    "    lL = max(0.0, best[\"tauL\"] - FINE_MARGIN); rL = min(1.0, best[\"tauL\"] + FINE_MARGIN)\n",
    "    candH2 = _grid(lH, rH, FINE_STEPS); candL2 = _grid(lL, rL, FINE_STEPS)\n",
    "\n",
    "    best2 = dict(best)\n",
    "    for tH in candH2:\n",
    "        ba_vec2 = np.empty_like(candL2)\n",
    "        for j, tL in enumerate(candL2):\n",
    "            ba_vec2[j] = _pooled_ba_for(tH, tL)\n",
    "        jmax = int(np.nanargmax(ba_vec2))\n",
    "        if float(ba_vec2[jmax]) > best2[\"BA_val\"]:\n",
    "            best2.update({\"BA_val\": float(ba_vec2[jmax]), \"tauH\": float(tH), \"tauL\": float(candL2[jmax])})\n",
    "\n",
    "    return best2  # {\"tauH\",\"tauL\",\"BA_val\"}\n",
    "\n",
    "# ---------------- outer LOSO with inner concatenation ----------------\n",
    "logo_outer = LeaveOneGroupOut()\n",
    "rows, pred_rows = [], []\n",
    "\n",
    "for fold_id, (tr_idx, te_idx) in enumerate(logo_outer.split(X_k, y_base.values, g_base.values), start=1):\n",
    "    train_mask = pd.Series(False, index=g_base.index); train_mask.iloc[tr_idx] = True\n",
    "    test_mask  = pd.Series(False, index=g_base.index);  test_mask.iloc[te_idx]  = True\n",
    "    test_sid = g_base.iloc[te_idx].iloc[0]\n",
    "\n",
    "    # ===== inner folds =====\n",
    "    inner_ids   = sorted(g_base[train_mask].unique())\n",
    "    inner_folds = choose_inner_folds_loso(inner_ids)\n",
    "    if VERBOSE:\n",
    "        print(f\"[inner folds] {len(inner_folds)} splits -> val subjects = {', '.join(inner_ids)}\")\n",
    "\n",
    "    # 連結valの器\n",
    "    val_scores_all, val_y_all, val_grp_all = [], [], []\n",
    "\n",
    "    # --- inner train に両群が居るか（群別τの前提） ---\n",
    "    inner_train_groups = fair_groups[train_mask]\n",
    "    if not ((\"High\" in set(inner_train_groups)) and (\"Low\" in set(inner_train_groups))):\n",
    "        raise RuntimeError(f\"[Cell5A] fold{fold_id}: inner-train に両群(High/Low)が必要（群別τの前提）\")\n",
    "\n",
    "    # ===== inner: train→val 予測の連結 =====\n",
    "    for inner_val in inner_folds:\n",
    "        val_mask  = g_base.isin(inner_val) & train_mask\n",
    "        trn_mask  = train_mask & (~val_mask)\n",
    "        if not trn_mask.any() or not val_mask.any():\n",
    "            continue\n",
    "\n",
    "        X_tr, y_tr = X_k[trn_mask], y_base[trn_mask]\n",
    "        X_vl, y_vl = X_k[val_mask], y_base[val_mask]\n",
    "        grp_vl     = fair_groups[val_mask].to_numpy()\n",
    "\n",
    "        # inner 学習\n",
    "        model_inner = fit_classifier(X_tr, y_tr)\n",
    "        sc_vl = predict_positive_score(model_inner, X_vl).astype(float)\n",
    "\n",
    "        # 連結（生の val 予測）\n",
    "        val_scores_all.append(sc_vl)\n",
    "        val_y_all.append(y_vl.to_numpy())\n",
    "        val_grp_all.append(grp_vl)\n",
    "\n",
    "        if VERBOSE:\n",
    "            nH = int((grp_vl==\"High\").sum()); nL = int((grp_vl==\"Low\").sum())\n",
    "            print(f\"[DBG] fold{fold_id}: inner_val size={len(y_vl):3d}  High={nH} Low={nL}\")\n",
    "\n",
    "    # 連結\n",
    "    if len(val_scores_all) == 0:\n",
    "        raise RuntimeError(f\"[Cell5A] fold{fold_id}: inner validation が空である．\")\n",
    "    s_val = np.concatenate(val_scores_all)\n",
    "    y_val = np.concatenate(val_y_all)\n",
    "    g_val = np.concatenate(val_grp_all)\n",
    "\n",
    "    # ===== 連結valで一度だけ τ を最適化（BA） =====\n",
    "    res_single = _single_tau_opt(s_val, y_val)\n",
    "    tau_single = float(res_single[\"tau\"])\n",
    "\n",
    "    res_wg = _wg_ba_opt_joint(s_val, y_val, g_val)\n",
    "    tauH_wg, tauL_wg = float(res_wg[\"tauH\"]), float(res_wg[\"tauL\"])\n",
    "\n",
    "    res_group = _group_ba_opt(s_val, y_val, g_val)\n",
    "    tauH_grp, tauL_grp = float(res_group[\"tauH\"]), float(res_group[\"tauL\"])\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"[DBG] fold{fold_id}: tau_single={tau_single:.6f} | \"\n",
    "              f\"Group-BA(tH,tL)=({tauH_grp:.6f},{tauL_grp:.6f}) | \"\n",
    "              f\"WG-BA(tH,tL)=({tauH_wg:.6f},{tauL_wg:.6f})\")\n",
    "\n",
    "    # ===== outer test =====\n",
    "    X_tr_o, y_tr_o = X_k[train_mask], y_base[train_mask]\n",
    "    X_te_o, y_te_o = X_k[test_mask],  y_base[test_mask]\n",
    "    grp_te         = fair_groups[test_mask].to_numpy()\n",
    "\n",
    "    model_outer = fit_classifier(X_tr_o, y_tr_o)\n",
    "    sc_te = predict_positive_score(model_outer, X_te_o).astype(float)\n",
    "\n",
    "    yhat_single = (sc_te >= tau_single).astype(int)\n",
    "    yhat_groupBA = (sc_te >= np.where(grp_te==\"High\", tauH_grp, tauL_grp)).astype(int)\n",
    "    yhat_wgBA    = (sc_te >= np.where(grp_te==\"High\", tauH_wg,  tauL_wg )).astype(int)\n",
    "\n",
    "    BA_single = _ba_binary(y_te_o.to_numpy(), yhat_single)\n",
    "    BA_group  = _ba_binary(y_te_o.to_numpy(), yhat_groupBA)\n",
    "    BA_wg     = _ba_binary(y_te_o.to_numpy(), yhat_wgBA)\n",
    "\n",
    "    if VERBOSE:\n",
    "        same_grp = bool(np.array_equal(yhat_single, yhat_groupBA))\n",
    "        same_wg  = bool(np.array_equal(yhat_single, yhat_wgBA))\n",
    "        nH_te = int((grp_te==\"High\").sum()); nL_te = int((grp_te==\"Low\").sum())\n",
    "        print(f\"[DBG] fold{fold_id}: TEST High={nH_te} Low={nL_te} | \"\n",
    "              f\"Single==Group-BA: {same_grp}  Single==WG-BA: {same_wg} | \"\n",
    "              f\"BA(single, group, wg)=({BA_single:.3f}, {BA_group:.3f}, {BA_wg:.3f})\")\n",
    "\n",
    "    rows.append({\n",
    "        \"fold_id\": int(fold_id),\n",
    "        \"test_id\": str(test_sid),\n",
    "        \"best_k\": int(best_k),\n",
    "        \"tau_single\": float(tau_single),\n",
    "        \"tau_high_GroupBA\": float(tauH_grp), \"tau_low_GroupBA\": float(tauL_grp),\n",
    "        \"tau_high_WGBA\": float(tauH_wg),     \"tau_low_WGBA\": float(tauL_wg),\n",
    "        \"BA_single\": float(BA_single),\n",
    "        \"BA_group\":  float(BA_group),\n",
    "        \"BA_group_WG\": float(BA_wg),\n",
    "        \"n_test\": int(len(y_te_o)),\n",
    "    })\n",
    "\n",
    "    # 予測詳細（BA専用命名）\n",
    "    for yy, ss, gg, ys, yg, yw in zip(y_te_o, sc_te, grp_te, yhat_single, yhat_groupBA, yhat_wgBA):\n",
    "        pred_rows.append({\n",
    "            \"fold_id\": int(fold_id),\n",
    "            \"test_id\": str(test_sid),\n",
    "            \"y_true\": int(yy),\n",
    "            \"proba\": float(ss),\n",
    "            \"group\": str(gg),\n",
    "            \"y_pred_single\": int(ys),\n",
    "            \"y_pred_group_BA\": int(yg),\n",
    "            \"y_pred_group_WG\": int(yw),\n",
    "        })\n",
    "    \n",
    "    print(f\"[DBG] fold{fold_id}: WG-joint val -> \"\n",
    "          f\"BA_H={res_wg['BA_H_val']:.3f}, BA_L={res_wg['BA_L_val']:.3f}, \"\n",
    "          f\"WG(BA)={res_wg['WG_BA_val']:.3f}, pooled(BA)={res_wg['BA_pooled_val']:.3f}, \"\n",
    "          f\"(tH,tL)=({tauH_wg:.4f},{tauL_wg:.4f})\")\n",
    "\n",
    "# ---------------- 出力 ----------------\n",
    "df_fold = pd.DataFrame(rows)\n",
    "df_pred = pd.DataFrame(pred_rows)\n",
    "df_fold.to_csv(groupaware_out(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "df_pred.to_csv(groupaware_out(\"GROUP_AWARE_PREDICTIONS.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 参考: プールAUC（確率は共通なので1つ）\n",
    "auc_pool = float(roc_auc_score(df_pred[\"y_true\"].to_numpy(), df_pred[\"proba\"].to_numpy()))\n",
    "\n",
    "def _pooled_ba(col):\n",
    "    y_true = df_pred[\"y_true\"].to_numpy()\n",
    "    yhat   = df_pred[col].to_numpy().astype(int)\n",
    "    return _ba_binary(y_true, yhat)\n",
    "\n",
    "summary = {\n",
    "    \"best_k\": int(best_k),\n",
    "    \"AUC_pooled\": auc_pool,  # 不要ならこのキーごと削除可\n",
    "    \"BA_pooled_single\": _pooled_ba(\"y_pred_single\"),\n",
    "    \"BA_pooled_group\":  _pooled_ba(\"y_pred_group_BA\"),\n",
    "    \"BA_pooled_group_WG\": _pooled_ba(\"y_pred_group_WG\"),\n",
    "    \"metric\": \"BA\",\n",
    "    \"n_samples\": int(len(df_pred)),\n",
    "    \"n_pos\": int((df_pred[\"y_true\"]==1).sum()),\n",
    "    \"n_neg\": int((df_pred[\"y_true\"]==0).sum()),\n",
    "}\n",
    "pd.DataFrame([summary]).to_csv(groupaware_out(\"GROUP_AWARE_SUMMARY.CSV\"), index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[Cell5A] Done: outer folds={len(df_fold)}, pooled AUC={auc_pool:.3f}\")\n",
    "print(f\"[Cell5A] Summary: {summary}\")\n",
    "\n",
    "# ---------------- 混同行列プロット（BA専用タイトル） ----------------\n",
    "def _draw_cm(cm, title, path):\n",
    "    fig, ax = plt.subplots(figsize=(7,6))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\", vmin=0, vmax=max(cm.max(),1))\n",
    "    labels = np.array([[\"TN\",\"FP\"],[\"FN\",\"TP\"]])\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            val = int(cm[i,j])\n",
    "            color = \"white\" if val > 0.6*im.get_clim()[1] else \"black\"\n",
    "            ax.text(j, i, f\"{labels[i,j]}\\n{val}\", ha=\"center\", va=\"center\",\n",
    "                    fontsize=18, fontweight=\"bold\", color=color)\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Pred:0\",\"Pred:1\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"True:0\",\"True:1\"], rotation=90, va=\"center\")\n",
    "    ax.set_title(title); ax.grid(False)\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=300); plt.close()\n",
    "\n",
    "y_pool = df_pred[\"y_true\"].to_numpy()\n",
    "cm_single = skm.confusion_matrix(y_pool, df_pred[\"y_pred_single\"],   labels=[0,1])\n",
    "cm_group  = skm.confusion_matrix(y_pool, df_pred[\"y_pred_group_BA\"], labels=[0,1])\n",
    "cm_wg     = skm.confusion_matrix(y_pool, df_pred[\"y_pred_group_WG\"], labels=[0,1])\n",
    "\n",
    "_draw_cm(cm_single, f\"Cell5A Single τ (BA={summary['BA_pooled_single']:.3f})\",\n",
    "         outpath(\"CONFMAT_CELL5A_SINGLE(BA).png\"))\n",
    "_draw_cm(cm_group,  f\"Cell5A Group-BA τ (BA={summary['BA_pooled_group']:.3f})\",\n",
    "         outpath(\"CONFMAT_CELL5A_GROUP(BA).png\"))\n",
    "_draw_cm(cm_wg,     f\"Cell5A WG-BA τ (BA={summary['BA_pooled_group_WG']:.3f})\",\n",
    "         outpath(\"CONFMAT_CELL5A_WG(BA).png\"))\n",
    "print(\"[Cell5A] Confusion matrices saved -> CONFMAT_CELL5A_*.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6eb4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 5B++: 確率スコア分布（OVERALL/群別/各Fold）＋ τ の中央値/IQR 帯・±表記 =====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- 設定 ----------\n",
    "BINS = 40\n",
    "LW = 1.5\n",
    "FS_TITLE, FS_LABEL, FS_LEGEND, FS_TICK = 30, 24, 20, 20\n",
    "\n",
    "# 色指定（ユーザ指定）\n",
    "COLOR_SICK = \"red\"   # True:Sick\n",
    "COLOR_NON  = \"blue\"  # True:Non-Sick\n",
    "\n",
    "# 線色（しきい値）\n",
    "COLOR_SINGLE = \"black\"\n",
    "COLOR_GROUP  = \"green\"\n",
    "COLOR_WG     = \"purple\"\n",
    "\n",
    "# 出力フォルダ（新規作成）\n",
    "RUN_ROOT = os.path.dirname(outpath(\"__dummy__\"))\n",
    "IMG_DIR  = os.path.join(RUN_ROOT, \"PROBA_DIST_BA\")\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "\n",
    "# ファイルパス\n",
    "SAVE_OVERALL_SvG   = os.path.join(IMG_DIR, \"OVERALL_SvGroup.png\")\n",
    "SAVE_OVERALL_SvWG  = os.path.join(IMG_DIR, \"OVERALL_SvWG.png\")\n",
    "SAVE_BYGROUP_SvG   = os.path.join(IMG_DIR, \"BYGROUP_SvGroup.png\")\n",
    "SAVE_BYGROUP_SvWG  = os.path.join(IMG_DIR, \"BYGROUP_SvWG.png\")\n",
    "\n",
    "# ---------- 入力 ----------\n",
    "GROUP_AWARE_DIR = os.path.join(OUT_DIR, \"GROUP_AWARE\", \"BA\")\n",
    "os.makedirs(GROUP_AWARE_DIR, exist_ok=True)\n",
    "\n",
    "def groupaware_path(filename: str) -> str:\n",
    "    return os.path.join(GROUP_AWARE_DIR, filename)\n",
    "\n",
    "pred_path = groupaware_path(\"GROUP_AWARE_PREDICTIONS.CSV\")\n",
    "fold_path = groupaware_path(\"GROUP_AWARE_THRESH_BY_FOLD.CSV\")\n",
    "if not (os.path.exists(pred_path) and os.path.exists(fold_path)):\n",
    "    raise FileNotFoundError(\"[Cell5B++] 必要CSVが見つからない（BA版 Cell 5A を先に実行）\")\n",
    "\n",
    "df_pred = pd.read_csv(pred_path, encoding=\"utf-8-sig\")\n",
    "df_fold = pd.read_csv(fold_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- モード自動判定（F1 or BA） ----------\n",
    "cols_f1 = {\"high\":\"tau_high_GroupF1\", \"low\":\"tau_low_GroupF1\", \"wgh\":\"tau_high_WGF1\", \"wgl\":\"tau_low_WGF1\"}\n",
    "cols_ba = {\"high\":\"tau_high_GroupBA\", \"low\":\"tau_low_GroupBA\", \"wgh\":\"tau_high_WGBA\", \"wgl\":\"tau_low_WGBA\"}\n",
    "\n",
    "if all(c in df_fold.columns for c in [cols_f1[\"high\"], cols_f1[\"low\"]]):\n",
    "    mode = \"F1\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_f1[\"high\"], cols_f1[\"low\"], cols_f1[\"wgh\"], cols_f1[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_F1\"\n",
    "elif all(c in df_fold.columns for c in [cols_ba[\"high\"], cols_ba[\"low\"]]):\n",
    "    mode = \"BA\"\n",
    "    c_high, c_low, c_wgh, c_wgl = cols_ba[\"high\"], cols_ba[\"low\"], cols_ba[\"wgh\"], cols_ba[\"wgl\"]\n",
    "    pred_group_col = \"y_pred_group_BA\"\n",
    "else:\n",
    "    raise RuntimeError(\"[Cell5B++] しきい値列が見つからない（F1/BAどちらかのCell 5Aの出力が必要）\")\n",
    "\n",
    "# ---------- 集約: 中央値/IQR（Q1〜Q3） ----------\n",
    "def _qstats(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    s = s[np.isfinite(s)]\n",
    "    if s.size == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan  # med, q1, q3, half_iqr\n",
    "    med = float(np.nanmedian(s))\n",
    "    q1, q3 = np.nanpercentile(s, [25, 75])\n",
    "    half = float((q3 - q1)/2.0)\n",
    "    return float(med), float(q1), float(q3), half\n",
    "\n",
    "tau_single_med, tau_single_q1, tau_single_q3, tau_single_half = _qstats(df_fold[\"tau_single\"])\n",
    "tau_high_med,   tau_high_q1,   tau_high_q3,   tau_high_half   = _qstats(df_fold[c_high])\n",
    "tau_low_med,    tau_low_q1,    tau_low_q3,    tau_low_half    = _qstats(df_fold[c_low])\n",
    "tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, tau_high_wg_half = _qstats(df_fold[c_wgh]) if c_wgh in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  tau_low_wg_half  = _qstats(df_fold[c_wgl]) if c_wgl in df_fold.columns else (np.nan, np.nan, np.nan, np.nan)\n",
    "\n",
    "# ---------- データ分解 ----------\n",
    "proba = pd.to_numeric(df_pred[\"proba\"], errors=\"coerce\").values\n",
    "ytrue = pd.to_numeric(df_pred[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "grp   = df_pred[\"group\"].astype(str).str.strip()\n",
    "\n",
    "p_sick = proba[ytrue == 1]   # True:Sick\n",
    "p_non  = proba[ytrue == 0]   # True:Non-Sick\n",
    "n_sick, n_non = len(p_sick), len(p_non)\n",
    "\n",
    "maskH = (grp == \"High\"); maskL = (grp == \"Low\")\n",
    "\n",
    "# ---------- ユーティリティ ----------\n",
    "def _style_axes(ax, title=None):\n",
    "    if title: ax.set_title(title, fontsize=FS_TITLE)\n",
    "    ax.set_xlabel(\"Predicted probability\", fontsize=FS_LABEL)\n",
    "    ax.set_ylabel(\"Density\", fontsize=FS_LABEL)\n",
    "    ax.tick_params(axis=\"both\", labelsize=FS_TICK)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "def _hist_overall(ax):\n",
    "    ax.hist(p_sick, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={n_sick})\", color=COLOR_SICK)\n",
    "    ax.hist(p_non,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={n_non})\", color=COLOR_NON)\n",
    "\n",
    "def _hist_bygroup(axes):\n",
    "    # High\n",
    "    p_sick_H = proba[(ytrue==1) & maskH]; p_non_H = proba[(ytrue==0) & maskH]\n",
    "    axes[0].hist(p_sick_H, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_H)})\", color=COLOR_SICK)\n",
    "    axes[0].hist(p_non_H,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_H)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[0], \"High group\")\n",
    "    # Low\n",
    "    p_sick_L = proba[(ytrue==1) & maskL]; p_non_L = proba[(ytrue==0) & maskL]\n",
    "    axes[1].hist(p_sick_L, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_sick_L)})\", color=COLOR_SICK)\n",
    "    axes[1].hist(p_non_L,  bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_non_L)})\", color=COLOR_NON)\n",
    "    _style_axes(axes[1], \"Low group\")\n",
    "\n",
    "def _vline_with_iqr(ax, x_med, q1, q3, color, ls, label_core):\n",
    "    if np.isfinite(x_med):\n",
    "        ax.axvline(x_med, color=color, linestyle=ls, linewidth=LW,\n",
    "                   label=f\"{label_core} = {x_med:.3f} ± {(q3-q1)/2:.3f}\" if (np.isfinite(q1) and np.isfinite(q3)) else f\"{label_core} = {x_med:.3f}\")\n",
    "    if np.isfinite(q1) and np.isfinite(q3):\n",
    "        ax.axvspan(q1, q3, color=color, alpha=0.12)\n",
    "\n",
    "# ---------- 1) OVERALL: Single vs Group（中央値・IQR帯・±表記） ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_med,   tau_high_q1,   tau_high_q3,   COLOR_GROUP,  \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(ax, tau_low_med,    tau_low_q1,    tau_low_q3,    COLOR_GROUP,  \"--\", f\"τ_low_{mode}\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs Group [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvG}\")\n",
    "\n",
    "# ---------- 2) OVERALL: Single vs WG（中央値・IQR帯・±表記） ----------\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "_hist_overall(ax)\n",
    "_vline_with_iqr(ax, tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(ax, tau_high_wg,    tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(ax, tau_low_wg,     tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "_style_axes(ax, title=f\"Probability distribution (OVERALL) — Single vs WG [{mode}]\")\n",
    "ax.legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_OVERALL_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_OVERALL_SvWG}\")\n",
    "\n",
    "# ---------- 3) BY_GROUP: Single vs Group（各群パネルに該当の IQR帯） ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "# Single は両段に表示\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "# High 段に High の Group τ、Low 段に Low の Group τ\n",
    "_vline_with_iqr(axes[0], tau_high_med, tau_high_q1, tau_high_q3, COLOR_GROUP, \"--\", f\"τ_high_{mode}\")\n",
    "_vline_with_iqr(axes[1], tau_low_med,  tau_low_q1,  tau_low_q3,  COLOR_GROUP, \"--\", f\"τ_low_{mode}\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvG}\")\n",
    "\n",
    "# ---------- 4) BY_GROUP: Single vs WG（各群パネルに該当の IQR帯） ----------\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9,10), sharex=True)\n",
    "_hist_bygroup(axes)\n",
    "_vline_with_iqr(axes[0], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[1], tau_single_med, tau_single_q1, tau_single_q3, COLOR_SINGLE, \"-\", \"τ_single\")\n",
    "_vline_with_iqr(axes[0], tau_high_wg, tau_high_wg_q1, tau_high_wg_q3, COLOR_WG, \":\", \"τ_high_WG\")\n",
    "_vline_with_iqr(axes[1], tau_low_wg,  tau_low_wg_q1,  tau_low_wg_q3,  COLOR_WG, \":\", \"τ_low_WG\")\n",
    "axes[0].legend(fontsize=FS_LEGEND); axes[1].legend(fontsize=FS_LEGEND)\n",
    "plt.tight_layout(); plt.savefig(SAVE_BYGROUP_SvWG, dpi=300); plt.close()\n",
    "print(f\"[Cell5B++] Saved -> {SAVE_BYGROUP_SvWG}\")\n",
    "\n",
    "# ---------- 5) Fold単位：OVERALL の確率分布と各Foldの τ（Single vs Group / Single vs WG） ----------\n",
    "#   - 各Foldのテストサンプルのみでヒストを作図\n",
    "#   - しきい値は「そのFold行」の値を直接表示（±は不要／IQR帯は使わない）\n",
    "for _, row in df_fold.iterrows():\n",
    "    fid = int(row[\"fold_id\"]) if \"fold_id\" in row else None\n",
    "    test_id = str(row.get(\"test_id\", f\"fold{fid}\"))\n",
    "    sub = df_pred[df_pred[\"fold_id\"] == fid] if \"fold_id\" in df_pred.columns and fid is not None else df_pred.copy()\n",
    "\n",
    "    p = pd.to_numeric(sub[\"proba\"], errors=\"coerce\").values\n",
    "    yt = pd.to_numeric(sub[\"y_true\"], errors=\"coerce\").values.astype(int)\n",
    "    p_s, p_n = p[yt==1], p[yt==0]\n",
    "\n",
    "    # 値（このFoldの τ）\n",
    "    t_single = float(row[\"tau_single\"])\n",
    "    t_high   = float(row[c_high]) if c_high in row else np.nan\n",
    "    t_low    = float(row[c_low])  if c_low  in row else np.nan\n",
    "    t_high_w = float(row[c_wgh])  if c_wgh  in row else np.nan\n",
    "    t_low_w  = float(row[c_wgl])  if c_wgl  in row else np.nan\n",
    "\n",
    "    # 出力パス\n",
    "    p_sg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvGroup.png\")\n",
    "    p_wg  = os.path.join(IMG_DIR, f\"FOLD{fid:02d}_{test_id}_OVERALL_SvWG.png\")\n",
    "\n",
    "    # Single vs Group\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high):   ax.axvline(t_high,   color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_high_{mode} = {t_high:.3f}\")\n",
    "    if np.isfinite(t_low):    ax.axvline(t_low,    color=COLOR_GROUP,  linestyle=\"--\", linewidth=LW, label=f\"τ_low_{mode}  = {t_low:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs Group [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_sg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_sg}\")\n",
    "\n",
    "    # Single vs WG\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.hist(p_s, bins=BINS, density=True, alpha=0.5, label=f\"True:Sick (n={len(p_s)})\", color=COLOR_SICK)\n",
    "    ax.hist(p_n, bins=BINS, density=True, alpha=0.5, label=f\"True:Non-Sick (n={len(p_n)})\", color=COLOR_NON)\n",
    "    if np.isfinite(t_single): ax.axvline(t_single, color=COLOR_SINGLE, linestyle=\"-\", linewidth=LW, label=f\"τ_single = {t_single:.3f}\")\n",
    "    if np.isfinite(t_high_w): ax.axvline(t_high_w, color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_high_WG = {t_high_w:.3f}\")\n",
    "    if np.isfinite(t_low_w):  ax.axvline(t_low_w,  color=COLOR_WG,     linestyle=\":\", linewidth=LW, label=f\"τ_low_WG  = {t_low_w:.3f}\")\n",
    "    _style_axes(ax, title=f\"[Fold {fid}] OVERALL — Single vs WG [{mode}]  (test={test_id})\")\n",
    "    ax.legend(fontsize=FS_LEGEND)\n",
    "    plt.tight_layout(); plt.savefig(p_wg, dpi=300); plt.close()\n",
    "    print(f\"[Cell5B++] Saved -> {p_wg}\")\n",
    "\n",
    "print(f\"[Cell5B++] All images saved in: {IMG_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
